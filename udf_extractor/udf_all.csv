"udf/spark_repos_0/103_zouzias_spark-lucenerdd/..src.test.scala.org.zouzias.spark.lucenerdd.spatial.shape.implicits.ShapeLuceneRDDImplicitsSpec.scala/udf/41.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(2), (row.getString(0), row.getString(1)))
"
"udf/spark_repos_0/103_zouzias_spark-lucenerdd/..src.test.scala.org.zouzias.spark.lucenerdd.spatial.shape.implicits.ShapeLuceneRDDImplicitsSpec.scala/udf/54.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(2), (row.getString(0), row.getString(1)))
"
"udf/spark_repos_0/103_zouzias_spark-lucenerdd/..src.test.scala.org.zouzias.spark.lucenerdd.spatial.shape.ShapeLuceneRDDLinkageSpec.scala/udf/46.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(1), row.getString(0))
"
"udf/spark_repos_0/103_zouzias_spark-lucenerdd/..src.test.scala.org.zouzias.spark.lucenerdd.spatial.shape.ShapeLuceneRDDLinkageSpec.scala/udf/52.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(1), row.getString(0))
"
"udf/spark_repos_0/11_markwhitey_SchoolBigDataAnalysis/..SchoolBigDataAnalysis.src.main.scala.com.itwang.DataProcessing.ConsumerProcessing.scala/udf/33.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(dscrp: String, dataStr: String) => if (""餐费支出"".equals(dscrp)) {
        val currHour = Integer.parseInt(dataStr.substring(11, 13))
        if (currHour >= 5 && currHour < 10) {
          ""breakfast""
        } else if (currHour >= 10 && currHour <= 15) {
          ""lunch""
        } else if (currHour > 15 && currHour <= 22) {
          ""dinner""
        } else {
          ""无规律就餐""
        }
      } else if (""用水支出"".equals(dscrp)) {
        ""用水支出""
      } else if (""商场购物"".equals(dscrp)) {
        ""商场购物""
      } else if (""购冷水支出"".equals(dscrp)) {
        ""用水支出""
      } else if (""购热水支出"".equals(dscrp)) {
        ""用水支出""
      } else if (""淋浴支出"".equals(dscrp)) {
        ""淋浴支出""
      } else if (""医疗支出"".equals(dscrp)) {
        ""医疗支出""
      } else {
        ""其他""
      }
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.dataquality.RSVDAnomalyDetection.scala/udf/116.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.dataquality.RSVDAnomalyDetection.scala/udf/98.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0)
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.ArchiveViewRegistrar.scala/udf/23.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, actor_id: Long) => arActorSplitsMap.value.getOrElse((wiki_db, actor_id), 1)
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.ArchiveViewRegistrar.scala/udf/27.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, actor_id: Long) => {
        val splits = arActorSplitsMap.value.getOrElse((wiki_db, actor_id), 1)
        (0 until splits).toArray
      }
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.LoggingViewRegistrar.scala/udf/22.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, actor_id: Long) => logActorSplitsMap.value.getOrElse((wiki_db, actor_id), 1)
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.LoggingViewRegistrar.scala/udf/26.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, actor_id: Long) => {
        val splits = logActorSplitsMap.value.getOrElse((wiki_db, actor_id), 1)
        (0 until splits).toArray
      }
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.LoggingViewRegistrar.scala/udf/39.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, comment_id: Long) => logCommentSplitsMap.value.getOrElse((wiki_db, comment_id), 1)
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.LoggingViewRegistrar.scala/udf/43.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, comment_id: Long) => {
        val splits = logCommentSplitsMap.value.getOrElse((wiki_db, comment_id), 1)
        (0 until splits).toArray
      }
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.RevisionViewRegistrar.scala/udf/23.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, actor_id: Long) => revActorSplitsMap.value.getOrElse((wiki_db, actor_id), 1)
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.RevisionViewRegistrar.scala/udf/27.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, actor_id: Long) => {
        val splits = revActorSplitsMap.value.getOrElse((wiki_db, actor_id), 1)
        (0 until splits).toArray
      }
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.RevisionViewRegistrar.scala/udf/40.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, comment_id: Long) => revCommentSplitsMap.value.getOrElse((wiki_db, comment_id), 1)
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.RevisionViewRegistrar.scala/udf/44.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(wiki_db: String, comment_id: Long) => {
        val splits = revCommentSplitsMap.value.getOrElse((wiki_db, comment_id), 1)
        (0 until splits).toArray
      }
"
"udf/spark_repos_0/11_wikimedia_analytics-refinery-source/..refinery-job.src.main.scala.org.wikimedia.analytics.refinery.job.mediawikihistory.sql.SQLHelper.scala/udf/37.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a1: mutable.WrappedArray[String]) => a1.distinct
"
"udf/spark_repos_0/123_jeffreyksmithjr_reactive-machine-learning-systems/..chapter-2.src.main.scala.com.reactivemachinelearning.SparkIntroduction.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val parts = line.split(',')
        LabeledPoint(parts(0).toDouble, Vectors.dense(locally {
          val _t_m_p_2 = parts(1).split(' ')
          _t_m_p_2.map(_.toDouble)
        }))
      }
"
"udf/spark_repos_0/123_jeffreyksmithjr_reactive-machine-learning-systems/..chapter-2.src.main.scala.com.reactivemachinelearning.SparkIntroduction.scala/udf/28.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val parts = line.split(',')
        LabeledPoint(parts(0).toDouble, Vectors.dense(locally {
          val _t_m_p_4 = parts(1).split(' ')
          _t_m_p_4.map(_.toDouble)
        }))
      }
"
"udf/spark_repos_0/123_jeffreyksmithjr_reactive-machine-learning-systems/..chapter-2.src.main.scala.com.reactivemachinelearning.SparkIntroduction.scala/udf/40.19.Dataset-LabeledPoint.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.mllib.regression.LabeledPoint]
Call: map

{
        case LabeledPoint(label, features) =>
          val prediction = model.predict(features)
          (prediction, label)
      }
"
"udf/spark_repos_0/123_jeffreyksmithjr_reactive-machine-learning-systems/..chapter-4.src.main.scala.com.reactivemachinelearning.FeatureGeneration.scala/udf/23.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => WordSequenceFeature(""words"", row.getSeq[String](0))
"
"udf/spark_repos_0/12_lifeomic_spark-vcf/..src.main.scala.com.lifeomic.variants.VCFResourceRelation.scala/udf/12.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(TEXT_VALUE).startsWith(""##"")
"
"udf/spark_repos_0/12_lifeomic_spark-vcf/..src.main.scala.com.lifeomic.variants.VCFResourceRelation.scala/udf/17.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(TEXT_VALUE).startsWith(""#"")
"
"udf/spark_repos_0/12_lifeomic_spark-vcf/..src.main.scala.com.lifeomic.variants.VCFResourceRelation.scala/udf/19.17.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

item => (item.getString(0), item.getString(1).split(""\t""))
"
"udf/spark_repos_0/12_lifeomic_spark-vcf/..src.main.scala.com.lifeomic.variants.VCFResourceRelation.scala/udf/25.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(TEXT_VALUE).startsWith(""#"")
"
"udf/spark_repos_0/12_lifeomic_spark-vcf/..src.main.scala.com.lifeomic.variants.VCFResourceRelation.scala/udf/47.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(TEXT_VALUE).startsWith(""##FORMAT"")
"
"udf/spark_repos_0/12_lifeomic_spark-vcf/..src.main.scala.com.lifeomic.variants.VCFResourceRelation.scala/udf/49.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getString(1)
"
"udf/spark_repos_0/12_lifeomic_spark-vcf/..src.main.scala.com.lifeomic.variants.VCFResourceRelation.scala/udf/57.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(TEXT_VALUE).startsWith(""##INFO"")
"
"udf/spark_repos_0/12_lifeomic_spark-vcf/..src.main.scala.com.lifeomic.variants.VCFResourceRelation.scala/udf/59.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getString(1)
"
"udf/spark_repos_0/1_396763284_wxp-spark/..recommend.src.main.scala.per.wxp.statistics.statistics.scala/udf/25.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Int) => simpleDateFormat.format(new Date(x * 1000L)).toInt
"
"udf/spark_repos_0/1_396763284_wxp-spark/..wxp-old-resource.src.main.scala.spark_ml.example.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_0/1_396763284_wxp-spark/..wxp-old-resource.src.main.scala.sqltest.rddtosql.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""staff_name: "" + teenager(1)
"
"udf/spark_repos_0/1_Abel-Huang_xspark-log/..src.main.scala.cn.abelib.spark.basic.DataFrameApp.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result.col(""age"") > 22
"
"udf/spark_repos_0/1_Abel-Huang_xspark-log/..src.main.scala.cn.abelib.spark.basic.DateFrameRDDApp.scala/udf/14.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_Abel-Huang_xspark-log/..src.main.scala.cn.abelib.spark.basic.DateFrameRDDApp.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

people.col(""age"") < 23
"
"udf/spark_repos_0/1_Abel-Huang_xspark-log/..src.main.scala.cn.abelib.spark.log.fusion.TopCountJob.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""20180407""
"
"udf/spark_repos_0/1_Abel-Huang_xspark-log/..src.main.scala.cn.abelib.spark.log.fusion.TopCountJob.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""20180407""
"
"udf/spark_repos_0/1_abo64_scala-spark-big-data/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/109.19.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((work, sex, age), avgPrimaryNeeds, avgWork, avgOther) =>
          TimeUsageRow(work, sex, age, avgPrimaryNeeds, avgWork, avgOther)
      }
"
"udf/spark_repos_0/1_achinnasamy_StructuredStreamingScala/..src.com.dmac.scalabasics.DatasetElucidator.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

each => each.length
"
"udf/spark_repos_0/1_achinnasamy_StructuredStreamingScala/..src.com.dmac.ss.SSJoin.scala/udf/12.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

each => StreamDTO(each)
"
"udf/spark_repos_0/1_adaltas_spark-streaming-scala/..src.main.scala.com.adaltas.taxistreaming.processing.TaxiProcessing.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""startLon"") >= lonWest && col(""startLon"") <= lonEast && col(""startLat"") >= latSouth && col(""startLat"") <= latNorth && col(""endLon"") >= lonWest && col(""endLon"") <= lonEast && col(""endLat"") >= latSouth && col(""endLat"") <= latNorth
"
"udf/spark_repos_0/1_adaltas_spark-streaming-scala/..src.main.scala.com.adaltas.taxistreaming.processing.TaxiProcessing.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""isStart"") === ""END""
"
"udf/spark_repos_0/1_anlei-cdh_SparkSource2/..src.main.scala.com.spark.ml.recommendation.CollaborativeFiltering.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_0/1_anlei-cdh_SparkSource2/..src.main.scala.com.spark.sql.MovieDataFrame.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 30
"
"udf/spark_repos_0/1_anlei-cdh_SparkSource2/..src.main.scala.com.spark.sql.MovieDataFrame.scala/udf/63.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          u => (u.getAs[String](""userId"").toLong, u.getAs[String](""age"").toInt + 1)
        }
"
"udf/spark_repos_0/1_arbc139_spark-playground/..src.main.scala.com.totorody.spark.DduckCount.scala/udf/24.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Long](""sum_sold_oct"") >= 50L
"
"udf/spark_repos_0/1_arbc139_spark-playground/..src.main.scala.com.totorody.spark.DduckCount.scala/udf/28.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Long](""sum_sold_nov"") >= 50L
"
"udf/spark_repos_0/1_arbc139_spark-playground/..src.main.scala.com.totorody.spark.GraphXExample2.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val vertexId = row.getAs[Integer](""station_cd"")
        val elements = (row.getAs[String](""station_name""), 0L, """")
        (vertexId.toLong, elements)
      }
"
"udf/spark_repos_0/1_arbc139_spark-playground/..src.main.scala.com.totorody.spark.MLExample2.scala/udf/32.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => LabeledPoint(row.getDouble(0), Vectors.dense(row.getDouble(1), row.getDouble(2), row.getDouble(3)))
      }
"
"udf/spark_repos_0/1_arbc139_spark-playground/..src.main.scala.com.totorody.spark.MLExample2.scala/udf/44.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
        row => (linRegModel.predict(row.getAs[org.apache.spark.ml.linalg.Vector](""features"")), row.getAs[Double](""label""))
      }
"
"udf/spark_repos_0/1_Arthur-Lanc_Time-Usage-with-Scala-and-Spark/..src.main.scala.timeusage.TimeUsage.scala/udf/83.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => TimeUsageRow(r.getAs(""working""), r.getAs(""sex""), r.getAs(""age""), r.getAs(""primaryNeeds""), r.getAs(""work""), r.getAs(""other""))
"
"udf/spark_repos_0/1_Arthur-Lanc_Time-Usage-with-Scala-and-Spark/..src.main.scala.timeusage.TimeUsage.scala/udf/89.19.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

k => TimeUsageRow(k._1._1, k._1._2, k._1._3, k._2, k._3, k._4)
"
"udf/spark_repos_0/1_ashishb888_spark-poc/..spark-scala-gs.src.main.scala.poc.spark.main.csv.CsvApp.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_0/1_ashishb888_spark-poc/..spark-scala-gs.src.main.scala.poc.spark.main.csv.CsvApp.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_0/1_ashishb888_spark-poc/..spark-sql.src.main.scala.poc.spark.sql.service.SparkSqlService.scala/udf/10.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_0/1_ashishb888_spark-poc/..spark-sql.src.main.scala.poc.spark.sql.service.SparkSqlService.scala/udf/14.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_0/1_BranY_MLTest/..src.main.scala.edu.nju.pasalab.RDDOperation.joinOperation.scala/udf/15.19.Dataset-((String, Double), (String, Int)).map","Type: org.apache.spark.sql.Dataset[((String, Double), (String, Int))]
Call: map

elem => (elem._1._1, elem._1._2, elem._2._2)
"
"udf/spark_repos_0/1_BranY_MLTest/..src.main.scala.edu.nju.pasalab.statisticalInfo.scala/udf/13.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split(""\\s+"")
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.dataframe.Basic.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.dataframe.DatasetConversion.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.dataframe.UDF.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

myNameFilter($""name"")
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.dataframe.UDF.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

salesFilter($""sales"", lit(2000.0d))
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.dataframe.UDF.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

stateFilter($""state"", array(lit(""CA""), lit(""MA""), lit(""NY""), lit(""NJ"")))
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.dataframe.UDF.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

multipleFilter($""state"", $""discount"", struct(lit(""CA""), lit(100.0d)))
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/58.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

struct _
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/67.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAtomic _
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/71.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

arrayLength _
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.JSON.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => s.replaceAllLiterally(""$"", """")
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.UDAF2.scala/udf/48.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.UDAF_Multi.scala/udf/43.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mystats
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.UDAF.scala/udf/41.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.UDF.scala/udf/16.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

westernState _
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.UDF.scala/udf/27.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

manyCustomers _
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.UDF.scala/udf/47.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

stateRegion _
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.UDF.scala/udf/62.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

discountRatio _
"
"udf/spark_repos_0/1_brightliming_SparkShowCase/..src.main.scala.sql.UDF.scala/udf/76.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeStruct _
"
"udf/spark_repos_0/1_data-hunters_metadata-digger/..src.main.scala.ai.datahunters.md.filter.analytics.SimilarMetatagsFilter.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

similaritiyCountsUDFs.geq(minPassing)
"
"udf/spark_repos_0/1_data-hunters_metadata-digger/..src.main.scala.ai.datahunters.md.filter.NotEmptyMetadataDir.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

notEmptyMapUDF(col(buildColName()))
"
"udf/spark_repos_0/1_data-hunters_metadata-digger/..src.main.scala.ai.datahunters.md.filter.NotEmptyTagFilter.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

notEmptyTagValueUDF(tag)(col(MetadataCol))
"
"udf/spark_repos_0/1_data-hunters_metadata-digger/..src.main.scala.ai.datahunters.md.processor.MetadataExtractor.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(EmbeddedMetadataSchemaConfig.FullFileTypeCol).notEqual(FileType.Unknown.toString)
"
"udf/spark_repos_0/1_davidpetro88_scala-playground/..Apache_Spark_with_Scala-Hands_On_with_Big_Data.SparkScalaCourse.src.com.sundogsoftware.spark.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""toxic"" =!= -1
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/35.22.Dataset-LabeledComment.filter","Type: org.apache.spark.sql.Dataset[jigsaw.LabeledComment]
Call: filter

$""comment_text"".isNull
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/39.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""comment_text"".isNull
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/49.24.Dataset-LabeledComment.filter","Type: org.apache.spark.sql.Dataset[jigsaw.LabeledComment]
Call: filter

col(cl) === 1
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/61.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(cl) === 1
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/70.22.Dataset-LabeledComment.filter","Type: org.apache.spark.sql.Dataset[jigsaw.LabeledComment]
Call: filter

$""comment_text"" === """"
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/74.22.Dataset-LabeledComment.filter","Type: org.apache.spark.sql.Dataset[jigsaw.LabeledComment]
Call: filter

$""id"" === ""00078f8ce7eb276d""
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/79.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Utils.getClasses(Utils.getCat(row.getAs[Int](0), row.getAs[Int](1), row.getAs[Int](2), row.getAs[Int](3), row.getAs[Int](4), row.getAs[Int](5)))
"
"udf/spark_repos_0/1_devang_ml/..src.main.scala.jigsaw.JigsawNB.scala/udf/81.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""items"" =!= Array[Double](0)
"
"udf/spark_repos_0/1_dhkdn9192_StockPatternStream/..src.main.scala.com.bigdata.CommonUtils.scala/udf/23.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val symb = row.getString(priceSize)
        val prices = locally {
          val _t_m_p_4 = 0 until priceSize
          _t_m_p_4.map(row.getLong(_).toDouble)
        }
        (symb, prices)
      }
"
"udf/spark_repos_0/1_dhkdn9192_StockPatternStream/..src.main.scala.com.bigdata.CommonUtils.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
        val symbCol = r.getAs[String](""symb"")
        val nameCol = r.getAs[String](""name"")
        (symbCol, nameCol)
      }
"
"udf/spark_repos_0/1_dhkdn9192_StockPatternStream/..src.main.scala.com.bigdata.CommonUtils.scala/udf/8.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val symb = row.getString(priceSize)
        val prices = locally {
          val _t_m_p_2 = 0 until priceSize
          _t_m_p_2.map(row.getLong(_).toDouble)
        }
        (symb, dates.zip(prices).toMap)
      }
"
"udf/spark_repos_0/1_dhkdn9192_StockPatternStream/..src.main.scala.com.bigdata.PatternFinder.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val slided = row.getAs[Seq[Seq[Double]]](0)
        val rt = row.getAs[Seq[Double]](1)
        val symb = row.getString(2)
        val combined = locally {
          val _t_m_p_2 = rt.zip(slided)
          _t_m_p_2.map(zipped => Seq(zipped._1) ++ zipped._2)
        }
        (combined, symb)
      }
"
"udf/spark_repos_0/1_dhkdn9192_StockPatternStream/..src.main.scala.com.bigdata.PatternFinder.scala/udf/37.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""symb"" === symb
"
"udf/spark_repos_0/1_dhkdn9192_StockPatternStream/..src.main.scala.com.bigdata.Preprocessor.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val slidedHist = locally {
          val _t_m_p_2 = locally {
            val _t_m_p_3 = 0 until histSize
            _t_m_p_3.map(row.getLong(_).toDouble)
          }.toArray.sliding(slideSize)
          _t_m_p_2.map(_.toArray)
        }.toArray
        val transposed = slidedHist.transpose
        val mins = locally {
          val _t_m_p_4 = transposed
          _t_m_p_4.map(_.min)
        }
        val durations = locally {
          val _t_m_p_5 = transposed
          _t_m_p_5.map(i => i.max - i.min)
        }
        val scaledAry = locally {
          val _t_m_p_6 = slidedHist
          _t_m_p_6.map { elem => 
            val scaled = locally {
              val _t_m_p_7 = elem.indices
              _t_m_p_7.map(i => spRound((elem(i) - mins(i)) / durations(i)))
            }.toArray
            scaled
          }
        }
        val symb = row.getString(histSize)
        (scaledAry, symb)
      }
"
"udf/spark_repos_0/1_dhkdn9192_StockPatternStream/..src.main.scala.com.bigdata.Preprocessor.scala/udf/54.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val rt = locally {
          val _t_m_p_9 = 0 until rtSize
          _t_m_p_9.map(row.getLong(_).toDouble)
        }.toArray
        val duration = rt.max - rt.min
        val scaled = locally {
          val _t_m_p_10 = rt
          _t_m_p_10.map(v => spRound((v - rt.min) / duration))
        }
        val symb = row.getString(rtSize)
        (scaled, symb)
      }
"
"udf/spark_repos_0/1_digital-thinking_dota2-analytics/..src.main.scala.com.ixeption.spark.dota2.util.Dota2Analytics.scala/udf/124.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col.radiant_win"" === true
"
"udf/spark_repos_0/1_digital-thinking_dota2-analytics/..src.main.scala.com.ixeption.spark.dota2.util.Dota2Analytics.scala/udf/128.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""players.player_slot"" < 128
"
"udf/spark_repos_0/1_digital-thinking_dota2-analytics/..src.main.scala.com.ixeption.spark.dota2.util.Dota2Analytics.scala/udf/132.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""players.player_slot"" >= 128
"
"udf/spark_repos_0/1_DinoZzhong_sparkmllib-and-alogrithm/..src.main.scala.dino.lr.playground.LrTrain.scala/udf/35.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs(""label"") != row.getAs(""prediction"")
"
"udf/spark_repos_0/1_edersoncorbari_scala-lab/..src.main.scala.io.github.edersoncorbari.graph.HierarchyEmployee.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_connect_by"".isNotNull
"
"udf/spark_repos_0/1_EdgarLGB_spark-demo/..src.main.scala.demo.MockAnalyser.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => {
        val source = r.getString(0)
        val timestamp = r.getString(1)
        val num = r.getLong(2)
        ""SUSPECT BEHAVIOR : Source %s requested %d times in the same second %s"".format(source, num, timestamp)
      }
"
"udf/spark_repos_0/1_Evan1987_Spark/..AdvancedAnalyticsWithSpark.src.main.scala.cigar.Chap03_ALS.ALSRecommend.scala/udf/127.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""positivePrediction"" > ($""negativePrediction"")
"
"udf/spark_repos_0/1_exstock_data-quality/..src.main.scala.com.hduser.udf.UDFAgent.scala/udf/13.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

indexOf(_: Seq[String], _: String)
"
"udf/spark_repos_0/1_exstock_data-quality/..src.main.scala.com.hduser.udf.UDFAgent.scala/udf/17.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

matches _
"
"udf/spark_repos_0/1_exstock_data-quality/..src.main.scala.com.hduser.udf.UDFAgent.scala/udf/21.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

regexReplace _
"
"udf/spark_repos_0/1_fcvane_scalaStudy/..src.main.scala.com.test.T20180816.test0816_6.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

people.col(""age"") > 30
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixDataSource.scala/udf/110.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixDataSource.scala/udf/17.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixDataSource.scala/udf/51.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixDataSource.scala/udf/92.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixEntryDataSource.scala/udf/16.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixEntryDataSource.scala/udf/25.19.Dataset-(Int, Int, Double).map","Type: org.apache.spark.sql.Dataset[(Int, Int, Double)]
Call: map

{
        case (user, item, _) =>
          MatrixEntry(user, item, 1.0d)
      }
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixEntryDataSource.scala/udf/38.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixEntryDataSource.scala/udf/47.19.Dataset-(Int, Int, Double).map","Type: org.apache.spark.sql.Dataset[(Int, Int, Double)]
Call: map

{
        case (item, user, _) =>
          MatrixEntry(item, user, 1.0d)
      }
"
"udf/spark_repos_0/1_fpopic_master-thesis/..src.main.scala.hr.fer.ztel.thesis.datasource.MatrixEntryDataSource.scala/udf/59.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_FUHENG0571_git-github.com-endymecy-AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.knn.KNNClassifier.scala/udf/65.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case (id, labels) =>
          val vector = new Array[Double](numClasses)
          var i = 0
          while (i < labels.length) {
            vector(labels(i).getDouble(0).toInt) += getWeight(labels(i))
            i += 1
          }
          val rawPrediction = Vectors.dense(vector)
          lazy val probability = raw2probability(rawPrediction)
          lazy val prediction = probability2prediction(probability)
          val values = new ArrayBuffer[Any]
          if ($(rawPredictionCol).nonEmpty) {
            values.append(rawPrediction)
          }
          if ($(probabilityCol).nonEmpty) {
            values.append(probability)
          }
          if ($(predictionCol).nonEmpty) {
            values.append(prediction)
          }
          (id, values)
      }
"
"udf/spark_repos_0/1_FUHENG0571_git-github.com-endymecy-AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.sampling.OverSampling.scala/udf/43.32.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(dependentColName)) === label
"
"udf/spark_repos_0/1_FUHENG0571_git-github.com-endymecy-AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.sampling.OverSampling.scala/udf/51.32.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(dependentColName)) === label
"
"udf/spark_repos_0/1_FUHENG0571_git-github.com-endymecy-AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.sampling.UnderSampling.scala/udf/44.30.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(dependentColName)) === label
"
"udf/spark_repos_0/1_FUHENG0571_git-github.com-endymecy-AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.sampling.UnderSampling.scala/udf/47.30.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(dependentColName)) === label
"
"udf/spark_repos_0/1_geyuegui_flumeTokafka/..RecommendSystem.recommender.statisticRecommender.src.main.scala.com.geyuegui.statisticAlgorithm.scala/udf/17.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Long) => simpleDateFormat.format(new Date(x * 1000L))
"
"udf/spark_repos_0/1_gkpavan001_Kafka-spark-ElasticSearch/..src.com.dbs.bootcamp.es.KafkaConnectorSparkStreamEsDemo.scala/udf/26.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

lines => {
        var line = lines.split("","")
        employee(line(0).split(""="")(1).toInt, line(1).split(""="")(1), line(2).split(""="")(1).dropRight(1).toInt)
      }
"
"udf/spark_repos_0/1_gkpavan001_Kafka-spark-ElasticSearch/..src.com.dbs.bootcamp.es.KafkaSparkStreamEsDemo.scala/udf/27.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.code.xyliu.demos.src.main.scala.com.dcjet.demo.Demo002.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 23
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.code.xyliu.demos.src.main.scala.com.dcjet.demo.Demo002.scala/udf/31.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 2
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.code.xyliu.demos.src.main.scala.com.dcjet.demo.Demo002.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

m => ""name:"" + m(0)
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.code.xyliu.demos.src.main.scala.com.dcjet.demo.Demo002.scala/udf/54.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

m => ""name:"" + m.getAs[String](""name"")
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.code.xyliu.demos.src.main.scala.com.dcjet.demo.Demo002.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

m => m.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..bigData.docs.spark-examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..ztyth日志系统.code.src.main.scala.com.dcjet.logCenter.LogArchive.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

configRow => {
        val config = Config(configRow.get(0).toString(), configRow.get(1).toString(), configRow.get(2).toString(), BigInt(configRow.get(3).toString()), configRow.get(4).toString())
        try {
          archive(config, spark)
        } catch {
          case ex: Exception =>
            println(""归档"" + config.name + ""出问题"" + ex)
        }
        """"
      }
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..ztyth日志系统.code.src.main.scala.com.dcjet.ztythLog.LogAnalysis.scala/udf/105.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.get(0).toString() + ""&"" + row.get(1).toString()
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..ztyth日志系统.ztythLogAnalyse.src.main.scala.com.dcjet.ztythLog.LogAnalysisDBDebug.scala/udf/195.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.get(0).toString() + ""&"" + row.get(1).toString()
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..ztyth日志系统.ztythLogAnalyse.src.main.scala.com.dcjet.ztythLog.LogAnalysisDBList.scala/udf/236.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.get(0).toString() + ""&"" + row.get(1).toString()
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..ztyth日志系统.ztythLogAnalyse.src.main.scala.com.dcjet.ztythLog.LogAnalysisDBParquetDB.scala/udf/190.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.get(0).toString() + ""&"" + row.get(1).toString()
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..ztyth日志系统.ztythLogAnalyse.src.main.scala.com.dcjet.ztythLog.LogAnalysisDBParquet.scala/udf/190.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.get(0).toString() + ""&"" + row.get(1).toString()
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..ztyth日志系统.ztythLogAnalyse.src.main.scala.com.dcjet.ztythLog.LogAnalysisDB.scala/udf/228.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.get(0).toString() + ""&"" + row.get(1).toString()
"
"udf/spark_repos_0/1_hyd-raiders_BigData/..ztyth日志系统.ztythLogAnalyse.src.main.scala.com.dcjet.ztythLog.LogAnalysis.scala/udf/105.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.get(0).toString() + ""&"" + row.get(1).toString()
"
"udf/spark_repos_0/1_iaka24_bigdata-project-real-time-sentiment-analysis-on-top-of-Twitter-s-streaming-API-/..Roma-master.src.main.scala.com.github.pedrovgs.roma.machinelearning.Corpus.scala/udf/23.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
          val sentiment = row.getAs[Int](sentimentColumnName)
          sentiment == negativeSentimentCsvValue || sentiment == positiveSentimentCsvValue
        }
"
"udf/spark_repos_0/1_iaka24_bigdata-project-real-time-sentiment-analysis-on-top-of-Twitter-s-streaming-API-/..Roma-master.src.main.scala.com.github.pedrovgs.roma.machinelearning.Corpus.scala/udf/28.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ row => 
        val label = if (row.getAs[Int](sentimentColumnName).equals(positiveSentimentCsvValue)) {
          positiveLabel
        } else {
          negativeLabel
        }
        (label, row.getAs[String](contentColumnName))
      }
"
"udf/spark_repos_0/1_issnoejasso_scala-spark-tutorial/..bin.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(""country"").===(""Afghanistan"")
"
"udf/spark_repos_0/1_issnoejasso_scala-spark-tutorial/..bin.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(AGE_MIDPOINT) < 20
"
"udf/spark_repos_0/1_issnoejasso_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(""country"").===(""Afghanistan"")
"
"udf/spark_repos_0/1_issnoejasso_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(AGE_MIDPOINT) < 20
"
"udf/spark_repos_0/1_issnoejasso_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/22.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.country == ""Afghanistan""
"
"udf/spark_repos_0/1_issnoejasso_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/29.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.age_midpoint.isDefined && response.age_midpoint.get < 20.0d
"
"udf/spark_repos_0/1_issnoejasso_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/36.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.salary_midpoint.isDefined
"
"udf/spark_repos_0/1_issnoejasso_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/41.19.Dataset-Response.map","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: map

response => locally {
        val _t_m_p_5 = response.salary_midpoint
        _t_m_p_5.map(point => Math.round(point / 20000) * 20000)
      }.orElse(None)
"
"udf/spark_repos_0/1_itdev-1210_big-data-analytics-projects-with-apache-spark/..src.main.scala.com.example.CreatingDatasets.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_itdev-1210_big-data-analytics-projects-with-apache-spark/..src.main.scala.com.example.ProgrammingGuideSQL.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_itdev-1210_big-data-analytics-projects-with-apache-spark/..src.main.scala.com.tomekl007.anomalydetection.RunKMeans.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
        val cluster = row.getAs[Int](""cluster"")
        val vec = row.getAs[Vector](""scaledFeatureVector"")
        Vectors.sqdist(centroids(cluster), vec) >= farthestDistanceBetweenTwoNormalClusters
      }
"
"udf/spark_repos_0/1_itdev-1210_big-data-analytics-projects-with-apache-spark/..src.main.scala.com.tomekl007.anomalydetection.RunKMeans.scala/udf/62.19.Dataset-Vector).map","Type: org.apache.spark.sql.Dataset[(Int, org.apache.spark.ml.linalg.Vector)]
Call: map

{
        case (cluster, vec) =>
          Vectors.sqdist(centroids(cluster), vec)
      }
"
"udf/spark_repos_0/1_itdev-1210_big-data-analytics-projects-with-apache-spark/..src.test.scala.com.tomekl007.SparkApisTests.scala/udf/28.22.Dataset-UserData.filter","Type: org.apache.spark.sql.Dataset[com.tomekl007.UserData]
Call: filter

_.userId == ""a""
"
"udf/spark_repos_0/1_james-hadoop_JamesHadoop/..src.main.scala.com.james.spark.hive.HiveSqlDemo.scala/udf/17.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      case Row(key: Int, value: String) =>
        s""Key: $key, Value: $value""
    }
"
"udf/spark_repos_0/1_james-hadoop_JamesHadoop/..src.main.scala.com.james.spark.sql.SparkSqlDemo.scala/udf/20.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_jcl10086_demo/..src.main.scala.org.jcl.core.HbaseSpark.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""createAt"" > st && $""createAt"" < et
"
"udf/spark_repos_0/1_Jeffersonmf_zap_sessionization_teste/..src.main.scala.core.EnrichmentEngine.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""anonymous_id"").equalTo(anonymous_id)
"
"udf/spark_repos_0/1_Jeffersonmf_zap_sessionization_teste/..src.main.scala.core.EnrichmentEngine.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""anonymous_id"").equalTo(anonymous_id)
"
"udf/spark_repos_0/1_Jeffersonmf_zap_sessionization_teste/..src.main.scala.core.EnrichmentEngine.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

result => result.getValuesMap[Any](List(""device_family"", ""count""))
"
"udf/spark_repos_0/1_Jeffersonmf_zap_sessionization_teste/..src.main.scala.core.EnrichmentEngine.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""anonymous_id"").equalTo(anonymous_id)
"
"udf/spark_repos_0/1_Jeffersonmf_zap_sessionization_teste/..src.main.scala.core.EnrichmentEngine.scala/udf/71.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

result => result.getValuesMap[Any](List(""os_family"", ""count""))
"
"udf/spark_repos_0/1_Jeffersonmf_zap_sessionization_teste/..src.main.scala.core.EnrichmentEngine.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""anonymous_id"").equalTo(anonymous_id)
"
"udf/spark_repos_0/1_Jeffersonmf_zap_sessionization_teste/..src.main.scala.core.EnrichmentEngine.scala/udf/82.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

result => result.getValuesMap[Any](List(""browser_family"", ""count""))
"
"udf/spark_repos_0/1_Jie-Yuan_tql-Spark/..src.main.scala.com.yuanjie.task.WordVectorTrain.scala/udf/26.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""title"".isNotNull
"
"udf/spark_repos_0/1_Jie-Yuan_tql-Spark/..src.main.scala.com.yuanjie.task.WordVectorTrain.scala/udf/28.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!$""title"".isNaN
"
"udf/spark_repos_0/1_Jie-Yuan_tql-Spark/..src.main.scala.com.yuanjie.task.WordVectorTrain.scala/udf/30.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

F.size($""words"") > 3
"
"udf/spark_repos_0/1_Jie-Yuan_tql-Spark/..src.main.scala.com.yuanjie.task.WordVectorTrain.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

F.size($""words"") > 16
"
"udf/spark_repos_0/1_jluchuang_keeptry-ml/..src.main.scala.cn.keeptry.ml.als.Audioscrobbler.scala/udf/100.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""user"") === userID
"
"udf/spark_repos_0/1_jluchuang_keeptry-ml/..src.main.scala.cn.keeptry.ml.als.Audioscrobbler.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (existingArtistIDs: _*)
"
"udf/spark_repos_0/1_jluchuang_keeptry-ml/..src.main.scala.cn.keeptry.ml.als.Audioscrobbler.scala/udf/112.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (recommendArtistIDs: _*)
"
"udf/spark_repos_0/1_jluchuang_keeptry-ml/..src.main.scala.cn.keeptry.ml.als.Audioscrobbler.scala/udf/191.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""positivePrediction"" > ($""negativePrediction"")
"
"udf/spark_repos_0/1_jluchuang_keeptry-ml/..src.main.scala.cn.keeptry.ml.als.Audioscrobbler.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val Array(user, artist, _*) = line.split(' ')
        (user.toInt, artist.toInt)
      }
"
"udf/spark_repos_0/1_jluchuang_keeptry-ml/..src.main.scala.cn.keeptry.ml.als.Audioscrobbler.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isin(goodID, badID)
"
"udf/spark_repos_0/1_jluchuang_keeptry-ml/..src.main.scala.cn.keeptry.ml.als.Audioscrobbler.scala/udf/79.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val Array(userID, artistID, count) = locally {
          val _t_m_p_7 = line.split(' ')
          _t_m_p_7.map(_.toInt)
        }
        val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)
        (userID, finalArtistID, count)
      }
"
"udf/spark_repos_0/1_joblasco_HowToWorkWithCSV/..src.main.scala.Caso_de_Uso.CasodeUso_Pelis.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" >= 537
"
"udf/spark_repos_0/1_joblasco_HowToWorkWithCSV/..src.main.scala.Dataframes.DFwithCSV.scala/udf/13.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""año"") === ""2000""
"
"udf/spark_repos_0/1_joblasco_HowToWorkWithCSV/..src.main.scala.Dataframes.DFwithCSV.scala/udf/15.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(""nota"") >= ""6,5""
"
"udf/spark_repos_0/1_joblasco_HowToWorkWithCSV/..src.main.scala.Datasets.DataSetBasicAPP.scala/udf/11.20.Dataset-Names.filter","Type: org.apache.spark.sql.Dataset[Datasets.Names]
Call: filter

namesObj => namesObj.name == ""George""
"
"udf/spark_repos_0/1_joblasco_HowToWorkWithCSV/..src.main.scala.WorkingWithCSV.CrearFile.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Edad"" >= 24
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""age"" > 70
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/30.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""race"" === ""Other""
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/47.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

info => ""Aged "" + info.getAs[String](""age"") + "", occupation is "" + info.getAs[String](""occupation"") + "", race is "" + info.getAs[String](""race"")
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/53.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""maritial_status"" === ""Divorced""
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/57.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""education"" === ""Doctorate"" and $""maritial_status"" === ""Divorced""
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/60.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""education"" === ""Doctorate""
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/64.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""education"" === ""Preschool"" and $""maritial_status"" === ""Divorced""
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/67.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""education"" === ""Preschool""
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/71.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""education"" === ""HS-grad"" and $""maritial_status"" === ""Divorced""
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.AdultBaseStatistics.scala/udf/74.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""education"" === ""HS-grad""
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..AdultBase.src.main.scala.AdultBase.database.DoctorateTableHive.scala/udf/31.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(marriage: String, id: Int, sex: String, occupation: String, education: String) =>
          val suffix = sex match {
            case ""Male"" =>
              s""His""
            case ""Female"" =>
              s""Her""
          }
          s""Doc. $id is $sex, $suffix marriage status is $marriage, occupation is $occupation""
      }
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..Examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_0/1_josephding23_SparkProjects/..NBADataBase.src.main.scala.NBADataBase.database.BasicStatistics.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Player"" === ""LeBron James""
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/116.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Comorbidities(r(0).toString, r(1).toString, if (r(2).toString.isEmpty) 0.0d else r(3).toString.toDouble, if (r(3).toString.isEmpty) 0.0d else r(3).toString.toDouble, if (r(4).toString.isEmpty) 0.0d else r(4).toString.toDouble, if (r(5).toString.isEmpty) 0.0d else r(5).toString.toDouble, if (r(6).toString.isEmpty) 0.0d else r(6).toString.toDouble, if (r(7).toString.isEmpty) 0.0d else r(7).toString.toDouble, if (r(8).toString.isEmpty) 0.0d else r(8).toString.toDouble, if (r(9).toString.isEmpty) 0.0d else r(9).toString.toDouble, if (r(10).toString.isEmpty) 0.0d else r(10).toString.toDouble, if (r(11).toString.isEmpty) 0.0d else r(11).toString.toDouble, if (r(12).toString.isEmpty) 0.0d else r(12).toString.toDouble, if (r(13).toString.isEmpty) 0.0d else r(13).toString.toDouble, if (r(14).toString.isEmpty) 0.0d else r(14).toString.toDouble, if (r(15).toString.isEmpty) 0.0d else r(15).toString.toDouble, if (r(16).toString.isEmpty) 0.0d else r(16).toString.toDouble, if (r(17).toString.isEmpty) 0.0d else r(17).toString.toDouble, if (r(18).toString.isEmpty) 0.0d else r(18).toString.toDouble, if (r(19).toString.isEmpty) 0.0d else r(19).toString.toDouble, if (r(20).toString.isEmpty) 0.0d else r(20).toString.toDouble, if (r(21).toString.isEmpty) 0.0d else r(21).toString.toDouble, if (r(22).toString.isEmpty) 0.0d else r(22).toString.toDouble, if (r(23).toString.isEmpty) 0.0d else r(23).toString.toDouble, if (r(24).toString.isEmpty) 0.0d else r(24).toString.toDouble, if (r(25).toString.isEmpty) 0.0d else r(25).toString.toDouble, if (r(26).toString.isEmpty) 0.0d else r(26).toString.toDouble, if (r(27).toString.isEmpty) 0.0d else r(27).toString.toDouble, if (r(28).toString.isEmpty) 0.0d else r(28).toString.toDouble, if (r(29).toString.isEmpty) 0.0d else r(29).toString.toDouble, if (r(30).toString.isEmpty) 0.0d else r(30).toString.toDouble, if (r(31).toString.isEmpty) 0.0d else r(31).toString.toDouble)
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/130.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => LabResult(r(0).toString, r(1).toString, new Date(dateFormat.parse(r(2).toString).getTime), r(3).toString, if (!r(4).toString.isEmpty) r(4).toString.toDouble else 0.0d)
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/144.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Diagnostic(r(0).toString, r(1).toString, new Date(dateFormat.parse(""1900-01-01 00:00:00"").getTime), r(2).toString, if (r(3).toString.isEmpty) 0 else r(3).toString.toInt)
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/158.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Medication(r(0).toString, r(1).toString, if (!r(2).toString.isEmpty) new Date(dateFormat.parse(r(2).toString).getTime) else if (!r(3).toString.isEmpty) new Date(dateFormat.parse(r(3).toString).getTime) else new Date(dateFormat.parse(""1900-01-01 00:00:00"").getTime), r(3).toString)
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/173.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => !r(4).toString.toLowerCase.contains(""discharge summary"")
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/175.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => Note(r(0).toString, r(1).toString, if (r(3).toString.isEmpty) {
        val dateString = if (r(2).toString.trim.length == 10) r(2).toString.trim + "" 00:00:00"" else r(2).toString.trim
        new Date(dateFormat.parse(dateString).getTime)
      } else {
        val dateString = if (r(3).toString.trim.length == 10) r(3).toString.trim + "" 00:00:00"" else r(3).toString.trim
        new Date(dateFormat.parse(dateString).getTime)
      }, r(4).toString, r(5).toString, r(6).toString)
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Patient(r(0).toString, if (r(1).toString.toLowerCase == ""m"") true else false, new Date(dateFormat.parse(r(2).toString).getTime), r(4).toString.toDouble, if (r(4).toString.toInt == 1 && r(3).toString.trim != """") new Date(dateFormat.parse(r(3).toString).getTime) else new Date(dateFormat.parse(""9000-01-01 00:00:00"").getTime), new Date(dateFormat.parse(""1971-01-01 00:00:00"").getTime), 0.0d)
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/53.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => PatientId(r(0).toString)
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/67.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => IcuStays(r(0).toString, if (r(1).toString.isEmpty) new Date(dateFormat.parse(""9000-01-01 00:00:00"").getTime) else new Date(dateFormat.parse(r(1).toString).getTime), if (r(2).toString.isEmpty) new Date(dateFormat.parse(""9000-01-01 00:00:00"").getTime) else new Date(dateFormat.parse(r(2).toString).getTime))
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/81.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Chart(r(0).toString, new Date(dateFormat.parse(r(1).toString).getTime))
"
"udf/spark_repos_0/1_joychak_mortality-prediction/..src.main.scala.com.datalogs.ioutils.LoadData.scala/udf/97.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Saps2(r(0).toString, r(1).toString, r(2).toString, if (r(3).toString.isEmpty) 0.0d else r(3).toString.toDouble, if (r(4).toString.isEmpty) 0.0d else r(4).toString.toDouble, if (r(5).toString.isEmpty) 0.0d else r(5).toString.toDouble, if (r(6).toString.isEmpty) 0.0d else r(6).toString.toDouble, if (r(7).toString.isEmpty) 0.0d else r(7).toString.toDouble, if (r(8).toString.isEmpty) 0.0d else r(8).toString.toDouble, if (r(9).toString.isEmpty) 0.0d else r(9).toString.toDouble, if (r(10).toString.isEmpty) 0.0d else r(10).toString.toDouble, if (r(11).toString.isEmpty) 0.0d else r(11).toString.toDouble, if (r(12).toString.isEmpty) 0.0d else r(12).toString.toDouble, if (r(13).toString.isEmpty) 0.0d else r(13).toString.toDouble, if (r(14).toString.isEmpty) 0.0d else r(14).toString.toDouble, if (r(15).toString.isEmpty) 0.0d else r(15).toString.toDouble, if (r(16).toString.isEmpty) 0.0d else r(16).toString.toDouble, if (r(17).toString.isEmpty) 0.0d else r(17).toString.toDouble, if (r(18).toString.isEmpty) 0.0d else r(18).toString.toDouble, if (r(19).toString.isEmpty) 0.0d else r(19).toString.toDouble)
"
"udf/spark_repos_0/1_jsxz_spark-projects/..movie-comment.src.main.scala.vip.anjun.movie.UsersAnalyzerDateFrame.scala/udf/68.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_0/1_jsxz_spark-projects/..movie-comment.src.main.scala.vip.anjun.movie.UsersAnalyzerDateFrame.scala/udf/73.23.Dataset-Rating.filter","Type: org.apache.spark.sql.Dataset[vip.anjun.movie.UsersAnalyzerDateFrame.Rating]
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/134.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""name000"", ""name001"", ""name002"", ""name003"", ""name004"") and !($""col0"" isin (""name001"", ""name002"", ""name003""))
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/106.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/115.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/88.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/97.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.core.src.test.scala.org.apache.spark.sql.PhoenixSuite.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""row005"", ""row001"", ""row002"") and !($""col0"" isin (""row001"", ""row002""))
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/102.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/106.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/73.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/77.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/89.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/101.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/66.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/73.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/66.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/77.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_0/1_kabass_big-data/..shc-1.1.1-1.6.examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.App.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.App.scala/udf/74.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.App.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.App.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.App.scala/udf/91.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.App.scala/udf/95.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.AvroSource.scala/udf/102.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.AvroSource.scala/udf/106.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/73.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/77.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.CompositeKey.scala/udf/89.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataCoder.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataCoder.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataCoder.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/101.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/66.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/73.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.DataType.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.HBaseSource.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.HBaseSource.scala/udf/66.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.HBaseSource.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.HBaseSource.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.HBaseSource.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.HBaseSource.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.JoinTablesFrom2Clusters.scala/udf/77.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-hbase-connector-shc.src.main.scala.sn.ka.spark_hbase_connector_shc.JoinTablesFrom2Clusters.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_0/1_kabass_big-data/..spark-sql-examples.src.main.scala.sn.ka.spark_sql_examples.JsonExamples.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..dk-job.src.main.scala.com.dk.job.builder.DKDataFrameBuilder.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.get(row.fieldIndex(""ENTITY_NAME"")).toString()
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..dk-job.src.main.scala.com.dk.storage_manager.DKStorageManager.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..dk-job.src.main.scala.com.dk.training.FraudDetectionHelperAndDemos.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

lower($""jobType"").contains(""engineer"")
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..dk-job.src.main.scala.com.dk.training.FraudDetectionHelperAndDemos.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

lower($""jobType"").isin(List(""chemical engineer"", ""teacher""): _*)
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..dk-job.src.main.scala.com.dk.training.FraudDetectionHelperAndDemos.scala/udf/35.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(fullString: String) => locally {
        val _t_m_p_4 = fullString.split("" "")
        _t_m_p_4.map(_.capitalize)
      }.mkString("" "")
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..dk-job.src.main.scala.com.dk.util.DKUdfUtility.scala/udf/13.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(r: Row, cols: Object) => {
        val colNames = cols.toString().split("","")
        val resultStatus = new StringBuilder()
        val colValue = new StringBuilder("""")
        var count: Int = 0
        locally {
          val _t_m_p_2 = r.toSeq
          _t_m_p_2.foreach { x => 
            if (x == null || x.equals("""")) {
              resultStatus.append(colNames(count)).append("" , "")
            }
            count = count + 1
          }
        }
        if (!resultStatus.isEmpty) {
          resultStatus.replace(resultStatus.length - 2, resultStatus.length - 1, """")
        }
        UdfOutput(resultStatus.toString(), colValue.toString(), s""columns contains the null values"")
      }
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark-azure-etl.src.main.scala.com.dk.job.standardization.DataStandardizer.scala/udf/20.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(column_name_value) === mapping.DESTINATION_COL_NAME
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark-azure-etl.src.main.scala.com.dk.job.standardization.DataStandardizer.scala/udf/22.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(source_name_column_value) === source || col(source_name_column_value) === All
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark-azure-etl.src.main.scala.com.dk.job.transformation.DataTypeTransformation.scala/udf/25.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

to_date_type_udf
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.crap.Crap4.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""IdentifierTypeCode"" === ""MRN""
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.crap.StandardizationExamples.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getString(0), genders.get(r.getString(1)), medics.get(r.getString(2)))
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.deltalake.DeltaLakeAcidDemo.scala/udf/24.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ i => 
        if (i > 9) {
          Thread.sleep(1000)
          throw new RuntimeException(""Error occured"")
        }
        i
      }
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.deltalake.SparkAcidIssues.scala/udf/21.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ i => 
        if (i > 50) {
          Thread.sleep(1000)
          throw new RuntimeException(""Error occured"")
        }
        i
      }
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.examples.DataFrameTransformation.scala/udf/85.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

dateFormatUdf
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.froddetection.FraudDetectionHelperAndDemos.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

lower($""jobType"").contains(""engineer"")
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.froddetection.FraudDetectionHelperAndDemos.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

lower($""jobType"").isin(List(""chemical engineer"", ""teacher""): _*)
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.froddetection.FraudDetectionHelperAndDemos.scala/udf/51.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(fullString: String) => locally {
        val _t_m_p_4 = fullString.split("" "")
        _t_m_p_4.map(_.capitalize)
      }.mkString("" "")
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.froddetection.Fraud.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""_corrupt_record"".isNotNull
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.type1type2.Type2DriverKuduHbase.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PatientID"").isin(patIdsToFetchFromIncrementalData: _*)
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.type1type2.Type2DriverPartitionByYear.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PatientID"").isin(patIdsToFetchFromIncrementalData: _*)
"
"udf/spark_repos_0/1_kachhdh1_Spark-practice/..spark_examples.src.type1type2.TypeOneDriver.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""CreatedYear"").isin(yearColsList: _*)
"
"udf/spark_repos_0/1_kilolee_SparkSQLProject/..src.main.scala.com.kilo.data.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_0/1_kilolee_SparkSQLProject/..src.main.scala.com.kilo.data.DataFrameRDDApp.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/1_kilolee_SparkSQLProject/..src.main.scala.com.kilo.data.DataSetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.kilo.data.DataSetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/1_kilolee_SparkSQLProject/..src.main.scala.com.kilo.log.TopNStatJob.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_kilolee_SparkSQLProject/..src.main.scala.com.kilo.log.TopNStatJobYARN.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_kilolee_SparkSQLProject/..src.main.scala.com.kilo.log.TopNStatJobYARN.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_kilolee_SparkSQLProject/..src.main.scala.com.kilo.log.TopNStatJobYARN.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_krish5989_AdUsageStatistics/..src.main.scala.AdUsageStatAppDF.scala/udf/27.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""guid"").rlike(""^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$"")
"
"udf/spark_repos_0/1_krish5989_AdUsageStatistics/..src.main.scala.AdUsageStatApp.scala/udf/29.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""guid"").rlike(""^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$"")
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.core.dadaAnalyse.dataAnalyse.scala/udf/34.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.core.dadaAnalyse.dataAnalyse.scala/udf/42.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.core.dadaAnalyse.dataAnalyse.scala/udf/49.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.core.dadaAnalyse.dataAnalyse.scala/udf/56.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\\t"")
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.core.spark_core.Customer_UDAF.scala/udf/33.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Customer_UDAF
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.ml.CreditCardDataExploratory.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
        val label = row.toSeq.last.asInstanceOf[String].toDouble
        label.equals(1.0d)
      }
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.ml.MLUtils.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val seq = row.toSeq
        val data = locally {
          val _t_m_p_2 = seq.drop(1).dropRight(2)
          _t_m_p_2.map(_.asInstanceOf[String].toDouble)
        }
        (Vectors.dense(data.toArray), seq.last.asInstanceOf[String].toDouble)
      }
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.ml.MLUtils.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataset(""label"") === 0
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.DSL_Spark_sql.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.MovieUserAnalyzerWithDataFrame.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""movieid = $MOVIE_ID""
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.Schema2DataFrame_reflect.scala/udf/16.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.Schema2DataFrame_reflect.scala/udf/32.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.Schema2DataFrame_reflect.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.Schema2DataFrame_reflect.scala/udf/41.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.Schema2DataFrame_StructType.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.SparkSQLSimpleExample.scala/udf/83.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          u => (u.getAs[String](""userID"").toLong, u.getAs[String](""age"").toInt + 1)
        }
"
"udf/spark_repos_0/1_lebinlebin_Spark-learning/..src.main.scala.org.training.spark.sql.udaf_Average_salary.scala/udf/38.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new udaf_Average_salary
"
"udf/spark_repos_0/1_liuzd123156_scala/..src.main.scala.com.ruozedata.spark.sql.DataSkew.DataSkewApp.scala/udf/13.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: String) => getRandomInt(randomScope) + ""-"" + x
"
"udf/spark_repos_0/1_liuzd123156_scala/..src.main.scala.com.ruozedata.spark.sql.DataSkew.DataSkewApp.scala/udf/17.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: String) => x.split(""-"")(1)
"
"udf/spark_repos_0/1_liuzd123156_scala/..src.main.scala.com.ruozedata.spark.sql.SparkSQLApp.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 20
"
"udf/spark_repos_0/1_liuzd123156_scala/..src.main.scala.com.ruozedata.spark.sql.SparkSQL_UDAF_APP.scala/udf/34.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new SparkSQL_UDAF_APP
"
"udf/spark_repos_0/1_livelybug_SparkScalaKafkaCassandra/..spark.src.main.scala.org.test.spark.KafkaTest.scala/udf/43.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => parseLog(line).getOrElse(null)
"
"udf/spark_repos_0/1_manikantamaddula_RealTime-Text-Summarization-SigSpace/..SourceCode.Spark_KMeans_FV.src.main.scala.testingCode.EM_NV.scala/udf/77.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

vec(""word"").isin(topwordterms.toList: _*)
"
"udf/spark_repos_0/1_manikantamaddula_RealTime-Text-Summarization-SigSpace/..SourceCode.Spark_KMeans_FV.src.main.scala.testingCode.Kmeans_NV.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

vec_scaled(""word"").isin(topwordterms.toList: _*)
"
"udf/spark_repos_0/1_manikantamaddula_RealTime-Text-Summarization-SigSpace/..SourceCode.Spark_KMeans_FV.src.main.scala.testingCode.SOM_Kmeans_DT.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

vec(""word"").isin(topwordterms.toList: _*)
"
"udf/spark_repos_0/1_manikantamaddula_RealTime-Text-Summarization-SigSpace/..SourceCode.Spark_KMeans_FV.src.main.scala.testingCode.SOM_SOM_NV.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

vec(""word"").isin(topwordterms.toList: _*)
"
"udf/spark_repos_0/1_manikantamaddula_RealTime-Text-Summarization-SigSpace/..SourceCode.Spark_KMeans_FV.src.main.scala.testingCode.testAllClasses.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

vec(""word"").isin(topwordterms.toList: _*)
"
"udf/spark_repos_0/1_manug25_Yelp-Data-Analytics/..streaming.src.main.scala.com.manu.yelp.stream.StreamData.scala/udf/18.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""data.jsontostructs(CAST(value AS STRING))"").isNotNull
"
"udf/spark_repos_0/1_MC-Zealot_spark-WB/..src.UserSimilarityByTags.TestScala.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split("":"")(0)
"
"udf/spark_repos_0/1_mpojeda84_car-data-stream-transform/..src.main.scala.com.mpojeda84.mapr.scala.Application.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

Helper.toJsonWithId
"
"udf/spark_repos_0/1_mpojeda84_car-data-stream-transform/..src.main.scala.com.mpojeda84.mapr.scala.Application.scala/udf/18.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Helper.valueIfInLastXDays
"
"udf/spark_repos_0/1_muhalfian_spark-structured-stream/..src.main.scala.com.muhalfian.spark.jobs.OnlineStream.scala/udf/31.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => RowArtifact.rowMasterDataUpdate(r)
"
"udf/spark_repos_0/1_nishantvaidya_spark-scala/..DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.test.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_0/1_niyangbaba_bigdata_project/..bigdata_project.src.main.scala.com.dxh.task.session.SessionStatTask.scala/udf/337.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityConcatUDAF
"
"udf/spark_repos_0/1_pkprzekwas_spark-flink-comparison/..spark.src.main.scala.examples.sql.Application.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".rlike(state)
"
"udf/spark_repos_0/1_project-sunbird_sunbird-core-dataproducts/..batch-models.src.main.scala.org.ekstep.analytics.job.report.AssessmentMetricsJob.scala/udf/144.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""courseid"") === courseId && col(""batchid"") === batchId
"
"udf/spark_repos_0/1_project-sunbird_sunbird-core-dataproducts/..batch-models.src.main.scala.org.ekstep.analytics.job.report.AssessmentMetricsJob.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

lower(col(""active"")).equalTo(""true"")
"
"udf/spark_repos_0/1_project-sunbird_sunbird-core-dataproducts/..batch-models.src.main.scala.org.ekstep.analytics.job.report.AssessmentMetricsJob.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

lower(col(""isdeleted"")) === ""false""
"
"udf/spark_repos_0/1_project-sunbird_sunbird-core-dataproducts/..batch-models.src.main.scala.org.ekstep.analytics.job.report.CourseMetricsJob.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

lower(col(""isdeleted"")) === ""false""
"
"udf/spark_repos_0/1_project-sunbird_sunbird-core-dataproducts/..batch-models.src.main.scala.org.ekstep.analytics.job.report.CourseMetricsJob.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""index"") === 1
"
"udf/spark_repos_0/1_project-sunbird_sunbird-core-dataproducts/..batch-models.src.main.scala.org.ekstep.analytics.job.report.CourseMetricsJob.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""index"") =!= 1
"
"udf/spark_repos_0/1_project-sunbird_sunbird-core-dataproducts/..batch-models.src.main.scala.org.ekstep.analytics.job.report.CourseMetricsJob.scala/udf/95.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""course_completion"").equalTo(100)
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.log.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.log.TopNStatJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.log.TopNStatJob.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.log.TopNStatJob.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.log.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.log.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.log.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.spark.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.iceberg.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/1_qiliangwang_Spark/..spark-sql.src.main.scala.com.iceberg.spark.DatasetApp.scala/udf/16.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.iceberg.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/1_rajeshvenkatesan_Hbase/..src.main.scala.com.hbase.write.WriteToHbase.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f => f.getAs(""_unit_id"").toString.forall(_.isDigit)
"
"udf/spark_repos_0/1_rajeshvenkatesan_Hbase/..src.main.scala.com.hbase.write.WriteToHbase.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f => !f.anyNull
"
"udf/spark_repos_0/1_rajeshvenkatesan_Hbase/..src.main.scala.com.streaming.Kafka.ReadFromKafka.scala/udf/15.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

x => udf(x.getAs(""value""))
"
"udf/spark_repos_0/1_rajeshvenkatesan_Hbase/..src.main.scala.com.streaming.Kafka.ReadFromKafka.scala/udf/19.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

x => !udf(x.getAs(""value""))
"
"udf/spark_repos_0/1_rayyildiz_spark-getting-started/..src.main.scala.dev.rayyildiz.intro.WordCount.scala/udf/10.20.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""and"")
"
"udf/spark_repos_0/1_rayyildiz_spark-getting-started/..src.main.scala.dev.rayyildiz.intro.WordCount.scala/udf/14.20.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""or"")
"
"udf/spark_repos_0/1_rayyildiz_spark-getting-started/..src.main.scala.dev.rayyildiz.intro.WordCount.scala/udf/18.20.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains("" "")
"
"udf/spark_repos_0/1_realtime-fraud-check_streaming-spark-kafka/..src.main.scala.streamprocessing.StreamProcess.scala/udf/11.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

decrypt(_)
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_github.scala/udf/119.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""language"").isin(languages_array: _*)
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_github.scala/udf/123.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""language"").isin(languages_array: _*)
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_github.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

pullRequestHistoryDF(""action"") === ""opened""
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_github.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

pullRequestHistoryDF(""action"") === ""opened""
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_github.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""year"") >= 2007 && col(""year"") <= 2019
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_github.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""year"") >= 2007 && col(""year"") <= 2019
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_github.scala/udf/93.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

issueEventsDF(""action"") === ""closed""
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_stackoverflow.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""language"").isin(languages_array: _*)
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_stackoverflow.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""_PostTypeId"") === 1
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.app_code.analysis_stackoverflow.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

questionsDF(""_AnswerCount"" === 0)
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.etl_code.etl_github.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!projectsDF_nonull(""language"").contains(""\\N"")
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.etl_code.etl_github.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""year"") >= 2007 && col(""year"") <= 2019
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.profiling_code.profile_github.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(colName).isNull || df(colName).isNaN
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.profiling_code.profile_github.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(colName) === "" ""
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.profiling_code.profile_github.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(colName) === """"
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.profiling_code.profile_stackoverflow.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(colName).isNull || df(colName).isNaN
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.profiling_code.profile_stackoverflow.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(colName) === "" ""
"
"udf/spark_repos_0/1_samarthtambad_big-data-pl/..src.main.scala.profiling_code.profile_stackoverflow.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(colName) === """"
"
"udf/spark_repos_0/1_shadowsama_spark/..spark.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_0/1_shadowsama_spark/..spark.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_shadowsama_spark/..spark.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_shadowsama_spark/..spark.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/1_shadowsama_spark/..spark.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/1_shadowsama_spark/..spark.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/1_shadowsama_spark/..spark.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_shadowsama_spark/..spark.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_0/1_ShellMount_BigDataExamples/..src.main.scala.com.setapi.sparkDemo.spark_sql_demo.LogAnalyzerSparkSQL.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getLong(1))
"
"udf/spark_repos_0/1_ShellMount_BigDataExamples/..src.main.scala.com.setapi.sparkDemo.spark_sql_demo.LogAnalyzerSparkSQL.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_0/1_ShellMount_BigDataExamples/..src.main.scala.com.setapi.sparkDemo.spark_sql_demo.LogAnalyzerSparkSQL.scala/udf/74.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_0/1_ShellMount_BigDataExamples/..src.main.scala.com.setapi.sparkDemo.spark_sql_demo.SparkSQLUDAF2.scala/udf/12.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(age: Int) => age + 100
"
"udf/spark_repos_0/1_ShellMount_BigDataExamples/..src.main.scala.com.setapi.sparkDemo.spark_sql_demo.SparkSQLUDAF2.scala/udf/37.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => {
        val Array(content_id, goods_price) = input.split(""-"")
        locally {
          val _t_m_p_3 = content_id.split("","").zip(goods_price.split("",""))
          _t_m_p_3.map({
            case (id, price) =>
              s""$id,$price""
          })
        }.mkString(""->"")
      }
"
"udf/spark_repos_0/1_ShellMount_BigDataExamples/..src.main.scala.com.setapi.sparkDemo.spark_sql_demo.SparkSQLUDAF2.scala/udf/50.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => {
        val Array(content_id, goods_price, goods_quantity) = input.split(""-"")
        locally {
          val _t_m_p_5 = locally {
            val _t_m_p_6 = content_id.split("","").zip(goods_price.split("",""))
            _t_m_p_6.map({
              case (id, price) =>
                s""$id,$price""
            })
          }.zip(goods_quantity.split("",""))
          _t_m_p_5.map({
            case (id_price, quantity) =>
              s""$id_price,$quantity""
          })
        }.mkString(""->"")
      }
"
"udf/spark_repos_0/1_ShellMount_BigDataExamples/..src.main.scala.com.setapi.sparkDemo.spark_sql_demo.SparkSQLUDAF.scala/udf/12.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(age: Int) => age + 100
"
"udf/spark_repos_0/1_SidharthaRay_omnicart-data-prep/..src.main.scala.com.disney.omnicart.StreamProcessor.scala/udf/21.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

record => parse(record)
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/113.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/51.23.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/98.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/553.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/557.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/585.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/589.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/897.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/906.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/80.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_0/1_SilenceYangXP_spark-thriftserver-HA/..hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.beifeng.sql.commons.DataFrameOperation.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 18
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.beifeng.sql.commons.UDAF.scala/udf/25.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new StringCount
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.beifeng.sql.commons.UDF.scala/udf/22.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length()
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.beifeng.sql.count.DailySale.scala/udf/28.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => Row(row(1), row(2))
        }
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.beifeng.sql.count.DailyUV.scala/udf/27.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => Row(row(1), row(2))
        }
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.io.SparkFileIO.scala/udf/70.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(""\t"")
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.io.SparkFileIO.scala/udf/74.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.mkString(""\t"")
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.io.SparkFutureIO.scala/udf/19.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => Tuple1(line)
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.test1.ml.CreditCardDataExploratory.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
        val label = row.toSeq.last.asInstanceOf[String].toDouble
        label.equals(1.0d)
      }
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.test1.ml.MLUtils.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val seq = row.toSeq
        val data = locally {
          val _t_m_p_2 = seq.drop(1).dropRight(2)
          _t_m_p_2.map(_.asInstanceOf[String].toDouble)
        }
        (Vectors.dense(data.toArray), seq.last.asInstanceOf[String].toDouble)
      }
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.test1.ml.MLUtils.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataset(""label"") === 0
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.test1.sql.MovieUserAnalyzerWithDataFrame.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""movieid = $MOVIE_ID""
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.test1.sql.SparkSQLSimpleExample.scala/udf/86.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          u => (u.getAs[String](""userID"").toLong, u.getAs[String](""age"").toInt + 1)
        }
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.testoffical.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.TestSpark.scala/udf/12.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""li"")
"
"udf/spark_repos_0/1_simonlisiyu_myhadoop/..spark2-test.src.main.scala.com.lsy.myhadoop.spark2.TestSpark.scala/udf/16.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""tom"")
"
"udf/spark_repos_0/1_stanford-futuredata_cs245-as2-public/..src.main.scala.edu.stanford.cs245.Tester.scala/udf/83.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

dist
"
"udf/spark_repos_0/1_vaijnathp_SparkStreamingAndRDDExample/..src.main.scala.polsys.sparkRDD.LongestWord.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

words => (words.length, words)
"
"udf/spark_repos_0/1_vaijnathp_SparkStreamingAndRDDExample/..src.main.scala.polsys.sparkRDD.LongestWord.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Rank"" === 1
"
"udf/spark_repos_0/1_vaijnathp_SparkStreamingAndRDDExample/..src.main.scala.polsys.structuredStreaming.AggregateSocketExample.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr = line.split("","")
        (arr(0), Timestamp.valueOf(arr(1)))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.blog.spark.ml.DecisionTreeExample.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_2 = arr(1).split(' ')
            _t_m_p_2.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.blog.spark.ml.featureprocessing.StringIndexerDemo.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_2 = arr(1).split(' ')
            _t_m_p_2.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.blog.spark.ml.featureprocessing.StringIndexerDemo.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_4 = arr(1).split(' ')
            _t_m_p_4.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.blog.spark.udf.UdfDemo.scala/udf/35.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length()
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.blog.spark.udf.UdfDemo.scala/udf/39.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAdult _
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.blog.spark.udf.UdfDemo.scala/udf/51.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length()
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.blog.spark.udf.UdfDemo.scala/udf/55.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAdult _
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.learning.spark.df.TestDf.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""col1"") === df(""col2"")
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.learning.spark.ml.featureprocessing.ReadData.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_2 = arr(1).split(' ')
            _t_m_p_2.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.learning.spark.ml.ReadDataDemo.scala/udf/122.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0).toDouble, Vectors.dense(locally {
            val _t_m_p_15 = arr(1).split(' ')
            _t_m_p_15.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.learning.spark.ml.ReadDataDemo.scala/udf/133.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0).toDouble, Vectors.dense(locally {
            val _t_m_p_17 = arr(1).split(' ')
            _t_m_p_17.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.learning.spark.ml.StatisticsDemo.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = locally {
            val _t_m_p_6 = line.split(""\t"")
            _t_m_p_6.map(_.toDouble)
          }
          (arr(0), arr(1), arr(2), arr(3), arr(4))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.learning.spark.ml.VectorIndexerDemo.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_2 = arr(1).split(' ')
            _t_m_p_2.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.learning.spark.sql.ReadDemo.scala/udf/16.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.learning.spark.sql.ReadDemo.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.spark.sql.DataFrameOperation.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 18
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.spark.sql.SparkExampleUDF.scala/udf/27.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.spark.sql.SparkExampleUDF.scala/udf/31.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAdult _
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.spark.sql.UDAF.scala/udf/21.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new StringCount
"
"udf/spark_repos_0/1_wangshubing1_my-spark/..src.main.scala.com.king.spark.sql.UDF.scala/udf/21.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length()
"
"udf/spark_repos_0/1_wfxian_Ad_DMP/..src.main.scala.com.tags.TagsContext.scala/udf/54.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

TagsUtil.oneUserId
"
"udf/spark_repos_0/1_ws0352_spark_learning/..src.main.scala.spark_learning.Library.scala/udf/14.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ref_height"") > 15.00d
"
"udf/spark_repos_0/1_ws0352_spark_learning/..src.main.scala.spark_learning.Library.scala/udf/19.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""ref_height"" > 15.00d
"
"udf/spark_repos_0/1_ws0352_spark_learning/..src.main.scala.spark_learning.Library.scala/udf/33.17.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.dataframe.LogEtlApp.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""traffic"" > 0
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.dataset.DataSetTest.scala/udf/14.19.Dataset-People.map","Type: org.apache.spark.sql.Dataset[com.wsk.spark.sql.dataset.DataSetTest.People]
Call: map

_.name
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.function.UDFApp.scala/udf/16.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(like: String) => like.split("","").size
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.hvie.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.SparkSqlExample.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.SparkSqlExample.scala/udf/36.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.SparkSqlExample.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.SparkSqlExample.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.SparkSqlExample.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.SparkSqlExample.scala/udf/92.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.SQLDataSourceExample.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/1_wsk1314zwr_spark-train/..src.main.scala.com.wsk.spark.sql.UserDefinedUntypedAggregation.scala/udf/32.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

WskAverage
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.logs.SQLLogAnalyzerSpark.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 30
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSpark.scala/udf/209.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSpark.scala/udf/213.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSpark.scala/udf/217.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSpark.scala/udf/221.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV1.scala/udf/172.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV1.scala/udf/176.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV1.scala/udf/180.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV1.scala/udf/184.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV2.scala/udf/182.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV2.scala/udf/186.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV2.scala/udf/190.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV2.scala/udf/194.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV3.scala/udf/192.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV3.scala/udf/196.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV3.scala/udf/200.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.meituan.product.AreaTop10ProductSparkV3.scala/udf/204.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.sql.SchemaSparkSQL.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""cnt"" > 100
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.sql.SparkSQLDemo.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""order_amt"" > 50
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.sql.TaoBaoBraJsonSpark.scala/udf/10.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sizeInfo: String) => sizeInfo.split(""\\;"")(0)
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.sql.TaoBaoBraJsonSpark.scala/udf/14.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AvgSalUDAF
"
"udf/spark_repos_0/1_xiaomengxun_sparkDemo/..spark.sql.TaoBaoBraJsonSpark.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(trim($""size_info"")) > 0
"
"udf/spark_repos_0/1_YHGui_scala/..ImoocSparkSQL.src.main.scala.com.imooc.log.TopNStatJob2.scala/udf/99.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_YHGui_scala/..ImoocSparkSQL.src.main.scala.com.imooc.log.TopNStatJob.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_YHGui_scala/..ImoocSparkSQL.src.main.scala.com.imooc.log.TopNStatJob.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_YHGui_scala/..ImoocSparkSQL.src.main.scala.com.imooc.log.TopNStatJob.scala/udf/75.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_YHGui_scala/..ImoocSparkSQL.src.main.scala.com.imooc.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 20
"
"udf/spark_repos_0/1_YHGui_scala/..ImoocSparkSQL.src.main.scala.com.imooc.spark.DataFrameRDDApp.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/1_YHGui_scala/..ImoocSparkSQL.src.main.scala.com.imooc.spark.DataFrameRDDApp.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/1_YHGui_scala/..ImoocSparkSQL.src.main.scala.com.imooc.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.imooc.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..eval.src.main.scala.com.johnsnowlabs.nlp.eval.NerCrfEvaluation.scala/udf/118.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""label"") =!= 'O'
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..eval.src.main.scala.com.johnsnowlabs.nlp.eval.NerCrfEvaluation.scala/udf/129.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getDouble(0), r.getDouble(1))
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..eval.src.main.scala.com.johnsnowlabs.nlp.eval.NerDLEvaluation.scala/udf/118.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""label"") =!= 'O'
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..eval.src.main.scala.com.johnsnowlabs.nlp.eval.NerDLEvaluation.scala/udf/129.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getDouble(0), r.getDouble(1))
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..eval.src.main.scala.com.johnsnowlabs.nlp.eval.NorvigSpellEvaluation.scala/udf/52.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !(row.mkString("""").isEmpty && row.length > 0)
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..eval.src.main.scala.com.johnsnowlabs.nlp.eval.SymSpellEvaluation.scala/udf/51.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !(row.mkString("""").isEmpty && row.length > 0)
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..src.main.scala.com.johnsnowlabs.nlp.Annotation.scala/udf/45.19.Dataset-AnnotationContainer.map","Type: org.apache.spark.sql.Dataset[com.johnsnowlabs.nlp.Annotation.AnnotationContainer]
Call: map

_.__annotation
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..src.main.scala.com.johnsnowlabs.nlp.Annotation.scala/udf/76.19.Dataset-AnnotationContainer.map","Type: org.apache.spark.sql.Dataset[com.johnsnowlabs.nlp.Annotation.AnnotationContainer]
Call: map

_.__annotation
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..src.main.scala.com.johnsnowlabs.nlp.functions.scala/udf/16.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

func(col(column)).as(column, meta)
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..src.main.scala.com.johnsnowlabs.nlp.training.POS.scala/udf/59.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.nonEmpty
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..src.main.scala.com.johnsnowlabs.nlp.training.POS.scala/udf/61.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => lineToTaggedDocument(line, delimiter)
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..src.main.scala.com.johnsnowlabs.nlp.training.POS.scala/udf/63.19.Dataset-TaggedDocument.map","Type: org.apache.spark.sql.Dataset[com.johnsnowlabs.nlp.training.TaggedDocument]
Call: map

{
        case TaggedDocument(sentence, taggedTokens) =>
          Annotations(sentence, createDocumentAnnotation(sentence), createPosAnnotation(sentence, taggedTokens))
      }
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..src.test.scala.com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteBehaviors.scala/udf/83.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""prediction"") === 1
"
"udf/spark_repos_0/1_yongsheng268_Spark-NLP/..src.test.scala.com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteBehaviors.scala/udf/87.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""prediction"") === 0
"
"udf/spark_repos_0/1_yuanmengnan_hadoop/..SparkLearning.SparkLearning1.src.main.scala.org.apache.spark.ml.Ensembles.EnsemblesSuite.scala/udf/125.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(p: Double, l: Double) =>
          (p, l)
      }
"
"udf/spark_repos_0/1_yuanmengnan_hadoop/..SparkLearning.SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromJson.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 21
"
"udf/spark_repos_0/1_yuanmengnan_hadoop/..SparkLearning.SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromTextReflection.scala/udf/25.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_0/1_yuanmengnan_hadoop/..SparkLearning.SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromTextReflection.scala/udf/32.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t.getAs[String](""name"")
"
"udf/spark_repos_0/1_yuanmengnan_hadoop/..SparkLearning.SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromTextReflection.scala/udf/39.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/1_yuanmengnan_hadoop/..SparkLearning.SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromTextSchema.scala/udf/33.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_0/1_yuanmengnan_hadoop/..SparkLearning.SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLReadParquetMethods.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""id"") > 12904870
"
"udf/spark_repos_0/1_yuanmengnan_hadoop/..SparkLearning.SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLReadParquetMethods.scala/udf/28.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""id: "" + t(0) + "" and modified:"" + t(1)
"
"udf/spark_repos_0/1_zhenchao125_spark0919/..spark-sql.src.main.scala.com.atguigu.saprk.sql.day01.udf.UDAFDemo1.scala/udf/10.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MySum
"
"udf/spark_repos_0/1_zhenchao125_spark0919/..spark-sql.src.main.scala.com.atguigu.saprk.sql.day01.udf.UDAFDemo2.scala/udf/10.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAvg
"
"udf/spark_repos_0/1_zhenchao125_spark0919/..spark-sql.src.main.scala.com.atguigu.saprk.sql.day02.project.SqlApp.scala/udf/9.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RemarkUDAF
"
"udf/spark_repos_0/1_zhicheng-ning_SparkMLlib/..src.com.mage.ml.association_rules.FPGrowthExample.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_0/1_zhicheng-ning_SparkMLlib/..src.com.mage.ml.lr.LinearRegression1.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val parts = line.split(',')
        val features: Array[Double] = locally {
          val _t_m_p_2 = parts(1).split(' ')
          _t_m_p_2.map(_.toDouble)
        }
        LabeledPoint(parts(0).toDouble, Vectors.dense(features))
      }
"
"udf/spark_repos_0/1_zhicheng-ning_SparkMLlib/..src.com.mage.ml.lr.LinearRegression2.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val parts = line.split(',')
        val features: immutable.Seq[String] = for (i <- 0 to 12) yield parts(i)
        LabeledPoint(parts(13).toDouble, Vectors.dense(locally {
          val _t_m_p_2 = features
          _t_m_p_2.map(_.toDouble)
        }.toArray))
      }
"
"udf/spark_repos_0/1_zhicheng-ning_SparkMLlib/..src.com.mage.ml.lr.LogisticRegression3.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val label: Double = row.getAs[Double](""label"")
        val features: SparseVector = row.getAs[SparseVector](""features"")
        val array = features.toArray
        val convertFeatures: linalg.Vector = Vectors.dense(array(0), array(1), array(0) - array(1))
        LabeledPoint(label, convertFeatures)
      }
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.log.TopNStatJob2.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.log.TopNStatJob.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.log.TopNStatJob.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.log.TopNStatJob.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.log.TopNStatJobYARN.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.log.TopNStatJobYARN.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.log.TopNStatJobYARN.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.spark.DataFrameApp.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.spark.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.zs.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/1_zhuangsen_sparksql/..src.main.scala.com.zs.spark.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.zs.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/1_zwbill82_spark-learn/..src.main.scala.com.bigdata.spark.sql.DataFrameCreateTest.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df.col(""id"") > 1 && df.col(""age"") === 30
"
"udf/spark_repos_0/1_zwbill82_spark-learn/..src.main.scala.com.bigdata.spark.sql.fuction.UDAFFuction.scala/udf/18.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new StringCount
"
"udf/spark_repos_0/1_zwbill82_spark-learn/..src.main.scala.com.bigdata.spark.sql.fuction.UDAFFuction.scala/udf/30.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AvgCaculator
"
"udf/spark_repos_0/1_zwbill82_spark-learn/..src.main.scala.com.bigdata.spark.sql.fuction.UDFFunction.scala/udf/18.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length
"
"udf/spark_repos_0/2_120534_Desertification-monitoring/..src.main.scala.chd.raster.ml.SimpleLogicRegression.scala/udf/30.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => t(1) + "":"" + t(0)
"
"udf/spark_repos_0/22_liumingmusic_HadoopLearning/..SparkSQL.src.main.scala.com.c503.sparksql.Spark_SQL_1.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 30
"
"udf/spark_repos_0/22_MrPowers_spark-spec/..src.main.scala.com.github.mrpowers.spark.spec.ml.classification.TitanicData.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Gender"").isNotNull && col(""Pclass"").isNotNull && col(""Age"").isNotNull && col(""SibSp"").isNotNull && col(""Parch"").isNotNull && col(""Fare"").isNotNull
"
"udf/spark_repos_0/22_MrPowers_spark-spec/..src.main.scala.com.github.mrpowers.spark.spec.ml.classification.TitanicData.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Gender"").isNotNull && col(""Survived"").isNotNull && col(""Pclass"").isNotNull && col(""Age"").isNotNull && col(""SibSp"").isNotNull && col(""Parch"").isNotNull && col(""Fare"").isNotNull
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-etl.src.main.scala.com.sope.etl.register.UDFRegistration.scala/udf/14.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udf
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-etl.src.main.scala.com.sope.etl.yaml.End2EndYaml.scala/udf/37.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udfInst
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.sql.dsl.package.scala/udf/211.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.sql.dsl.package.scala/udf/217.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

locally {
          val _t_m_p_24 = locally {
            val _t_m_p_25 = routingConditions
            _t_m_p_25.map {
              condition => not(condition)
            }
          }
          _t_m_p_24.reduce(_ and _)
        }
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.sql.dsl.package.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.sql.dsl.package.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.sql.package.scala/udf/133.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.sql.package.scala/udf/136.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(filterCondition)
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.sql.udfs.package.scala/udf/86.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udf
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.utils.etl.DimensionTable.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$ChangeStatus"" === Insert
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.utils.etl.DimensionTable.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$ChangeStatus"" === Update
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.utils.etl.DimensionTable.scala/udf/73.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$ChangeStatus"" === NoChangeDelete
"
"udf/spark_repos_0/24_mayur2810_sope/..sope-spark.src.main.scala.com.sope.spark.utils.etl.DimensionTable.scala/udf/77.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$ChangeStatus"" === Invalid
"
"udf/spark_repos_0/24_WhiteFangBuck_OReilly-2018/..src.main.scala.com.cloudera.workshop.solutions.classifier.SpamClassification.scala/udf/44.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_0/24_WhiteFangBuck_OReilly-2018/..src.main.scala.com.cloudera.workshop.solutions.classifier.TelcoChurnPrediction.scala/udf/37.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_0/28_sjyttkl_spark_learning/..src.main.scala.com.sjyttkl.bigdata.spark_sql.SparkSQL04_UDAF.scala/udf/15.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udf
"
"udf/spark_repos_0/29_PacktPublishing_Scala-Machine-Learning-Projects/..Chapter09.src.main.scala.com.packt.ScalaML.FraudDetection.FraudDetection2.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""day1""
"
"udf/spark_repos_0/29_PacktPublishing_Scala-Machine-Learning-Projects/..Chapter09.src.main.scala.com.packt.ScalaML.FraudDetection.FraudDetection2.scala/udf/74.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""day2""
"
"udf/spark_repos_0/29_PacktPublishing_Scala-Machine-Learning-Projects/..Chapter09.src.main.scala.com.packt.ScalaML.FraudDetection.FraudDetection2.scala/udf/92.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Class"" === ""0""
"
"udf/spark_repos_0/29_PacktPublishing_Scala-Machine-Learning-Projects/..Chapter09.src.main.scala.com.packt.ScalaML.FraudDetection.FraudDetection2.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Class"" === ""1""
"
"udf/spark_repos_0/29_PacktPublishing_Scala-Machine-Learning-Projects/..Chapter09.src.main.scala.com.packt.ScalaML.FraudDetection.H2OCreditFraud.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""day1""
"
"udf/spark_repos_0/29_PacktPublishing_Scala-Machine-Learning-Projects/..Chapter09.src.main.scala.com.packt.ScalaML.FraudDetection.H2OCreditFraud.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""day2""
"
"udf/spark_repos_0/29_PacktPublishing_Scala-Machine-Learning-Projects/..Chapter09.src.main.scala.com.packt.ScalaML.FraudDetection.H2OCreditFraud.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Class"" === ""0""
"
"udf/spark_repos_0/29_PacktPublishing_Scala-Machine-Learning-Projects/..Chapter09.src.main.scala.com.packt.ScalaML.FraudDetection.H2OCreditFraud.scala/udf/75.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Class"" === ""1""
"
"udf/spark_repos_0/2_abulbasar_SparkScalaExamples/..src.main.scala.com.example.cassandra.StreamToCassandra.scala/udf/17.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(isnull(col(""id"")))
"
"udf/spark_repos_0/2_abulbasar_SparkScalaExamples/..src.main.scala.com.example.dataframe.DatasetExample.scala/udf/38.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

toStock
"
"udf/spark_repos_0/2_abulbasar_SparkScalaExamples/..src.main.scala.com.example.streaming.StructuredStreamFileSourceAppV2.scala/udf/27.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val tokens = line.split("","")
        MyStock2(tokens(0), tokens(1).toDouble, tokens(2).toDouble, tokens(3).toDouble, tokens(4).toDouble, tokens(5).toDouble, tokens(6).toDouble, tokens(7))
      }
"
"udf/spark_repos_0/2_ahyh_spark-learn/..src.main.scala.com.yh.spark.learn.sql.join.IPLocationsSQL2.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_0/2_ahyh_spark-learn/..src.main.scala.com.yh.spark.learn.sql.join.IPLocationsSQL2.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = IPUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_0/2_ahyh_spark-learn/..src.main.scala.com.yh.spark.learn.sql.join.IPLocationsSQL2.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(ipNum: Long) => {
        val ipRulesInExecutor: Array[(Long, Long, String)] = broadCastRef.value
        val index = IPUtils.binarySearch(ipRulesInExecutor, ipNum)
        var province = ""未知""
        if (index != -1) {
          province = ipRulesInExecutor(index)._3
        }
        province
      }
"
"udf/spark_repos_0/2_ahyh_spark-learn/..src.main.scala.com.yh.spark.learn.sql.join.IPLocationsSQL.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_0/2_ahyh_spark-learn/..src.main.scala.com.yh.spark.learn.sql.join.IPLocationsSQL.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = IPUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_0/2_ahyh_spark-learn/..src.main.scala.com.yh.spark.learn.sql.join.SparkSQLJoinTest.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split("","")
        val country = fields(0)
        val name = fields(1)
        (country, name)
      }
"
"udf/spark_repos_0/2_ahyh_spark-learn/..src.main.scala.com.yh.spark.learn.sql.join.SparkSQLJoinTest.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split("","")
        val id = fields(0).toLong
        val name = fields(1)
        val country = fields(2)
        (id, name, country)
      }
"
"udf/spark_repos_0/2_babatunde-abdulquddus_building-data-pipeline/..spark.src.main.scala.io.data.spark.IoTStreamingExample.scala/udf/28.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

SensorRecord.apply
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.AlwaysNullConstraint.scala/udf/7.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(columnName).isNotNull
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.ColumnColumnConstraint.scala/udf/8.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

constraintColumn
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.ConditionalColumnConstraint.scala/udf/8.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!statement || implication
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.DateFormatConstraint.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

cannotBeDate(new Column(columnName))
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.JoinableConstraint.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(""count"") > 1
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.NeverNullConstraint.scala/udf/7.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(columnName).isNull
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.RegexConstraint.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

doesNotMatch(new Column(columnName))
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.StringColumnConstraint.scala/udf/7.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

constraintString
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.TypeConversionConstraint.scala/udf/14.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(castedColumnName).isNull && originalColumn.isNotNull
"
"udf/spark_repos_0/2_datamindedbe_wharlord/..src.main.scala.be.dataminded.wharlord.constraints.UniqueKeyConstraint.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(""count"") > 1
"
"udf/spark_repos_0/2_FlorentF9_spark-config-example/..src.main.scala.CmdWordCount.scala/udf/22.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

not(lower(col(""value"")).isin(stopWords: _*))
"
"udf/spark_repos_0/2_FlorentF9_spark-config-example/..src.main.scala.CmdWordCount.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= minCount
"
"udf/spark_repos_0/2_FlorentF9_spark-config-example/..src.main.scala.ConfigWordCount.scala/udf/17.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

not(lower(col(""value"")).isin(settings.stopWords: _*))
"
"udf/spark_repos_0/2_FlorentF9_spark-config-example/..src.main.scala.ConfigWordCount.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= settings.minCount
"
"udf/spark_repos_0/2_guanpengju1_dmp_16/..src.main.scala.com.pengju.dmp.tags.TagsContext.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

TagsUtils.hasSomeUserIdCondition
"
"udf/spark_repos_0/2_HelloMan_xsql/..spark-runtime.src.main.scala.org.xsql.stream.spark.source.KafkaReader.scala/udf/54.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

F.udf { (line: String) => Regex2Json.toJson(line, pattern, ddl) }
"
"udf/spark_repos_0/2_iwannab1_Kdata/..src.main.scala.raonbit.spark.ml.DataSplitter.scala/udf/20.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

target + "" == "" + cat
"
"udf/spark_repos_0/2_loukwn_similarity-and-range-queries-spark-thesis/..src.main.scala.modules.MinMaxModule.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

tempDocDF(""filterValue"")
"
"udf/spark_repos_0/2_mcarmonaa_QuerySetApp/..src.main.scala.tech.sourced.queryset.BoaQueries.scala/udf/160.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'lang === lang
"
"udf/spark_repos_0/2_medale_scala-spark/..analytics.src.main.scala.com.uebercomputing.scalaspark.analytics.HelloSparkDatasetWorld.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ fullName => 
        val firstLast = fullName.split("" "")
        firstLast(0)
      }
"
"udf/spark_repos_0/2_meliodaseren_spark-sql-demo/..src.com.iii.sparksql101.DatasetAPIDemo2.scala/udf/20.19.Dataset-Transaction.map","Type: org.apache.spark.sql.Dataset[com.iii.sparksql101.Transaction]
Call: map

tx => (s""${tx.account.firstName} ${tx.account.lastName}"", tx.account.number)
"
"udf/spark_repos_0/2_meliodaseren_spark-sql-demo/..src.com.iii.sparksql101.runBasicDataFrameExample.scala/udf/15.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/2_meliodaseren_spark-sql-demo/..src.com.iii.sparksql101.runInferSchemaExample.scala/udf/18.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/2_meliodaseren_spark-sql-demo/..src.com.iii.sparksql101.runInferSchemaExample.scala/udf/22.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/2_meliodaseren_spark-sql-demo/..src.com.iii.sparksql101.runProgrammaticSchemaExample.scala/udf/26.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/23.26.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: filter

_.id % 100 != 0
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/25.24.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: filter

_.id % 101 != 0
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/27.22.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: filter

_.id % 102 != 0
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/29.20.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: filter

_.id % 103 != 0
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/63.24.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: map

d => Data(d.id + 1L)
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/65.22.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: map

d => Data(d.id + 1L)
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/67.20.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: map

d => Data(d.id + 1L)
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/69.18.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: map

d => Data(d.id + 1L)
"
"udf/spark_repos_0/2_ngtrkhoa_spark-sql-worksharing/..spark-sql-perf.src.main.scala.com.databricks.spark.sql.perf.RunBenchmark.scala/udf/66.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'thisRunTimeMs.isNotNull
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.log.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.log.TopNStatJob.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.log.TopNStatJob.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.log.TopNStatJob.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.log.TopNStatJobYARN.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.log.TopNStatJobYARN.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.log.TopNStatJobYARN.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.spark.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.perkinl.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/2_perkinls_SparkSQLProject/..SparkSqlCaculate.src.main.scala.com.perkinl.spark.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.perkinl.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/2_piyushpatel2005_spark-scala/..chapter03App.src.main.scala.org.sia.chapter03App.GithubDay.scala/udf/20.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isEmp
"
"udf/spark_repos_0/2_piyushpatel2005_spark-scala/..chapter03App.src.main.scala.org.sia.chapter03App.GithubDay.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

sqlFunc($""login"")
"
"udf/spark_repos_0/2_piyushpatel2005_spark-scala/..lambda-pluralsight.spark-lambda.src.main.scala.streaming.StreamingJob.scala/udf/42.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
            r => ((r.getString(0), r.getLong(1)), ActivityByProduct(r.getString(0), r.getLong(1), r.getLong(2), r.getLong(3), r.getLong(4)))
          }
"
"udf/spark_repos_0/2_piyushpatel2005_spark-scala/..SparkScalaCourse.src.com.piyushpatel2005.spark.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.piyushpatel2005.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_0/2_quocanh_spark-cherry/..src.main.scala.org.quocanh.spark.cherry.Cherry.scala/udf/124.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(c).isNotNull
"
"udf/spark_repos_0/2_quocanh_spark-cherry/..src.main.scala.org.quocanh.spark.cherry.Cherry.scala/udf/127.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(c).isNull
"
"udf/spark_repos_0/2_quocanh_spark-cherry/..src.main.scala.org.quocanh.spark.cherry.Cherry.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""__count"") === 1
"
"udf/spark_repos_0/2_Saevel_Spark-Lambda/..src.main.scala.prv.zielony.spark.lambda.accounts.AccountFactory.scala/udf/68.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""dt"".as[Long] >= timestamp.toLong
"
"udf/spark_repos_0/2_Shehats_bootiful-spark-reactive/..src.main.scala.com.sparks.sparkscala.spark.FlightX.scala/udf/53.19.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[com.sparks.sparkscala.spark.Flight]
Call: map

flights => Airport(flights.origin, flights.origin.hashCode)
"
"udf/spark_repos_0/2_Shehats_bootiful-spark-reactive/..src.main.scala.com.sparks.sparkscala.spark.FlightX.scala/udf/56.19.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[com.sparks.sparkscala.spark.Flight]
Call: map

flights => Airport(flights.destination, flights.destination.hashCode)
"
"udf/spark_repos_0/2_Shehats_bootiful-spark-reactive/..src.main.scala.com.sparks.sparkscala.spark.FlightX.scala/udf/66.19.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[com.sparks.sparkscala.spark.Flight]
Call: map

flight => Route(flight.origin.hashCode, flight.destination.hashCode, flight.distance, flight.getDate())
"
"udf/spark_repos_0/2_Shehats_bootiful-spark-reactive/..src.main.scala.com.sparks.sparkscala.spark.FlightX.scala/udf/71.19.Dataset-Route.map","Type: org.apache.spark.sql.Dataset[com.sparks.sparkscala.spark.Route]
Call: map

_.getEdge
"
"udf/spark_repos_0/2_Shehats_bootiful-spark-reactive/..src.main.scala.com.sparks.sparkscala.spark.FlightX.scala/udf/76.19.Dataset-Airport.map","Type: org.apache.spark.sql.Dataset[com.sparks.sparkscala.spark.Airport]
Call: map

x => (x.id, x.name)
"
"udf/spark_repos_0/2_ToWorkit_Spark-SQL/..sql.src.main.scala.com.logs.log.TopNStatJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_ToWorkit_Spark-SQL/..sql.src.main.scala.com.logs.log.TopNStatJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_0/2_ToWorkit_Spark-SQL/..sql.src.main.scala.com.logs.spark.DataFrame2RDD.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") < 20
"
"udf/spark_repos_0/2_ToWorkit_Spark-SQL/..sql.src.main.scala.com.logs.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_0/2_ToWorkit_Spark-SQL/..sql.src.main.scala.com.logs.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.logs.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_0/2_wangwei420625_spark/..SparkCore.src.main.scala.com.sposter.sparksql.ScalaSparkSQLByReflection.scala/udf/20.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_0/32_joandre_MCL_spark/..src.main.scala.org.apache.spark.mllib.clustering.MCLModel.scala/udf/38.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          case Row(id: Long, cluster: Long) =>
            Assignment(id, cluster)
        }
"
"udf/spark_repos_0/32_joandre_MCL_spark/..src.main.scala.org.apache.spark.mllib.clustering.MCL.scala/udf/174.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Assignment(row.getInt(0).toLong, row.getInt(1).toLong)
"
"udf/spark_repos_0/32_joandre_MCL_spark/..src.test.scala.org.apache.spark.mllib.clustering.MCLSuite.scala/udf/163.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(group: mutable.WrappedArray[Long]) =>
          (group.max, group)
      }
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/103.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!convertedDF(colName).isNaN
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/110.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.getAs[Int](0) == 0
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/187.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!valueDF(colName).isNaN
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/189.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

r => isFiniteFun(r.getAs(0))
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/287.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (colName, isNaNFun(row.getAs(0)), 1L)
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/348.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!valueDF(featureName).isNaN
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/350.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

r => isFiniteFun(r.getAs(0))
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/453.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_ => 0
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/459.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!colDF(field.name).isNaN
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/461.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_ => 1
"
"udf/spark_repos_0/37_gopro_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/470.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_ => 1
"
"udf/spark_repos_0/39_Trigl_SparkLearning/..src.main.scala.com.trigl.spark.main.ALSEvaluation.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val userid = row.getString(0).hashCode
        val itemid = row.getString(1).hashCode
        val rating = row.getDouble(2)
        val time = row.getLong(3) % 10
        (userid, itemid, rating, time)
      }
"
"udf/spark_repos_0/3_aleks-sidorenko_code-challenge/..course.coursera.capstone.observatory.src.main.scala.observatory.Extraction.scala/udf/20.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""lat"".isNotNull && $""lon"".isNotNull
"
"udf/spark_repos_0/3_aleks-sidorenko_code-challenge/..course.coursera.capstone.observatory.src.main.scala.observatory.Extraction.scala/udf/22.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""lat"" =!= 0.0d && $""lon"" =!= 0.0d
"
"udf/spark_repos_0/3_aleks-sidorenko_code-challenge/..course.coursera.capstone.observatory.src.main.scala.observatory.Extraction.scala/udf/24.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => Station(id = StationId(r.getAs[String](""stn""), r.getAs[String](""wban"")), location = Location(r.getAs[Double](""lat""), r.getAs[Double](""lon"")))
"
"udf/spark_repos_0/3_aleks-sidorenko_code-challenge/..course.coursera.capstone.observatory.src.main.scala.observatory.Extraction.scala/udf/31.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Temperature(stationId = StationId(r.getAs[String](""stn""), r.getAs[String](""wban"")), date = Date(year = year, month = r.getAs[Int](""month""), day = r.getAs[Int](""day"")), temperature = r.getAs[Double](""temperature""))
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/104.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!convertedDF(normalizedColName).isNaN
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/111.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.getAs[Int](0) == 0
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/188.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!valueDF(colName).isNaN
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/190.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

r => isFiniteFun(r.getAs(0))
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/288.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (colName, isNaNFun(row.getAs(0)), 1L)
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/349.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!valueDF(featureName).isNaN
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/351.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

r => isFiniteFun(r.getAs(0))
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/454.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_ => 0
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/460.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!colDF(field.name).isNaN
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/462.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_ => 1
"
"udf/spark_repos_0/3_chesterxgchen_facets-overview-spark/..src.main.scala.features.stats.spark.FeatureStatsGenerator.scala/udf/471.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_ => 1
"
"udf/spark_repos_0/3_deepakshingavi_CoordinatesComparison/..src.main.scala.com.ds.training.CoordinateLocator.scala/udf/52.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(USER_ID_COLUMN).isin(map.keys.toList: _*)
"
"udf/spark_repos_0/3_fucusy_dataframe2html/..src.main.scala.io.github.fucusy.Viz.scala/udf/37.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

F.col(rowOrderCol) === i
"
"udf/spark_repos_0/3_liangfb_h2oaisolution/..pocs.AirlinesWithWeatherDemo.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'Dest === ""ORD""
"
"udf/spark_repos_0/3_liangfb_h2oaisolution/..pocs.CityBikeSharingDemo.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'HourLocal === 12
"
"udf/spark_repos_0/3_liangfb_h2oaisolution/..pocs.DeepLearningDemo.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'Dest === ""SFO""
"
"udf/spark_repos_0/3_luochana_spark_Logs/..spark_project.src.main.scala.Streaming.kafkaConsumer.scala/udf/19.23.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_0/3_luochana_spark_Logs/..spark_project.src.main.scala.Streaming.kafkaConsumer.scala/udf/21.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

lines => {
          val infos = lines.split(""\t"")
          val ip = infos(0)
          val time = DateUtils.parseTime(infos(1))
          var courseId = ""0""
          val url = infos(2).split("" "")(1)
          if (url.startsWith(""/class"")) {
            val courceHTML = url.split(""/"")(2)
            courseId = courceHTML.substring(0, courceHTML.lastIndexOf("".""))
          }
          CheckLog(ip, time, courseId, infos(3), infos(4))
        }
"
"udf/spark_repos_0/3_luochana_spark_Logs/..spark_project.src.main.scala.Streaming.kafkaConsumer.scala/udf/39.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_0/3_luochana_spark_Logs/..spark_project.src.main.scala.Streaming.kafkaConsumer.scala/udf/41.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

lines => {
        val infos = lines.split(""\t"")
        val ip = infos(0)
        val time = DateUtils.parseTime(infos(1))
        var courseId = ""0""
        val url = infos(2)
        CheckLog(ip, time, courseId, infos(3), infos(4))
      }
"
"udf/spark_repos_0/3_madhutv_spark-simple-datatransform/..src.main.scala.org.singaj.simpletrans.SimpleTransformer.scala/udf/62.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

cond
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.dataset.Flight.scala/udf/27.22.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[dataset.Flight.Flight]
Call: filter

flight => flight.crsdephour == 10
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.dataset.Flight.scala/udf/34.22.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[dataset.Flight.Flight]
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.dataset.Flight.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.dataset.Flight.scala/udf/49.22.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[dataset.Flight.Flight]
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.dataset.Flight.scala/udf/57.22.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[dataset.Flight.Flight]
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/104.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/106.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/111.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/113.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/34.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/90.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/92.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/97.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.machinelearning.Flight.scala/udf/99.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""pred_rf"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/64.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""pred_rf""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/71.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/73.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""pred_rf"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/78.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/80.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""pred_rf"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/85.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/87.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""pred_rf""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/92.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.parquet.QueryFlight.scala/udf/94.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""pred_rf""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/64.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""pred_rf"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""pred_rf""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/75.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/77.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""pred_rf"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/82.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/84.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""pred_rf"")
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/89.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 0.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/91.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""pred_rf""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/96.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_rf"" === 1.0d
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-flightdelay.src.main.scala.sparkmaprdb.QueryFlight.scala/udf/98.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""pred_rf""))
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-structuredstreaming.src.main.scala.sparkmaprdb.QueryUber.scala/udf/34.22.Dataset-UberwId.filter","Type: org.apache.spark.sql.Dataset[sparkmaprdb.QueryUber.UberwId]
Call: filter

$""_id"" <= ""1""
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-structuredstreaming.src.main.scala.streaming.StructuredStreamingConsumer.scala/udf/57.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(message: String) => parseUber(message)
"
"udf/spark_repos_0/3_mapr-demos_mapr-spark2-ebook/..mapr-spark-structuredstreaming.src.main.scala.streaming.StructuredStreamingConsumer.scala/udf/68.19.Dataset-UberC.map","Type: org.apache.spark.sql.Dataset[streaming.StructuredStreamingConsumer.UberC]
Call: map

uber => createUberwId(uber)
"
"udf/spark_repos_0/3_udemy_spark-statistics/..src.main.scala.com.udemy.statistics.spark.algorithm.Sampling.scala/udf/59.28.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

_ != mostFrequentValue.value
"
"udf/spark_repos_0/43_zaleslaw_Spark-Tutorial/..MLlib.src.main.scala.jbreak.define_classes.jb_2_Classified_with_Decision_Trees.scala/udf/38.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

lambdaCheckClasses
"
"udf/spark_repos_0/43_zaleslaw_Spark-Tutorial/..MLlib.src.main.scala.jbreak.eatable.jb_4_Classified_with_SVM.scala/udf/30.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

lambdaCheckClasses
"
"udf/spark_repos_0/43_zaleslaw_Spark-Tutorial/..MLlib.src.main.scala.knn.Wrong_Animals_Classified_with_kNN.scala/udf/38.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

lambdaCheckClasses
"
"udf/spark_repos_0/43_zaleslaw_Spark-Tutorial/..Spark-Core.src.main.scala.DataFrames.Ex_4_Datasets.scala/udf/16.21.Dataset-stateNamesRow.map","Type: org.apache.spark.sql.Dataset[DataFrames.Ex_4_Datasets.stateNamesRow]
Call: map

x => (x.name, x.state, x.count)
"
"udf/spark_repos_0/43_zaleslaw_Spark-Tutorial/..Spark-Core.src.main.scala.DataFrames.Ex_4_Datasets.scala/udf/18.22.Dataset-(String, String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, String, Int)]
Call: filter

e => e._2 == ""NY""
"
"udf/spark_repos_0/43_zaleslaw_Spark-Tutorial/..Spark-Core.src.main.scala.DataFrames.Ex_6_UDF.scala/udf/13.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

lambdaIsWorldWarTwoYear
"
"udf/spark_repos_0/43_zaleslaw_Spark-Tutorial/..Spark-Core.src.main.scala.DataFrames.Ex_7_UDAF.scala/udf/15.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new LongestWord
"
"udf/spark_repos_0/4_andfanilo_spark-project-template/..src.main.scala.com.github.andfanilo.template.apps.DatasetApp.scala/udf/11.19.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[com.github.andfanilo.template.apps.Person]
Call: map

p => (p.age * 2, p.name)
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.geosparksql.UDF.UdfRegistrator.scala/udf/16.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

f
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.SamplingCube.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => f.getAs[SimplePoint](0).toString
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.SamplingIcebergCube.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => f.getAs[SimplePoint](0).toString
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.TabulaNoSamS.scala/udf/32.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(f).isNotNull
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.TabulaNoSamS.scala/udf/45.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(f).isNull
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.TabulaNoSamS.scala/udf/67.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => f.getAs[SimplePoint](0).toString
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.Tabula.scala/udf/122.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""${cubeLocalMeasureName(0)} > $icebergThreshold""
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.Tabula.scala/udf/36.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(f).isNotNull
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.Tabula.scala/udf/49.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(f).isNull
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.cubes.Tabula.scala/udf/80.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => f.getAs[SimplePoint](0).toString
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.utils.CommonFunctions.scala/udf/56.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(queryAttributes(i)).isNull
"
"udf/spark_repos_0/4_DataSystemsLab_Tabula/..src.main.scala.org.datasyslab.samplingcube.utils.CommonFunctions.scala/udf/60.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(queryAttributes(i)).===(attributeValues(i))
"
"udf/spark_repos_0/51_swoop-inc_spark-records/..src.main.scala.com.swoop.spark.records.package.scala/udf/28.22.Dataset-Rec.filter","Type: org.apache.spark.sql.Dataset[Rec]
Call: filter

env.dataFilter
"
"udf/spark_repos_0/51_swoop-inc_spark-records/..src.main.scala.com.swoop.spark.records.package.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

env.dataFilter
"
"udf/spark_repos_0/51_swoop-inc_spark-records/..src.main.scala.com.swoop.spark.records.package.scala/udf/9.22.Dataset-Rec.filter","Type: org.apache.spark.sql.Dataset[Rec]
Call: filter

_.issues.isDefined
"
"udf/spark_repos_0/51_swoop-inc_spark-records/..src.main.scala.com.swoop.spark.records.RootCauseAnalysis.scala/udf/60.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Issue.idMessageFor _
"
"udf/spark_repos_0/5_codingdebugallday_spark-explore/..src.main.scala.org.abigballofmud.structured.app.SyncApp.scala/udf/124.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""event.payload.op"".===(""c"") || $""event.payload.op"".===(""u"")
"
"udf/spark_repos_0/5_codingdebugallday_spark-explore/..src.main.scala.org.abigballofmud.structured.app.SyncApp.scala/udf/128.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""event.payload.op"".===(""d"")
"
"udf/spark_repos_0/5_codingdebugallday_spark-explore/..src.main.scala.org.abigballofmud.structured.demo.RealTimeSyncApp.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""event.payload.op"".===(""c"")
"
"udf/spark_repos_0/5_codingdebugallday_spark-explore/..src.main.scala.org.abigballofmud.structured.sink.JdbcSinkDemo.scala/udf/74.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""event.payload.op"".===(""c"") || $""event.payload.op"".===(""u"")
"
"udf/spark_repos_0/5_codingdebugallday_spark-explore/..src.main.scala.org.abigballofmud.structured.sink.JdbcSinkDemo.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""event.payload.op"".===(""d"")
"
"udf/spark_repos_0/5_codingdebugallday_spark-explore/..src.main.scala.org.abigballofmud.structured.source.FileSource.scala/udf/16.22.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[org.abigballofmud.structured.source.User]
Call: filter

_.age > 18
"
"udf/spark_repos_0/5_codingdebugallday_spark-explore/..src.main.scala.org.abigballofmud.structured.window.WindowsDemo2.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val split: Array[String] = line.split("","")
        (split(0), split(1))
      }
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.ExploreModel.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.ExploreModel.scala/udf/21.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          case row =>
            (row.getInt(0), locally {
              val _t_m_p_4 = row.getSeq[Float](1)
              _t_m_p_4.map(_.toDouble)
            }.toArray)
        }
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.ExploreModel.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case row =>
          (row.getInt(0), locally {
            val _t_m_p_6 = row.getSeq[Float](1)
            _t_m_p_6.map(_.toDouble)
          }.toArray)
      }
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.ExploreModel.scala/udf/54.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getInt(1)
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.MoviesALSBatch.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.MoviesALSBatch.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"".notEqual(Double.NaN)
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.MoviesALSBatch.scala/udf/71.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          case row =>
            Math.pow(row.getFloat(2) - meanRating, 2)
        }
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.MoviesALS.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch07.MoviesALS.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"".notEqual(Double.NaN)
"
"udf/spark_repos_0/5_datadance_book2/..rocket.src.main.scala.com.koala.ch11.GenTrainingData.scala/udf/49.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ x => 
        val label = x(0)
        val terms = locally {
          val _t_m_p_10 = locally {
            val _t_m_p_11 = x(1).toString.replace(""["", """").replace(""]"", """").split('|')
            _t_m_p_11.map(v => (indexes.get(v).getOrElse(-1) + 1, 1))
          }.sortBy(_._1)
          _t_m_p_10.map(x => x._1 + "":"" + x._2)
        }.mkString("" "")
        label.toString + "" "" + terms
      }
"
"udf/spark_repos_0/5_DataSystemsGroupUT_Distributed-SmartML/..DSmartML.src.main.scala.org.apache.spark.ml.classification.LDA.scala/udf/57.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$(labelCol) + ""== "" + label
"
"udf/spark_repos_0/5_DataSystemsGroupUT_Distributed-SmartML/..DSmartML.src.main.scala.org.apache.spark.ml.classification.QDA.scala/udf/55.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$(labelCol) + ""== "" + label
"
"udf/spark_repos_0/5_DataSystemsGroupUT_Distributed-SmartML/..DSmartML.src.main.scala.org.dsmartml.MetadataManager.scala/udf/276.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

cond
"
"udf/spark_repos_0/5_DataSystemsGroupUT_Distributed-SmartML/..DSmartML.src.main.scala.org.dsmartml.MetadataManager.scala/udf/723.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(c) < 0
"
"udf/spark_repos_0/5_DataSystemsGroupUT_Distributed-SmartML/..DSmartML.src.main.scala.org.dsmartml.MetadataManager.scala/udf/84.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

cond
"
"udf/spark_repos_0/5_devmindset_sparksql/..DataAnalysis911.scala/udf/49.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x(0) + "" -->"" + x(2).toString
"
"udf/spark_repos_0/5_devmindset_sparksql/..DataAnalysis911.scala/udf/55.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_0/5_devmindset_sparksql/..OrderItemAna2.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => Order(row.getAs[String](0).toInt, row.getAs[String](1), row.getAs[String](3))
      }
"
"udf/spark_repos_0/5_devmindset_sparksql/..OrderItemAna2.scala/udf/28.22.Dataset-Order.filter","Type: org.apache.spark.sql.Dataset[com.deb.spark.sql.OrderItemAna2.Order]
Call: filter

_.order_status == ""COMPLETE""
"
"udf/spark_repos_0/5_devmindset_sparksql/..OrderItemAna2.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => OrderDetails(row.getAs[String](1).toInt, row.getAs[String](4).toFloat)
      }
"
"udf/spark_repos_0/5_devmindset_sparksql/..OrderItemAnalysis.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""order_status"" === ""COMPLETE""
"
"udf/spark_repos_0/5_devmindset_sparksql/..SensorDataAnalysis.scala/udf/44.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (new Integer(x(7).toString), x(8).toString)
"
"udf/spark_repos_0/5_devmindset_sparksql/..SensorDataAnalysis.scala/udf/48.22.Dataset-(Integer, String).filter","Type: org.apache.spark.sql.Dataset[(Integer, String)]
Call: filter

x => if (x._1 == 1) true else false
"
"udf/spark_repos_0/5_devmindset_sparksql/..SensorDataAnalysis.scala/udf/53.22.Dataset-(Integer, String).map","Type: org.apache.spark.sql.Dataset[(Integer, String)]
Call: map

x => (x._2, 1)
"
"udf/spark_repos_0/5_devmindset_sparksql/..TransactionDataAnalysis.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => Transaction(row.getAs[Long](0), row.getAs[Long](1).toInt, row.getAs[Long](2))
      }
"
"udf/spark_repos_0/5_devmindset_sparksql/..TransactionDataAnalysis.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Percentage"") <= 0.3d
"
"udf/spark_repos_0/5_jsnowacki_spark-nlp-tools/..src.main.scala.com.sigdelta.spark.nlp.tools.LanguageDetector.scala/udf/30.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(text: String) => detect(text)
"
"udf/spark_repos_0/5_lanchiang_data-knoller/..data-prep.dataprep-model.src.main.scala.de.hpi.isg.dataprep.ConversionHelper.scala/udf/116.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => indexArray.contains(row)
"
"udf/spark_repos_0/5_lanchiang_data-knoller/..data-prep.dataprep-model.src.main.scala.de.hpi.isg.dataprep.ConversionHelper.scala/udf/48.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => resultArray.contains(row)
"
"udf/spark_repos_0/5_lanchiang_data-knoller/..data-prep.dataprep-model.src.main.scala.de.hpi.isg.dataprep.ConversionHelper.scala/udf/98.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => indexArray.contains(row)
"
"udf/spark_repos_0/5_lanchiang_data-knoller/..data-prep.dataprep-simple.src.test.scala.de.hpi.isg.dataprep.experiment.SuggestableRemovePreambleTest.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => SuggestableRemovePreamble.valueLengthHistogram(row)
"
"udf/spark_repos_0/5_lanchiang_data-knoller/..data-prep.dataprep-simple.src.test.scala.de.hpi.isg.dataprep.experiment.SuggestableRemovePreambleTest.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => SuggestableRemovePreamble.valueLengthHistogram(row)
"
"udf/spark_repos_0/5_lanchiang_data-knoller/..data-prep.dataprep-simple.src.test.scala.de.hpi.isg.dataprep.plot.PlotHistogramDifferenceTest.scala/udf/20.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => SuggestableRemovePreamble.valueLengthHistogram(row)
"
"udf/spark_repos_0/61_radanalyticsio_silex/..src.main.scala.io.radanalytics.silex.app.app.scala/udf/51.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

fb
"
"udf/spark_repos_0/61_radanalyticsio_silex/..src.main.scala.io.radanalytics.silex.frame.labeledPoint.scala/udf/49.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double) => label
      }
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.Engine.scala/udf/42.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

reposDf(""id"").isin(repositoryIds: _*)
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.Engine.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

refsDf(""name"").isin(referenceNames: _*)
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.Engine.scala/udf/56.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

commitsDf(""hash"").isin(commitHashes: _*)
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.package.scala/udf/102.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

($""name"" === HeadRef).or($""name"" === LocalHeadRef)
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.package.scala/udf/113.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""reference_name"" === name
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.package.scala/udf/118.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""name"" === name
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.package.scala/udf/28.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

customUDF(session)
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.package.scala/udf/54.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""is_remote"") === true
"
"udf/spark_repos_0/67_src-d_jgit-spark-connector/..src.main.scala.tech.sourced.engine.package.scala/udf/97.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

($""reference_name"" === HeadRef).or($""reference_name"" === LocalHeadRef)
"
"udf/spark_repos_0/6_bpn1_ingestion/..src.main.scala.de.hpi.ingestion.textmining.re.RelationClassifier.scala/udf/150.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

data(""label"") === 0
"
"udf/spark_repos_0/6_dimajix_flowman/..flowman-core.src.main.scala.com.dimajix.flowman.model.Relation.scala/udf/117.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(partitionName).isin(values: _*)
"
"udf/spark_repos_0/6_dimajix_flowman/..flowman-spec.src.main.scala.com.dimajix.flowman.spec.mapping.FilterMapping.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_0/6_dimajix_flowman/..flowman-spec.src.main.scala.com.dimajix.flowman.spec.mapping.RankMapping.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""flowman_gen_rank"") === 1
"
"udf/spark_repos_0/6_dimajix_flowman/..flowman-spec.src.main.scala.com.dimajix.flowman.spec.mapping.TransitiveChildrenMapping.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

locally {
        val _t_m_p_10 = locally {
          val _t_m_p_11 = allColumns
          _t_m_p_11.map(_.isNotNull)
        }
        _t_m_p_10.reduce(_ && _)
      }
"
"udf/spark_repos_0/6_dimajix_flowman/..flowman-spec.src.main.scala.com.dimajix.flowman.spec.mapping.TransitiveChildrenMapping.scala/udf/73.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

locally {
          val _t_m_p_13 = locally {
            val _t_m_p_14 = parentColumns
            _t_m_p_14.map(c => col(c).isNotNull)
          }
          _t_m_p_13.reduce(_ && _)
        }
"
"udf/spark_repos_0/6_dimajix_flowman/..flowman-spec.src.main.scala.com.dimajix.flowman.spec.mapping.TransitiveChildrenMapping.scala/udf/89.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

locally {
          val _t_m_p_18 = locally {
            val _t_m_p_19 = childColumns
            _t_m_p_19.map(c => col(c).isNotNull)
          }
          _t_m_p_18.reduce(_ && _)
        }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryAnalysis.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), Map(1 -> row.getDouble(13)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryAnalysis.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), Map(2 -> row.getDouble(8)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryAnalysis.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), Map(3 -> row.getDouble(16)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryAnalysis.scala/udf/40.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), Map(4 -> row.getDouble(13)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryBLYhdBrandsContrast.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), Seq(row.getString(3)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryBLYhdBrandsContrast.scala/udf/17.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(2), row.getString(5))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryBLYhdBrandsContrast.scala/udf/32.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getInt(0), if (row.isNullAt(12) || row.get(12).toString.equalsIgnoreCase(""null"")) null else row.getString(12))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryGoodsDist.scala/udf/108.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val category_sid = row.getInt(0)
        val range_no = row.getInt(1)
        val p_type = row.getInt(4)
        val sale_sum = row.getInt(5)
        ((category_sid, p_type), (range_no, sale_sum))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryGoodsDist.scala/udf/61.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val avg_epc = row.getDouble(2)
        val avg_ssr = row.getDouble(3)
        var avg_epc_score = 0.0d
        if (avg_epc < 0.2d || avg_epc > 0.8d) {
          avg_epc_score = 0.2d * bc_epc_weight.value
        } else if (0.4d < avg_epc && avg_epc <= 0.8d) {
          avg_epc_score = 0.5d * bc_epc_weight.value
        } else if (0.2d <= avg_epc && avg_epc <= 0.4d) {
          avg_epc_score = 1 * bc_epc_weight.value
        }
        var avg_ssr_score = 0.0d
        if (avg_ssr < 0.2d || avg_ssr > 0.8d) {
          avg_ssr_score = 0.2d * bc_ssr_weight.value
        } else if (0.2d <= avg_ssr && avg_ssr <= 0.4d || 0.6d <= avg_ssr && avg_ssr <= 0.8d) {
          avg_ssr_score = 0.5d * bc_ssr_weight.value
        } else if (0.4d < avg_ssr && avg_ssr < 0.6d) {
          avg_ssr_score = 1 * bc_ssr_weight.value
        }
        (row.getInt(0), (row.getString(1), row.getDouble(2), avg_epc_score, row.getDouble(3), avg_ssr_score))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryGoodsDist.scala/udf/98.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val category_sid = row.getInt(0)
        val range_no = row.getInt(1)
        val p_type = row.getInt(4)
        val sale_sum = row.getInt(5)
        ((category_sid, p_type), sale_sum)
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryMatch.scala/udf/20.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(1), row.getString(2))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryOperation.scala/udf/47.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val level = row.getInt(2)
        var stock_sku_rate = 0.0d
        if (!row.get(3).isInstanceOf[Double]) {
          stock_sku_rate = 0.0d
        } else {
          stock_sku_rate = row.getDouble(3)
        }
        var ave_prop_fill_rate = 0.0d
        if (!row.get(4).isInstanceOf[Double]) {
          ave_prop_fill_rate = 0.0d
        } else {
          ave_prop_fill_rate = row.getDouble(4)
        }
        var shelf_sale_ratio = 0.0d
        if (!row.get(7).isInstanceOf[Double]) {
          shelf_sale_ratio = 0.0d
        } else {
          shelf_sale_ratio = row.getDouble(7)
        }
        var ave_price_adjustment_time = 0.0d
        if (!row.get(5).isInstanceOf[Double]) {
          ave_price_adjustment_time = 0.0d
        } else {
          ave_price_adjustment_time = row.getDouble(5)
        }
        var ave_on_off_shelf_time = 0.0d
        if (!row.get(6).isInstanceOf[Double]) {
          ave_on_off_shelf_time = 0.0d
        } else {
          ave_on_off_shelf_time = row.getDouble(6)
        }
        (level, Seq((ave_price_adjustment_time, ave_on_off_shelf_time, stock_sku_rate, ave_prop_fill_rate, shelf_sale_ratio)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryOperation.scala/udf/84.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val category_sid = row.getInt(0)
        val category_name = row.getString(1)
        val level = row.getInt(2)
        var stock_sku_rate = 0.0d
        if (!row.get(3).isInstanceOf[Double]) {
          stock_sku_rate = 0.0d
        } else {
          stock_sku_rate = row.getDouble(3)
        }
        var ave_prop_fill_rate = 0.0d
        if (!row.get(4).isInstanceOf[Double]) {
          ave_prop_fill_rate = 0.0d
        } else {
          ave_prop_fill_rate = row.getDouble(4)
        }
        var shelf_sale_ratio = 0.0d
        if (!row.get(7).isInstanceOf[Double]) {
          shelf_sale_ratio = 0.0d
        } else {
          shelf_sale_ratio = row.getDouble(7)
        }
        var ave_price_adjustment_time = 0.0d
        if (!row.get(5).isInstanceOf[Double]) {
          ave_price_adjustment_time = 0.0d
        } else {
          ave_price_adjustment_time = row.getDouble(5)
        }
        var ave_on_off_shelf_time = 0.0d
        if (!row.get(6).isInstanceOf[Double]) {
          ave_on_off_shelf_time = 0.0d
        } else {
          ave_on_off_shelf_time = row.getDouble(6)
        }
        (level, (category_sid, category_name, level, stock_sku_rate, ave_prop_fill_rate, shelf_sale_ratio, ave_price_adjustment_time, ave_on_off_shelf_time))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryPromotion.scala/udf/23.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(1), (row.getString(0), row.getString(2), row.getString(3)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryPromotion.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getLong(1), row.getString(0))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryYhdItemMatch.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val comment = row.getInt(5)
        val cateUrl = row.getString(1)
        (cateUrl, Seq((comment, row)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryYhdItemMatch.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getString(1))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryYhdPrice.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val cateUrl = row.getString(1)
        (cateUrl, row)
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.category.CategoryYhdPrice.scala/udf/26.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val yhdCateUrl = row.getString(7)
        (yhdCateUrl, row)
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.OrderAndStoreAnalysis3.scala/udf/203.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.toSeq
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.OrderAndStoreAnalysis4.scala/udf/99.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""storename"" === ""世纪联华中环百联店"" || $""storename"" === ""华联超市重庆北店""
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.OrderAndStoreAnalysis.scala/udf/175.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.anyNull) null else (row.getLong(0).toString, row.getDouble(1), row.getString(2))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.OrderAndStoreAnalysis.scala/udf/181.23.Dataset-(String, Double, String).filter","Type: org.apache.spark.sql.Dataset[(String, Double, String)]
Call: filter

_ != null
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.OrderAndStoreAnalysis.scala/udf/185.20.Dataset-(String, Double, String).map","Type: org.apache.spark.sql.Dataset[(String, Double, String)]
Call: map

s => ((s._1, s._3), Seq(s._2))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/113.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
            r => (if (r.isNullAt(0) || r.get(0) == null || r.get(0).toString.equalsIgnoreCase(""null"")) -1 else r.getLong(0).toInt, r.getString(1), r.getLong(2).toInt)
          }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/152.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0)) -1 else r.getLong(0), (if (r.isNullAt(1)) -1 else r.getLong(1), r.getString(2), if (r.isNullAt(3)) -1 else r.getLong(3), r.getString(4), if (r.isNullAt(5)) -1 else r.getLong(5), r.getString(6), if (r.isNullAt(7)) -1 else r.getLong(7), r.getString(8), if (r.isNullAt(9)) -1 else r.getLong(9), r.getString(10)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/159.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(2) || r.get(2) == null || r.get(2).toString.equalsIgnoreCase(""null"")) -99 else r.getLong(2), (r.getString(0), r.getString(1), if (r.isNullAt(3) || r.get(3).toString.equalsIgnoreCase(""null"")) 0 else r.getDouble(3).toInt))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0)) -1 else r.getLong(0), (if (r.isNullAt(1)) -1 else r.getLong(1), r.getString(2), if (r.isNullAt(3)) -1 else r.getLong(3), r.getString(4), if (r.isNullAt(5)) -1 else r.getLong(5), r.getString(6), if (r.isNullAt(7)) -1 else r.getLong(7), r.getString(8), if (r.isNullAt(9)) -1 else r.getLong(9), r.getString(10)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/214.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0) || r.get(0) == ""null"") -99L else r.getLong(0), (r.getString(1), r.getDouble(2)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (r.getString(0), (if (r.isNullAt(1)) 0 else r.getLong(1), if (r.isNullAt(2)) 0 else r.getLong(2), if (r.isNullAt(3)) 0 else r.getLong(3)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/302.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0)) -1 else r.getLong(0), (if (r.isNullAt(1)) -1 else r.getLong(1), r.getString(2), if (r.isNullAt(3)) -1 else r.getLong(3), r.getString(4), if (r.isNullAt(5)) -1 else r.getLong(5), r.getString(6), if (r.isNullAt(7)) -1 else r.getLong(7), r.getString(8), if (r.isNullAt(9)) -1 else r.getLong(9), r.getString(10)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/311.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
            r => (if (r.isNullAt(0) | r.get(0) == ""null"") -1 else r.getLong(0), r.getString(1), r.getInt(2), r.getInt(3))
          }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0) || r.get(0).toString.equalsIgnoreCase(""null"")) -99L else r.getString(0).toLong, (if (r.isNullAt(1)) 0 else r.getLong(1), if (r.isNullAt(2)) 0 else r.getLong(2), r.getString(3), if (r.isNullAt(4)) 0 else r.getLong(4)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/370.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.get(0) == ""null"" | r.isNullAt(0)) -99 else r.getString(0).toLong, if (r.isNullAt(1) | r.get(1) == ""null"") 0.0d else r.getDouble(1))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/423.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0)) -99 else r.getLong(0), if (r.isNullAt(1)) 0 else r.getLong(1))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/472.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0)) -99 else r.getLong(0), if (r.isNullAt(1)) 0 else r.getLong(1))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/525.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
            r => ((r.getInt(0), r.getInt(1)), Seq(r.getDouble(2)))
          }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/587.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
              r => (if (r.isNullAt(0)) -1 else r.getLong(0).toInt, r.getString(1), if (r.isNullAt(2)) -1 else r.getLong(2).toInt, r.getString(3), if (r.isNullAt(4)) -1 else r.getLong(4).toInt, r.getString(5), if (r.isNullAt(6)) -1 else r.getLong(6).toInt, r.getString(7), if (r.isNullAt(8)) -1 else r.getLong(8).toInt, r.getString(9), r.getString(10), if (r.isNullAt(11)) 0 else r.getDouble(11), r.getString(12), r.getString(13), if (r.isNullAt(14)) 0 else r.getDouble(14).toInt, r.getString(15))
            }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/659.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0)) -1 else r.getLong(0), (if (r.isNullAt(1)) -1 else r.getLong(1), r.getString(2), if (r.isNullAt(3)) -1 else r.getLong(3), r.getString(4), if (r.isNullAt(5)) -1 else r.getLong(5), r.getString(6), if (r.isNullAt(7)) -1 else r.getLong(7), r.getString(8), if (r.isNullAt(9)) -1 else r.getLong(9), r.getString(10)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/668.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          r => (if (r.isNullAt(0) || r.get(2).toString.equalsIgnoreCase(""null"")) -99 else r.getLong(0), (r.getString(1), if (r.isNullAt(2) || r.get(2).toString.equalsIgnoreCase(""null"")) 0.0d else r.getDouble(2), if (r.isNullAt(3) || r.get(3).toString.equalsIgnoreCase(""null"")) 0.0d else r.getDouble(3), if (r.isNullAt(4) || r.get(4).toString.equalsIgnoreCase(""null"")) null else r.getString(4)))
        }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/737.28.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
                r => (if (r.isNullAt(0)) -1 else r.getLong(0).toInt, r.getString(1), if (r.isNullAt(2)) -1 else r.getLong(2).toInt, r.getString(3), if (r.isNullAt(4)) -1 else r.getLong(4).toInt, r.getString(5), if (r.isNullAt(6)) -1 else r.getLong(6).toInt, r.getString(7), if (r.isNullAt(8)) -1 else r.getLong(8).toInt, r.getString(9), r.getString(10), r.getString(11), r.getString(12), r.getString(13), if (r.isNullAt(14)) 0.0d else r.getDouble(14))
              }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/834.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (r.getString(0), r.getString(1), r.getString(2), if (r.isNullAt(3)) -1 else r.getInt(3), if (r.isNullAt(4)) -1 else r.getInt(4), if (r.isNullAt(5)) -1 else r.getLong(5).toInt, r.getString(6), if (r.isNullAt(7)) -1 else r.getLong(7).toInt, r.getString(8), if (r.isNullAt(9)) -1 else r.getLong(9).toInt, r.getString(10), if (r.isNullAt(11)) -1 else r.getLong(11).toInt, r.getString(12), if (r.isNullAt(13)) -1 else r.getLong(13).toInt, r.getString(14), if (r.isNullAt(15)) 0.0d else r.getDouble(15))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/896.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => (if (r.isNullAt(0)) -1 else r.getLong(0), (if (r.isNullAt(1)) -1 else r.getLong(1), r.getString(2), if (r.isNullAt(3)) -1 else r.getLong(3), r.getString(4), if (r.isNullAt(5)) -1 else r.getLong(5), r.getString(6), if (r.isNullAt(7)) -1 else r.getLong(7), r.getString(8), if (r.isNullAt(9)) -1 else r.getLong(9), r.getString(10)))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.CategoryStatistic.scala/udf/903.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          r => (if (r.isNullAt(0) || r.get(0).toString.equalsIgnoreCase(""null"")) -99 else r.getLong(0), (r.getString(1), r.getString(2), if (r.isNullAt(3) || r.get(3).toString.equalsIgnoreCase(""null"")) 0.0d else r.getDouble(3), if (r.isNullAt(4) || r.get(4).toString.equalsIgnoreCase(""null"")) -1 else r.getInt(4), if (r.isNullAt(5) || r.get(5).toString.equalsIgnoreCase(""null"")) -1 else r.getInt(5)))
        }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/122.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.anyNull) null else (row.getLong(0).toString, row.getDouble(1))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/170.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getLong(0).toString, (row.getString(1), row.getString(2), row.getDouble(3)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/212.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.anyNull) null else (row.getLong(0).toString, (row.getDouble(1), row.getDouble(2), row.getLong(3).toInt))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/231.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.anyNull) null else (row.getLong(0).toString, (row.getDouble(1), row.getDouble(2), row.getLong(3).toInt))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/255.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getLong(0).toString, row.getLong(1).toInt)
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/291.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getLong(0).toString, (row.getLong(1).toInt, row.getLong(2).toInt))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/323.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getLong(0).toString, (row.getLong(1).toInt, row.getLong(2).toInt))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/367.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
              row => if (row.anyNull) null else (row.getLong(0).toString, Seq((row.getString(1), row.getString(2), row.getString(3), row.getDouble(4), row.getString(5))))
            }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/40.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.anyNull) null else (row.getLong(0).toString, (row.getLong(1).toInt, row.getLong(2).toInt, row.getLong(3).toInt))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/444.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getLong(0).toString, (row.getString(1), row.getString(2), row.getDouble(3)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/551.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getString(0), row.getDouble(1), row.getString(2))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalysis2.scala/udf/65.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.isNullAt(0)) null else if (!row.isNullAt(4) && row.getLong(0) != row.getLong(4)) (row.getLong(0).toString, row.getLong(4).toString) else if (!row.isNullAt(3) && row.getLong(0) != row.getLong(3)) (row.getLong(0).toString, row.getLong(3).toString) else if (!row.isNullAt(2) && row.getLong(0) != row.getLong(2)) (row.getLong(0).toString, row.getLong(2).toString) else if (!row.isNullAt(1) && row.getLong(0) != row.getLong(1)) (row.getLong(0).toString, row.getLong(1).toString) else null
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalyze.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.isNullAt(0) | row.isNullAt(1)) null else (row.getLong(0), row.getString(1), if (row.isNullAt(2)) null else row.getLong(2), if (row.isNullAt(3)) null else row.getLong(3), if (row.isNullAt(4)) null else row.getLong(4))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalyze.scala/udf/208.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.anyNull) null else (row.getString(0), row.getString(1), row.getString(2), row.getString(3), row.getString(4))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalyze.scala/udf/345.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getString(0), row.getString(1), row.getString(2))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalyze.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.anyNull) null else (row.getLong(0), row.getString(1), row.getString(2), row.getInt(3), row.getDouble(4))
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalyze.scala/udf/407.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getString(2), (row.getString(0), row.getString(1), row.getDouble(3)))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalyze.scala/udf/438.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => if (row.anyNull) null else (row.getLong(0).toString, row.getDouble(1), row.getDouble(2).toInt, row.getLong(3).toInt, row.getLong(4).toInt)
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalyze.scala/udf/457.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getLong(0).toString, (row.getDouble(1), row.getDouble(2).toInt))
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.product.GoodsAnalyze.scala/udf/489.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => if (row.anyNull) null else (row.getLong(0).toString, row.getString(1), row.getString(2), row.getDouble(3), row.getDouble(4).toInt)
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.recommend.OrderStoreLngLatToHBase.scala/udf/26.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
            row => (row.getString(0), if (row.isNullAt(1) || row.get(1).toString.equalsIgnoreCase(""null"")) -1.0d else row.getDouble(1), if (row.isNullAt(2) || row.get(2).toString.equalsIgnoreCase(""null"")) -1.0d else row.getDouble(2), row.getDouble(3), row.getDouble(4), row.getString(5), row.getString(6), row.getString(7), row.getString(8), row.getString(9), row.getString(10), row.getString(11), row.getString(12), row.getString(13))
          }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.recommend.StoreZoneAnalysis.scala/udf/35.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
              row => (row.getString(0), if (row.isNullAt(1) || row.get(1).toString.equalsIgnoreCase(""null"")) -1.0d else row.getDouble(1), if (row.isNullAt(2) || row.get(2).toString.equalsIgnoreCase(""null"")) -1.0d else row.getDouble(2), row.getDouble(3), row.getDouble(4), row.getString(5), row.getString(6), row.getString(7), row.getString(8), row.getString(9), row.getString(10), row.getString(11), row.getString(12), row.getString(13))
            }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.search.ExportOrderDataToHbase.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val featureBytes = Bytes.toBytes(""feature"")
        val length = row.length
        val orderNo = row.getString(16)
        val goodsCode = row.getString(20)
        val put = new Put(Bytes.toBytes(orderNo + ""-"" + goodsCode))
        for (i <- 0 until length) {
          if (!row.isNullAt(i)) put.addColumn(featureBytes, Bytes.toBytes(columnArray(i)), Bytes.toBytes(row.get(i).toString))
        }
        (new ImmutableBytesWritable(Bytes.toBytes("""")), put)
      }
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.similarity.MerchantShopGoods.scala/udf/34.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

($""sid"" !== ""null"") && ($""store_sid"" !== ""null"")
"
"udf/spark_repos_0/6_lastbus_recommend/..src.main.scala.com.bl.bigdata.util.DataBaseUtil.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => locally {
        val _t_m_p_2 = row.toSeq
        _t_m_p_2.map(r => if (r == null) null else r.toString)
      }.toArray
"
"udf/spark_repos_0/6_tashoyan_telecom-streaming/..predictor-spark.src.main.scala.com.github.tashoyan.telecom.predictor.SparkSocketWindowWordCount.scala/udf/18.22.Dataset-Timestamp).filter","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: filter

_._1.nonEmpty
"
"udf/spark_repos_0/6_tashoyan_telecom-streaming/..predictor-spark.src.main.scala.com.github.tashoyan.telecom.predictor.SparkSocketWindowWordCount.scala/udf/33.21.Dataset-WindowWordCount.map","Type: org.apache.spark.sql.Dataset[com.github.tashoyan.telecom.predictor.SparkSocketWindowWordCount.WindowWordCount]
Call: map

{
          _.copy(emitTimestamp = new Timestamp(System.currentTimeMillis()))
        }
"
"udf/spark_repos_0/6_tashoyan_telecom-streaming/..predictor-spark.src.main.scala.com.github.tashoyan.telecom.predictor.SparkSocketWindowWordCount.scala/udf/37.19.Dataset-WindowWordCount.map","Type: org.apache.spark.sql.Dataset[com.github.tashoyan.telecom.predictor.SparkSocketWindowWordCount.WindowWordCount]
Call: map

_.toString
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.ImplicitEncodingTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.ImplicitEncodingTest.scala/udf/38.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.ImplicitEncodingTest.scala/udf/40.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""name"".isNotNull
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.ImplicitEncodingTest.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"".isNotNull
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.KryoEncodingTest.scala/udf/28.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.KryoEncodingTest.scala/udf/32.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.KryoEncodingTest.scala/udf/36.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkDFTest.scala/udf/104.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkDFTest.scala/udf/132.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkDFTest.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkDFTest.scala/udf/59.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkDFTest.scala/udf/61.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""name"".isNotNull
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkDFTest.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"".isNotNull
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkDFTest.scala/udf/91.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0) + "", Age: "" + teenager(1)
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkDFTest.scala/udf/97.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"") + "", Age: "" + teenager.getAs[Long](""age"")
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkTestFileTest.scala/udf/104.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkTestFileTest.scala/udf/106.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""name"".isNotNull
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkTestFileTest.scala/udf/112.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"".isNotNull
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkTestFileTest.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkTestFileTest.scala/udf/59.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkTestFileTest.scala/udf/61.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""name"".isNotNull
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkTestFileTest.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"".isNotNull
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/..src.main.scala.com.inbravo.spark.SparkTestFileTest.scala/udf/97.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/...worksheet.src.com.inbravo.spark.RandomSpark.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/...worksheet.src.com.inbravo.spark.RandomSpark.scala/udf/55.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_0/7_inbravo_scala-feature-set/...worksheet.src.com.inbravo.spark.RandomSpark.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_0/7_nestorpersist_dataset-transform/..demo.src.main.scala.com.persist.DstDemo.scala/udf/26.19.Dataset-ABC.map","Type: org.apache.spark.sql.Dataset[com.persist.ABC]
Call: map

abc => CA(abc.b, abc.a * 2 + abc.a)
"
"udf/spark_repos_0/8_mapr-demos_mapr-es-db-spark-payment/..src.main.scala.sparkmaprdb.ETLPayment.scala/udf/41.22.Dataset-Payment.filter","Type: org.apache.spark.sql.Dataset[sparkmaprdb.ETLPayment.Payment]
Call: filter

$""amount"" > 1000
"
"udf/spark_repos_0/8_mapr-demos_mapr-es-db-spark-payment/..src.main.scala.sparkmaprdb.ETLPayment.scala/udf/46.19.Dataset-Payment.map","Type: org.apache.spark.sql.Dataset[sparkmaprdb.ETLPayment.Payment]
Call: map

payment => createPaymentwId(payment)
"
"udf/spark_repos_0/8_mapr-demos_mapr-es-db-spark-payment/..src.main.scala.sparkmaprdb.ETLPayment.scala/udf/52.22.Dataset-PaymentwId.filter","Type: org.apache.spark.sql.Dataset[sparkmaprdb.ETLPayment.PaymentwId]
Call: filter

_.physician_id == ""214250""
"
"udf/spark_repos_0/8_mapr-demos_mapr-es-db-spark-payment/..src.main.scala.sparkmaprdb.QueryPayment.scala/udf/29.22.Dataset-PaymentwId.filter","Type: org.apache.spark.sql.Dataset[sparkmaprdb.QueryPayment.PaymentwId]
Call: filter

$""amount"" > 2000
"
"udf/spark_repos_0/8_mapr-demos_mapr-es-db-spark-payment/..src.main.scala.sparkmaprdb.QueryPayment.scala/udf/38.22.Dataset-PaymentwId.filter","Type: org.apache.spark.sql.Dataset[sparkmaprdb.QueryPayment.PaymentwId]
Call: filter

$""amount"" > 1000
"
"udf/spark_repos_0/8_mapr-demos_mapr-es-db-spark-payment/..src.main.scala.sparkmaprdb.QueryPayment.scala/udf/43.22.Dataset-PaymentwId.filter","Type: org.apache.spark.sql.Dataset[sparkmaprdb.QueryPayment.PaymentwId]
Call: filter

$""_id"".like(""98485%"")
"
"udf/spark_repos_0/8_mapr-demos_mapr-es-db-spark-payment/..src.main.scala.sparkmaprdb.QueryPayment.scala/udf/48.22.Dataset-PaymentwId.filter","Type: org.apache.spark.sql.Dataset[sparkmaprdb.QueryPayment.PaymentwId]
Call: filter

$""_id"".like(""%_02/%"")
"
"udf/spark_repos_0/8_phatak-dev_spark-two-migration/..spark-one.src.main.scala.com.madhukaraphatak.spark.migration.sparkone.DFMapExample.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getDouble(3)
"
"udf/spark_repos_0/8_phatak-dev_spark-two-migration/..spark-two.src.main.scala.com.madhukaraphatak.spark.migration.sparktwo.DFMapExample.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getDouble(3)
"
"udf/spark_repos_0/9_aertslab_GRNBoost/..src.main.scala.org.aertslab.grnboost.GRNBoost.scala/udf/126.22.Dataset-ExpressionByGene.filter","Type: org.apache.spark.sql.Dataset[org.aertslab.grnboost.ExpressionByGene]
Call: filter

e => targets contains e.gene
"
"udf/spark_repos_0/9_aertslab_GRNBoost/..src.main.scala.org.aertslab.grnboost.GRNBoost.scala/udf/138.22.Dataset-Regulation.filter","Type: org.apache.spark.sql.Dataset[org.aertslab.grnboost.Regulation]
Call: filter

$""include"" === 1
"
"udf/spark_repos_0/9_aertslab_GRNBoost/..src.main.scala.org.aertslab.grnboost.GRNBoost.scala/udf/98.24.Dataset-ExpressionByGene.filter","Type: org.apache.spark.sql.Dataset[org.aertslab.grnboost.ExpressionByGene]
Call: filter

e => estimationTargets contains e.gene
"
"udf/spark_repos_0/9_aertslab_GRNBoost/..src.main.scala.org.aertslab.grnboost.util.RankUtils.scala/udf/41.24.Dataset-ExpressionByGene.filter","Type: org.apache.spark.sql.Dataset[org.aertslab.grnboost.ExpressionByGene]
Call: filter

e => tfs.contains(e.gene)
"
"udf/spark_repos_1/10_EDS-APHP_UimaOnSpark/..src.main.scala.fr.aphp.wind.uima.spark.MimicSectionExtractor.scala/udf/22.19.Dataset-(Int, String).map","Type: org.apache.spark.sql.Dataset[(Int, String)]
Call: map

row => tt.analyzeText(row._1, 1, row._2)
"
"udf/spark_repos_1/10_EDS-APHP_UimaOnSpark/..src.main.scala.fr.aphp.wind.uima.spark.SentenceExtract.scala/udf/23.19.Dataset-Text.map","Type: org.apache.spark.sql.Dataset[fr.aphp.wind.uima.spark.SentenceSegmenter.Text]
Call: map

x => tt.analyzeText(x.text).asInstanceOf[String]
"
"udf/spark_repos_1/10_knoldus_structured-streaming-application/..src.main.scala.knolx.spark.MultiStreamHandler.scala/udf/49.17.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

{
      case (deviceId, unit) =>
        (deviceId, CurrentPowerConsumption(Option(unit).fold(0d)(_.toDouble)))
    }
"
"udf/spark_repos_1/10_renxiangnan_strider/..src.main.scala.engine.core.sparkop.op.litematop.LiteMatBGPNode.scala/udf/36.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

liteMatFilters.head._2(inputDF(s""$columnName_0""))
"
"udf/spark_repos_1/10_renxiangnan_strider/..src.main.scala.engine.core.sparkop.op.litematop.LiteMatBGPNode.scala/udf/44.32.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

liteMatFilters.head._2(inputDF(s""$columnName_0""))
"
"udf/spark_repos_1/10_renxiangnan_strider/..src.main.scala.engine.core.sparkop.op.litematop.LiteMatBGPNode.scala/udf/46.30.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

liteMatFilters(1)._2(inputDF(s""$columnName_1""))
"
"udf/spark_repos_1/10_renxiangnan_strider/..src.main.scala.engine.core.sparkop.op.litematop.LiteMatBGPNode.scala/udf/56.34.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

liteMatFilters.head._2(inputDF(s""$columnName_0""))
"
"udf/spark_repos_1/10_renxiangnan_strider/..src.main.scala.engine.core.sparkop.op.litematop.LiteMatBGPNode.scala/udf/58.32.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

liteMatFilters(1)._2(inputDF(s""$columnName_1""))
"
"udf/spark_repos_1/10_renxiangnan_strider/..src.main.scala.engine.core.sparkop.op.litematop.LiteMatBGPNode.scala/udf/60.30.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

liteMatFilters(2)._2(inputDF(s""$columnName_2""))
"
"udf/spark_repos_1/10_renxiangnan_strider/..src.main.scala.engine.core.sparkop.op.SparkFilter.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterUDF(df(s""$columnName""))
"
"udf/spark_repos_1/11_FusionDB_fusiondb/..fql.src.test.scala.org.apache.spark.sql.fdb.service.postgresql.PgDialectSuite.scala/udf/36.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

() => ""test""
"
"udf/spark_repos_1/12_JerryLead_SparkFaultBench/..src.main.scala.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/13_AbsaOSS_hyperdrive/..ingestor-default.src.main.scala.za.co.absa.hyperdrive.ingestor.implementation.writer.parquet.ParquetPartitioningStreamWriter.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(COL_DATE) === lit(reportDate)
"
"udf/spark_repos_1/15_ebiznext_comet-data-pipeline/..src.main.scala.com.ebiznext.comet.job.ingest.IngestionJob.scala/udf/174.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""not ($condition)""
"
"udf/spark_repos_1/15_ebiznext_comet-data-pipeline/..src.main.scala.com.ebiznext.comet.job.ingest.SimpleJsonIngestionJob.scala/udf/31.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""_corrupt_record"".isNotNull
"
"udf/spark_repos_1/15_ebiznext_comet-data-pipeline/..src.main.scala.com.ebiznext.comet.job.metrics.MetricsJob.scala/udf/152.23.Dataset-MetricRecord.map","Type: org.apache.spark.sql.Dataset[com.ebiznext.comet.job.ingest.MetricRecord]
Call: map

converter.toSqlCompatible
"
"udf/spark_repos_1/15_ebiznext_comet-data-pipeline/..src.test.scala.com.ebiznext.comet.job.metrics.MetricsJobSpec.scala/udf/167.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getString(0), r.getString(1), r.getString(2))
"
"udf/spark_repos_1/15_ebiznext_comet-data-pipeline/..src.test.scala.com.ebiznext.comet.job.metrics.MetricsJobSpec.scala/udf/177.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getString(0), r.getString(1), r.getString(2))
"
"udf/spark_repos_1/15_ebiznext_comet-data-pipeline/..src.test.scala.com.ebiznext.comet.job.metrics.MetricsJobSpec.scala/udf/187.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getString(0), r.getString(1), r.getString(2))
"
"udf/spark_repos_1/15_ebiznext_comet-data-pipeline/..src.test.scala.com.ebiznext.comet.udf.TestUdf.scala/udf/8.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

concatWithSpace
"
"udf/spark_repos_1/16_Wasabi1234_Spark-MLlib-Tutorial/..src.cluster.Ida.Main.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val label = row.getString(4) match {
          case ""Iris-setosa"" => 0
          case ""Iris-versicolor"" => 1
          case ""Iris-virginica"" => 2
        }
        (row.getString(0).toDouble, row.getString(1).toDouble, row.getString(2).toDouble, row.getString(3).toDouble, label, random.nextDouble())
      }
"
"udf/spark_repos_1/16_Wasabi1234_Spark-MLlib-Tutorial/..src.cluster.kmeans.Main.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val label = row.getString(4) match {
          case ""Iris-setosa"" => 0
          case ""Iris-versicolor"" => 1
          case ""Iris-virginica"" => 2
        }
        (row.getString(0).toDouble, row.getString(1).toDouble, row.getString(2).toDouble, row.getString(3).toDouble, label, random.nextDouble())
      }
"
"udf/spark_repos_1/16_Wasabi1234_Spark-MLlib-Tutorial/..src.iris.Main.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val label = row.getString(4) match {
          case ""Iris-setosa"" => 0
          case ""Iris-versicolor"" => 1
          case ""Iris-virginica"" => 2
        }
        (row.getString(0).toDouble, row.getString(1).toDouble, row.getString(2).toDouble, row.getString(3).toDouble, label, random.nextDouble())
      }
"
"udf/spark_repos_1/16_Wasabi1234_Spark-MLlib-Tutorial/..src.isotonic.Main.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](0).toDouble, row.getString(1).toDouble, rand.nextDouble())
"
"udf/spark_repos_1/16_Wasabi1234_Spark-MLlib-Tutorial/..src.pca.Main.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val label = row.getString(4) match {
          case ""Iris-setosa"" => 0
          case ""Iris-versicolor"" => 1
          case ""Iris-virginica"" => 2
        }
        (row.getString(0).toDouble, row.getString(1).toDouble, row.getString(2).toDouble, row.getString(3).toDouble, label, random.nextDouble())
      }
"
"udf/spark_repos_1/16_Wasabi1234_Spark-MLlib-Tutorial/..src.rs.Main.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/16_Wasabi1234_Spark-MLlib-Tutorial/..src.sentiment_analysis.Main.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => (locally {
        val _t_m_p_2 = line.split("" "")
        _t_m_p_2.filter(!_.equals("" ""))
      }, 0, rand.nextDouble())
"
"udf/spark_repos_1/16_Wasabi1234_Spark-MLlib-Tutorial/..src.sentiment_analysis.Main.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => (locally {
        val _t_m_p_4 = line.split("" "")
        _t_m_p_4.filter(!_.equals("" ""))
      }, 1, rand.nextDouble())
"
"udf/spark_repos_1/17_EIS-Bonn_Squerall/..src.main.scala.org.squerall.SparkExecutor.scala/udf/132.32.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

whereString
"
"udf/spark_repos_1/17_EIS-Bonn_Squerall/..src.main.scala.org.squerall.SparkExecutor.scala/udf/141.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

finalDF(column).like(operand_value._2.replace(""\"""", """"))
"
"udf/spark_repos_1/17_EIS-Bonn_Squerall/..src.main.scala.org.squerall.SparkExecutor.scala/udf/174.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!ndf(column).equalTo(skipValue)
"
"udf/spark_repos_1/18_hortonworks-spark_cloud-integration/..cloud-examples.src.test.scala.org.apache.spark.sql.sources.AbstractCloudRelationTest.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_1/18_hortonworks-spark_cloud-integration/..cloud-examples.src.test.scala.org.apache.spark.sql.sources.AbstractCloudRelationTest.scala/udf/131.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_1/18_hortonworks-spark_cloud-integration/..cloud-examples.src.test.scala.org.apache.spark.sql.sources.AbstractCloudRelationTest.scala/udf/137.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_1/18_hortonworks-spark_cloud-integration/..cloud-examples.src.test.scala.org.apache.spark.sql.sources.AbstractCloudRelationTest.scala/udf/143.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.centrifuge.sql.centrifuge_sql.scala/udf/195.66.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mergeAnnotations _
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.centrifuge.sql.centrifuge_sql.scala/udf/208.64.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mergeAnnotations _
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.centrifuge.sql.centrifuge_sql.scala/udf/229.62.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

cleanAnnotation _
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.centrifuge.sql.centrifuge_sql.scala/udf/23.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

f
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.centrifuge.sql.Explore.scala/udf/57.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

to_age _
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.centrifuge.sql.Explore.scala/udf/61.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

non_empty_string _
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.centrifuge.sql.Explore.scala/udf/65.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

add_annotations _
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.centrifuge.sql.Explore.scala/udf/69.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

flatten_annotations[Annotation] _
"
"udf/spark_repos_1/18_univalence_centrifuge/..src.main.scala.io.univalence.centrifuge.Executor.scala/udf/70.25.Dataset-M.map","Type: org.apache.spark.sql.Dataset[M]
Call: map

_._2
"
"udf/spark_repos_1/1_aabdelmohsen_LogsIngestion_Spark_Kafka_Cassandra/..src.main.java.com.log.ingestion.LogsIngestion.scala/udf/29.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => line.get(0).toString().split("","")
"
"udf/spark_repos_1/1_abiratsis_IoT-device-stream-kafka/..src.main.scala.com.IoT.Consumer.KafkaToHBaseJob.scala/udf/20.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getString(0)
"
"udf/spark_repos_1/1_AdamF42_BondoraCreditRiskPrevision/..core.src.main.scala.it.unibo.datapreprocessor.DataPreprocessor.scala/udf/111.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Correlation"").between(0.7d, 1)
"
"udf/spark_repos_1/1_AdamF42_BondoraCreditRiskPrevision/..core.src.main.scala.it.unibo.datapreprocessor.DataPreprocessor.scala/udf/154.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(x).isNull || df(x) === """" || df(x).isNaN
"
"udf/spark_repos_1/1_alghimo_spark-pipelines/..src.main.scala.org.alghimo.sparkPipelines.dataManager.CsvFileDataManager.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filter.get
"
"udf/spark_repos_1/1_alghimo_spark-pipelines/..src.main.scala.org.alghimo.sparkPipelines.dataManager.HiveDataManager.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filter.get
"
"udf/spark_repos_1/1_AlpineNow_PluginSDK/..plugin-spark.src.main.scala.com.alpine.plugin.core.spark.reporting.NullDataReportingUtils.scala/udf/25.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

removeCondition
"
"udf/spark_repos_1/1_AlpineNow_PluginSDK/..plugin-spark.src.main.scala.com.alpine.plugin.core.spark.reporting.NullDataReportingUtils.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!removeCondition
"
"udf/spark_repos_1/1_amitsenapati_coursera-big-data-analysis-with-scala-and-spark/..src.main.scala.timeusage.TimeUsage.scala/udf/85.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => TimeUsageRow(r.getAs[String](""working""), r.getAs[String](""sex""), r.getAs[String](""age""), r.getAs[Double](""primaryNeeds""), r.getAs[Double](""work""), r.getAs[Double](""other""))
"
"udf/spark_repos_1/1_amitsenapati_coursera-big-data-analysis-with-scala-and-spark/..src.main.scala.timeusage.TimeUsage.scala/udf/91.19.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((age, sex, working), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, primaryNeeds, work, other)
      }
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1-bin-hadoop2.7.examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.main.scala.org.apache.spark.mllib.util.MLUtils.scala/udf/71.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""same_bucket"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""same_bucket"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/30.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") > distFP
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/34.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") < distFN
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distUDF(col(s""a.$inputCol""), col(s""b.$inputCol"")) < threshold
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/64.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""distCol"") < threshold
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.VectorAssemblerSuite.scala/udf/95.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

vectorUDF($""features"") > 1
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.fpm.FPGrowthSuite.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""freq"") === col(""expectedFreq"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.main.scala.org.apache.spark.sql.execution.command.AnalyzePartitionCommand.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(filter)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/14.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!$""value"".startsWith(options.comment.toString)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

length(trim($""value"")) > 0
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/102.24.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/141.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % 2L === 0L
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/156.27.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

func
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/203.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/218.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/49.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

func
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetSuite.scala/udf/1263.20.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.execution.SQLExecutionSuite.scala/udf/80.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ x => 
        while (!SQLExecutionSuite.canProgress) {
          Thread.sleep(1)
        }
        x
      }
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.FilterPushdownBenchmark.scala/udf/53.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.SessionStateSuite.scala/udf/44.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: String).length + (_: Int)
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.SessionStateSuite.scala/udf/54.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: Int) + 1
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.core.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/87.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/545.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/549.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/577.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/581.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/895.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/904.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.orc.OrcReadBenchmark.scala/udf/125.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.orc.OrcReadBenchmark.scala/udf/244.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.orc.OrcReadBenchmark.scala/udf/65.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.orc.OrcReadBenchmark.scala/udf/95.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_1/1_aydare_datasciencecoursera/..spark-2.3.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_1/1_Bentipe_spark-learning/..src.main.scala.com.learn.Main.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

randomData.col(""country"") === ""Cyprus""
"
"udf/spark_repos_1/1_bezta-company_bigDataDevPfm/..spark.src.main.java.bzt.SparkSQLDemo.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""gov_id"" === ""锦江区""
"
"udf/spark_repos_1/1_big-datai_restsparkserver/..src.main.scala.cassandra.AudienceJobs.scala/udf/30.19.Dataset-urls.map","Type: org.apache.spark.sql.Dataset[cassandra.urls]
Call: map

{
        l => if (l != null && l.url != null && !l.url.isEmpty) {
          urls(l.url.drop(6).dropRight(3).trim, l.fingerprintid)
        } else {
          urls("""", """")
        }
      }
"
"udf/spark_repos_1/1_big-datai_restsparkserver/..src.main.scala.cassandra.AudienceJobs.scala/udf/38.20.Dataset-urls.filter","Type: org.apache.spark.sql.Dataset[cassandra.urls]
Call: filter

l => !l.fingerprintid.isEmpty && !l.url.isEmpty
"
"udf/spark_repos_1/1_BrianSetz_spark-bootcamp/..src.main.scala.nl.rug.sc.SparkExample.scala/udf/32.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Int](""number"")
"
"udf/spark_repos_1/1_BrianSetz_spark-bootcamp/..src.main.scala.nl.rug.sc.SparkExample.scala/udf/34.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/1_BrianSetz_spark-bootcamp/..src.main.scala.nl.rug.sc.SparkExample.scala/udf/47.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/1_BrianSetz_spark-bootcamp/..src.main.scala.nl.rug.sc.SparkExample.scala/udf/62.22.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[nl.rug.sc.Person]
Call: map

person => person.grade
"
"udf/spark_repos_1/1_BrianSetz_spark-bootcamp/..src.main.scala.nl.rug.sc.SparkExample.scala/udf/80.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pop"" < 10000
"
"udf/spark_repos_1/1_cpeixin_sparkmlib/..src.main.scala.Bayes.native_bayes_CN.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: String, features: Vector) =>
          LabeledPoint(label.toDouble, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_1/1_cpeixin_sparkmlib/..src.main.scala.Bayes.native_bayes_CN.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: String, features: Vector) =>
          LabeledPoint(label.toDouble, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_1/1_dachoume_CodesRepository/..src.scala.dao.Write2HBase.scala/udf/15.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => {
          val arr = x.split(""\\|"")
          (arr(0), arr(1), arr(2))
        }
"
"udf/spark_repos_1/1_dachoume_CodesRepository/..src.scala.sparkcore.teach_cou.MultiSort.scala/udf/11.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("" "")
"
"udf/spark_repos_1/1_dachoume_CodesRepository/..src.scala.sparkcore.teach_cou.Teach_Cou.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => {
        val arr = x.split("" "", -1)
        Weight(arr(0), arr(1))
      }
"
"udf/spark_repos_1/1_dachoume_CodesRepository/..src.scala.sparkcore.teach_cou.Test2.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => {
        val arr = x.split("" "", -1)
        Weight(arr(0), arr(1))
      }
"
"udf/spark_repos_1/1_dachoume_CodesRepository/..src.scala.sparkcore.teach_cou.Test2.scala/udf/18.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Cou_UDAF2
"
"udf/spark_repos_1/1_dataengi_spark-data-ingest/..src.main.scala.com.ImportBatch.scala/udf/53.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

remap(meta, _)
"
"udf/spark_repos_1/1_dataengi_spark-data-ingest/..src.main.scala.com.ImportBatch.scala/udf/55.26.Dataset-DSHeader,).filter","Type: org.apache.spark.sql.Dataset[(model.DSHeader,)]
Call: filter

skipNulls _
"
"udf/spark_repos_1/1_dataengi_spark-data-ingest/..src.main.scala.com.ImportBatch.scala/udf/57.21.Dataset-DSHeader,).map","Type: org.apache.spark.sql.Dataset[(model.DSHeader,)]
Call: map

_._1
"
"udf/spark_repos_1/1_dataengi_spark-data-ingest/..src.main.scala.com.ImportStreaming.scala/udf/122.27.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

remap(meta, _)
"
"udf/spark_repos_1/1_dataengi_spark-data-ingest/..src.main.scala.com.ImportStreaming.scala/udf/124.28.Dataset-DSHeader,).filter","Type: org.apache.spark.sql.Dataset[(model.DSHeader,)]
Call: filter

skipNulls _
"
"udf/spark_repos_1/1_dataengi_spark-data-ingest/..src.main.scala.com.ImportStreaming.scala/udf/126.23.Dataset-DSHeader,).map","Type: org.apache.spark.sql.Dataset[(model.DSHeader,)]
Call: map

_._1
"
"udf/spark_repos_1/1_dxb2018_sparksql-train/..src.main.scala.com.xb.bigdata.chapter04.DataFrameAPIApp.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df.col(""state"") === ""CA""
"
"udf/spark_repos_1/1_dxb2018_sparksql-train/..src.main.scala.com.xb.bigdata.chapter04.DatasetApp.scala/udf/12.19.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[com.xb.bigdata.chapter04.DatasetApp.Person]
Call: map

x => x.name
"
"udf/spark_repos_1/1_dxb2018_sparksql-train/..src.main.scala.com.xb.bigdata.chapter04.InteroperatingRDDApp.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => ""Name:"" + x.getAs[String](""name"")
"
"udf/spark_repos_1/1_dxb2018_sparksql-train/..src.main.scala.com.xb.bigdata.chapter05.DataSourceApp.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 30
"
"udf/spark_repos_1/1_dxb2018_sparksql-train/..src.main.scala.com.xb.bigdata.chapter05.DataSourceApp.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 30
"
"udf/spark_repos_1/1_dxb2018_sparksql-train/..src.main.scala.com.xb.bigdata.chapter05.DataSourceApp.scala/udf/66.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val splits = x.getString(0).split("","")
        splits(0).trim
      }
"
"udf/spark_repos_1/1_dxb2018_sparksql-train/..src.main.scala.com.xb.bigdata.chapter06.HiveSourceApp.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" < 30
"
"udf/spark_repos_1/1_dxb2018_sparksql-train/..src.main.scala.com.xb.bigdata.chapter06.UDFFunctionApp.scala/udf/17.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => s.split("","").size
"
"udf/spark_repos_1/1_fanzhongyu_spark-mllib/..src.main.scala.com.timamllib.arithmetic.collaborativefiltering.CollaborativeFiltering.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/1_fanzhongyu_spark-mllib/..src.main.scala.com.timamllib.arithmetic.collaborativefiltering.CollaborativeFilteringTest1.scala/udf/14.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split("","")
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/40.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/92.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_1/1_fharenheit_template-spark-app/..src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/32.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_1/1_football-ml_web/..app.utils.CSVUtil.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_(0) != header(0)
"
"udf/spark_repos_1/1_Gigitsu_Functional-Programming-in-Scala-Capstone/..src.main.scala.observatory.Extraction.scala/udf/19.21.Dataset-StationWithTemperatures.map","Type: org.apache.spark.sql.Dataset[observatory.Extraction.StationWithTemperatures]
Call: map

st => (Date(st.year, st.month, st.day), Location(st.latitude, st.longitude), (st.temperature - 32) / 1.8d)
"
"udf/spark_repos_1/1_guardian_ml-example/..ml.src.main.scala.com.gu.mlExample.Runner.scala/udf/23.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""received_date >= '$yesterday'""
"
"udf/spark_repos_1/1_guardian_ml-example/..ml.src.main.scala.com.gu.mlExample.Runner.scala/udf/25.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

s""received_date <= '$date'""
"
"udf/spark_repos_1/1_guardian_ml-example/..ml.src.main.scala.com.gu.mlExample.Runner.scala/udf/39.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""received_date >= '$yesterday'""
"
"udf/spark_repos_1/1_guardian_ml-example/..ml.src.main.scala.com.gu.mlExample.Runner.scala/udf/41.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

s""received_date <= '$date'""
"
"udf/spark_repos_1/1_hchauvin_health-dataset-generator/..generator-target-eds.src.main.scala.fr.aphp.wind.eds.generator.target.eds.access.scala/udf/34.21.Dataset-ProviderRole.map","Type: org.apache.spark.sql.Dataset[fr.aphp.wind.eds.generator.target.eds.access.ProviderRole]
Call: map

pr => (rand.nextLong, pr.careSiteId, pr.providerId, ""Provider"", pr.roleId)
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.AppUseActionBased.scala/udf/335.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

v => v.mkString(splitChar)
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.AppUseActionBased.scala/udf/386.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

v => v.mkString(splitChar).replace(""""""
"""""", """")
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.AppUseActionBased.scala/udf/84.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(splitChar)
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.sexmlib.AppUseSex.scala/udf/147.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.sexmlib.AppUseSex.scala/udf/151.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.sexmlib.AppUseSex.scala/udf/169.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.sexmlib.AppUseSex.scala/udf/173.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.sexmlib.AppUseSex.scala/udf/24.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
        case Row(lable: Int, features: String) =>
          (lable, features)
      }
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.sexmlib.AppUseSex.scala/udf/32.24.Dataset-(Int, String).filter","Type: org.apache.spark.sql.Dataset[(Int, String)]
Call: filter

{
          line => if (line._2 != null && line._2.nonEmpty) true else false
        }
"
"udf/spark_repos_1/1_heiheiwangergou_DMP-system-source-code/..sexmodel.sexmlib.AppUseSex.scala/udf/36.19.Dataset-(Int, String).map","Type: org.apache.spark.sql.Dataset[(Int, String)]
Call: map

{
        line => line._1 + "" "" + line._2
      }
"
"udf/spark_repos_1/1_heyxyw_bigdata/..bigdatastudy.spark.src.main.scala.com.zhouq.spark.sql.join.IPSearchLocationSQL2.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fileds: Array[String] = line.split(""[|]"")
        val startNum: Long = fileds(2).toLong
        val endNum: Long = fileds(3).toLong
        val province: String = fileds(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_1/1_heyxyw_bigdata/..bigdatastudy.spark.src.main.scala.com.zhouq.spark.sql.join.IPSearchLocationSQL2.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fileds: Array[String] = line.split(""[|]"")
        val ip: String = fileds(1)
        val ipNum: Long = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_1/1_heyxyw_bigdata/..bigdatastudy.spark.src.main.scala.com.zhouq.spark.sql.join.IPSearchLocationSQL2.scala/udf/33.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(ipNum: Long) => {
        val ipRulesInExecutor: Array[(Long, Long, String)] = broadcastRef.value
        val index: Int = MyUtils.binarySearch(ipRulesInExecutor, ipNum)
        var province = ""未知""
        if (index != -1) {
          province = ipRulesInExecutor(index)._3
        }
        province
      }
"
"udf/spark_repos_1/1_heyxyw_bigdata/..bigdatastudy.spark.src.main.scala.com.zhouq.spark.sql.join.IPSearchLocationSQL.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fileds: Array[String] = line.split(""[|]"")
        val startNum: Long = fileds(2).toLong
        val endNum: Long = fileds(3).toLong
        val province: String = fileds(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_1/1_heyxyw_bigdata/..bigdatastudy.spark.src.main.scala.com.zhouq.spark.sql.join.IPSearchLocationSQL.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fileds: Array[String] = line.split(""[|]"")
        val ip: String = fileds(1)
        val ipNum: Long = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_1/1_heyxyw_bigdata/..bigdatastudy.spark.src.main.scala.com.zhouq.spark.sql.join.JoinDemo.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fileds: Array[String] = line.split("","")
        val ename: String = fileds(0)
        val cname: String = fileds(1)
        (cname, ename)
      }
"
"udf/spark_repos_1/1_heyxyw_bigdata/..bigdatastudy.spark.src.main.scala.com.zhouq.spark.sql.join.JoinDemo.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fileds: Array[String] = line.split("","")
        val id: Long = fileds(0).toLong
        val name: String = fileds(1)
        val nation: String = fileds(2)
        (id, name, nation)
      }
"
"udf/spark_repos_1/1_heyxyw_bigdata/..bigdatastudy.spark.src.main.scala.com.zhouq.spark.sql.SQLFavTeache.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val tIndex = line.lastIndexOf(""/"") + 1
        val teacher = line.substring(tIndex)
        val host = new URL(line).getHost
        val sIndex = host.indexOf(""."")
        val subject = host.substring(0, sIndex)
        (subject, teacher)
      }
"
"udf/spark_repos_1/1_hgf-anu_DMP_hgf/..src.main.scala.com.tags.TagContext.scala/udf/60.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val userId: String = TagUtils.getOneUserId(row)
          val listAd: List[(String, Int)] = TagAdType.makeTags(row)
          val listAppname: List[(String, Int)] = TagAppname.makeTags(row, broadcast_app_dict)
          val listBusiness: List[(String, Int)] = TagBusiness.makeTags(row)
          val listDevice: List[(String, Int)] = TagDevice.makeTags(row)
          val listDitch: List[(String, Int)] = TagDitch.makeTags(row)
          val listKeyword: List[(String, Int)] = TagKeyword.makeTags(row, broadcast_banWords)
          val listLocation: List[(String, Int)] = TagLocation.makeTags(row)
          (userId, listAd ++ listAppname ++ listBusiness ++ listDevice ++ listDitch ++ listKeyword ++ listLocation)
        }
"
"udf/spark_repos_1/1_honeyAndSw_coursera-scala/..course4.week4-timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/111.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      r => TimeUsageRow(r.getAs(""working""), r.getAs(""sex""), r.getAs(""age""), r.getAs(""primaryNeeds""), r.getAs(""work""), r.getAs(""other""))
    }
"
"udf/spark_repos_1/1_honeyAndSw_coursera-scala/..course4.week4-timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/123.20.Dataset-(Key, Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[(Key, Double, Double, Double)]
Call: map

v => TimeUsageRow(v._1._1, v._1._2, v._1._3, v._2, v._3, v._4)
"
"udf/spark_repos_1/1_honeyAndSw_coursera-scala/..course5.observatory.src.main.scala.observatory.Extraction.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

stations(""latitude"").isNotNull && stations(""longitude"").isNotNull
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.DataSource.DataSourceApp.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val splits: Array[String] = x.getString(0).split("","")
        val builder = new StringBuilder()
        builder.append(splits(0).trim).append("","").append(splits(1).trim)
        builder.toString()
      }
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.DataSource.DataSourceApp.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""cnt"" > 100
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.DataSource.DataSourceApp.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""cnt"" > 100
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.DataSource.UDFFunctionApp.scala/udf/18.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => s.split("","").size
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.project.sql.TopNStats.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.project.sql.TopNStats.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.project.sql.TopNStats.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.spark.learn.sql.dataframe.DataframeAPIDemo.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

people.col(""age"") > 19
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.spark.learn.sql.dataframe.Datasets.scala/udf/11.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.bigdata.spark.learn.sql.dataframe.Datasets.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.spark.learn.sql.dataframe.RDD2Dataframe.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.spark.learn.sql.dataframe.RDD2Dataframe.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.SparkSession.DatasetApp.scala/udf/12.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

x => x + 1
"
"udf/spark_repos_1/1_HuichuanLI_Spark-SQL/..sparksql-train.src.main.scala.com.bigdata.SparkSession.InteroperatingRDDApp.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => ""Name:"" + x.getAs[String](""name"")
"
"udf/spark_repos_1/1_hung-phan_coursera-scala-spark-big-data/..src.main.scala.timeusage.TimeUsage.scala/udf/104.19.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working = working, sex = sex, age = age, primaryNeeds = roundTo1Decimal(primaryNeeds), work = roundTo1Decimal(work), other = roundTo1Decimal(other))
      }
"
"udf/spark_repos_1/1_hung-phan_coursera-scala-spark-big-data/..src.main.scala.timeusage.TimeUsage.scala/udf/98.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(working = row.getAs[String](""working""), sex = row.getAs[String](""sex""), age = row.getAs[String](""age""), primaryNeeds = row.getAs[Double](""primaryNeeds""), work = row.getAs[Double](""work""), other = row.getAs[Double](""other""))
"
"udf/spark_repos_1/1_inspur-bigdata_iot-stream-app/..src.main.java.myspark.MQTTStreamWordCount.scala/udf/20.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

_._1
"
"udf/spark_repos_1/1_JDZW2018_sparkDemo/..src.main.scala.cn.com.yusys.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_1/1_Jeffersonmf_teste-nasa-logs/..src.main.scala.core.ParserEngine.scala/udf/35.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dfParsedLines.col(""code"").equalTo(""404"")
"
"udf/spark_repos_1/1_johngodoi_learning-spark/..src.main.scala.com.johngodoi.spark.lego.Count.scala/udf/11.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getAs[String](""parent_id"")
"
"udf/spark_repos_1/1_linbojin_scala-courses/..04-scala-spark-big-data.week4.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/111.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
"
"udf/spark_repos_1/1_linbojin_scala-courses/..05-scala-capstone.solutions.observatory.src.main.scala.observatory.Extraction.scala/udf/22.24.Dataset-Station.filter","Type: org.apache.spark.sql.Dataset[observatory.Station]
Call: filter

$""lat"".isNotNull
"
"udf/spark_repos_1/1_linbojin_scala-courses/..05-scala-capstone.solutions.observatory.src.main.scala.observatory.Extraction.scala/udf/24.22.Dataset-Station.filter","Type: org.apache.spark.sql.Dataset[observatory.Station]
Call: filter

$""lon"".isNotNull
"
"udf/spark_repos_1/1_luodesong_adver/..src.main.scala.com.luodesong.adver.util.RegisterUtil.scala/udf/7.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

QFUdf.getAgeRange _
"
"udf/spark_repos_1/1_micho10_observatory/..src.main.scala.observatory.Extraction.scala/udf/16.19.Dataset-StnTempReading.map","Type: org.apache.spark.sql.Dataset[observatory.StnTempReading]
Call: map

reading => (ReadingDate(year, reading.month, reading.day), Location(reading.lat, reading.lon), reading.temperature)
"
"udf/spark_repos_1/1_mskimm_gbt-scala/..src.test.scala.com.github.mskimm.testing.Datasets.scala/udf/10.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
            val xs = line.split(""\t"")
            val label = xs(0).toDouble
            val features = locally {
              val _t_m_p_2 = xs.drop(1)
              _t_m_p_2.map(_.toDouble)
            }
            (label, Vectors.dense(features))
          }
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.AppStream.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => CarEvent(r.getString(0))
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.ScalaUDAFExample.scala/udf/33.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

obj
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.SensorAnalysis.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => sensor(row.getString(0), row.getString(1), row.getInt(2), row.getString(3), row.getString(4), row.getLong(5), row.getLong(6), row.getDouble(7), row.getDouble(8), row.getLong(9))
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.SensorDataSave.scala/udf/27.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => sensor(row.getString(0), row.getString(1), row.getInt(2), row.getString(3), row.getString(4), row.getLong(5), row.getLong(6), row.getDouble(7), row.getDouble(8), row.getLong(9))
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.SensorUDAF.scala/udf/34.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

obj
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.StreamStaticJoin.scala/udf/31.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => sensor(row.getString(0), row.getString(1), row.getInt(2), row.getString(3), row.getString(4), row.getLong(5), row.getLong(6), row.getDouble(7), row.getDouble(8), row.getLong(9))
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.YelpAnalysis.scala/udf/35.22.Dataset-business.filter","Type: org.apache.spark.sql.Dataset[com.naresh.org.YelpAnalysis.business]
Call: filter

row => row.state == ""SC""
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.YelpAnalysis.scala/udf/44.22.Dataset-checkin.filter","Type: org.apache.spark.sql.Dataset[com.naresh.org.YelpAnalysis.checkin]
Call: filter

r => r.checkins > 0
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.YelpAnalysis.scala/udf/48.22.Dataset-checkin.filter","Type: org.apache.spark.sql.Dataset[com.naresh.org.YelpAnalysis.checkin]
Call: filter

r => r.checkins > 0
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.YelpAnalysis.scala/udf/53.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getString(1), row.getList(2).size)
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.YelpAnalysis.scala/udf/58.22.Dataset-review.filter","Type: org.apache.spark.sql.Dataset[com.naresh.org.YelpAnalysis.review]
Call: filter

row => row.stars != null
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SampleStreamApp.src.main.scala.com.naresh.org.YelpAnalysis.scala/udf/73.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => {
        val temp = row.getString(0)
        var ret = true
        try {
          var t = temp.toInt
        } catch {
          case e: Exception =>
            ret = false
        }
        ret
      }
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SparkBankingApp.src.main.scala.com.naresh.org.AppSpark.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""loan_id"").isNull
"
"udf/spark_repos_1/1_ndulam_KafkaSparkStreams/..SparkBankingApp.src.main.scala.com.naresh.org.AppSpark.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""bank"").isNotNull
"
"udf/spark_repos_1/1_noraibrahimi_IntelliDotaIC/..src.main.scala.classification.OutliersDetection.scala/udf/16.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataframe.col(col) > leftB && dataframe.col(col) < rightB
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Call.scala/udf/63.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""_c1"").isin(phoneNumbers.distinct: _*) && col(""_c5"").isin(""HGMTC"", ""HGMOC"") || col(""_c26"").isin(lacCollection.distinct: _*) && col(""_c5"").isin(""HGMTC"", ""HGMOC"") || col(""_c23"").isin(cellCollection.distinct: _*) && col(""_c5"").isin(""HGMTC"", ""HGMOC"")
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Call.scala/udf/84.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""_c0"").isin(locally {
              val _t_m_p_3 = phoneNumbers
              _t_m_p_3.map(""7"" + (_))
            }: _*) && col(""_c3"").isin(List(""OUTGOING_OPPS_CALL"", ""INCOMING_TPPS_CALL""): _*) || col(""_c14"").substr(6, 5).isin(lacCollection.distinct: _*) && col(""_c3"").isin(List(""OUTGOING_OPPS_CALL"", ""INCOMING_TPPS_CALL""): _*) || col(""_c14"").substr(11, 5).isin(cellCollection.distinct: _*) && col(""_c3"").isin(List(""OUTGOING_OPPS_CALL"", ""INCOMING_TPPS_CALL""): _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.CallThread.scala/udf/18.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""FIRST_PHONE_NUMBER"").isin(request.identifier: _*) && (col(""DATE_TIME"") >= request.startPeriod.toString(""yyyy-MM-dd HH:mm:ss"") && col(""DATE_TIME"") <= request.endPeriod.toString(""yyyy-MM-dd HH:mm:ss""))
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.CallThread.scala/udf/33.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

expression && (col(""DATE_TIME"") >= request.startPeriod.toString(""yyyy-MM-dd HH:mm:ss"") && col(""DATE_TIME"") <= request.endPeriod.toString(""yyyy-MM-dd HH:mm:ss""))
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.CallThread.scala/udf/44.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""FIRST_PHONE_NUMBER"").isin(locally {
            val _t_m_p_4 = request.identifier
            _t_m_p_4.map(""7"" + (_))
          }: _*) && (col(""DATE_TIME"") >= request.startPeriod.toString(""yyyy-MM-dd HH:mm:ss"") && col(""DATE_TIME"") <= request.endPeriod.toString(""yyyy-MM-dd HH:mm:ss""))
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.CallThread.scala/udf/62.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

expression && (col(""DATE_TIME"") >= request.startPeriod.toString(""yyyy-MM-dd HH:mm:ss"") && col(""DATE_TIME"") <= request.endPeriod.toString(""yyyy-MM-dd HH:mm:ss""))
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.CallThread.scala/udf/72.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(request.identifier: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.CallThread.scala/udf/80.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(locally {
            val _t_m_p_8 = request.identifier
            _t_m_p_8.map(""7"" + (_))
          }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.CallThread.scala/udf/91.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(locally {
            val _t_m_p_10 = request.identifier
            _t_m_p_10.map(""7"" + (_))
          }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Contract.scala/udf/28.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""SUBSCRIBER_NO"").isin(phoneNumbers.distinct: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Contract.scala/udf/51.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""SUBSCRIBER_NO"").isin(phoneNumbers.distinct: _*) && col(""SOC"").like(""BCARD%"")
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Contract.scala/udf/70.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""work_telno"").isin(phoneNumbers.distinct: _*) || col(""home_telno"").isin(phoneNumbers.distinct: _*) || col(""ctn_1"").isin(phoneNumbers.distinct: _*) || col(""ctn_2"").isin(phoneNumbers.distinct: _*) || col(""ctn_3"").isin(phoneNumbers.distinct: _*) || col(""ctn_4"").isin(phoneNumbers.distinct: _*) || col(""ctn_5"").isin(phoneNumbers.distinct: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.ContractThread.scala/udf/14.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(request.identifier: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.ContractThread.scala/udf/22.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(request.identifier: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.ContractThread.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""work_telno"").isin(request.identifier.distinct: _*) || col(""home_telno"").isin(request.identifier.distinct: _*) || col(""ctn_1"").isin(request.identifier.distinct: _*) || col(""ctn_2"").isin(request.identifier.distinct: _*) || col(""ctn_3"").isin(request.identifier.distinct: _*) || col(""ctn_4"").isin(request.identifier.distinct: _*) || col(""ctn_5"").isin(request.identifier.distinct: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Internet.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""EXTERNAL_ID"").isin(locally {
          val _t_m_p_2 = phoneNumbers
          _t_m_p_2.map(""7"" + (_))
        }: _*) && col(""MERCHANT_ID"") =!= ""MFS""
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.InternetThread.scala/udf/12.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(locally {
          val _t_m_p_2 = request.identifier
          _t_m_p_2.map(""7"" + (_))
        }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Money.scala/udf/30.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""MERCHANT_ID"") === ""MFS"" && col(""EXTERNAL_ID"").isin(locally {
              val _t_m_p_2 = phoneNumbers
              _t_m_p_2.map(""7"" + (_))
            }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Money.scala/udf/52.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""EXTERNAL_ID"").isin(locally {
              val _t_m_p_4 = phoneNumbers
              _t_m_p_4.map(""7"" + (_))
            }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Money.scala/udf/74.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(col(""MTR_COMMENT"").like(""%Cancel%"") || col(""MTR_COMMENT"").rlike(""BT_A_[0-9]+_([0-9]+)"")) && col(""EXTERNAL_ID"").isin(locally {
              val _t_m_p_6 = phoneNumbers
              _t_m_p_6.map(""7"" + (_))
            }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.MoneyThread.scala/udf/14.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(locally {
          val _t_m_p_2 = request.identifier
          _t_m_p_2.map(""7"" + (_))
        }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.MoneyThread.scala/udf/25.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(locally {
          val _t_m_p_4 = request.identifier
          _t_m_p_4.map(""7"" + (_))
        }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.MoneyThread.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHONE_NUMBER"").isin(locally {
          val _t_m_p_6 = request.identifier
          _t_m_p_6.map(""7"" + (_))
        }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Portability.scala/udf/12.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""up"") === ""21""
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Portability.scala/udf/41.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""mob_num"").isin(identifiers: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Redirection.scala/udf/10.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""msisdn"").isin(locally {
          val _t_m_p_2 = identifiers
          _t_m_p_2.map(""7"" + (_))
        }: _*)
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Roaming.scala/udf/10.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""SchemaServiceNumber"").like(""WSMS:%"")
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Roaming.scala/udf/14.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""Description"").rlike(""\\?*DO_\\?*NOT_\\?*USE"")
"
"udf/spark_repos_1/1_nurzhannogerbek_robocop/..src.main.scala.reports.Roaming.scala/udf/64.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Msisdn"").isin(locally {
          val _t_m_p_4 = identifiers
          _t_m_p_4.map(""7"" + (_))
        }: _*)
"
"udf/spark_repos_1/1_pazinio_Commits/..src.main.scala.com.commits.Main.scala/udf/17.19.Dataset-NameByDate.map","Type: org.apache.spark.sql.Dataset[com.commits.NameByDate]
Call: map

r => new NameByDay(date = r.date, dateWithNoTime = r.date.split("" "")(0), dayOfWeek = LocalDate.parse(r.date.split("" "")(0)).getDayOfWeek.toString, name = r.name)
"
"udf/spark_repos_1/1_pazinio_Commits/..src.main.scala.com.commits.Main.scala/udf/22.21.Dataset-NameByDay.map","Type: org.apache.spark.sql.Dataset[com.commits.NameByDay]
Call: map

r => (r, 1)
"
"udf/spark_repos_1/1_pazinio_Commits/..src.main.scala.com.commits.Main.scala/udf/24.19.Dataset-NameByDay, Int)).map","Type: org.apache.spark.sql.Dataset[(String, (com.commits.NameByDay, Int))]
Call: map

x => new TotalBySpecificDate(dateWithNoTime = x._2._1.dateWithNoTime, dayOfWeek = x._2._1.dayOfWeek, total = x._2._2)
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.app.loadods.main.OdsPat.scala/udf/11.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(col: String) => {
        val res = ((Int.MaxValue.toLong - col.hashCode) % 30).toInt
        DateUtil.getNdaysAgo(res)
      }
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.app.loadods.test.CountMain.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val databaseNm = row.getAs[String](""databaseName"")
        broadcast.value.sql(s""use `$databaseNm`"")
        val xx = locally {
          val _t_m_p_2 = broadcast.value.sql(""show tables"")
          _t_m_p_2.map(tRow => tRow.getAs[String](""tableName""))
        }.collect()
        (databaseNm, xx)
      }
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.app.loadods.test.CountMain.scala/udf/19.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

tRow => tRow.getAs[String](""tableName"")
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.app.loadods.test.CountMain.scala/udf/26.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val databaseNm = row.getAs[String](""databaseNm"")
        val tableNm = row.getAs[String](""tableNm"")
        broadcast.value.sql(s""use `$databaseNm`"")
        val sql = s""show partitions $tableNm""
        println(sql)
        val partionsDF = broadcast.value.sql(sql)
        partionsDF.show()
        (databaseNm, tableNm)
      }
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.dataframe.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.dataframe.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.dataframe.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.dataset.ActionOperation.scala/udf/28.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => 1
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.dataset.DatasetApp1.scala/udf/11.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.dataset.DatasetApp.scala/udf/14.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.lp.test.sql.dataset.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.dataset.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.lp.test.sql.dataset.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.project.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.project.TopNStatJob.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.project.TopNStatJob.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.project.TopNStatJob.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.project.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.project.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.sql.project.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.structstreaming.KafkaSourceOperator.StructuredKafkaTypes.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

each => {
        val buf = each.split("" "")
        fruit(buf(0), buf(1).toLong)
      }
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.structstreaming.KafkaSourceOperator.StructuredSessionizationFlatmap.scala/udf/16.19.Dataset-(Long, String).map","Type: org.apache.spark.sql.Dataset[(Long, String)]
Call: map

{
        case (timestamp, fruitCol) =>
          Event(sessionId = fruitCol, timestamp)
      }
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.structstreaming.KafkaSourceOperator.StructuredSessionizationMap.scala/udf/16.19.Dataset-(Long, String).map","Type: org.apache.spark.sql.Dataset[(Long, String)]
Call: map

{
        case (timestamp, fruitCol) =>
          Event(sessionId = fruitCol, timestamp)
      }
"
"udf/spark_repos_1/1_perkinls_spark-local-train/..src.main.scala.com.lp.test.structstreaming.KafkaSourceOperator.TestForeachWriter.scala/udf/24.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toString
"
"udf/spark_repos_1/1_petrospgithub_onlinearima/..src.main.scala.evaluation.Accuracy.scala/udf/28.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""horizon="" + horizon
"
"udf/spark_repos_1/1_philip-healy_spark-demo/..src.main.scala.com.github.philip_healy.sparkdemo.DataFrameDemo.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sex"" === ""male"" || $""sex"" === ""female""
"
"udf/spark_repos_1/1_ptndoss_Spark_Scala_Learning/..src.main.scala.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(""country"").===(""Afghanistan"")
"
"udf/spark_repos_1/1_ptndoss_Spark_Scala_Learning/..src.main.scala.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(AGE_MIDPOINT) < 20
"
"udf/spark_repos_1/1_ptndoss_Spark_Scala_Learning/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/22.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.country == ""Afghanistan""
"
"udf/spark_repos_1/1_ptndoss_Spark_Scala_Learning/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/29.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.age_midpoint.isDefined && response.age_midpoint.get < 20.0d
"
"udf/spark_repos_1/1_ptndoss_Spark_Scala_Learning/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/36.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.salary_midpoint.isDefined
"
"udf/spark_repos_1/1_ptndoss_Spark_Scala_Learning/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/41.19.Dataset-Response.map","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: map

response => locally {
        val _t_m_p_5 = response.salary_midpoint
        _t_m_p_5.map(point => Math.round(point / 20000) * 20000)
      }.orElse(None)
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.meta.MetaParse.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 4
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.AbstractModel.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 4
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.AbstractTagModel.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 4
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.basic.BasicModel2.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 4
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.basic.BasicModel.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 4
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.rmd.BpModel.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""loc_url"".isNotNull
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.rmd.BpModel.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""userId"".isNotNull && $""productId"".isNotNull && validate_product_udf($""productId"")
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.rule.GenderModel.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 4
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.rule.GenderModel.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 5
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.rule.JobModel.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 4
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.rule.JobModel.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 5
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.models.statistics.AgeRangeModel.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 5
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.tools.TagTools.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 5
"
"udf/spark_repos_1/1_q12138s_Project-UserPortrait/..tags_model.src.main.scala.cn.itcast.tags.tools.TagTools.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""level"" === 5
"
"udf/spark_repos_1/1_railrem_group_project/..src.main.java.tu.Clustering.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""population"").isNotNull
"
"udf/spark_repos_1/1_recognai_rdf-processing/..src.main.scala.com.recognai.rdf.spark.enrichment.WikidataEnrichment.scala/udf/23.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => Triple(r.getString(0), r.getString(1), parserProperty(r.getString(2)))
"
"udf/spark_repos_1/1_recognai_rdf-processing/..src.main.scala.com.recognai.rdf.spark.enrichment.WikidataEnrichment.scala/udf/30.19.Dataset-Triple, (String, String)).map","Type: org.apache.spark.sql.Dataset[(com.recognai.rdf.spark.operations.Triple, (String, String))]
Call: map

{
        case (triple, subjectsRelation) =>
          Triple(subjectsRelation._2, triple.Predicate, triple.Object)
      }
"
"udf/spark_repos_1/1_recognai_rdf-processing/..src.main.scala.com.recognai.rdf.spark.enrichment.WikidataEnrichment.scala/udf/41.22.Dataset-Triple.filter","Type: org.apache.spark.sql.Dataset[com.recognai.rdf.spark.operations.Triple]
Call: filter

_.Object.literal.isDefined
"
"udf/spark_repos_1/1_recognai_rdf-processing/..src.main.scala.com.recognai.rdf.spark.enrichment.WikidataEnrichment.scala/udf/46.21.Dataset-Triple.map","Type: org.apache.spark.sql.Dataset[com.recognai.rdf.spark.operations.Triple]
Call: map

t => (t.Subject, getURIName(t.Subject))
"
"udf/spark_repos_1/1_recognai_rdf-processing/..src.main.scala.com.recognai.rdf.spark.enrichment.WikidataEnrichment.scala/udf/48.19.Dataset-Triple).map","Type: org.apache.spark.sql.Dataset[((String, String), com.recognai.rdf.spark.operations.Triple)]
Call: map

{
        case (t1, t2) =>
          (t2.Subject, t1._1)
      }
"
"udf/spark_repos_1/1_recognai_rdf-processing/..src.main.scala.com.recognai.rdf.spark.SchemaExtraction.scala/udf/9.19.Dataset-Subject.map","Type: org.apache.spark.sql.Dataset[com.recognai.rdf.spark.operations.Subject]
Call: map

s => (s.getType(), propertiesDefinitions(s))
"
"udf/spark_repos_1/1_senior-sigan_kaggle-amazon-fine-food-reviews/..parser.src.main.scala.it.sevenbits.amazonfinefoods.App.scala/udf/43.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.trim.replaceAll(""[^\\p{Alpha}]+"", """")
"
"udf/spark_repos_1/1_senior-sigan_kaggle-amazon-fine-food-reviews/..parser.src.main.scala.it.sevenbits.amazonfinefoods.App.scala/udf/45.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.nonEmpty
"
"udf/spark_repos_1/1_senior-sigan_kaggle-amazon-fine-food-reviews/..parser.src.main.scala.it.sevenbits.amazonfinefoods.App.scala/udf/71.23.Dataset-CommentChunk.map","Type: org.apache.spark.sql.Dataset[it.sevenbits.amazonfinefoods.CommentChunk]
Call: map

ch => ch.copy(Text = ch.Text.trim)
"
"udf/spark_repos_1/1_senior-sigan_kaggle-amazon-fine-food-reviews/..parser.src.main.scala.it.sevenbits.amazonfinefoods.App.scala/udf/73.24.Dataset-CommentChunk.filter","Type: org.apache.spark.sql.Dataset[it.sevenbits.amazonfinefoods.CommentChunk]
Call: filter

_.Text.nonEmpty
"
"udf/spark_repos_1/1_Shasidhar_kafka-streaming/..src.main.scala.com.shashidhar.KafkaFromOffsets.scala/udf/12.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_1/1_Shasidhar_kafka-streaming/..src.main.scala.com.shashidhar.KafkaWordCount.scala/udf/12.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_1/1_Shasidhar_kafka-streaming/..src.main.scala.com.shashidhar.RecoveringCheckpoint.scala/udf/12.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.com.scala.spark.usecase.LogisticAnalytic.scala/udf/34.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""Distance"") < 140000 && df(""Weight"") > 1000
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DataFrameDemo.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""name"").isNotNull
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DataFrameDemo.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 21
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DataframeUsingCSV.scala/udf/35.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""rating"") > 3
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DataframeUsingCSV.scala/udf/43.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""rating"") === 3 && df(""movieId"") >= 30
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DataframeUsingCSV.scala/udf/51.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""movieId"") <= 30
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DataframeUsingCSV.scala/udf/59.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""movieId"") <= 30
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DataframeUsingCSV.scala/udf/65.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""movieId"") <= 300
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DataframeUsingCSV.scala/udf/69.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""movieId"") >= 300
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DatasetDemo.scala/udf/31.21.Dataset-Department).map","Type: org.apache.spark.sql.Dataset[(practise.scala.spark.dfds.Employee, practise.scala.spark.dfds.Department)]
Call: map

record => Record(record._1.name, record._1.age, record._1.salary, record._1.departmentId, record._2.name)
"
"udf/spark_repos_1/1_shishir11_Scala_Spark/..src.main.scala.practise.scala.spark.dfds.DatasetDemo.scala/udf/33.22.Dataset-Record.filter","Type: org.apache.spark.sql.Dataset[practise.scala.spark.dfds.Record]
Call: filter

record => record.age > 25
"
"udf/spark_repos_1/1_sjsmi1e_Spark-learing/..src.main.scala.sql.SparkUDFTest.scala/udf/22.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAvg
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.CancelOrderCount.scala/udf/19.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => {
        val serialid = JSON.parseObject(value).getInteger(""serialid"")
        serialid match {
          case null => 0
          case _ => serialid
        }
      }
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.HotBrand.scala/udf/20.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => {
        val brandid = JSON.parseObject(value).getInteger(""brandid"")
        brandid match {
          case null => 0
          case _ => brandid
        }
      }
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.HotProduct.scala/udf/24.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => pattern.findFirstMatchIn(value) match {
        case Some(s) =>
          s.group(0).toInt
        case None =>
          0
      }
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.HotSerial.scala/udf/19.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => {
        val serialid = JSON.parseObject(value).getInteger(""serialid"")
        serialid match {
          case null => 0
          case _ => serialid
        }
      }
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.OrderCount.scala/udf/19.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => {
        val serialid = JSON.parseObject(value).getInteger(""serialid"")
        serialid match {
          case null => 0
          case _ => serialid
        }
      }
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.PayOrderCount.scala/udf/19.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => {
        val serialid = JSON.parseObject(value).getInteger(""serialid"")
        serialid match {
          case null => 0
          case _ => serialid
        }
      }
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.Product2Buy.scala/udf/20.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => {
        val brandid = JSON.parseObject(value).getInteger(""brandid"")
        brandid match {
          case null => 0
          case _ => brandid
        }
      }
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.Product2Cart.scala/udf/20.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => {
        val brandid = JSON.parseObject(value).getInteger(""brandid"")
        brandid match {
          case null => 0
          case _ => brandid
        }
      }
"
"udf/spark_repos_1/1_soufunlab_sparkdemo/..compute.src.main.scala.com.demo.compute.jobs.business.Product2Order.scala/udf/20.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => {
        val brandid = JSON.parseObject(value).getInteger(""brandid"")
        brandid match {
          case null => 0
          case _ => brandid
        }
      }
"
"udf/spark_repos_1/1_spatil6_Spark-2-features/..Spark2Features.src.core.learning.ExploreSpark.scala/udf/115.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!sqlfunc(col(""v""))
"
"udf/spark_repos_1/1_spatil6_Spark-2-features/..Spark2Features.src.core.learning.ExploreSpark.scala/udf/48.19.Dataset-Bank.map","Type: org.apache.spark.sql.Dataset[core.learning.Bank]
Call: map

_.age
"
"udf/spark_repos_1/1_spatil6_Spark-2-features/..Spark2Features.src.core.learning.ExploreSpark.scala/udf/56.22.Dataset-Bank.filter","Type: org.apache.spark.sql.Dataset[core.learning.Bank]
Call: filter

p => p.age == 23
"
"udf/spark_repos_1/1_sudhirsrepo_SparkSql/..src.main.scala.com.sudhir.spark.sql.WorkingWithDataSource.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_1/1_sudhirsrepo_SparkSql/..src.main.scala.com.sudhir.spark.sql.WorkingWithFiles.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 25
"
"udf/spark_repos_1/1_sudhirsrepo_SparkSql/..src.main.scala.com.sudhir.spark.sql.WorkingWithFiles.scala/udf/36.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/1_sudhirsrepo_SparkSql/..src.main.scala.com.sudhir.spark.sql.WorkingWithFiles.scala/udf/58.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ""Name: "" + row(0)
"
"udf/spark_repos_1/1_sudhirsrepo_SparkSql/..src.main.scala.com.sudhir.spark.sql.WorkingWithFiles.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attr => ""Name: "" + attr.getAs[String](""name"")
"
"udf/spark_repos_1/1_sudhirsrepo_SparkSql/..src.main.scala.com.sudhir.spark.sql.WorkingWithFiles.scala/udf/67.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attr => ""Age: "" + attr.getAs(""age"")
"
"udf/spark_repos_1/1_sudhirsrepo_SparkSql/..src.main.scala.com.sudhir.spark.sql.WorkingWithFiles.scala/udf/73.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_1/1_sudhirsrepo_SparkSql/..src.main.scala.com.sudhir.spark.sql.WorkingWithFiles.scala/udf/79.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_1/1_suixinfengye_bigdata/..src.main.scala.spark.analysis.MovieKeyWords.scala/udf/58.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AggComments
"
"udf/spark_repos_1/1_suixinfengye_bigdata/..src.main.scala.spark.analysis.MovieKeyWords.scala/udf/67.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(movieid: String, content: String) =>
          val ansjInstance = bansj.value
          val keyWords = ansjInstance.getKeywords(content).toString
          (movieid, keyWords)
        case Row(movieid: String, content1: String, content2: String) =>
          val ansjInstance = bansj.value
          val keyWords = ansjInstance.getKeywords(content1 + content2).toString
          (movieid, keyWords)
      }
"
"udf/spark_repos_1/1_sunshinenum_dns/..src.main.scala.chen.extractFeatures.scoreNames.scala/udf/17.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[Stream[String]](""ngrams"").toList.mkString(""/"")
"
"udf/spark_repos_1/1_sunshinenum_dns/..src.main.scala.chen.extractFeatures.scoreNames.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

word => (word, 1)
"
"udf/spark_repos_1/1_sunshinenum_dns/..src.main.scala.chen.extractFeatures.scoreNames.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0) -> row.getDouble(1)
"
"udf/spark_repos_1/1_sunshinenum_dns/..src.main.scala.chen.extractFeatures.scoreNames.scala/udf/58.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs[Stream[String]](""names"").toList.mkString(""""), row.getAs[Stream[String]](""ngrams"").toList.mkString(""/""))
"
"udf/spark_repos_1/1_sunshinenum_dns/..src.main.scala.chen.extractFeatures.scoreNames.scala/udf/60.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

row => locally {
          val _t_m_p_11 = row._2.split(""/"")
          _t_m_p_11.map(r => row._1 + ""/"" + r)
        }
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.examples.src.test.scala.tour.SparkStructuredStreams.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (r: Row) => WordCount(r.getAs[String](0), r.getAs[Long](1)) }
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/20.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.binary _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/24.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.binaryWithSubType _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/28.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.dbPointer _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/32.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.javaScript _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/36.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.javaScriptWithScope _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/40.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.maxKey _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/44.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.minKey _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/48.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.objectId _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/52.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.regularExpression _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/56.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.regularExpressionWithOptions _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/60.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.symbol _
"
"udf/spark_repos_1/1_supernovel_Docker-DataProcess/..mongo-spark.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/64.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.timestamp _
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.ex1_products.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""_c0"").contains(""John"")
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.movies.MovieDataFillingBadValues.scala/udf/15.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

replaceEmptyStr _
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.movies.MovieDataFillingBadValues.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x == 1900
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.movies.MovieDataFillingBadValues.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x != 1900
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.movies.MovieDataFillingBadValues.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.convertYear _
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.movies.MovieData.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.convertYear _
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.movies.RatingData.scala/udf/50.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getCurrentHour _
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.movies.RatingData.scala/udf/56.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

assignTod _
"
"udf/spark_repos_1/1_supreme-core_MyMachineLearningExamples/..src.main.scala.sparkML.src.recommendation.FeatureExtraction.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.AnalyzeWebLog.scala/udf/31.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

parseDate(_: String): Long
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ContentOverlaysProcessing.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""resource_type"" === ""movie""
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeMongoAnalysis.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseMongoStats
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeMongoAnalysis.scala/udf/27.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseMongoStats
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeMongoAnalysis.scala/udf/32.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!$""resource"".startsWith(""MX"") && !$""resource"".startsWith(""Published"")
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeMongoAnalysis.scala/udf/36.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!$""resource"".startsWith(""MX"") && !$""resource"".startsWith(""Published"")
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeResolverAnalysis.scala/udf/28.22.Dataset-DanubeResolverTab.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeResolverTab]
Call: filter

$""pubId"".between(lower, upper) || $""pubId"".between(nonJtLower, nonJtUpper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeResolverAnalysis.scala/udf/35.22.Dataset-DanubeResolverTab.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeResolverTab]
Call: filter

$""pubId"".between(lower, upper) || $""pubId"".between(jtLower, jtUpper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeResolverJengaOnly.scala/udf/24.22.Dataset-DanubeResolverTab.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeResolverTab]
Call: filter

$""pubId"".between(lower, upper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeResolverJengaOnly.scala/udf/31.22.Dataset-DanubeResolverTab.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeResolverTab]
Call: filter

$""pubId"".between(lower, upper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeStatesAnalysis2.scala/udf/25.22.Dataset-DanubeStates.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeStates]
Call: filter

$""pubId"".between(nonJtLower, nonJtUpper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeStatesAnalysis2.scala/udf/36.22.Dataset-DanubeStates.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeStates]
Call: filter

$""pubId"".between(jtLower, jtUpper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeStatesAnalysis.scala/udf/24.22.Dataset-DanubeNonJTState.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeNonJTState]
Call: filter

$""pubId"".between(nonJtLower, nonJtUpper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.DanubeStatesAnalysis.scala/udf/35.22.Dataset-DanubeJTState.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeJTState]
Call: filter

$""pubId"".between(jtLower, jtUpper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.FlightSample.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""year"" >= 1990
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.FlightSample.scala/udf/25.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""depdelay"" >= 15
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.FlightSample.scala/udf/29.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""depdelay"" >= 60
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.FlightSample.scala/udf/33.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cancelled"" === 1
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.FlightSample.scala/udf/37.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cancelled"" === 1
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStartCv.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseRating
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStartCv.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseRating
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStartCv.scala/udf/28.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseMovie
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStartCv.scala/udf/53.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""userId"" === pUserId
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseRating
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/25.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseRating
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/29.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseMovie
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/46.19.Dataset-Rating.map","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Rating]
Call: map

_.movieId
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/51.22.Dataset-Movie.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Movie]
Call: filter

mv => !pMovieIds.contains(mv.id)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/59.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""userId"" === pUserId
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/65.24.Dataset-Rating.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Rating]
Call: filter

$""userId"" === sUserId
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/67.19.Dataset-Rating.map","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Rating]
Call: map

_.movieId
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/71.22.Dataset-Movie.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Movie]
Call: filter

mv => !sMovieIds.contains(mv.id)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSColdStart.scala/udf/78.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""userId"" === sUserId
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSMongo.scala/udf/104.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""prediction"".isNaN
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSMongo.scala/udf/67.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""prediction"".isNaN
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALSMongo.scala/udf/83.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""prediction"".isNaN
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALS.scala/udf/100.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""prediction"".isNaN
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALS.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseRating
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALS.scala/udf/25.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseRating
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALS.scala/udf/29.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

mlParser.parseMovie
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALS.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""prediction"".isNaN
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALS.scala/udf/79.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""prediction"".isNaN
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALS.scala/udf/91.19.Dataset-Rating.map","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Rating]
Call: map

_.movieId
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.MovieLensALS.scala/udf/96.22.Dataset-Movie.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Movie]
Call: filter

movie => !pMovieIds.contains(movie.id)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailMX.scala/udf/26.22.Dataset-DanubeResolverRaw.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeResolverRaw]
Call: filter

$""roviId"".between(lower, upper) && $""state"" === state
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailMX.scala/udf/33.22.Dataset-DanubeResolverRaw.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeResolverRaw]
Call: filter

$""roviId"".between(lower, upper) && $""state"" === state
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailMX.scala/udf/43.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""resource"" === res
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailMX.scala/udf/46.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""resource"" === res
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailMX.scala/udf/50.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""non_jt_dirty_size"".isNull || $""jt_dirty_size"".isNull
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailsJenga.scala/udf/25.22.Dataset-DanubeResolverRaw.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeResolverRaw]
Call: filter

$""pubId"".between(lower, upper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailsJenga.scala/udf/32.22.Dataset-DanubeResolverRaw.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.DanubeResolverRaw]
Call: filter

$""pubId"".between(lower, upper)
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailsJenga.scala/udf/42.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""resource"" === res
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.ResolverDetailsJenga.scala/udf/45.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""resource"" === res
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.SparkSessionZipsExample.scala/udf/22.22.Dataset-Zips.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Zips]
Call: filter

_.pop > 40000
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.SparkSessionZipsExample.scala/udf/29.22.Dataset-Zips.filter","Type: org.apache.spark.sql.Dataset[org.freemind.spark.sql.Zips]
Call: filter

$""state"" === ""CA""
"
"udf/spark_repos_1/1_threecuptea_spark_tutorial_2/..src.main.scala.org.freemind.spark.sql.SparkSessionZipsExample.scala/udf/39.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(z: String) => z.toInt
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.base.BaseModel.scala/udf/67.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(newTagsId: String, oldTagsId: String) => if (StringUtils.isBlank(newTagsId)) {
        oldTagsId
      } else if (StringUtils.isBlank(oldTagsId)) {
        newTagsId
      } else {
        (newTagsId.split("","") ++ oldTagsId.split("","")).toSet.mkString("","")
      }
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.matchtag.GenderModel2.scala/udf/17.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

row => (row._2, row._1)
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.matchtag.JobModel.scala/udf/33.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(1), row.getLong(0))
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.matchtag.NationalityModel.scala/udf/18.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val sourceStr: String = row.getAs(""rule"").toString
        locally {
          val _t_m_p_2 = sourceStr.split(""##"")
          _t_m_p_2.map(kv => {
            val arr: Array[String] = kv.split(""="")
            (arr(0), arr(1))
          })
        }
      }
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.matchtag.PoliticalFaceModel.scala/udf/33.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val id = row.getAs(""id"").toString
        val rule = row.getAs(""rule"").toString
        (rule, id)
      }
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.matchtag.SimpleGenderModel.scala/udf/32.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(1), row.getLong(0))
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.ml.BPModel.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'productId.isNotNull
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.ml.BPModel.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'userId.isNotNull && 'productId.isNotNull && 'rating.isNotNull
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.ml.PSMModel.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'psm.isNotNull
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.ml.RFEModel.scala/udf/45.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

row => (row._2, row._1)
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.ml.RFMModel.scala/udf/61.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

row => (row._2, row._1)
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.statistics.AgeModel.scala/udf/15.19.Dataset-(Long, String).map","Type: org.apache.spark.sql.Dataset[(Long, String)]
Call: map

t => {
        val arr: Array[String] = t._2.split(""-"")
        (arr(0), arr(1), t._1)
      }
"
"udf/spark_repos_1/1_TryingEnjoyYingYingYing_userprofile_dj/..model.src.main.scala.cn.itcast.up.statistics.CycleModel.scala/udf/19.19.Dataset-(Long, String).map","Type: org.apache.spark.sql.Dataset[(Long, String)]
Call: map

t => {
        val arr: Array[String] = t._2.split(""-"")
        (arr(0), arr(1), t._1)
      }
"
"udf/spark_repos_1/1_vokie123456_dmp/..src.main.scala.tag.DataTag.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

TagUtils.hasneedOneUserId
"
"udf/spark_repos_1/1_vovanmix_scala-challenges/..spark.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/91.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      row => TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
    }
"
"udf/spark_repos_1/1_vovanmix_scala-challenges/..spark.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/99.19.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, primaryNeeds, work, other)
      }
"
"udf/spark_repos_1/1_wkwzzc_StatisticalLearningMethod/..src.main.scala.CH5_DecisionTree.DecisionTreeModel.scala/udf/29.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""ftsName"" =!= labelCol
"
"udf/spark_repos_1/1_Xingbei99_bike_share_platform_data_analysis/..src.main.scala.com.bike_platform_data_analysis.process.RetentionRateProcess.scala/udf/16.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""user_age_days"") === 1
"
"udf/spark_repos_1/1_Xingbei99_bike_share_platform_data_analysis/..src.main.scala.com.bike_platform_data_analysis.process.RetentionRateProcess.scala/udf/21.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""user_age_days"") === 3
"
"udf/spark_repos_1/1_Xingbei99_bike_share_platform_data_analysis/..src.main.scala.com.bike_platform_data_analysis.process.RetentionRateProcess.scala/udf/26.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""user_age_days"") === 7
"
"udf/spark_repos_1/1_yilong2001_yl-streaming-kafka-hbase/..src.main.scala.com.example.streaming.demo.simple.StructuredStreamSqlUdf.scala/udf/34.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new IsAbnormal2().isAbnormalEx _
"
"udf/spark_repos_1/1_Yiran-wu_iwantfind-tool/..src.main.scala.com.iwantfind.DataFramesAnalyser.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getLong(0), 1)
"
"udf/spark_repos_1/1_YuhanSun_CSE512-Project-Phase2-Template/..src.main.scala.cse512.HotcellAnalysis.scala/udf/16.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 0)
"
"udf/spark_repos_1/1_YuhanSun_CSE512-Project-Phase2-Template/..src.main.scala.cse512.HotcellAnalysis.scala/udf/20.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 1)
"
"udf/spark_repos_1/1_YuhanSun_CSE512-Project-Phase2-Template/..src.main.scala.cse512.HotcellAnalysis.scala/udf/24.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupTime: String) => HotcellUtils.CalculateCoordinate(pickupTime, 2)
"
"udf/spark_repos_1/1_YuhanSun_CSE512-Project-Phase2-Template/..src.main.scala.cse512.HotzoneAnalysis.scala/udf/13.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(string: String) => string.replace(""("", """").replace("")"", """")
"
"udf/spark_repos_1/1_YuhanSun_CSE512-Project-Phase2-Template/..src.main.scala.cse512.HotzoneAnalysis.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => HotzoneUtils.ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day01.DatasetDemo2.scala/udf/10.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("" "")
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day01.DatasetDemo3.scala/udf/10.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("" "")
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day01.WordCountDataFrame.scala/udf/29.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(name: String) => 1
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.IPLocation2.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => {
        val split = t.split(""\\|"")
        (split(2).toLong, split(3).toLong, split(6))
      }
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.IPLocation2.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => IpUtils.ip2Long(str.split(""\\|"")(1))
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.IPLocation.scala/udf/12.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => {
        val split = t.split(""\\|"")
        (split(2).toLong, split(3).toLong, split(6))
      }
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.IPLocation.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => IpUtils.ip2Long(str.split(""\\|"")(1))
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.IPLocationUDF.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => {
        val split = t.split(""\\|"")
        (split(2).toLong, split(3).toLong, split(6))
      }
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.IPLocationUDF.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => {
        val split = t.split(""\\|"")
        (split(2).toLong, split(3).toLong, split(6))
      }
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.IPLocationUDF.scala/udf/30.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => IpUtils.ip2Long(str.split(""\\|"")(1))
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.IPLocationUDF.scala/udf/37.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(longIp: Long) => IpUtils.binarySearchProvince(longIp, ipRules)
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.JoinDemo.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => {
        val split = str.split("" "")
        (split(0), split(1))
      }
"
"udf/spark_repos_1/1_ZhangJin1988_spark-sql/..src.main.scala.cn.zhangjin.spark.day02.JoinDemo.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => {
        val split = str.split("" "")
        val name = split(0)
        val age = split(1)
        val pcode = split(2)
        (name, age, pcode)
      }
"
"udf/spark_repos_1/1_zhenchao125_spark0722/..spark-sql-project.src.main.scala.com.atguigu.sql.project.SqlApp.scala/udf/19.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RemarkUDAF
"
"udf/spark_repos_1/1_zhenchao125_spark0722/..spark-sql.src.main.scala.com.atguigu.sql.day01.FDDSDemo1.scala/udf/14.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.atguigu.sql.day01.Person]
Call: filter

p => p.age > 10
"
"udf/spark_repos_1/1_zhenchao125_spark0722/..spark-sql.src.main.scala.com.atguigu.sql.day01.FDDSDemo1.scala/udf/18.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.atguigu.sql.day01.Person]
Call: filter

$""age"" > 10
"
"udf/spark_repos_1/1_zhenchao125_spark0722/..spark-sql.src.main.scala.com.atguigu.sql.day01.FDDSDemo1.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 10
"
"udf/spark_repos_1/1_zhenchao125_spark0722/..spark-sql.src.main.scala.com.atguigu.sql.day02.udf.MySumDemo.scala/udf/11.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MySum
"
"udf/spark_repos_1/1_zidenis_spark-overflow/..src.main.scala.br.ufrn.dimap.forall.spark.SparkOverflow.scala/udf/102.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(document: String) => cleanDocument(document)
"
"udf/spark_repos_1/1_zidenis_spark-overflow/..src.main.scala.br.ufrn.dimap.forall.spark.SparkOverflow.scala/udf/248.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(id: Long, countVector: Vector) =>
          (id, countVector)
      }
"
"udf/spark_repos_1/1_zidenis_spark-overflow/..src.main.scala.br.ufrn.dimap.forall.spark.SparkOverflow.scala/udf/98.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(tags: String) => isSparkRelated(tags)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase2.src.main.scala.cse512.SpatialQuery.scala/udf/39.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase2.src.main.scala.cse512.SpatialQuery.scala/udf/53.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase2.src.main.scala.cse512.SpatialQuery.scala/udf/65.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => ST_Within(pointString1, pointString2, distance)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase2.src.main.scala.cse512.SpatialQuery.scala/udf/79.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => ST_Within(pointString1, pointString2, distance)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase3.src.main.scala.cse512.HotcellAnalysis.scala/udf/17.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 0)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase3.src.main.scala.cse512.HotcellAnalysis.scala/udf/21.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 1)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase3.src.main.scala.cse512.HotcellAnalysis.scala/udf/25.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupTime: String) => HotcellUtils.CalculateCoordinate(pickupTime, 2)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase3.src.main.scala.cse512.HotcellAnalysis.scala/udf/44.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x1: Int, y1: Int, z1: Int, x2: Int, y2: Int, z2: Int) => HotcellUtils.ST_Within(x1, y1, z1, x2, y2, z2)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase3.src.main.scala.cse512.HotcellAnalysis.scala/udf/53.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sum: Int) => HotcellUtils.gScore(meanCount, std, numCells, sum)
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase3.src.main.scala.cse512.HotzoneAnalysis.scala/udf/13.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(string: String) => string.replace(""("", """").replace("")"", """")
"
"udf/spark_repos_1/1_zishanfu_NYC-Taxi-Trip-Analysis/..Phase3.src.main.scala.cse512.HotzoneAnalysis.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => HotzoneUtils.ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/180.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => bcProperties.value.contains(t.p)
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/202.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.p == RDFS.domain.toString
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/206.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.p == RDFS.range.toString
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/210.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.p == RDFS.subClassOf.toString
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/214.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.p == RDFS.subPropertyOf.toString
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/223.29.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.s == tp.getSubject.toString() || t.o == tp.getSubject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/228.27.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.p != RDF.`type`.toString
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/233.29.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.s == tp.getSubject.toString() || t.o == tp.getSubject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/305.29.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.s == tp.getSubject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/310.29.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.o == tp.getObject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/73.24.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.s.equals(s)
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/79.24.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.p.equals(p)
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataframe.scala/udf/85.24.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.o.equals(o)
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/187.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => bcProperties.value.contains(t.p)
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/223.25.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.s == tp.getSubject.toString() || t.o == tp.getSubject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/228.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.p != RDF.`type`.toString
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/233.25.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.s == tp.getSubject.toString() || t.o == tp.getSubject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/249.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.o == tp.getObject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/271.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.o == tp.getObject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/29.22.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.p == predicate.toString
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/294.25.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.o == tp.getObject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/312.25.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.s == tp.getSubject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/317.25.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.o == tp.getObject.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/324.23.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

_.o == tp.getPredicate.toString()
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/80.24.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.s.equals(s)
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/86.24.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.p.equals(p)
"
"udf/spark_repos_1/25_SANSA-Stack_SANSA-Inference/..sansa-inference-spark.src.main.scala.net.sansa_stack.inference.spark.backwardchaining.BackwardChainingReasonerDataset.scala/udf/92.24.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[net.sansa_stack.inference.spark.backwardchaining.RDFTriple]
Call: filter

t => t.o.equals(o)
"
"udf/spark_repos_1/29_ispras_pu4spark/..src.main.scala.ru.ispras.pu4spark.GradualReductionPULearner.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(prevLabel) === GradualReductionPULearner.undefLabel && curDF(curLabel) === GradualReductionPULearner.relNegLabel
"
"udf/spark_repos_1/29_ispras_pu4spark/..src.main.scala.ru.ispras.pu4spark.GradualReductionPULearner.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) === GradualReductionPULearner.posLabel
"
"udf/spark_repos_1/29_ispras_pu4spark/..src.main.scala.ru.ispras.pu4spark.GradualReductionPULearner.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) === GradualReductionPULearner.relNegLabel
"
"udf/spark_repos_1/29_ispras_pu4spark/..src.main.scala.ru.ispras.pu4spark.GradualReductionPULearner.scala/udf/35.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) !== GradualReductionPULearner.undefLabel
"
"udf/spark_repos_1/29_ispras_pu4spark/..src.main.scala.ru.ispras.pu4spark.GradualReductionPULearner.scala/udf/48.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) === GradualReductionPULearner.relNegLabel
"
"udf/spark_repos_1/29_ispras_pu4spark/..src.main.scala.ru.ispras.pu4spark.GradualReductionPULearner.scala/udf/52.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) === GradualReductionPULearner.relNegLabel
"
"udf/spark_repos_1/29_ispras_pu4spark/..src.main.scala.ru.ispras.pu4spark.TraditionalPULearner.scala/udf/19.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(prevLabel) === TraditionalPULearner.undefLabel && curDF(curLabel) === TraditionalPULearner.relNegLabel
"
"udf/spark_repos_1/29_ispras_pu4spark/..src.main.scala.ru.ispras.pu4spark.TraditionalPULearner.scala/udf/27.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) !== TraditionalPULearner.undefLabel
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/134.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""name000"", ""name001"", ""name002"", ""name003"", ""name004"") and !($""col0"" isin (""name001"", ""name002"", ""name003""))
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/114.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..core.src.test.scala.org.apache.spark.sql.PhoenixSuite.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""row005"", ""row001"", ""row002"") and !($""col0"" isin (""row001"", ""row002""))
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.phoenix.connector.PhoenixTest.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""COL1"") === ""test_row_1"" && df(""ID"") === 1L
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/64.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/88.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/100.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSourceBigDataTest.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSourceBigDataTest.scala/udf/53.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSourceBigDataTest.scala/udf/57.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSourceBigDataTest.scala/udf/67.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSourceBigDataTest.scala/udf/74.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSourceBigDataTest.scala/udf/81.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobAccessing2Clusters.scala/udf/79.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobAccessing2Clusters.scala/udf/83.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""5""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/59.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""40""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""20"" && $""key"" >= ""1""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.SqlAnalyze.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.SqlAnalyze.scala/udf/38.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""col1""
"
"udf/spark_repos_1/29_lw309637554_alicloud-hbase-spark-examples/..examples.src.main.scala.org.apache.spark.structstreaming.StructStreamingWordCount.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""value"" =!= """"
"
"udf/spark_repos_1/2_aamend_texata-r2-2017/..src.main.scala.com.aamend.texata.BreakingNewsExtractor.scala/udf/20.22.Dataset-GKG.filter","Type: org.apache.spark.sql.Dataset[com.aamend.texata.gdelt.GKG]
Call: filter

c => c.themes.contains(""ENV_GAS"") || c.themes.contains(""ENV_OIL"")
"
"udf/spark_repos_1/2_aamend_texata-r2-2017/..src.main.scala.com.aamend.texata.Brent.scala/udf/11.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

year(col(""DATE"")) >= 2015
"
"udf/spark_repos_1/2_aamend_texata-r2-2017/..src.main.scala.com.aamend.texata.gdelt.package.scala/udf/63.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!_.startsWith(""DATE"")
"
"udf/spark_repos_1/2_aamend_texata-r2-2017/..src.main.scala.com.aamend.texata.gdelt.package.scala/udf/65.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseGkg
"
"udf/spark_repos_1/2_aamend_texata-r2-2017/..src.main.scala.com.aamend.texata.gdelt.package.scala/udf/71.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseEvent
"
"udf/spark_repos_1/2_aamend_texata-r2-2017/..src.main.scala.com.aamend.texata.Influencers.scala/udf/18.22.Dataset-GKG.filter","Type: org.apache.spark.sql.Dataset[com.aamend.texata.gdelt.GKG]
Call: filter

c => c.themes.contains(""ENV_GAS"") || c.themes.contains(""ENV_OIL"")
"
"udf/spark_repos_1/2_adamdec_coursera-scala-spark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/104.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, round1(primaryNeeds), round1(work), round1(other))
      }
"
"udf/spark_repos_1/2_andersonkmi_kaggle-london-crime-data-spark/..src.main.scala.org.sharpsw.spark.LondonCrimeDataExplorer.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""year"" === year
"
"udf/spark_repos_1/2_anirudh985_TwitterSentimentAnalysis/..src.main.scala.Configure.Main.scala/udf/58.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Double](""predictedLabel"")
"
"udf/spark_repos_1/2_anirudh985_TwitterSentimentAnalysis/..src.main.scala.usingNaiveBayes.NaiveBayesModelUtil.scala/udf/33.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

{
        row => row.getAs[Double](""label"") == row.getAs[Double](""predictedLabel"")
      }
"
"udf/spark_repos_1/2_APEX-Network_Enterprise-CPX-Wallet/..src.main.scala.com.chinapex.sparksql.DashboardSpark.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ x => 
        val ipValue = x.get(0).asInstanceOf[String]
        IpResolutionService.geoDataFromMem(ipValue).city
      }
"
"udf/spark_repos_1/2_APEX-Network_Enterprise-CPX-Wallet/..src.main.scala.com.chinapex.sparksql.DashboardSpark.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

cc => {
        val city = cc.get(0).asInstanceOf[String]
        val count = cc.get(1).asInstanceOf[Long]
        val ll = longLats.find(_.city == city)
        (city, locally {
          val _t_m_p_3 = ll
          _t_m_p_3.map(x => x.long)
        }.getOrElse(0.0d), locally {
          val _t_m_p_4 = ll
          _t_m_p_4.map(x => x.lat)
        }.getOrElse(0.0d), count)
      }
"
"udf/spark_repos_1/2_bigO88_rmj-spark-with-drools-rule-engine/..src.main.scala.com.rmj.engine.SparkDataframeType.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""country"") === ""India""
"
"udf/spark_repos_1/2_bigO88_rmj-spark-with-drools-rule-engine/..src.main.scala.com.rmj.engine.SparkPivot.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""country"") === ""India""
"
"udf/spark_repos_1/2_cclient_spark-streaming-kafka-offset-mysql/..src.main.scala.com.github.cclient.spark.Offset.scala/udf/25.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""step="" + lastStep
"
"udf/spark_repos_1/2_CCutecute_DMP-YJX_ZZY_GPY/..src.main.scala.dmp.datatreating.Treating.scala/udf/13.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split("","")
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/134.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""name000"", ""name001"", ""name002"", ""name003"", ""name004"") and !($""col0"" isin (""name001"", ""name002"", ""name003""))
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/114.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..core.src.test.scala.org.apache.spark.sql.PhoenixSuite.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""row005"", ""row001"", ""row002"") and !($""col0"" isin (""row001"", ""row002""))
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/64.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/88.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/100.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobAccessing2Clusters.scala/udf/79.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobAccessing2Clusters.scala/udf/83.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""5""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/59.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""40""
"
"udf/spark_repos_1/2_chentiantai_spark-hgraph-connector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""20"" && $""key"" >= ""1""
"
"udf/spark_repos_1/2_cyrnguyen_NoSQLProject/..data_import.spark-data-import.src.main.scala.com.sparkProject.LoadEvents.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Actor1Code"") === ""USAGOV"" || col(""Actor1Code"") === ""USAGOV"" && col(""Actor1CountryCode"") === ""USA""
"
"udf/spark_repos_1/2_cyrnguyen_NoSQLProject/..data_import.spark-data-import.src.main.scala.com.sparkProject.LoadMentions.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""MentionType"") === 1
"
"udf/spark_repos_1/2_dgadiraju_ccdemo/..src.main.scala.retail_db.GetDailyProductRevenue.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""order_status"").isin(""COMPLETE"", ""CLOSED"")
"
"udf/spark_repos_1/2_DmitryPukhov_spark-trade/..src.main.scala.pro.dmitrypukhov.sparktrade.lambda.batch.BaseProcessor.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

convert
"
"udf/spark_repos_1/2_DmitryPukhov_spark-trade/..src.main.scala.pro.dmitrypukhov.sparktrade.lambda.speed.CandlesStream.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

converter.asCandle
"
"udf/spark_repos_1/2_DmitryPukhov_spark-trade/..src.main.scala.pro.dmitrypukhov.sparktrade.lambda.speed.TicksStream.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

converter.asTick
"
"udf/spark_repos_1/2_geoHeil_graphFrameStarter/..src.main.scala.myOrg.ExampleSQL.scala/udf/30.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""b.fraud"" === 1
"
"udf/spark_repos_1/2_geoHeil_graphFrameStarter/..src.main.scala.myOrg.ExampleSQL.scala/udf/60.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""b.fraud"" === 1
"
"udf/spark_repos_1/2_inbravo_spark-feature-set/..src.main.scala.com.inbravo.mr.utils.MovieUtils.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.apply(0).toString.contains(genre)
"
"udf/spark_repos_1/2_inbravo_spark-feature-set/..src.main.scala.com.inbravo.mr.utils.MovieUtils.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

column => column.apply(2).toString.contains(genre)
"
"udf/spark_repos_1/2_jingruhou_sparkLab/..src.SparkRDD.test1.scala/udf/28.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_1/2_jingruhou_sparkLab/..src.SparkSQL.Reflection.scala/udf/23.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_1/2_jingruhou_sparkLab/..src.SparkSQL.Reflection.scala/udf/30.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t.getAs[String](""name"")
"
"udf/spark_repos_1/2_jingruhou_sparkLab/..src.SparkSQL.Reflection.scala/udf/37.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_1/2_Jon10011_sparkStreaming/..src.main.scala.com.songdong.sql.SparkSQL03_UDAF.scala/udf/14.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udaf
"
"udf/spark_repos_1/2_LeeMoonCh_spark_ml_simple/..src.main.scala.com.lc.study.no5classification.n1classification.DecisionTreeClassifierDemo.scala/udf/18.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => dt.predict(f.getAs[Vector](""features""))
"
"udf/spark_repos_1/2_LeeMoonCh_spark_ml_simple/..src.main.scala.com.lc.study.no5classification.n1classification.RandomDorestClassifierDemo.scala/udf/12.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => {
      var age = 0.0d
      val ageA = f.getAs[String](""Age"")
      if (StringUtils.isNotBlank(ageA)) {
        age = ageA.toDouble
      } else {
        age = 1001
      }
      (f.getString(1).trim.toDouble, f.getString(2).trim.toDouble, f.getString(4), age, f.getString(6).trim.toDouble, f.getString(7).trim.toDouble)
    }
"
"udf/spark_repos_1/2_LeeMoonCh_spark_ml_simple/..src.main.scala.com.lc.study.no5classification.n1classification.RandomDorestClassifierDemo.scala/udf/29.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => {
      var age = 0.0d
      val ageA = f.getAs[String](""Age"")
      if (StringUtils.isNotBlank(ageA)) {
        age = ageA.toDouble
      } else {
        age = 1001
      }
      (f.getString(1).trim.toDouble, f.getString(3), age, f.getString(5).trim.toDouble, f.getString(6).trim.toDouble)
    }
"
"udf/spark_repos_1/2_liuxinsi_spark-recommender/..src.main.scala.com.lxs.recommender.tag.TAGRecommender.scala/udf/131.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getAs[String](""TAG"")
"
"udf/spark_repos_1/2_liuxinsi_spark-recommender/..src.main.scala.com.lxs.recommender.tag.TAGRecommender.scala/udf/250.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => Order(r.getAs[String](""UID""), r.getAs[String](""PID""), r.getAs[Int](""ORDER_COUNT""))
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.utility.GHCNData.scala/udf/26.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

apply
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.utility.GHCNStation.scala/udf/27.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

apply
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.GHCNStationsColor2018.scala/udf/20.22.Dataset-GHCNData.filter","Type: org.apache.spark.sql.Dataset[utility.GHCNData]
Call: filter

_.element == ""TMAX""
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SimpleAppSQL.scala/udf/10.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""val"")
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/24.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != firstLine
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/26.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

CensusData.parseLine
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/31.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/36.21.Dataset-CensusData.map","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: map

_.age
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/42.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.age >= 50
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/46.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/50.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.maritalStatus == ""Married-civ-spouse""
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/54.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/58.20.Dataset-CensusData.map","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: map

_.age
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/69.23.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.hoursPerWeek > 40
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/14.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""income"" === "">50K""
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/20.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'age >= 50
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'income === "">50K""
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'maritalStatus === ""Married-civ-spouse""
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'income === "">50K""
"
"udf/spark_repos_1/2_MarkCLewis_SIGCSE2019Spark/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/43.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'hoursPerWeek > 40
"
"udf/spark_repos_1/2_MartinCayuelas_clickFinder/..src.main.scala.utils.Tools.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataset(""label"") === 0
"
"udf/spark_repos_1/2_ONSdigital_sbr-enterprise-assembler/..src.main.scala.closures.BaseClosure.scala/udf/101.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ruToLinks(row, appconf)
"
"udf/spark_repos_1/2_ONSdigital_sbr-enterprise-assembler/..src.main.scala.closures.BaseClosure.scala/udf/108.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => louToLinks(row, appconf)
"
"udf/spark_repos_1/2_ONSdigital_sbr-enterprise-assembler/..src.main.scala.closures.BaseClosure.scala/udf/115.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => leuToLinks(row, appconf)
"
"udf/spark_repos_1/2_ONSdigital_sbr-enterprise-assembler/..src.main.scala.closures.BaseClosure.scala/udf/146.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => rowToLocalUnit(row, appconf)
"
"udf/spark_repos_1/2_ONSdigital_sbr-enterprise-assembler/..src.main.scala.closures.BaseClosure.scala/udf/163.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => rowToLegalUnit(row, appconf)
"
"udf/spark_repos_1/2_ONSdigital_sbr-enterprise-assembler/..src.main.scala.closures.BaseClosure.scala/udf/180.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => rowToReportingUnit(row, appconf)
"
"udf/spark_repos_1/2_ONSdigital_sbr-enterprise-assembler/..src.main.scala.closures.BaseClosure.scala/udf/197.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => rowToEnt(row, appconf)
"
"udf/spark_repos_1/2_PacktPublishing_Big-Data-Analytics-Projects-with-Apache-Spark/..src.main.scala.com.example.CreatingDatasets.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_1/2_PacktPublishing_Big-Data-Analytics-Projects-with-Apache-Spark/..src.main.scala.com.example.ProgrammingGuideSQL.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_1/2_PacktPublishing_Big-Data-Analytics-Projects-with-Apache-Spark/..src.main.scala.com.tomekl007.anomalydetection.RunKMeans.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
        val cluster = row.getAs[Int](""cluster"")
        val vec = row.getAs[Vector](""scaledFeatureVector"")
        Vectors.sqdist(centroids(cluster), vec) >= farthestDistanceBetweenTwoNormalClusters
      }
"
"udf/spark_repos_1/2_PacktPublishing_Big-Data-Analytics-Projects-with-Apache-Spark/..src.main.scala.com.tomekl007.anomalydetection.RunKMeans.scala/udf/62.19.Dataset-Vector).map","Type: org.apache.spark.sql.Dataset[(Int, org.apache.spark.ml.linalg.Vector)]
Call: map

{
        case (cluster, vec) =>
          Vectors.sqdist(centroids(cluster), vec)
      }
"
"udf/spark_repos_1/2_PacktPublishing_Big-Data-Analytics-Projects-with-Apache-Spark/..src.test.scala.com.tomekl007.SparkApisTests.scala/udf/28.22.Dataset-UserData.filter","Type: org.apache.spark.sql.Dataset[com.tomekl007.UserData]
Call: filter

_.userId == ""a""
"
"udf/spark_repos_1/2_Thpffcj_cloud-computing/..spark-mllib.src.main.scala.cn.edu.nju.DataProcessing.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs(""behavior"").equals(""play"")
"
"udf/spark_repos_1/2_Thpffcj_cloud-computing/..spark-mllib.src.main.scala.cn.edu.nju.DataProcessing.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => gameNumber.get(row.getAs(""gameName"").toString) > 2
"
"udf/spark_repos_1/2_Thpffcj_cloud-computing/..spark-mllib.src.main.scala.cn.edu.nju.DataProcessing.scala/udf/51.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val userId = row.getAs(""userId"").toString
        val gameName = row.getAs(""gameName"").toString
        var duration = (row.getAs(""duration"").toString.toDouble / maxTimeMap.get(gameName) * 10).formatted(""%.2f"")
        if (duration.toDouble < 1.0d) {
          duration = ""1.0""
        }
        val gameId = gameMap.get(gameName)
        val random = rand.nextDouble()
        (userId, gameId, gameName, duration, random)
      }
"
"udf/spark_repos_1/2_Thpffcj_cloud-computing/..spark-mllib.src.main.scala.cn.edu.nju.EmotionAnalysis.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => (locally {
        val _t_m_p_2 = line.split("" "")
        _t_m_p_2.filter(!_.equals("" ""))
      }, 0, rand.nextDouble())
"
"udf/spark_repos_1/2_Thpffcj_cloud-computing/..spark-mllib.src.main.scala.cn.edu.nju.EmotionAnalysis.scala/udf/26.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => (locally {
        val _t_m_p_4 = line.split("" "")
        _t_m_p_4.filter(!_.equals("" ""))
      }, 1, rand.nextDouble())
"
"udf/spark_repos_1/2_Thpffcj_cloud-computing/..spark-mllib.src.main.scala.cn.edu.nju.EmotionAnalysis.scala/udf/50.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => locally {
        val _t_m_p_6 = line.split("" "")
        _t_m_p_6.filter(!_.equals("" ""))
      }
"
"udf/spark_repos_1/2_Thpffcj_cloud-computing/..spark-streaming.src.main.scala.cn.edu.nju.BatchProcess.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val name = row.getAs(""name"").toString
        val types = ""game""
        val recommendations_up = row.getAs(""recommendations_up"").toString
        val date = DateUtils.tranTimestampToString(row.getAs(""time""))
        println((name, types, recommendations_up, date))
        (name, types, recommendations_up, date)
      }
"
"udf/spark_repos_1/2_VirtusLab_spark_sql_under_the_hood/..src.main.scala.com.virtuslab.sparksql.MainClass.scala/udf/13.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.virtuslab.sparksql.MainClass.Person]
Call: filter

_.age.exists(_ > 65)
"
"udf/spark_repos_1/2_YuhanSun_CSE512-Project-Phase1-Template/..src.main.scala.cse512.SpatialQuery.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => true
"
"udf/spark_repos_1/2_YuhanSun_CSE512-Project-Phase1-Template/..src.main.scala.cse512.SpatialQuery.scala/udf/32.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => true
"
"udf/spark_repos_1/2_YuhanSun_CSE512-Project-Phase1-Template/..src.main.scala.cse512.SpatialQuery.scala/udf/45.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => true
"
"udf/spark_repos_1/2_YuhanSun_CSE512-Project-Phase1-Template/..src.main.scala.cse512.SpatialQuery.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => true
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.etl.AccesslogETL2.scala/udf/84.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$curHourTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.etl.AccesslogETL2.scala/udf/88.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>'$beginTime' and acctime<'$curHourTime' ""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.etl.AccesslogETL4.scala/udf/79.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$curHourTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.etl.AccesslogETL4.scala/udf/83.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>'$beginTime' and acctime<'$curHourTime' ""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.etl.AccesslogETL.scala/udf/93.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$curHourTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.etl.AccesslogETL.scala/udf/97.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>'$preHourTime' and acctime<'$curHourTime' ""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcLogQuery.scala/udf/176.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$beginTime' and acctime<='$endTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcLogQuery.scala/udf/181.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$beginTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcLogQuery.scala/udf/184.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime<='$endTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcLogQuery.scala/udf/190.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$beginTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcLogQuery.scala/udf/195.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime<='$endTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcLogQuery.scala/udf/92.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(strIp: String) => ip2Long(strIp)
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcLogQuery.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/105.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

condition
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/155.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$beginTime' and acctime<='$endTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/160.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$beginTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/163.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime<='$endTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/169.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$beginTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/174.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime<='$endTime'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""hid='$hid'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/78.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(strIp: String) => ip2Long(strIp)
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessOrcTextLogQuery.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessQueryMain.scala/udf/36.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""url='$b64Url'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessQueryMain.scala/udf/42.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""domain='$domain'""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.AccessTextLogQuery.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""acctime>='$beginTime' and acctime<='$endTime' ""
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.IDCAccessLogMR.scala/udf/100.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

strfilter
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.query.IDCAccessLogMR.scala/udf/47.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(strIp: String) => {
            var result: Long = 0
            try {
              if (strIp != null && !"""".equals(strIp.trim())) {
                val position1 = strIp.indexOf(""."")
                val position2 = strIp.indexOf(""."", position1 + 1)
                val position3 = strIp.indexOf(""."", position2 + 1)
                if (position1 > -1 && position2 > -1 && position3 > -1) {
                  val ip0 = strIp.substring(0, position1).toLong
                  val ip1 = strIp.substring(position1 + 1, position2).toLong
                  val ip2 = strIp.substring(position2 + 1, position3).toLong
                  val ip3 = strIp.substring(position3 + 1).toLong
                  result = (ip0 << 24) + (ip1 << 16) + (ip2 << 8) + ip3
                }
              }
            } catch {
              case e: Exception =>
                e.printStackTrace()
            }
            result
          }
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.stat.AccessLogStatHour1.scala/udf/54.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(domain: String) => {
        var topDomain = """"
        pattern.matcher(domain.trim).matches() match {
          case true =>
            val arrDomain = domain.split(""\\."")
            val len = arrDomain.length
            var last_1 = ""-1""
            var last_2 = ""-1""
            var last_3 = ""-1""
            if (len > 2) {
              last_1 = arrDomain(len - 1)
              last_2 = arrDomain(len - 2)
              last_3 = arrDomain(len - 3)
            } else if (len > 1) {
              last_1 = arrDomain(len - 1)
              last_2 = arrDomain(len - 2)
            }
            if (bdv_inDomain.contains(last_1)) {
              topDomain = last_2 + ""."" + last_1
            } else if (bdv_countryDomain.contains(last_1)) {
              if (bdv_inDomain.contains(last_2) || bdv_areaDomain.contains(last_2 + ""."" + last_1)) {
                topDomain = last_3 + ""."" + last_2 + ""."" + last_1
              } else {
                topDomain = last_2 + ""."" + last_1
              }
            } else {
              topDomain = """"
            }
            topDomain
          case _ =>
            ""-1""
        }
      }
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.stat.AccessLogStatHour2.scala/udf/53.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(domain: String) => {
        var topDomain = """"
        pattern.matcher(domain.trim).matches() match {
          case true =>
            val arrDomain = domain.split(""\\."")
            val len = arrDomain.length
            var last_1 = ""-1""
            var last_2 = ""-1""
            var last_3 = ""-1""
            if (len > 2) {
              last_1 = arrDomain(len - 1)
              last_2 = arrDomain(len - 2)
              last_3 = arrDomain(len - 3)
            } else if (len > 1) {
              last_1 = arrDomain(len - 1)
              last_2 = arrDomain(len - 2)
            }
            if (bdv_inDomain.contains(last_1)) {
              topDomain = last_2 + ""."" + last_1
            } else if (bdv_countryDomain.contains(last_1)) {
              if (bdv_inDomain.contains(last_2) || bdv_areaDomain.contains(last_2 + ""."" + last_1)) {
                topDomain = last_3 + ""."" + last_2 + ""."" + last_1
              } else {
                topDomain = last_2 + ""."" + last_1
              }
            } else {
              topDomain = """"
            }
            topDomain
          case _ =>
            ""-1""
        }
      }
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.stat.AccessLogStatHour.scala/udf/111.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(domain: String) => {
        var matchForeign = 0
        val regex = "".*\\.(\\w+)(?:\\:\\d+|)$"".r
        if (regex.pattern.matcher(domain).matches()) {
          val regex(suffix) = domain
          if (dbv_foreDomain.contains(suffix)) {
            matchForeign = 1
          }
        }
        matchForeign
      }
"
"udf/spark_repos_1/2_zcw5116_DpiNew/..src.main.scala.com.zyuc.dpi.stat.AccessLogStatHour.scala/udf/167.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(domain: String) => {
        var topDomain = """"
        pattern.matcher(domain.trim).matches() match {
          case true =>
            val arrDomain = domain.split(""\\."")
            val len = arrDomain.length
            var last_1 = ""-1""
            var last_2 = ""-1""
            var last_3 = ""-1""
            if (len > 2) {
              last_1 = arrDomain(len - 1)
              last_2 = arrDomain(len - 2)
              last_3 = arrDomain(len - 3)
            } else if (len > 1) {
              last_1 = arrDomain(len - 1)
              last_2 = arrDomain(len - 2)
            }
            if (bdv_inDomain.contains(last_1)) {
              topDomain = last_2 + ""."" + last_1
            } else if (bdv_countryDomain.contains(last_1)) {
              if (bdv_inDomain.contains(last_2) || bdv_areaDomain.contains(last_2 + ""."" + last_1)) {
                topDomain = last_3 + ""."" + last_2 + ""."" + last_1
              } else {
                topDomain = last_2 + ""."" + last_1
              }
            } else {
              topDomain = """"
            }
            topDomain
          case _ =>
            ""-1""
        }
      }
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day01.option.TypeOpt.scala/udf/12.24.Dataset-People.filter","Type: org.apache.spark.sql.Dataset[com.atguigu.structure.streaming.day01.option.People]
Call: filter

_.age > 20
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day01.option.TypeOpt.scala/udf/14.19.Dataset-People.map","Type: org.apache.spark.sql.Dataset[com.atguigu.structure.streaming.day01.option.People]
Call: map

_.name
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day02.duplicate.DropDumplicate1.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), Timestamp.valueOf(arr(1)), arr(2))
      }
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day02.joint.SteamingStatic1.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val splits = line.split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day02.joint.SteamingStatic.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val splits = line.split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day02.joint.SteamSteamJoint.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), arr(1), Timestamp.valueOf(arr(2)))
      }
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day02.joint.SteamSteamJoint.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), arr(1).toInt, Timestamp.valueOf(arr(2)))
      }
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day02.sink.KafkaSinkBatch.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0) + "","" + row.getLong(1)
"
"udf/spark_repos_1/2_zhenchao125_structure-streaming/..src.main.scala.com.atguigu.structure.streaming.day02.window.Window1.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val split: Array[String] = line.split("","")
        (split(0), split(1))
      }
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.common.FeatureUtils.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""df_type"") === 0
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.common.FeatureUtils.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""df_type"") === 1
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.JData.JDataByTimeSeries.OrderAndActionCluster.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(user_id: Int, types: Int, prediction: Int, date: Timestamp) =>
          val new_prediction = allMap(prediction)
          (user_id.toString, types, new_prediction, date)
      }
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.JData.JDataByTimeSeries.OrderAndActionCluster.scala/udf/72.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""o_user_id"".isNotNull
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.JData.JDataByTimeSeries.OrderAndActionCluster.scala/udf/74.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ row => 
        val userID = row.getInt(5)
        val date = row.getTimestamp(7)
        val sku = row.getInt(6)
        val aType = 0
        val aNum = 0
        val oNum = row.getInt(8)
        val types = 1
        (userID, date, sku, aType, aNum, oNum, types)
      }
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.JData.JDataByTimeSeries.OrderAndActionCluster.scala/udf/89.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""user_id"".isNotNull
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.JData.JDataByTimeSeries.OrderAndActionCluster.scala/udf/91.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""o_user_id"".isNull
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.JData.JDataByTimeSeries.OrderAndActionCluster.scala/udf/93.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ row => 
        val userID = row.getInt(0)
        val date = row.getTimestamp(2)
        val sku = row.getInt(1)
        val aType = row.getInt(4)
        val aNum = row.getInt(3)
        val oNum = 0
        val types = 0
        (userID, date, sku, aType, aNum, oNum, types)
      }
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.eloRecommendation.DataCollect.scala/udf/27.22.Dataset-caseTransactions.filter","Type: org.apache.spark.sql.Dataset[org.lzy.kaggle.eloRecommendation.DataCollect.caseTransactions]
Call: filter

_.authorized_flag == 1
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.eloRecommendation.DataCollect.scala/udf/31.22.Dataset-caseTransactions.filter","Type: org.apache.spark.sql.Dataset[org.lzy.kaggle.eloRecommendation.DataCollect.caseTransactions]
Call: filter

_.authorized_flag == 0
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.eloRecommendation.DataExplore.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => ThanOneMerchants.contains(row.getString(0))
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.eloRecommendation.DataExplore.scala/udf/64.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""authorized_flag"" === ""Y""
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.eloRecommendation.DataExplore.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""authorized_flag"" === ""N""
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.googleAnalytics.GetMeansOfResult.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

raw => {
        val fullVisitorId = raw.getString(0).toString
        val transactionRevenue = math.expm1(raw.getString(1).toDouble) match {
          case a if a >= 1000000 => 1000000
          case b => b
        }
        (fullVisitorId, transactionRevenue)
      }
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.FeatureExact.scala/udf/173.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val id = row.getString(0)
          var arr = Array[Double]()
          for (i <- columns.indices) {
            arr = arr :+ row.getDouble(i + 1)
          }
          (id, arr)
        }
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.FeatureExact.scala/udf/212.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.FeatureExact.scala/udf/216.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isNonUgly_udf($""id"")
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.FeatureExact.scala/udf/246.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val id = row.getString(0)
        val means: Double = locally {
          val _t_m_p_25 = transact_cols.indices
          _t_m_p_25.map(i => {
            val x = row.getDouble(i + 1)
            if (x == 0) 0 else math.log1p(x)
          })
        }.sum / transact_cols.length
        (id, math.expm1(means))
      }
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.FeatureExact.scala/udf/283.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""compiled_leak"" > 0
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.FeatureExact.scala/udf/287.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""compiled_leak"" === ($""target"")
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.FeatureExact.scala/udf/326.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val id = row.getString(0)
        val means: Double = locally {
          val _t_m_p_33 = transact_cols.indices
          _t_m_p_33.map(i => {
            val x = row.getDouble(i + 1)
            if (x == 0) 0 else math.log1p(x)
          })
        }.sum / transact_cols.length
        (id, math.expm1(means))
      }
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.FeatureExact.scala/udf/353.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""compiled_leak"" > 0
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.ShowFeatures.scala/udf/25.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""58e2e02e6"" =!= 0d
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.TrainModel.scala/udf/70.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""type"" === 0
"
"udf/spark_repos_1/39_kobelzy_SparkML/..src.main.scala.org.lzy.kaggle.kaggleSantander.TrainModel.scala/udf/74.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""type"" === 1
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/15.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

replaceEmptyStr _
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x == 1900
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x != 1900
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.convertYear _
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_04.scala.2.0.0.src.main.scala.org.sparksamples.MovieData.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.convertYear _
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_04.scala.2.0.0.src.main.scala.org.sparksamples.RatingData.scala/udf/49.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getCurrentHour _
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_04.scala.2.0.0.src.main.scala.org.sparksamples.RatingData.scala/udf/55.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

assignTod _
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_05.2.0.0.scala-spark-app.src.main.scala.com.spark.recommendation.FeatureExtraction.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_08.scala.2.0.0.src.main.scala.org.sparksamples.kmeans.BisectingKMeans.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""prediction == "" + i
"
"udf/spark_repos_1/39_ml-resources_spark-ml/..Chapter_08.scala.2.0.0.src.main.scala.org.sparksamples.kmeans.MovieLensKMeans.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""prediction == "" + i
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.clustering.algorithms.Kmeans.scala/udf/41.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => (f.getInt(f.size - 1), f.getInt(0).toLong)
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.kge.linkprediction.crossvalidation.kFold.scala/udf/25.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""k"" =!= i
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.kge.linkprediction.crossvalidation.kFold.scala/udf/31.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""k"" === i
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalWithDataframeCrossJoin.scala/udf/104.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Jsim"" > 0.6d
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalyDetectionWithCountVetcorizerModel.scala/udf/106.27.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetA.id"".isNotNull
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalyDetectionWithCountVetcorizerModel.scala/udf/108.25.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetB.id"".isNotNull
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalyDetectionWithCountVetcorizerModel.scala/udf/110.23.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetA.id"" =!= ($""datasetB.id"")
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalyDetectionWithCountVetcorizerModel.scala/udf/283.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""value < $lowerRange or value > $upperRange""
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalyDetectionWithCountVetcorizerModel.scala/udf/290.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

matcher _
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalyWithHashingTF.scala/udf/113.27.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetA.id"".isNotNull
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalyWithHashingTF.scala/udf/115.25.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetB.id"".isNotNull
"
"udf/spark_repos_1/39_SANSA-Stack_SANSA-ML/..sansa-ml-spark.src.main.scala.net.sansa_stack.ml.spark.outliers.anomalydetection.AnomalyWithHashingTF.scala/udf/117.23.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetA.id"" =!= ($""datasetB.id"")
"
"udf/spark_repos_1/3_anupama2504_Apache-Spark2.0-With_SCALA/..SparkScala.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_1/3_in-cal_incal-spark_ml/..src.main.scala.examples.SimpleClustering.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
        val length = r(0).asInstanceOf[Double]
        val width = r(1).asInstanceOf[Double]
        val clusterClass = r(2).asInstanceOf[Int]
        (clusterClass + 1, length, width)
      }
"
"udf/spark_repos_1/3_in-cal_incal-spark_ml/..src.main.scala.org.incal.spark_ml.transformers.SamplingTransformer.scala/udf/25.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df.col(""labelString"") === labelOrAlias
"
"udf/spark_repos_1/3_in-cal_incal-spark_ml/..src.main.scala.org.incal.spark_ml.transformers.SamplingTransformer.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""labelString"").isin(labels: _*)
"
"udf/spark_repos_1/3_michalzeman_laPlatform/..data-platform.common-data-api.src.main.scala.com.la.platform.batch.services.TransformDataServiceImpl.scala/udf/30.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val strVector = row.getAs[String](""message"")
          val stringArray = strVector.split("","")
          val label = stringArray(stringArray.length - 1).toDouble
          val features = Vectors.dense(mapFeatures(stringArray(0).toDouble, stringArray(1).toDouble, 6).toArray)
          new LabeledPoint(label, features)
        }
"
"udf/spark_repos_1/3_mraad_spark-pico-path/..src.main.scala.com.esri.PicoPathApp.scala/udf/21.19.Dataset-PicoPath.map","Type: org.apache.spark.sql.Dataset[com.esri.PicoPath]
Call: map

pp => {
        val prevTime = pp.pt.getTime
        val nextTime = pp.nt.getTime
        val seconds = (nextTime - prevTime) / 1000L
        val meters = Haversine.distance(pp.py, pp.px, pp.ny, pp.nx)
        val kph = 3.6d * meters / seconds
        val dy = pp.ny - pp.py
        val dx = pp.nx - pp.px
        val deg = math.atan2(dy, dx).toDegrees
        (pp.id, meters, seconds, kph, deg, pp.wkt)
      }
"
"udf/spark_repos_1/3_mraad_spark-pico-path/..src.main.scala.com.esri.TrackApp.scala/udf/25.19.Dataset-Track.map","Type: org.apache.spark.sql.Dataset[com.esri.Track]
Call: map

track => (track.trackID, track.wkt)
"
"udf/spark_repos_1/3_mraad_spark-pico-path/..src.main.scala.com.esri.TrackApp.scala/udf/9.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new TrackAssembler()
"
"udf/spark_repos_1/3_nielsenbe_Spark-Wiki-Parser/..src.main.scala.com.github.nielsenbe.sparkwikiparser.wikipedia.sparkdbbuild.DatabaseBuildFull.scala/udf/23.22.Dataset-WikipediaPage.filter","Type: org.apache.spark.sql.Dataset[com.github.nielsenbe.sparkwikiparser.wikipedia.WikipediaPage]
Call: filter

_.parserMessage != ""SUCCESS""
"
"udf/spark_repos_1/3_PacktPublishing_Modern-Scala-Projects/..Chapter05.src.main.scala.com.packt.modern.chapter5.OutlierAlgorithm.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

labelledFeatureVectorRow => probabilityDensity(labelledFeatureVectorRow.getAs(0), broadcastVariable.value)
"
"udf/spark_repos_1/3_PacktPublishing_Modern-Scala-Projects/..Chapter05.src.main.scala.com.packt.modern.chapter5.OutlierAlgorithm.scala/udf/52.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          probRow => if (probRow.getDouble(0) < broadcastTerm) {
            1.0d
          } else 0.0d
        }
"
"udf/spark_repos_1/3_PacktPublishing_Modern-Scala-Projects/..Chapter05.src.main.scala.com.packt.modern.chapter5.OutlierAlgorithm.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

labelAndPrediction => labelAndPrediction.getAs(""PDF"") == targetLabel && labelAndPrediction.get(1) == finalPrediction
"
"udf/spark_repos_1/3_PacktPublishing_Modern-Scala-Projects/..Chapter07.src.main.scala.com.packt.modern.chapter7.RecSystem.scala/udf/13.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

salesOrder => Rating(salesOrder.getInt(0), salesOrder.getInt(2), salesOrder.getDouble(6))
"
"udf/spark_repos_1/3_PacktPublishing_Modern-Scala-Projects/..Chapter07.src.main.scala.com.packt.modern.chapter7.RecSystem.scala/udf/29.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

salesLead => (salesLead.getInt(0), salesLead.getInt(2))
"
"udf/spark_repos_1/3_phuonglh_vlp/..tag.src.main.scala.vlp.tag.Tagger.scala/udf/76.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_1/3_phuonglh_vlp/..tcl.src.main.scala.vlp.tcl.Classifier.scala/udf/105.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getString(1), row.getDouble(2).toInt, row.getAs[DenseVector](3))
"
"udf/spark_repos_1/3_phuonglh_vlp/..tcl.src.main.scala.vlp.tcl.Classifier.scala/udf/132.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[DenseVector](0).values
"
"udf/spark_repos_1/3_phuonglh_vlp/..tcl.src.main.scala.vlp.tcl.Classifier.scala/udf/79.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_1/3_phuonglh_vlp/..tdp.src.main.scala.vlp.tdp.Classifier.scala/udf/164.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_1/3_phuonglh_vlp/..vdr.src.main.scala.vlp.vdr.Restorer.scala/udf/123.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getSeq[String](0)
"
"udf/spark_repos_1/3_phuonglh_vlp/..vdr.src.main.scala.vlp.vdr.Restorer.scala/udf/126.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getSeq[String](0)
"
"udf/spark_repos_1/3_phuonglh_vlp/..vdr.src.main.scala.vlp.vdr.Restorer.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_1/3_phuonglh_vlp/..vdr.src.main.scala.vlp.vdr.Restorer.scala/udf/78.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Seq[String]](0), row.getAs[Seq[String]](1))
"
"udf/spark_repos_1/3_phuonglh_vlp/..vdr.src.main.scala.vlp.vdr.Restorer.scala/udf/86.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

p => if (p._1 == p._2) 1 else 0
"
"udf/spark_repos_1/3_phuonglh_vlp/..vdr.src.main.scala.vlp.vdr.Restorer.scala/udf/88.22.Dataset-Int.filter","Type: org.apache.spark.sql.Dataset[Int]
Call: filter

_ > 0
"
"udf/spark_repos_1/3_phuonglh_vlp/..vdr.src.main.scala.vlp.vdr.Restorer.scala/udf/95.26.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

p => p._1 != p._2
"
"udf/spark_repos_1/3_qianxiyong_MovieRecommendSystem/..recommender.ContentRecommender.src.main.scala.com.atguigu.content.ContentRecommender.scala/udf/21.19.Dataset-Movie.map","Type: org.apache.spark.sql.Dataset[com.atguigu.content.Movie]
Call: map

x => (x.mid, x.name, locally {
        val _t_m_p_2 = x.genres
        _t_m_p_2.map(c => if (c == ' ') ' ' else c)
      })
"
"udf/spark_repos_1/3_qianxiyong_MovieRecommendSystem/..recommender.ContentRecommender.src.main.scala.com.atguigu.content.ContentRecommender.scala/udf/36.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          case row =>
            (row.getAs[Int](""mid""), row.getAs[SparseVector](""features"").toArray)
        }
"
"udf/spark_repos_1/3_qianxiyong_MovieRecommendSystem/..recommender.StatisticsRecommender.src.main.scala.com.atguigu.statistics.StatisticsRecommender.scala/udf/34.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Int) => simpleDateFormat.format(new Date(x * 1000L)).toInt
"
"udf/spark_repos_1/3_yangzheng10_DianpingCommnetDataAnalysis/..dazhong_comment_analysis.src.main.scala.cn.yz0515.Step02_AtisticalData.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Int, features: Vector) =>
          LabeledPoint(label.toInt, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_1/3_yangzheng10_DianpingCommnetDataAnalysis/..dazhong_comment_analysis.src.main.scala.cn.yz0515.Step02_AtisticalDataV2.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Int, features: Vector) =>
          LabeledPoint(label.toInt, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_1/3_yangzheng10_DianpingCommnetDataAnalysis/..Step02_AtisticalData.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Int, features: Vector) =>
          LabeledPoint(label.toInt, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_1/3_yangzheng10_DianpingCommnetDataAnalysis/..Step02_AtisticalDataV2.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Int, features: Vector) =>
          LabeledPoint(label.toInt, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_1/3_yanxiaole_spark-imbalanced-learn/..src.main.scala.imblearn.undersampling.RandomUnderSampler.scala/udf/12.22.Dataset-LabeledPoint.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.ml.feature.LabeledPoint]
Call: filter

p => p.label == 1
"
"udf/spark_repos_1/3_yanxiaole_spark-imbalanced-learn/..src.main.scala.imblearn.undersampling.RandomUnderSampler.scala/udf/21.22.Dataset-LabeledPoint.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.ml.feature.LabeledPoint]
Call: filter

p => p.label == 0
"
"udf/spark_repos_1/3_yanxiaole_spark-imbalanced-learn/..src.main.scala.imblearn.undersampling.RandomUnderSampler.scala/udf/25.22.Dataset-LabeledPoint.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.ml.feature.LabeledPoint]
Call: filter

p => p.label == 1
"
"udf/spark_repos_1/3_yanxiaole_spark-imbalanced-learn/..src.main.scala.imblearn.undersampling.RandomUnderSampler.scala/udf/9.22.Dataset-LabeledPoint.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.ml.feature.LabeledPoint]
Call: filter

p => p.label == 0
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparkml.NOAAClustering.scala/udf/23.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
      val id = line.substring(0, 11)
      val lat = line.substring(12, 20).trim.toDouble
      val lon = line.substring(21, 30).trim.toDouble
      val elev = line.substring(31, 37).trim.toDouble
      val name = line.substring(41, 71)
      Station(id, lat, lon, elev, name)
    }
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparkml.NOAAClustering.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'cluster === cluster
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparkml.NOAAClustering.scala/udf/61.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'measure === ""TMAX""
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparkml.NOAAClustering.scala/udf/65.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'measure === ""TMIN""
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparkml.NOAAClustering.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'measure === ""PRCP""
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparksql.BLSTyped.scala/udf/18.20.Dataset-LAData.filter","Type: org.apache.spark.sql.Dataset[sparksql.LAData]
Call: filter

'id.endsWith(""03"") && 'year === 2016 && 'period === ""M10""
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparksql.BLSTyped.scala/udf/22.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
      val p = locally {
        val _t_m_p_3 = line.split(""\t"")
        _t_m_p_3.map(_.trim)
      }
      Series(p(0), p(2), p(3), p(6))
    }
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparksql.BLSTyped.scala/udf/35.20.Dataset-ZipData.filter","Type: org.apache.spark.sql.Dataset[sparksql.ZipData]
Call: filter

'lat.isNotNull
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparksql.BLSTyped.scala/udf/39.17.Dataset-((String, String), Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String), Double, Double)]
Call: map

{
      case ((county, state), lat, lon) =>
        ZipCountyData(lat, lon, state, county)
    }
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparksql.NOAAData.scala/udf/33.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""mtype"" === ""TMAX""
"
"udf/spark_repos_1/47_MarkCLewis_BigDataAnalyticswithSpark/..src.main.scala.sparksql.NOAAData.scala/udf/37.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'mtype === ""TMIN""
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexBall.scala/udf/46.27.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => x.getAs[Double](""MaxProb"")
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexCH.scala/udf/67.27.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => x.getAs[Double](""MaxProb"")
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexDB.scala/udf/33.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => Vectors.sqdist(centroide, x.getAs[org.apache.spark.ml.linalg.Vector](""features""))
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexDB.scala/udf/62.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => Vectors.sqdist(centroide, x.getAs[org.apache.spark.ml.linalg.Vector](""features""))
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexDB.scala/udf/93.27.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => x.getAs[Double](""MaxProb"")
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexHartigan.scala/udf/45.27.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => x.getAs[Double](""MaxProb"")
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexKL.scala/udf/114.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => Vectors.sqdist(centroideDataSet.asML, x.getAs[org.apache.spark.ml.linalg.Vector](""features""))
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexKL.scala/udf/159.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => Vectors.sqdist(centroideDataSet.asML, x.getAs[org.apache.spark.ml.linalg.Vector](""features""))
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexKL.scala/udf/48.29.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => x.getAs[Double](""MaxProb"")
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.indexes.IndexKL.scala/udf/69.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => Vectors.sqdist(centroideDataSet.asML, x.getAs[org.apache.spark.ml.linalg.Vector](""features""))
"
"udf/spark_repos_1/4_DanielTizon_ClusteringMetrics/..src.main.scala.clustering.metrics.Utils.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => {
        val data = x.getAs[org.apache.spark.ml.linalg.Vector](""features"")
        var isOutlier = false
        for (i <- 0 to data.size - 1) {
          if (data(i) > factor || data(i) < -factor) isOutlier = true
        }
        !isOutlier
      }
"
"udf/spark_repos_1/4_marianokamp_neartime/..e2e.src.main.scala.persistence.PersistenceDemo.scala/udf/83.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => 1
        }
"
"udf/spark_repos_1/4_mraad_arcgis-alluxio/..src.main.scala.com.esri.HexApp.scala/udf/32.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(d: Double) => d.toMercatorX
"
"udf/spark_repos_1/4_mraad_arcgis-alluxio/..src.main.scala.com.esri.HexApp.scala/udf/36.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(d: Double) => d.toMercatorY
"
"udf/spark_repos_1/4_mraad_arcgis-alluxio/..src.main.scala.com.esri.HexApp.scala/udf/41.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Double, y: Double) => hexGrid.convertXYToRowCol(x, y).toLong
"
"udf/spark_repos_1/4_sysgears_akka-spark-pipeline/..src.main.scala.services.github.spark.GitHubGraphXService.scala/udf/29.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

name => (centerVertexID, name.getAs[String](""name""))
"
"udf/spark_repos_1/4_sysgears_akka-spark-pipeline/..src.main.scala.services.github.spark.GitHubGraphXService.scala/udf/50.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getAs[Long](""id"")
"
"udf/spark_repos_1/4_sysgears_akka-spark-pipeline/..src.main.scala.services.github.spark.GitHubGraphXService.scala/udf/54.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ ver => 
          val src = ver.getAs[Long](""id"")
          (src, dst, RelationProp(1))
        }
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.customudf.SparkUDF.scala/udf/15.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeDT(_: String, _: String, _: String)
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.examples.DataFrame_Joins.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => FinalResult(x.getInt(0), x.getString(1), x.getInt(2), x.getString(6), x.getInt(3))
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.examples.DataFrame_Joins.scala/udf/33.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""empName"").like(""%Revanth%"")
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.examples.DataFrame.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => AggregatedEmpData(row.getInt(0), row.getString(1), row.getString(2), row.getLong(3), row.getLong(4), row.getInt(5), Utills.getTime())
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.examples.DataFramesRollup.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => FinalResultRollup(x.getString(0), x.getString(1), x.getString(2), x.getString(3), x.getString(4), x.getString(5), x.getLong(16), x.getLong(17), x.getDouble(18), x.getString(9), x.getString(10), x.getString(11), x.getString(12), x.getString(13), x.getString(14), x.getString(15))
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.examples.Spark_Hive_ORC.scala/udf/32.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Stock Entry: "" + t.toString
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.examples.SparkJDBC.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

employee(""SALARY"") > 7000
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.examples.Spark_StructType.scala/udf/31.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0) + "","" + ""Age: "" + t(1)
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.usecases.FlightDataAnalysis.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""DepDelay"" > 15
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.usecases.loganalysis.LogAnalyzerSQL.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getLong(1))
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.usecases.loganalysis.LogAnalyzerSQL.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-1.5.src.main.scala.com.spark.usecases.loganalysis.LogAnalyzerSQL.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.cassandra.ExportCassandraData.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""timestamp > cast('"" + getPrevDate(1) + ""' as timestamp) and timestamp <= cast('"" + getPrevDate(0) + ""' as timestamp)""
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.cassandra.ExportCassandraData.scala/udf/48.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""tid"" === id
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.cassandra.export.ExportCassandraData.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""timestamp > cast('"" + getPrevDate(1) + ""' as timestamp) and timestamp <= cast('"" + getPrevDate(0) + ""' as timestamp)""
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.cassandra.export.ExportCassandraData.scala/udf/48.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""tid"" === id
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.cassandra.export.Export_Cassandra_Data.scala/udf/51.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""timestamp >= cast('"" + startTime + ""' as timestamp) and timestamp < cast('"" + endTime + ""' as timestamp)""
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.cassandra.export.Export_Cassandra_Table_Data.scala/udf/40.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""timestamp >= cast('"" + startTime + ""' as timestamp) and timestamp < cast('"" + endTime + ""' as timestamp)""
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.cassandra.FilterCassandraData.scala/udf/17.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getInt(0)
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.cassandra.FilterCassandraData.scala/udf/23.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filt
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.custom.SemiStructuredUtilUDF.scala/udf/58.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

struct _
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.custom.SemiStructuredUtilUDF.scala/udf/67.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAtomic _
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.custom.SemiStructuredUtilUDF.scala/udf/71.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

arrayLength _
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.custom.UDF.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

myNameFilter($""name"")
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.custom.UDF.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

salesFilter($""sales"", lit(2000.0d))
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.custom.UDF.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

stateFilter($""state"", array(lit(""CA""), lit(""MA""), lit(""NY""), lit(""NJ"")))
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.custom.UDF.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

multipleFilter($""state"", $""discount"", struct(lit(""CA""), lit(100.0d)))
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.dataframes.DatasetConversion.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.dataset.SemiStructuredData.scala/udf/14.19.Dataset-University.map","Type: org.apache.spark.sql.Dataset[com.spark2.dataset.SemiStructuredData.University]
Call: map

s => s""${s.name} is  ${2017 - s.yearFounded} years old""
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.dataset.WordCountDS.scala/udf/17.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != """"
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.elasticsearch.ESQuerying.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df.col(""customer_id"").equalTo(""121"").and(df.col(""purchase_id"").equalTo(""234""))
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.window.functions.SwitchCPUMemStats.scala/udf/34.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

"""" + tsColumn + "" >= cast('"" + startTime + ""' as timestamp) and "" + tsColumn + "" < cast('"" + endTime + ""' as timestamp)""
"
"udf/spark_repos_1/53_Re1tReddy_Spark/..Spark-2.1.src.main.scala.com.spark2.window.functions.SwitchCPUMemStats.scala/udf/48.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""cpu_util > 30""
"
"udf/spark_repos_1/5_DanferWang_Data-Analytics-Using-Spark/..Decision-Tree-and-Random-Forest-Prediction.decisionTreeRandomForestPredict.scala/udf/62.19.Dataset-Double.map","Type: org.apache.spark.sql.Dataset[Double]
Call: map

_ / total
"
"udf/spark_repos_1/5_marino-serna_ParallelTool/..src.main.scala.com.github.marino_serna.parallel_tool.DataBaseStorage.scala/udf/78.36.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$partitionField"" === partitionValue
"
"udf/spark_repos_1/5_marino-serna_ParallelTool/..src.main.scala.com.github.marino_serna.parallel_tool.ParallelTool.scala/udf/58.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""method_name"" === methodName
"
"udf/spark_repos_1/5_matlongo_spark-projects/..AudienceCrossDevicer.src.main.scala.AudienceCrossDevicer.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

index_filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.aggregators.Aggregations.scala/udf/116.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""distance < %s"".format(value_dictionary(""umbraldist"").toInt)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.aggregators.Aggregations.scala/udf/138.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""transport_id"") =!= """" && col(""transport_id"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.Coronavirus.scala/udf/115.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""day"") === col(""day_original"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.Coronavirus.scala/udf/145.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""day"") === col(""day_original"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.Coronavirus.scala/udf/163.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""day"") === col(""day_original"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.matchers.GeoCodeMatcherOldPipeline.scala/udf/37.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.matchers.GeoCodeMatcher.scala/udf/38.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.matchers.GeoSparkMatcher.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.matchers.GeoSparkMatcher.scala/udf/93.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getUserData
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Coronavirus.src.main.scala.matchers.PolygonMatcher.scala/udf/46.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..CrossDevicer.src.main.scala.CrossDevicer.scala/udf/140.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(col(""new_segment"")) > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..CrossDevicer.src.main.scala.CrossDevicer.scala/udf/243.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(col(""new_segment"")) > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataCustomAudiences.src.main.scala.GetDataCustomAudience.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""country"").isin(countries: _*) && col(""event_type"").isin(event_types: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.datasets.Dataset.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.datasets.Dataset.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.datasets.DatasetSegmentTriplets.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.datasets.UrlUtils.scala/udf/111.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.datasets.UrlUtils.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(country)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.datasets.UrlUtils.scala/udf/58.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.datasets.UrlUtils.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""keyword"").rlike(""[a-z]{2,}"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.datasets.UrlUtils.scala/udf/77.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataDemo.src.main.scala.features.TrainingFeatures.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""count >= %s"".format(minSupport)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataInsights.src.main.scala.AggregateData.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segments"").isin(taxo_segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataInsights.src.main.scala.DataInsights.scala/udf/45.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""id_partner"").cast(""int"") < 1500
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUrls.src.main.scala.datasets.DatasetKeywordContent.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""content_keys"").rlike(""[a-z]{2,}"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUrls.src.main.scala.datasets.DatasetSegmentsBranded.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"").isin(branded_segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUrls.src.main.scala.datasets.UrlUtils.scala/udf/104.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUrls.src.main.scala.datasets.UrlUtils.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(country)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUrls.src.main.scala.datasets.UrlUtils.scala/udf/58.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUrls.src.main.scala.datasets.UrlUtils.scala/udf/70.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUrls.src.main.scala.GenerateDatasets.scala/udf/33.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""date > %s"".format(start)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUrls.src.main.scala.GenerateDatasets.scala/udf/35.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segments"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..DataUserAgent.src.main.scala.GetDataUserAgent.scala/udf/16.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""event_type IN ('pv', 'campaign') AND length(user_agent)>0 AND country IN (%s)"".format(locally {
            val _t_m_p_4 = countries
            _t_m_p_4.map(""'%s'"".format(_))
          }.mkString("", ""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicerAutoBot.src.main.scala.aggregators.Aggregations.scala/udf/116.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""distance < %s"".format(value_dictionary(""umbraldist"").toInt)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicerAutoBot.src.main.scala.aggregators.Aggregations.scala/udf/138.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""transport_id"") =!= """" && col(""transport_id"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicerAutoBot.src.main.scala.matchers.GeoCodeMatcherOldPipeline.scala/udf/37.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicerAutoBot.src.main.scala.matchers.GeoCodeMatcher.scala/udf/35.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicerAutoBot.src.main.scala.matchers.GeoSparkMatcher.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicerAutoBot.src.main.scala.matchers.GeoSparkMatcher.scala/udf/93.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getUserData
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicerAutoBot.src.main.scala.matchers.PolygonMatcher.scala/udf/46.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicerAutoBot.src.main.scala.random-debugging.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""_c2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicer.src.main.scala.aggregators.Aggregations.scala/udf/116.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""distance < %s"".format(value_dictionary(""umbraldist"").toInt)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicer.src.main.scala.aggregators.Aggregations.scala/udf/138.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""transport_id"") =!= """" && col(""transport_id"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicer.src.main.scala.matchers.GeoCodeMatcherOldPipeline.scala/udf/37.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicer.src.main.scala.matchers.GeoCodeMatcher.scala/udf/38.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicer.src.main.scala.matchers.GeoSparkMatcher.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicer.src.main.scala.matchers.GeoSparkMatcher.scala/udf/93.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getUserData
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicer.src.main.scala.matchers.PolygonMatcher.scala/udf/46.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GeoDevicer.src.main.scala.random-debugging.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""_c2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GetPII.src.main.scala.GetPiiMonth.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ml_sh2"").isNotNull || col(""mb_sh2"").isNotNull || col(""nid_sh2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..GetPII.src.main.scala.GetPii.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ml_sh2"").isNotNull || col(""mb_sh2"").isNotNull || col(""nid_sh2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..HomeJobsJr.src.main.scala.crossdevicer.CrossDevicer.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""frequency"") > value_dictionary(""minFreq"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..HomeJobsJr.src.main.scala.homejobs.HomeJobs.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

if (value_dictionary(""UseType"") == ""home"") {
        col(""Hour"") >= value_dictionary(""HourFrom"").toInt || col(""Hour"") <= value_dictionary(""HourTo"").toInt
      } else {
        col(""Hour"") <= value_dictionary(""HourFrom"").toInt && col(""Hour"") >= value_dictionary(""HourTo"").toInt && !date_format(col(""Time""), ""EEEE"").isin(List(""Saturday"", ""Sunday""): _*)
      }
"
"udf/spark_repos_1/5_matlongo_spark-projects/..HomeJobs.src.main.scala.HomeJobs.scala/udf/34.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(country)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..HomeJobs.src.main.scala.HomeJobs.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

if (UseType == ""home"") {
        col(""Hour"") >= HourFrom || col(""Hour"") <= HourTo
      } else {
        col(""Hour"") <= HourFrom && col(""Hour"") >= HourTo && !date_format(col(""Time""), ""EEEE"").isin(List(""Saturday"", ""Sunday""): _*)
      }
"
"udf/spark_repos_1/5_matlongo_spark-projects/..HomeJobs.src.main.scala.HomeJobs.scala/udf/75.19.Dataset-Record).map","Type: org.apache.spark.sql.Dataset[(String, main.scala.Record)]
Call: map

row => (row._2.ad_id, row._2.id_type, row._2.freq, row._2.geocode, row._2.avg_latitude, row._2.avg_longitude)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Keywiser.src.main.scala.crossdevicer.AudienceCrossDevicer.scala/udf/15.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

index_filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Keywiser.src.main.scala.keywiser.Keywiser.scala/udf/128.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

t._2
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Keywiser.src.main.scala.keywiser.Keywiser.scala/udf/190.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

domain_filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Keywiser.src.main.scala.TermSearch.scala/udf/135.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..KeywordIngestion.src.main.scala.keywordIngestion.scala/udf/119.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..KeywordIngestion.src.main.scala.keywordIngestion.scala/udf/85.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..KeywordIngestion.src.main.scala.TestJoin.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..KeywordIngestion.src.main.scala.TestJoin.scala/udf/77.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..LookAlike.src.main.scala.Item2Item.scala/udf/110.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..LookAlike.src.main.scala.Item2Item.scala/udf/245.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..LookAlike.src.main.scala.LookAlike.scala/udf/105.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""feature"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..NLP.src.main.scala.crossdevicer.AudienceCrossDevicer.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

index_filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..NSE_Assignment.src.main.scala.crossdevicer.CrossDevicer.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""frequency"") > value_dictionary(""minFreq"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..NSE_Assignment.src.main.scala.crossdevicer.CrossDevicer.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""frequency"") >= value_dictionary(""minFreq"").toInt
"
"udf/spark_repos_1/5_matlongo_spark-projects/..NSE_Assignment.src.main.scala.homejobs.HomeJobs.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

if (value_dictionary(""UseType"") == ""home"") {
        col(""Hour"") >= value_dictionary(""HourFrom"").toInt || col(""Hour"") <= value_dictionary(""HourTo"").toInt
      } else {
        col(""Hour"") <= value_dictionary(""HourFrom"").toInt && col(""Hour"") >= value_dictionary(""HourTo"").toInt && !date_format(col(""Time""), ""EEEE"").isin(List(""Saturday"", ""Sunday""): _*)
      }
"
"udf/spark_repos_1/5_matlongo_spark-projects/..POICrossDevicer.src.main.scala.POICrossdevicerJson.scala/udf/48.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..POICrossDevicer.src.main.scala.POICrossdevicer.scala/udf/33.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(country)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..POICrossDevicer.src.main.scala.POI_Json_Analysis.scala/udf/55.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(value_dictionary(""country""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..PremiumPartner.src.main.scala.DataExporter.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length($""device_id"") > 0 && $""id_partner"".isin(ids_partners: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..PremiumPartner.src.main.scala.DataExporter.scala/udf/17.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

filters
"
"udf/spark_repos_1/5_matlongo_spark-projects/..PremiumPartner.src.main.scala.PremiumPartnerData.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""id_partner"").isin(ids_partners: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Profiler.src.main.scala.Profiler.scala/udf/115.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""activity"") > activity_min
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Profiler.src.main.scala.Profiler.scala/udf/123.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""appstotal"") > app_min
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Profiler.src.main.scala.Profiler.scala/udf/131.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment_total"") > third_party_min
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Profiler.src.main.scala.Profiler.scala/udf/140.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""detections"") >= location_min
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.BigRandom.scala/udf/112.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.BigRandom.scala/udf/265.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(taxo_segs: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.BigRandom.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""seg_id"").isin(segList: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.BigRandom.scala/udf/87.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_27
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.crossdevicer.AudienceCrossDevicer.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

index_filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.DumpBase.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""id_partner = %s"".format(id_partner)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.DunnhumbyEnrichment.scala/udf/105.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.DunnhumbyEnrichment.scala/udf/116.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""day >= %s AND country in (%s)"".format(piiDateFrom, countries)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.DunnhumbyEnrichment.scala/udf/120.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

query
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.DunnhumbyEnrichmentSegments.scala/udf/105.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.DunnhumbyEnrichmentSegments.scala/udf/120.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""day >= %s AND country in (%s)"".format(piiDateFrom, countries)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.DunnhumbyEnrichmentSegments.scala/udf/128.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

query
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elMejorRandom.scala/udf/123.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

if (value_dictionary(""UseType"") == ""home"") {
        col(""Hour"") >= value_dictionary(""HourFrom"") || col(""Hour"") <= value_dictionary(""HourTo"")
      } else {
        col(""Hour"") <= value_dictionary(""HourFrom"") && col(""Hour"") >= value_dictionary(""HourTo"") && !date_format(col(""Time""), ""EEEE"").isin(List(""Saturday"", ""Sunday""): _*)
      }
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elMejorRandom.scala/udf/144.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"") isin (35360, 35361, 35362, 35363)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elMejorRandom.scala/udf/150.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"") isin (35360, 35361, 35362, 35363)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elMejorRandom.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""_c2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elMejorRandom.scala/udf/193.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"").isin(List(""99593"", ""5022"", ""920"", ""275"", ""2660"", ""302"", ""48174""): _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elMejorRandom.scala/udf/253.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elSegundomejorRandom.scala/udf/123.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

if (value_dictionary(""UseType"") == ""home"") {
        col(""Hour"") >= value_dictionary(""HourFrom"") || col(""Hour"") <= value_dictionary(""HourTo"")
      } else {
        col(""Hour"") <= value_dictionary(""HourFrom"") && col(""Hour"") >= value_dictionary(""HourTo"") && !date_format(col(""Time""), ""EEEE"").isin(List(""Saturday"", ""Sunday""): _*)
      }
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elSegundomejorRandom.scala/udf/144.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"") isin (35360, 35361, 35362, 35363)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elSegundomejorRandom.scala/udf/150.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"") isin (35360, 35361, 35362, 35363)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elSegundomejorRandom.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""_c2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elSegundomejorRandom.scala/udf/193.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"").isin(List(""99593"", ""5022"", ""920"", ""275"", ""2660"", ""302"", ""48174""): _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elSegundomejorRandom.scala/udf/253.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.elSegundomejorRandom.scala/udf/302.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment_id"").isin(segmentos: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GCBACampaings.scala/udf/76.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""values"")) > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataForAudience.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(taxonomy: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataForAudience.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(taxonomy: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataTripletsCSV.scala/udf/62.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataXPAR.scala/udf/165.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(taxonomy: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataXPAR.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataXPAR.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataXPBR.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataXPBR.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataXPCL.scala/udf/175.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(taxonomy: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataXPCL.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetDataXPCL.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetURLsForAudience.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.GetURLsForAudience.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Randomgrafia.scala/udf/123.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PROVCODE"") === ""02""
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Randomgrafia.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Randomgrafia.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1004.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = 'AR' AND event_type = 'pv' AND ("" + query + "") AND NOT lower(url) LIKE '%mapa.buenosaires%' AND NOT lower(url) LIKE '%miba.buenosaires%' AND NOT lower(url) LIKE '%www.buenosaires%transporte%'""
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1040.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""url"").contains(""taringa.net"") && not(col(""url"").contains(""api.retargetly""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1174.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""nid_sh2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1182.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""nid_sh2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1219.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ml_sh2"").isNotNull or col(""mb_sh2"").isNotNull or col(""nid_sh2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1225.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""summary"") === ""count""
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1408.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1418.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"").isin(taxo: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1538.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"").cast(IntegerType) < 5000 && col(""country"").isin(countries: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1541.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"").cast(IntegerType) < 5000
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1560.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

arrIntersect(col(""_c2""))
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1571.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""array_contains(_c2, '%s')"".format(id)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1600.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1625.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

array_equifax_filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1642.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

t._2
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1880.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""feature"").isin(factualSegments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/1947.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""de_geo_pulseplus_postal_code"").isin(zipcodes: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/203.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(country)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/220.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= 2
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/225.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= 20
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/230.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= 80
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/259.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= 2
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/264.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= 20
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/269.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= 80
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/292.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(country)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/380.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""device_type"") === ""RTG""
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/432.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""country = '%s'"".format(country)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/524.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""id_partner"") === ""349"" && col(""all_segments"").isin(List(""76522,76536,76543""): _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/575.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""name0"") =!= """" && col(""name0"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/589.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""country"") === ""AR""
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/598.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""len"") < 11
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/606.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""len"") < 11
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/749.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Hour"") >= 16 || col(""Hour"") <= 5 || date_format(col(""datetime""), ""EEEE"").isin(List(""Sunday""): _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/791.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Hour"") >= 19 || col(""Hour"") <= 8 || date_format(col(""Time""), ""EEEE"").isin(List(""Saturday"", ""Sunday""): _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/869.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!date_format(col(""Time""), ""EEEE"").isin(List(""Saturday"", ""Sunday""): _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/873.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

array_contains(col(""third_party""), 1192) || array_contains(col(""third_party""), 1191) || array_contains(col(""third_party""), 1193) || array_contains(col(""third_party""), 1190) || array_contains(col(""third_party""), 1194) || array_contains(col(""third_party""), 1069) || array_contains(col(""third_party""), 1195)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/910.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""url"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/917.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""url"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Random.scala/udf/921.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""url"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Retroactivation.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"").cast(IntegerType) < 5000 && col(""country"").isin(countries: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Retroactivation.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"").cast(IntegerType).isin(partners: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.Retroactivation.scala/udf/91.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.SampleURLs.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.SampleURLs.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.SampleURLs.scala/udf/96.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""domain"").isin(domain_not_shareable: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.TestRules.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""id_partner"").isin(partners: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.unBuenRandom.scala/udf/117.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

if (value_dictionary(""UseType"") == ""home"") {
        col(""Hour"") >= value_dictionary(""HourFrom"") || col(""Hour"") <= value_dictionary(""HourTo"")
      } else {
        col(""Hour"") <= value_dictionary(""HourFrom"") && col(""Hour"") >= value_dictionary(""HourTo"") && !date_format(col(""Time""), ""EEEE"").isin(List(""Saturday"", ""Sunday""): _*)
      }
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.unBuenRandom.scala/udf/138.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"") isin (35360, 35361, 35362, 35363)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.unBuenRandom.scala/udf/144.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"") isin (35360, 35361, 35362, 35363)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.unBuenRandom.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""_c2"").isNotNull
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.unBuenRandom.scala/udf/187.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""feature"").isin(List(""99593"", ""5022"", ""920"", ""275"", ""2660"", ""302"", ""48174""): _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Random.src.main.scala.unBuenRandom.scala/udf/246.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reporter.src.main.scala.ReporterPipeline.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"") < 5000 && !col(""id_partner"").isin(blacklist: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reporter.src.main.scala.Reporter.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reporter.src.main.scala.Streaming.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"") < 5000 && !col(""id_partner"").isin(blacklist: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reports.src.main.scala.platformsData.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(col(""d2"").isNotNull || col(""d10"").isNotNull || col(""d11"").isNotNull || col(""d13"").isNotNull || col(""d14"").isNotNull) && col(""country"").isin(countries: _*) && col(""event_type"").isin(event_types: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reports.src.main.scala.platformsReport.scala/udf/108.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(taxo_segs: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reports.src.main.scala.platformsReport.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""country"").isin(countries: _*) && col(""segment"").isin(taxo_segs: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reports.src.main.scala.platformsReport.scala/udf/95.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(taxo_segs: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reports.src.main.scala.pvAlertReport.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""median"") >= lit(median_thr)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reports.src.main.scala.pvAlertReport.scala/udf/66.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") < lit(low_thr)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reports.src.main.scala.RemovedSegmentsReport.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""removed_segment"").isin(taxo_segs: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Reports.src.main.scala.RemovedSegments.scala/udf/21.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""removed_segments"").isNotNull && col(""country"").isin(countries: _*) && col(""event_type"").isin(event_types: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.crossdevicer.AudienceCrossDevicer.scala/udf/15.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

index_filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.GetAudience.scala/udf/264.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

commonFilter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.GetAudience.scala/udf/280.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query(""filter"").toString
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.GetAudience.scala/udf/285.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query(""filter"").toString
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.GetAudience.scala/udf/322.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

commonFilter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.GetAudience.scala/udf/347.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""segments"")) > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.GetAudience.scala/udf/353.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

commonFilter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.GetAudience.scala/udf/364.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query(""filter"").toString
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.GetAudience.scala/udf/424.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[String], ys: Seq[String]) => xs.intersect(ys).size > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.devicer.Retroactivations.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"").cast(IntegerType) < 5000
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.pipeline.Streaming.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"").cast(IntegerType) < 5000 && col(""country"").isin(countries: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.pipeline.Streaming.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0 && col(""event_type"").isin(event_types: _*) && col(""id_partner"").cast(IntegerType) < 5000
"
"udf/spark_repos_1/5_matlongo_spark-projects/..Retroactivations.src.main.scala.pipeline.Streaming.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""device_id"")) > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..SamplePublicis.src.main.scala.generateOrganic.scala/udf/96.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(col(""gral_segments"")) > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..SamplePublicis.src.main.scala.OrganicSegments.scala/udf/132.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""segment"").isin(taxo_general_b.value: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..SamplePublicisTmp.src.main.scala.generateOrganic.scala/udf/96.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(col(""gral_segments"")) > 0
"
"udf/spark_repos_1/5_matlongo_spark-projects/..SamplePublicisTmp.src.main.scala.OrganicSegments.scala/udf/125.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""segment"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..ShareThisUSIngester.src.main.scala.ByTwoOutput.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""country"" === ""US""
"
"udf/spark_repos_1/5_matlongo_spark-projects/..ShareThisUSIngester.src.main.scala.EstidMapper.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" >= start && $""day"" <= end
"
"udf/spark_repos_1/5_matlongo_spark-projects/..ShareThisUSIngester.src.main.scala.ShareThisUS.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

($""_c5"" === ""*"" || $""_c5"".isNull || $""_c5"" === ($""extension"")) && ($""_c2"" === ""*"" || $""_c2"".isNull || $""_c2"" === ($""path"")) && when($""_c3"".isNull, true).otherwise(arr_compare(split($""_c3"", ""&""), $""query"") || $""_c3"" === ""*"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..UniqueCalculator.src.main.scala.crossdevicer.AudienceCrossDevicer.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

index_filter
"
"udf/spark_repos_1/5_matlongo_spark-projects/..UniqueCalculator.src.main.scala.UniqueCalculator.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..UniqueCalculator.src.main.scala.UniqueCalculator.scala/udf/64.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""segment"").isin(segments: _*)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..URLContentEmbedding.src.main.scala.MeanWordsEmbedder.scala/udf/110.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""lang"" === ""sp""
"
"udf/spark_repos_1/5_matlongo_spark-projects/..URLContentEmbedding.src.main.scala.MeanWordsEmbedder.scala/udf/112.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""domain"" =!= ($""word"")
"
"udf/spark_repos_1/5_matlongo_spark-projects/..UrlIngester.src.main.scala.UrlIngester.scala/udf/114.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""length(%s)>0"".format(field)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..UrlIngester.src.main.scala.UrlIngester.scala/udf/160.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""len <= %s"".format(url_limit)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..UrlIngester.src.main.scala.UrlIngester.scala/udf/171.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""country = '%s'"".format(country)
"
"udf/spark_repos_1/5_matlongo_spark-projects/..UrlIngester.src.main.scala.UrlIngester.scala/udf/80.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

query_generic_domains
"
"udf/spark_repos_1/5_ThoughtWorksInc_streaming-data-pipeline/..FileChecker.src.main.scala.com.free2wheelers.apps.StationReportValidator.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""longitude"".rlike(LAT_LONG_REGEX)
"
"udf/spark_repos_1/5_ThoughtWorksInc_streaming-data-pipeline/..FileChecker.src.main.scala.com.free2wheelers.apps.StationReportValidator.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""latitude"".rlike(LAT_LONG_REGEX)
"
"udf/spark_repos_1/5_ThoughtWorksInc_streaming-data-pipeline/..FileChecker.src.main.scala.com.free2wheelers.apps.StationReportValidator.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""total"") > DUPLICATION_THRESHOLD
"
"udf/spark_repos_1/5_ThoughtWorksInc_streaming-data-pipeline/..StationConsumer.src.main.scala.com.free2wheelers.apps.StationApp.scala/udf/52.23.Dataset-StationStatus).map","Type: org.apache.spark.sql.Dataset[((Double, Double), com.free2wheelers.apps.StationStatus)]
Call: map

_._2
"
"udf/spark_repos_1/5_ThoughtWorksInc_streaming-data-pipeline/..StationConsumer.src.main.scala.com.free2wheelers.apps.StationApp.scala/udf/54.21.Dataset-StationStatus.map","Type: org.apache.spark.sql.Dataset[com.free2wheelers.apps.StationStatus]
Call: map

parseDateTimeToIsoFormat
"
"udf/spark_repos_1/5_ThoughtWorksInc_streaming-data-pipeline/..StationConsumer.src.main.scala.com.free2wheelers.apps.StationApp.scala/udf/56.22.Dataset-StationStatus.filter","Type: org.apache.spark.sql.Dataset[com.free2wheelers.apps.StationStatus]
Call: filter

row => !"""".equals(row.last_updated)
"
"udf/spark_repos_1/61_nadimbahadoor_learn-spark/..source-code.learn-spark.src.main.scala.com.allaboutscala.learn.spark.dataframe.DataFrameOperations.scala/udf/38.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => toQuestion(row)
"
"udf/spark_repos_1/61_nadimbahadoor_learn-spark/..source-code.learn-spark.src.main.scala.com.allaboutscala.learn.spark.sql.SparkSQL_Tutorial.scala/udf/56.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

prefixStackoverflow _
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/101.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/103.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/108.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/110.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/87.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/89.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/94.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_1/6_caroljmcdonald_mapr-sparkml-churn/..src.main.scala.example.Churn.scala/udf/96.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_1/6_empcl_e-commerceDataAnalysisSystem/..src.main.scala.empcl.spark.product.AreaProductTop3Spark.scala/udf/21.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupContactDistinctUDF
"
"udf/spark_repos_1/6_empcl_e-commerceDataAnalysisSystem/..src.main.scala.empcl.spark.session.UserSessionStatAnalyzeSpark.scala/udf/306.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => {
        var flag = false
        val session_ids = row.getString(1)
        val session_id = row.getString(2)
        val sessionIdArr = session_ids.split("","")
        if (sessionIdArr.contains(session_id)) {
          flag = true
        }
        flag
      }
"
"udf/spark_repos_1/6_empcl_e-commerceDataAnalysisSystem/..src.main.scala.empcl.spark.session.UserSessionStatAnalyzeSpark.scala/udf/334.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val action_time = row.getString(2)
        val hour = action_time.split("" "")(1).split("":"")(0)
        val user_id = row.getString(0)
        val session_id = row.getString(1)
        val search_keyword = row.getString(3)
        val click_category_id = row.getString(4)
        val page_id = row.getString(5)
        val click_product_id = row.getString(6)
        val order_category_ids = row.getString(7)
        val order_product_ids = row.getString(8)
        val pay_category_ids = row.getString(9)
        val pay_product_ids = row.getString(10)
        HourDetailSession(hour, session_id, user_id, action_time, search_keyword, click_category_id, page_id, click_product_id, order_category_ids, order_product_ids, pay_category_ids, pay_product_ids)
      }
"
"udf/spark_repos_1/6_empcl_e-commerceDataAnalysisSystem/..src.main.scala.empcl.spark.session.UserSessionStatAnalyzeSpark.scala/udf/561.23.Dataset-CategoryIdSessionCounts.filter","Type: org.apache.spark.sql.Dataset[empcl.spark.session.CategoryIdSessionCounts]
Call: filter

cisc => {
        var flag = false
        val category_id = cisc.category_id
        if (bc_selectedCategory.contains(category_id)) {
          flag = true
        }
        flag
      }
"
"udf/spark_repos_1/6_empcl_e-commerceDataAnalysisSystem/..src.main.scala.empcl.spark.session.UserSessionStatAnalyzeSpark.scala/udf/77.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => {
        var flag = true
        val keyWordsInfo = row.getString(3)
        val clickCategoryInfo = row.get(4)
        val age = row.getString(13)
        val professional = row.getString(14)
        val city = row.getString(15)
        val sex = row.getString(16)
        val _startAge = StringUtils.getFieldFromConcatString(parameter, ""\\|"", Constants.PARAM_START_AGE).getOrElse("""")
        val _endAge = StringUtils.getFieldFromConcatString(parameter, ""\\|"", Constants.PARAM_END_AGE).getOrElse("""")
        if (!ValidUtils.between(age, _startAge, _endAge)) {
          flag = false
        }
        val _professional = StringUtils.getFieldFromConcatString(parameter, ""\\|"", Constants.PARAM_PROFESSIONAL).getOrElse("""")
        if (!ValidUtils.in(professional, _professional)) {
          flag = false
        }
        val _city = StringUtils.getFieldFromConcatString(parameter, ""\\|"", Constants.PARAM_CITY).getOrElse("""")
        if (!ValidUtils.in(city, _city)) {
          flag = false
        }
        val _sex = StringUtils.getFieldFromConcatString(parameter, ""\\|"", Constants.PARAM_SEX).getOrElse("""")
        if (!ValidUtils.equals(sex, _sex)) {
          flag = false
        }
        val searchKeyWords = StringUtils.getFieldFromConcatString(parameter, ""\\|"", Constants.PARAM_SEARCHKEYWORDS).getOrElse("""")
        if (!ValidUtils.in(keyWordsInfo, searchKeyWords)) {
          flag = false
        }
        val _clickCategory = StringUtils.getFieldFromConcatString(parameter, ""\\|"", Constants.PARAM_CLICKCATEGORY).getOrElse("""")
        if (!ValidUtils.in(clickCategoryInfo, _clickCategory)) {
          flag = false
        }
        flag
      }
"
"udf/spark_repos_1/6_empcl_e-commerceDataAnalysisSystem/..src.test.scala.empcl.DateTest.scala/udf/15.19.Dataset-TestUser.map","Type: org.apache.spark.sql.Dataset[empcl.TestUser]
Call: map

user => {
        var r1 = 0
        var r2 = 0
        var r3 = 0
        var r4 = 0
        val u1 = user.user1
        val u2 = user.user2
        val u3 = user.user3
        val u4 = user.user4
        if (StringUtils.isNotEmpty(u1)) {
          r1 = r1 + 1
        }
        if (StringUtils.isNotEmpty(u2)) {
          r2 = r2 + 1
        }
        if (StringUtils.isNotEmpty(u3)) {
          r3 = r3 + 1
        }
        if (StringUtils.isNotEmpty(u4)) {
          r4 = r4 + 1
        }
        Count(r1, r2, r3, r4)
      }
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.dataframe.operations.WorkingWithDataframes.scala/udf/80.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
        (* : String) => matchDayOfWeek(*)
      }
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.learning.chapter3.FunctionalQueryDataset.scala/udf/14.19.Dataset-Customers.map","Type: org.apache.spark.sql.Dataset[com.spark.learning.chapter3.FunctionalQueryDataset.Customers]
Call: map

customer => customer.name
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.learning.chapter3.FunctionalQueryDataset.scala/udf/28.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => s.length
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.mllib.fpgrowth.FPGrowthAlgo.scala/udf/40.19.Dataset-AssocoationClass.map","Type: org.apache.spark.sql.Dataset[com.spark.mllib.fpgrowth.FPGrowthAlgo.AssocoationClass]
Call: map

rows => rows.antecedent
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.mllib.fpgrowth.FPGrowthAlgo.scala/udf/56.24.Dataset-TransactionProduct.filter","Type: org.apache.spark.sql.Dataset[com.spark.mllib.fpgrowth.FPGrowthAlgo.TransactionProduct]
Call: filter

rows => {
          var isPresent = false
          breakable {
            for (i <- arrayAntecedent) {
              if (rows.product.split("","").sameElements(i)) {
                isPresent = true
                break
              }
            }
          }
          isPresent
        }
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.mllib.fpgrowth.FPGrowthAlgo.scala/udf/87.22.Dataset-AssocoationClass.filter","Type: org.apache.spark.sql.Dataset[com.spark.mllib.fpgrowth.FPGrowthAlgo.AssocoationClass]
Call: filter

_.antecedent.sameElements(locally {
        val _t_m_p_4 = antecedent.split("","")
        _t_m_p_4.map(value => value.trim)
      })
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.mllib.fpgrowth.FPGrowthAlgo.scala/udf/96.22.Dataset-AssocoationClass.filter","Type: org.apache.spark.sql.Dataset[com.spark.mllib.fpgrowth.FPGrowthAlgo.AssocoationClass]
Call: filter

_.consequent.equalsIgnoreCase(consequent.trim())
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.sql.UDFExmapleOne.scala/udf/49.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getMax _
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.sql.UDFExmapleOne.scala/udf/53.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getAvg _
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.sql.UDFExmapleOne.scala/udf/57.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getMonth _
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.sql.UDFExmapleOne.scala/udf/61.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getYear _
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.sql.UDFExmapleOne.scala/udf/65.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getDay _
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.two.updates.DatasetVsDataFrame.scala/udf/15.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.spark.two.updates.DatasetVsDataFrame.Sales]
Call: map

_.itemId
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.two.updates.JoiningDatasets.scala/udf/21.21.Dataset-Department).map","Type: org.apache.spark.sql.Dataset[(com.spark.two.updates.JoiningDatasets.Employee, com.spark.two.updates.JoiningDatasets.Department)]
Call: map

record => Record(record._1.name, record._1.age, record._1.salary, record._1.departmentId, record._2.name)
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.two.updates.JoiningDatasets.scala/udf/23.22.Dataset-Record.filter","Type: org.apache.spark.sql.Dataset[com.spark.two.updates.JoiningDatasets.Record]
Call: filter

record => record.age > 25
"
"udf/spark_repos_1/6_itspawanbhardwaj_spark-learning/..src.main.scala.com.spark.two.updates.RDDToDataSet.scala/udf/40.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

value => value == ""hello""
"
"udf/spark_repos_1/6_treselle-systems_sfo_fire_service_call_analysis_using_spark/..src.main.scala.FireServiceCallAnalysisDS.scala/udf/47.22.Dataset-fireServiceCall.filter","Type: org.apache.spark.sql.Dataset[com.treselle.fscalls.analysis.FireServiceCallAnalysisDS.fireServiceCall]
Call: filter

fireServiceCall => fireServiceCall.CallYear == 2016
"
"udf/spark_repos_1/7_aosama_MachineLearningSamples/..src.main.scala.org.ibrahim.ezmachinelearning.RFWithSurrogateCensusIncomeExample.scala/udf/96.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Double, y: Double) => scala.math.pow(x - y, 2)
"
"udf/spark_repos_1/7_chandnipatelTW_twdu2b/..Monitoring.src.main.scala.com.free2wheelers.apps.MonitoringApp.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 1
"
"udf/spark_repos_1/7_chandnipatelTW_twdu2b/..Monitoring.src.main.scala.com.free2wheelers.apps.MonitoringApp.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""docks_available"" < 0
"
"udf/spark_repos_1/7_chandnipatelTW_twdu2b/..Monitoring.src.main.scala.com.free2wheelers.apps.MonitoringApp.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""bikes_available"" < 0
"
"udf/spark_repos_1/7_chandnipatelTW_twdu2b/..Monitoring.src.main.scala.com.free2wheelers.apps.MonitoringApp.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isOutsideOfNYC($""latitude"", $""longitude"") && isOutsideOfSF($""latitude"", $""longitude"")
"
"udf/spark_repos_1/7_chandnipatelTW_twdu2b/..StationConsumer.src.main.scala.com.free2wheelers.apps.StationApp.scala/udf/24.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

convertTimestampToISO
"
"udf/spark_repos_1/7_devmindset_sparkscalainterview/..Xebia.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"" === ""WB"" || $""state"" === ""HR""
"
"udf/spark_repos_1/7_devmindset_sparkscalainterview/..Xebia.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""gender"" === ""F""
"
"udf/spark_repos_1/7_nikhilbhide_AmazonAWS/..SparkS3Analytics.src.com.nik.spark.S3LogParserAnalytics.scala/udf/68.22.Dataset-S3LogEntry.filter","Type: org.apache.spark.sql.Dataset[com.nik.spark.S3LogEntry]
Call: filter

record => record.bytesSent != -999
"
"udf/spark_repos_1/7_nikhilbhide_AmazonAWS/..SparkS3FindDuplicateFiles.src.com.nik.spark.S3FindDuplicateFiles.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""duplicateFileCount"" > 1
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.FeatureGenerator.scala/udf/38.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

x => x.getString(0) != null && x.getString(1) != null && x.getString(2) != null
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.FeatureGenerator.scala/udf/42.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

x => x.getString(0).split("","").size > 1 && x.getString(1).split("","").size > 1
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.FeatureGenerator.scala/udf/46.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => generateFeaturesFromPair(x.getString(0), x.getString(1), """", """", """", Some(x.getString(2).trim match {
        case ""T"" => 1.0d
        case ""F"" => 0.0d
      }))
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.FeatureGenerator.scala/udf/57.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""citizenship"" === citizenship
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.FeatureGenerator.scala/udf/84.25.Dataset-MatchRecord.filter","Type: org.apache.spark.sql.Dataset[matching.ml.datamining.worldcheck.MatchRecord]
Call: filter

x => x.name != null && x.name != ""null"" && x.name.nonEmpty
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.FeatureGenerator.scala/udf/86.23.Dataset-MatchRecord.filter","Type: org.apache.spark.sql.Dataset[matching.ml.datamining.worldcheck.MatchRecord]
Call: filter

x => x.name.split("","").size > 1
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.FeatureGenerator.scala/udf/93.20.Dataset-MatchRecord.map","Type: org.apache.spark.sql.Dataset[matching.ml.datamining.worldcheck.MatchRecord]
Call: map

x => generateFeaturesFromPair(s1, x.name, x.uid, x.personType, x.citizenship)
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.WorldCheckAliases.scala/udf/214.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

x => citizenships.contains(x.getString(citizenshipIndex))
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.datamining.worldcheck.WorldCheckAliases.scala/udf/228.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""e/i"" !== ""e""
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.MatchingStreamingMLMain.scala/udf/57.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.getDouble(2) == 1.0d
"
"udf/spark_repos_1/7_steveSK_Matching-ML/..src.main.scala.matching.ml.MatchingStreamMLJob.scala/udf/59.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.getDouble(2) == 1.0d
"
"udf/spark_repos_1/8_allwefantasy_spark-ml-example/..src.main.java.exmaple.KK.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_1/8_allwefantasy_spark-ml-example/..src.main.java.nobleprog.BatchSparkSQL.scala/udf/30.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sep: String, co: mutable.WrappedArray[String]) => co.mkString(sep)
"
"udf/spark_repos_1/8_allwefantasy_spark-ml-example/..src.main.java.nobleprog.DataFrameExample.scala/udf/29.22.Dataset-DC.filter","Type: org.apache.spark.sql.Dataset[nobleprog.DC]
Call: filter

f => f.number > 3
"
"udf/spark_repos_1/8_bebee4java_spark-notes/..spark-sql.src.main.scala.org.spark.notes.DataFrameOperate.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"").between(10, 20)
"
"udf/spark_repos_1/8_bebee4java_spark-notes/..spark-sql.src.main.scala.org.spark.notes.DataFrameOperate.scala/udf/42.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 10
"
"udf/spark_repos_1/8_bebee4java_spark-notes/..spark-sql.src.main.scala.org.spark.notes.DataFrameOperate.scala/udf/44.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(""name"").isin(""wangwu"", ""lisi"")
"
"udf/spark_repos_1/8_bebee4java_spark-notes/..spark-sql.src.main.scala.org.spark.notes.SparkSQLUDAF.scala/udf/16.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

myCount
"
"udf/spark_repos_1/8_bebee4java_spark-notes/..spark-sql.src.main.scala.org.spark.notes.SparkSQLUDAF.scala/udf/22.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

myAvg
"
"udf/spark_repos_1/8_bebee4java_spark-notes/..spark-sql.src.main.scala.org.spark.notes.SparkSQLUDF.scala/udf/19.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

format_date
"
"udf/spark_repos_1/8_bebee4java_spark-notes/..spark-sql.src.main.scala.org.spark.notes.SparkSQLUDF.scala/udf/30.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udf[String, Long](score_tag)
"
"udf/spark_repos_1/8_bebee4java_spark-notes/..spark-streaming.src.main.scala.org.spark.war.stream.SparkStreamingApp.scala/udf/98.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getInt(0).toString
"
"udf/spark_repos_1/8_data-commons_prep-buddy/..src.main.scala.com.thoughtworks.datacommons.prepbuddy.analyzers.AnalyzableDataset.scala/udf/18.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

definition.isComplete(_)
"
"udf/spark_repos_1/8_data-commons_prep-buddy/..src.main.scala.com.thoughtworks.datacommons.prepbuddy.analyzers.AnalyzableDataset.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

record => {
          val stringValue: String = if (record.anyNull) null else record(0).toString
          FieldType.inferField(stringValue) != expected.dataType
        }
"
"udf/spark_repos_1/8_Salamahin_joinwiz/..joinwiz_core.src.main.scala.joinwiz.spark.package.scala/udf/22.21.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

func
"
"udf/spark_repos_1/8_Salamahin_joinwiz/..joinwiz_core.src.main.scala.joinwiz.spark.package.scala/udf/34.24.Dataset-L.filter","Type: org.apache.spark.sql.Dataset[L]
Call: filter

predicate
"
"udf/spark_repos_1/98_projectglow_glow/..core.src.main.scala.io.projectglow.transformers.splitmultiallelics.VariantSplitter.scala/udf/32.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(numAlleles: Int, ploidy: Int, alleleIdx: Int) => refAltColexOrderIdxArray(numAlleles, ploidy, alleleIdx)
"
"udf/spark_repos_1/9_dhinojosa_spark-training/..spark-api.src.test.scala.com.xyzcorp.SparkBasicRDDSpec.scala/udf/127.20.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

x => x + 1
"
"udf/spark_repos_1/9_dhinojosa_spark-training/..spark-api.src.test.scala.com.xyzcorp.SparkBasicRDDSpec.scala/udf/144.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getInt(0)
"
"udf/spark_repos_1/9_dhinojosa_spark-training/..spark-api.src.test.scala.com.xyzcorp.SparkDatasetSpec.scala/udf/103.25.Dataset-Trade.filter","Type: org.apache.spark.sql.Dataset[com.xyzcorp.Trade]
Call: filter

_.getLocalDate.getYear == 2017
"
"udf/spark_repos_1/9_dhinojosa_spark-training/..spark-api.src.test.scala.com.xyzcorp.SparkDatasetSpec.scala/udf/31.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => s.length
"
"udf/spark_repos_1/9_dhinojosa_spark-training/..spark-api.src.test.scala.com.xyzcorp.SparkDatasetSpec.scala/udf/41.22.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

x => x % 2 == 0
"
"udf/spark_repos_1/9_dhinojosa_spark-training/..spark-api.src.test.scala.com.xyzcorp.SparkDatasetSpec.scala/udf/59.24.Dataset-Trade.filter","Type: org.apache.spark.sql.Dataset[com.xyzcorp.Trade]
Call: filter

t => t.close > t.open
"
"udf/spark_repos_1/9_dhinojosa_spark-training/..spark-api.src.test.scala.com.xyzcorp.SparkDatasetSpec.scala/udf/70.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""Date"".endsWith(""16"")
"
"udf/spark_repos_1/9_dhinojosa_spark-training/..spark-api.src.test.scala.com.xyzcorp.SparkDatasetSpec.scala/udf/79.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""Date"".endsWith(""16"")
"
"udf/spark_repos_1/9_gerdreiss_learning-spark/..src.main.scala.com.github.learningspark.udemy1.DataFrames.scala/udf/24.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.github.learningspark.udemy1.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_1/9_goibibo_dataplatform_utils/..src.main.scala.com.goibibo.dp.utils.DeltaUtils.scala/udf/224.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

to_sorted_str
"
"udf/spark_repos_1/9_LinMingQiang_spark-learn/..spark-sql.src.main.scala.com.spark.learn.Test.scala/udf/21.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getDayHourFromTimestamp
"
"udf/spark_repos_1/9_LinMingQiang_spark-learn/..spark-sql.src.main.scala.com.spark.learn.UDFATest.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Row.merge(r, Row(""hello""))
"
"udf/spark_repos_2/10_piotr-kalanski_data-quality-monitoring/..src.main.scala.com.datawizards.dqm.filter.FilterByYearMonthDayColumns.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""year"") === lit(year) && col(""month"") === lit(month) && col(""day"") === lit(day)
"
"udf/spark_repos_2/11_aamend_spark-gdelt/..src.main.scala.com.aamend.spark.gdelt.package.scala/udf/72.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

GdeltParser.parseGkg
"
"udf/spark_repos_2/11_aamend_spark-gdelt/..src.main.scala.com.aamend.spark.gdelt.package.scala/udf/83.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

GdeltParser.parseEvent
"
"udf/spark_repos_2/11_aamend_spark-gdelt/..src.main.scala.com.aamend.spark.gdelt.package.scala/udf/94.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

GdeltParser.parseMention
"
"udf/spark_repos_2/13_hn5092_spring4spark/..src.main.scala.com.pajk.bigdata.SparkProcess.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.ARIMA.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(residual + lag + q).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.ARIMA.scala/udf/74.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + prefix + lag + maxPQ).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.ARIMA.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(residual + lag + q).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.ARMA.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + lag + maxPQ).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.ARMA.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(residual + lag + q).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.ARMA.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + lag + maxPQ).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.ARMA.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(residual + lag + q).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.ARYuleWalker.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + ""_lag_"" + p).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.AutoRegression.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + prefix + lag + p).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.AutoRegression.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + prefix + lag + p).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.MovingAverage.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(residual + lag + q).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.MovingAverage.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(residual + lag + q).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.TimeSeriesDiff.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + ""_lag_"" + lag).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.TimeSeriesDiff.scala/udf/24.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(diff1Col + ""_lag_"" + 1).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.TimeSeriesUtil.scala/udf/161.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + ""_lag_"" + 1).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.TimeSeriesUtil.scala/udf/165.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(diff1Col + ""_lag_"" + maxLag).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.TimeSeriesUtil.scala/udf/172.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(diff1Col + ""_lag_"" + 1).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.TimeSeriesUtil.scala/udf/176.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(diff2Col + ""_lag_"" + maxLag).isNotNull
"
"udf/spark_repos_2/15_liao-iu_scalaTS/..scalaTS.src.main.scala.com.suning.mlaas.spark.mllib.ts.TimeSeriesUtil.scala/udf/50.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inputCol + ""_lag_"" + i).isNotNull
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameCreateSpec.scala/udf/127.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""name"") equalTo ""John""
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameCreateSpec.scala/udf/131.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""age"") < 30
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameCreateSpec.scala/udf/135.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""address.state"") equalTo ""NY""
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameCreateSpec.scala/udf/139.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""jaddress.city"") equalTo ""Charlotte""
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/101.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoIntersects polygon(point(2, 2), point(-2, 2), point(2, 4), point(2, 2))
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/105.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""point"") geoWithin rectangle(-2, 2, -2, 2)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/109.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""point"") geoWithin rectangle(2, 4, -2, 2)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/244.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(attribute) geoWithin rectangle(-1, 1, -1, 1)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/25.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoIntersects circle(point(1, 0), 1)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/29.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoIntersects circle(point(3, 0), 1)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoWithin circle(point(1, 0), 2)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoWithin circle(point(1, 0), 1)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/41.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoContains circle(point(0, 0), 0.5d)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoContains circle(point(1, 0), 1)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""rect"") geoIntersects rectangle(1, 3, 1, 3)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/53.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""rect"") geoIntersects rectangle(3, 5, 0, 2)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/57.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""rect"") geoWithin rectangle(-1, 3, -1, 3)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/61.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""rect"") geoWithin rectangle(1, 3, 1, 3)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/65.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""rect"") geoContains rectangle(0.5d, 1.5d, 0.5d, 1.5d)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/69.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""rect"") geoContains rectangle(1, 3, 1, 3)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/73.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""point"") geoWithin rectangle(0, 2, 0, 2)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/77.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""point"") geoWithin rectangle(2, 3, 2, 3)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/81.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoIntersects lineString(point(0, 0), point(0, 2), point(2, 2))
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/85.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoIntersects lineString(point(0, 2), point(2, 2), point(2, 0))
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/89.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoWithin rectangle(-2, 2, -2, 2)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/93.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoWithin rectangle(0, 2, -2, 2)
"
"udf/spark_repos_2/16_InsightEdge_insightedge/..insightedge-core.src.test.scala.org.apache.spark.sql.insightedge.dataframe.DataFrameSpatialSpec.scala/udf/97.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""circle"") geoIntersects polygon(point(0, 0), point(0, 2), point(2, 2), point(0, 0))
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.it.scala.au.com.agl.arc.plugins.UDFPluginTest.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => UDFPluginTest.strReverse(s)
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.transform.DiffTransform.scala/udf/39.24.Dataset-Row).filter","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, org.apache.spark.sql.Row)]
Call: filter

col(""_1"").isNotNull
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.transform.DiffTransform.scala/udf/41.22.Dataset-Row).filter","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, org.apache.spark.sql.Row)]
Call: filter

col(""_2"").isNotNull
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.transform.DiffTransform.scala/udf/45.22.Dataset-Row).filter","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, org.apache.spark.sql.Row)]
Call: filter

col(""_2"").isNull
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.transform.DiffTransform.scala/udf/49.22.Dataset-Row).filter","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, org.apache.spark.sql.Row)]
Call: filter

col(""_1"").isNull
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.udf.UDF.scala/udf/48.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getJSONDoubleArray _
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.udf.UDF.scala/udf/52.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getJSONIntArray _
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.udf.UDF.scala/udf/56.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getJSONLongArray _
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.udf.UDF.scala/udf/60.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getRandom _
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.validate.EqualityValidate.scala/udf/112.23.Dataset-Row).filter","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, org.apache.spark.sql.Row)]
Call: filter

col(""_2"").isNull
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.main.scala.au.com.agl.arc.validate.EqualityValidate.scala/udf/116.23.Dataset-Row).filter","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, org.apache.spark.sql.Row)]
Call: filter

col(""_1"").isNull
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.test.scala.au.com.agl.arc.extract.ImageExtractSuite.scala/udf/84.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""image.origin"".contains(targetFile.replace(""file:"", ""file://""))
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.test.scala.au.com.agl.arc.plugins.UDFPluginTest.scala/udf/12.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDFPluginTest.addTwenty _
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.test.scala.au.com.agl.arc.plugins.UDFPluginTest.scala/udf/8.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDFPluginTest.addTen _
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.test.scala.au.com.agl.arc.transform.DiffTransformSuite.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""integerDatum"" === 17
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.test.scala.au.com.agl.arc.transform.DiffTransformSuite.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""integerDatum"" === 34
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.test.scala.au.com.agl.arc.transform.DiffTransformSuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""integerDatum"" === 35
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.test.scala.au.com.agl.arc.util.TestDataUtils.scala/udf/29.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""actual"").isNull
"
"udf/spark_repos_2/18_AGLEnergy_arc/..src.test.scala.au.com.agl.arc.util.TestDataUtils.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""expected"").isNull
"
"udf/spark_repos_2/18_databrickslabs_dataframe-rules-engine/..src.main.scala.com.databricks.labs.validation.Validator.scala/udf/114.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'Failed === true
"
"udf/spark_repos_2/1_ada-discovery_ada-app/..server.src.main.scala.org.ada.server.services.ml.MachineLearningService.scala/udf/162.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
        val id = r(0).asInstanceOf[String]
        val clazz = r(1).asInstanceOf[Int]
        (id, clazz + 1)
      }
"
"udf/spark_repos_2/1_ada-discovery_ada-app/..web-ncer.app.runnables.ml.LogisticRegressionExample.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

data(""label"").===(1)
"
"udf/spark_repos_2/1_ada-discovery_ada-app/..web-ncer.app.runnables.ml.LogisticRegressionExample.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

data(""label"").===(0)
"
"udf/spark_repos_2/1_afoerster_spark-incremental-merge/..src.main.scala.SparkMerge.scala/udf/69.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

originalRecords(partitionColumn).isin(partitions: _*)
"
"udf/spark_repos_2/1_amitnema_spark-coach/..src.main.scala.org.apn.spark.dsl.WordCount.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => RowFactory.create(x.trim())
"
"udf/spark_repos_2/1_amuradyan_SparkStreamingHDFSDemo/..src.main.scala.example.ScalaSparkStreamingHDFSDemo.scala/udf/15.22.Dataset-Entity.filter","Type: org.apache.spark.sql.Dataset[example.ScalaSparkStreamingHDFSDemo.Entity]
Call: filter

_.tt == 1
"
"udf/spark_repos_2/1_amuradyan_SparkStreamingHDFSDemo/..src.main.scala.example.ScalaSparkStreamingHDFSDemo.scala/udf/19.22.Dataset-Entity.filter","Type: org.apache.spark.sql.Dataset[example.ScalaSparkStreamingHDFSDemo.Entity]
Call: filter

_.tt == 2
"
"udf/spark_repos_2/1_amuradyan_SparkStreamingHDFSDemo/..src.main.scala.example.ScalaSparkStreamingHDFSDemo.scala/udf/23.22.Dataset-Entity.filter","Type: org.apache.spark.sql.Dataset[example.ScalaSparkStreamingHDFSDemo.Entity]
Call: filter

_.tt == 3
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.main.scala.org.apache.spark.mllib.util.MLUtils.scala/udf/71.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""same_bucket"")
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""same_bucket"")
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/30.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") > distFP
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/34.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") < distFN
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distUDF(col(s""a.$inputCol""), col(s""b.$inputCol"")) < threshold
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/64.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""distCol"") < threshold
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.feature.VectorAssemblerSuite.scala/udf/95.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

vectorUDF($""features"") > 1
"
"udf/spark_repos_2/1_Andy112646_TrafficProject/..IdeaProjects.BigData.spark-2.3.1.mllib.src.test.scala.org.apache.spark.ml.fpm.FPGrowthSuite.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""freq"") === col(""expectedFreq"")
"
"udf/spark_repos_2/1_aravinthsci_spark-streamingapp/..src.main.scala.com.streamapp.kafka.KafkaStructuredStreaming.scala/udf/16.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_2/1_aravinthsci_spark-streamingapp/..src.main.scala.com.streamapp.kafka.KafkatoCassandra.scala/udf/16.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

r => r.split("","")
"
"udf/spark_repos_2/1_arunansu_CourseraScala/..observatory.src.main.scala.observatory.Extraction.scala/udf/20.19.Dataset-Joined.map","Type: org.apache.spark.sql.Dataset[observatory.Joined]
Call: map

j => (StationDate(j.day, j.month, j.year), Location(j.latitude, j.longitude), j.temperature)
"
"udf/spark_repos_2/1_buildlackey_spark-structured-streaming-vs-dstreams/..src.main.scala.com.lackey.stream.examples.dataset.StructuredStreamingTopSensorState.scala/udf/15.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""rank"") <= 1
"
"udf/spark_repos_2/1_carbon-01_spark/..spark_sql.src.main.scala.com.atguigu.sql.day01.Test02.scala/udf/9.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MySum
"
"udf/spark_repos_2/1_carbon-01_spark/..spark_sql.src.main.scala.com.atguigu.sql.day01.Test04.scala/udf/14.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MySumAvg
"
"udf/spark_repos_2/1_carbon-01_spark/..spark_sql.src.main.scala.com.atguigu.sql.day01.Test04.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !(row.isNullAt(0) || row.isNullAt(1))
"
"udf/spark_repos_2/1_carbon-01_spark/..spark_sql.src.main.scala.com.atguigu.sql.day01.Test05.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !(row.isNullAt(0) || row.isNullAt(1))
"
"udf/spark_repos_2/1_carbon-01_spark/..spark_sql.src.main.scala.com.atguigu.sql.day01.Test07.scala/udf/8.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => s.toUpperCase()
"
"udf/spark_repos_2/1_carbon-01_spark/..sprk-sql-project.src.main.scala.com.atuguigu.spark.sql.Sql0830.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ReMark
"
"udf/spark_repos_2/1_ccalvomartinez_SparkScalaExercices/..src.main.scala.com.carolina.calvo.domain.Domain.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        acumulator.add(1)
        Transaction(Client(acumulator.value, row.getAs[String](""Name"").trim, row.getAs[String](""Account_Created"").toLong), parseToDate(row.getAs[String](""Transaction_date""), ""M/d/yy h:mm""), row.getAs[String](""Price"").toDouble, row.getAs[String](""Description"").trim, getCategory(row.getAs[String](""Description"").trim), row.getAs(""Payment_Type""), Geolocation(row.getAs[String](""Latitude"").toDouble, row.getAs[String](""Longitude"").toDouble, row.getAs[String](""City"").trim, """"))
      }
"
"udf/spark_repos_2/1_ccalvomartinez_SparkScalaExercices/..src.main.scala.com.carolina.calvo.domain.Domain.scala/udf/58.19.Dataset-Transaction.map","Type: org.apache.spark.sql.Dataset[com.carolina.calvo.model.Transaction]
Call: map

tr => {
        tr.geolocation.country = getCountry(tr.geolocation.latitude, tr.geolocation.longitude)
        tr
      }
"
"udf/spark_repos_2/1_CCHHHHH_dataimport/..src.main.java.com.importdataset.data.dataControlUtils.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.getLong(0) % 30 == 0
"
"udf/spark_repos_2/1_chengwenquan_Spark_structured/..src.main.scala.day02.eventtime.EventTimeDemo02.scala/udf/11.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

e => {
          val splits = e.split("","")
          (splits(0), splits(1))
        }
"
"udf/spark_repos_2/1_chengwenquan_Spark_structured/..src.main.scala.day02.join.StreamingStatic.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

e => {
        val splits: mutable.ArrayOps[String] = e.getString(0).split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_2/1_chengwenquan_Spark_structured/..src.main.scala.day02.join.StreamToStream.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

e => {
        val splits: Array[String] = e.split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_2/1_chengwenquan_Spark_structured/..src.main.scala.day02.join.StreamToStream.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

e => {
        val splits: Array[String] = e.split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_2/1_chengwenquan_Spark_structured/..src.main.scala.day02.watermark.DropduplicateDemo.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

e => {
        val splits: Array[String] = e.split("","")
        (splits(0), Timestamp.valueOf(splits(1)), splits(2))
      }
"
"udf/spark_repos_2/1_CODAIT_streaming-integration-sample/..src.main.scala.org.codait.streaming.SparkJobMultiStreams2.scala/udf/31.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

x => x.startsWith(""0"")
"
"udf/spark_repos_2/1_CODAIT_streaming-integration-sample/..src.main.scala.org.codait.streaming.SparkJobMultiStreams2.scala/udf/48.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

x => x.startsWith(""1"")
"
"udf/spark_repos_2/1_CODAIT_streaming-integration-sample/..src.main.scala.org.codait.streaming.SparkJobMultiStreams.scala/udf/31.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

x => x.startsWith(""0"")
"
"udf/spark_repos_2/1_CODAIT_streaming-integration-sample/..src.main.scala.org.codait.streaming.SparkJobMultiStreams.scala/udf/48.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

x => x.startsWith(""1"")
"
"udf/spark_repos_2/1_curryli_SparkLearn/..CF_card_mchnt.src.example.card_mchnt_test.scala/udf/103.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

predictionsImplicit(""ranks"") < 100
"
"udf/spark_repos_2/1_curryli_SparkLearn/..CF_card_mchnt.src.example.card_mchnt_test.scala/udf/90.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

mchnt_df_pairs(""mchnt_cd"").isin(test_mchntlist: _*)
"
"udf/spark_repos_2/1_curryli_SparkLearn/..CF_card_mchnt.src.example.recom_rank.scala/udf/73.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

test_mchnt(""ranks"") < 20
"
"udf/spark_repos_2/1_dadanhrn_npmvuln/..src.main.scala.npmvuln.jobs.AdvisoryDfBuilder.scala/udf/10.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Name"") =!= ""Malicious Package""
"
"udf/spark_repos_2/1_dadanhrn_npmvuln/..src.main.scala.npmvuln.jobs.AdvisoryDfBuilder.scala/udf/12.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""Versions"") =!= ""*""
"
"udf/spark_repos_2/1_dadanhrn_npmvuln/..src.main.scala.npmvuln.jobs.DependsOnEdgesBuilder.scala/udf/9.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

func = row => {
        val dependentId: VertexId = row.getAs[VertexId](""ReleaseId"")
        val dependencyId: VertexId = row.getAs[VertexId](""ProjectId"")
        val constraint: String = row.getAs[String](""Constraint"")
        val dependencyProp: DependsOnEdge = new DependsOnEdge(constraint)
        new Edge[DependsOnEdge](dependencyId, dependentId, dependencyProp)
      }
"
"udf/spark_repos_2/1_dadanhrn_npmvuln/..src.main.scala.npmvuln.jobs.PackageStateVerticesBuilder.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val releaseId: VertexId = row.getAs[VertexId](""ReleaseId"")
        val packageName: String = row.getAs[String](""Project"")
        val version: String = row.getAs[String](""Release"")
        val from: Instant = row.getAs[Timestamp](""Date"").toInstant
        val to: Instant = row.getAs[Timestamp](""NextReleaseDate"").toInstant
        val latestPeriod: Interval = Interval.of(from, to)
        val packageStateVertex: PackageStateVertex = new PackageStateVertex(packageName, version, latestPeriod)
        (releaseId, packageStateVertex)
      }
"
"udf/spark_repos_2/1_dadanhrn_npmvuln/..src.main.scala.npmvuln.jobs.PackageVerticesBuilder.scala/udf/9.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val projectId: VertexId = row.getAs[VertexId](""ProjectId"")
        val projectName: String = row.getAs[String](""Project"")
        val packageVertex: PackageVertex = new PackageVertex(projectName)
        (projectId, packageVertex)
      }
"
"udf/spark_repos_2/1_dadanhrn_npmvuln/..src.main.scala.npmvuln.jobs.SnapshotEdgesBuilder.scala/udf/9.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val packageId: VertexId = row.getAs[VertexId](""ProjectId"")
        val packageStateId: VertexId = row.getAs[VertexId](""ReleaseId"")
        val packageName: String = row.getAs[String](""Project"")
        val version: String = row.getAs[String](""Release"")
        val snapshotProp: SnapshotEdge = new SnapshotEdge(packageName, version)
        new Edge[SnapshotEdge](packageStateId, packageId, snapshotProp)
      }
"
"udf/spark_repos_2/1_dadanhrn_npmvuln/..src.main.scala.npmvuln.jobs.VulnerabilityDfBuilder.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val vertexId: VertexId = row.getAs[VertexId](""ReleaseId"")
        val vulnId: String = row.getAs[String](""Id"")
        val vulnName: String = row.getAs[String](""Name"")
        val severity: String = row.getAs[String](""Severity"")
        val releaseDate: Instant = row.getAs[Timestamp](""Date"").toInstant
        val nextReleaseDate: Instant = row.getAs[Timestamp](""NextReleaseDate"").toInstant
        val period: Interval = Interval.of(releaseDate, nextReleaseDate)
        (vertexId, new VulnProperties(vulnId, vulnName, severity, period))
      }
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.dataAwareJoin.data.SkewedDataGenerator.scala/udf/18.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

Key
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.dataAwareJoin.data.UniformDataGenerator.scala/udf/24.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

Key
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.dataAwareJoin.join.IterativeBroadcastJoin.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""pass"") === lit(iteration)
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.dataAwareJoin.join.SliceJoinDetail.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

e => key_part_bc.value.get(e(0).toString) != None
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.dataAwareJoin.join.SliceJoinDetail.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

e => key_part_bc.value.get(e(0).toString) == None
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.dataAwareJoin.join.SliceJoin.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

e => key_part_bc.value.get(e(0).toString) != None
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.dataAwareJoin.join.SliceJoin.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

e => key_part_bc.value.get(e(0).toString) == None
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.iterativeBroadcastJoin.data.SkewedDataGenerator.scala/udf/18.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

Key
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.iterativeBroadcastJoin.data.UniformDataGenerator.scala/udf/24.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

Key
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.iterativeBroadcastJoin.join.IterativeBroadcastJoin2.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""pass"") === lit(iteration)
"
"udf/spark_repos_2/1_eric27yang_Spark-scala/..src.main.scala.org.iscas.iterativeBroadcastJoin.join.IterativeBroadcastJoin.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""pass"") === lit(iteration)
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/100.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Dimension
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/104.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Envelope
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/108.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_ExteriorRing
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/112.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeometryN
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/116.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeometryType
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/120.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_InteriorRingN
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/124.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsClosed
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/128.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsCollection
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/132.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsEmpty
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/136.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsRing
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/140.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsSimple
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/144.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsValid
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/148.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_NumGeometries
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/152.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_NumPoints
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/156.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PointN
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/160.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_X
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/164.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Y
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/92.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Boundary
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/96.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CoordDim
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/14.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CastToPoint
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/18.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CastToPolygon
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/22.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CastToLineString
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/26.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_ByteArray
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/101.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Point
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/105.64.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PointFromGeoHash
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/109.61.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PointFromText
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/113.60.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PointFromWKB
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/117.55.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Polygon
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/121.63.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PolygonFromText
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/37.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromGeoHash
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/41.62.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromGeoHash
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/45.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKT
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/49.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKT
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/53.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKT
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/57.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKB
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/61.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_LineFromText
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/65.60.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MLineFromText
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/69.62.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MPointFromText
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/73.61.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MPolyFromText
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/77.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakeBBOX
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/81.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakeBox2D
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/85.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakeLine
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/89.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakePoint
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/93.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakePointM
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/97.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakePolygon
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/24.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AsBinary
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/28.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AsGeoJSON
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/32.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AsLatLonText
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/36.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AsText
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/40.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeoHash
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricProcessingFunctions.scala/udf/48.66.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_antimeridianSafeGeom
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricProcessingFunctions.scala/udf/52.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_antimeridianSafeGeom
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricProcessingFunctions.scala/udf/56.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_BufferPoint
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/102.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Distance
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/106.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Length
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/110.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_DistanceSphere
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/114.68.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AggregateDistanceSphere
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/118.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_LengthSphere
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/122.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ch
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/42.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Translate
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/46.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Contains
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/50.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Covers
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/54.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Crosses
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/58.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Disjoint
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/62.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Equals
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/66.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Intersects
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/70.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Overlaps
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/74.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Touches
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/78.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Within
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/82.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Relate
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/86.55.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_RelateBool
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/90.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Area
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/94.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_ClosestPoint
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/98.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Centroid
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-sql.src.main.scala.org.locationtech.geomesa.spark.GeometricDistanceFunctions.scala/udf/21.60.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_DistanceSpheroid
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-sql.src.main.scala.org.locationtech.geomesa.spark.GeometricDistanceFunctions.scala/udf/25.69.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AggregateDistanceSpheroid
"
"udf/spark_repos_2/1_eric-erki_geomesa/..geomesa-spark.geomesa-spark-sql.src.main.scala.org.locationtech.geomesa.spark.GeometricDistanceFunctions.scala/udf/29.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_LengthSpheroid
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..hive.src.main.java.com.li.hive.Check.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[Long](""userId"")
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..hive.src.main.java.com.li.hive.Check.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        f => u.value.contains(f.getAs[Long](0))
      }
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..mllib.src.main.java.com.li.mllib.word.tfidf.apple.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: String, features: Vector) =>
          LabeledPoint(label.toDouble, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.ActiveComputing.scala/udf/40.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RowColStat()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.ActiveComputing.scala/udf/46.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.ActiveComputing.scala/udf/50.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new ActiveComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.ActiveComputing.scala/udf/54.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RetentionCompute()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.ActiveComputing.scala/udf/58.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new ActiveComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.AdvancedComputing.scala/udf/40.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RowColStat()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.AdvancedComputing.scala/udf/46.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.AdvancedComputing.scala/udf/50.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.CreateData.scala/udf/25.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RowColStat()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.CreateData.scala/udf/30.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.CreateData.scala/udf/34.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.RetentionComputing.scala/udf/39.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RowColStat()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.RetentionComputing.scala/udf/45.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.RetentionComputing.scala/udf/49.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.RetentionComputing.scala/udf/74.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RetentionCompute()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.RowColComputing.scala/udf/38.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RowColStat()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.UDFRowColStatComputing.scala/udf/40.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RowColStat()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.UDFRowColStatComputing.scala/udf/47.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.data.UDFRowColStatComputing.scala/udf/51.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/31.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CompareValueRate()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/35.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CompareValueSum()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/39.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new Compare()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/43.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CompareRate()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/49.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new RowColStat()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/53.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AdvancedComputing()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/57.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new HighMathJava()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/61.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new SumA()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/65.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new DiscountA()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.Tongbi.scala/udf/69.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CountA()
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.function.udaf.UDAFunction.scala/udf/16.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new UserDefinedAggregateFunction() {
        override def inputSchema: StructType = DataTypes.createStructType(util.Arrays.asList(DataTypes.createStructField(""food_amount"", DataTypes.DoubleType, true), DataTypes.createStructField(""channel_name"", DataTypes.StringType, true)))
        override def bufferSchema: StructType = DataTypes.createStructType(util.Arrays.asList(DataTypes.createStructField(""sum_food_amount"", DataTypes.DoubleType, true), DataTypes.createStructField(""count_food_amount"", DataTypes.IntegerType, true), DataTypes.createStructField(""count_channel_name"", DataTypes.IntegerType, true)))
        override def dataType: DataType = DataTypes.DoubleType
        override def deterministic = true
        override def initialize(buffer: MutableAggregationBuffer): Unit = {
          buffer.update(0, 0.00d)
          buffer.update(1, 0)
          buffer.update(2, 0)
        }
        override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
          buffer.update(0, buffer.getDouble(0) + input.getDouble(0))
          buffer.update(1, buffer.getInt(1) + 1)
          buffer.update(2, buffer.getInt(2) + 1)
        }
        override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
          buffer1.update(0, buffer1.getDouble(0) + buffer2.getDouble(0))
          buffer1.update(1, buffer1.getInt(1) + buffer2.getInt(1))
          buffer1.update(2, buffer1.getInt(2) + buffer2.getInt(2))
        }
        override def evaluate(row: Row): Any = row.getDouble(0) + row.getInt(2) + row.getDouble(0) / row.getInt(1)
      }
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.scala.HiveWrite.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.scala.Json2Row.scala/udf/33.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String, str2: String) => str.hashCode % 2 + 1
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.scala.UDAFScala.scala/udf/22.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new StringCount
"
"udf/spark_repos_2/1_fjl121029xx_myspark/..sql.src.main.java.com.li.sql.scala.UDFScala.scala/udf/20.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.hashCode % 2
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/102.35.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""ID_USER"") === lit(row.getInt(0))
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/119.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""total_reviews >= $minReviews""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/152.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""total_reviews >= $minReviews AND total_recipes >= total_reviews""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/339.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""ID_USER = $userID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/343.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""ID_USER = $userID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/347.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""ID_USER = $userID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/35.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""CALORIES"") === lit(0).cast(FloatType)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/357.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""ID_RECIPE = $recipeID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/362.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""ID_RECIPE = $recipeID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender_old.scala/udf/71.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col >= lit(i)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/104.35.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""USER_ID"") === lit(row.getInt(0))
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/121.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""total_reviews >= $minReviews AND total_recipes >= $minRecipes""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/154.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""total_reviews >= $minReviews AND total_recipes >= $minRecipes""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""CALORIES"") === lit(0).cast(FloatType)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/360.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""USER_ID = $userID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/364.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""USER_ID = $userID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/368.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""USER_ID = $userID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/378.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""RECIPE_ID = $recipeID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/384.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""RECIPE_ID = $recipeID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.NonSupervisedRecommender.scala/udf/73.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col >= lit(i)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.Preprocesser.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ID"") =!= lit(0)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.PropagationRecommender.scala/udf/35.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""count >= $minReviews""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.allrecipes.PropagationRecommender.scala/udf/47.32.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

s""USER_ID = $userID""
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.IngredientPreprocessing.scala/udf/249.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""FLAG"") === lit(0)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.IngredientPreprocessing.scala/udf/257.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""FLAG"") === lit(0)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.IngredientPreprocessing.scala/udf/29.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ID_INGREDIENT"") =!= lit(0)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.IngredientPreprocessing.scala/udf/46.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ID_INGREDIENT"") =!= lit(0)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.IngredientPreprocessing.scala/udf/59.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ID_INGREDIENT"") =!= lit(0)
"
"udf/spark_repos_2/1_fmendezlopez_TFM-Spark/..src.main.scala.es.uam.eps.tfm.fmendezlopez.utils.SparkUtils.scala/udf/129.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""ROW_NUM <= $k""
"
"udf/spark_repos_2/1_gdonthu27_SparkScala/..startup.src.main.scala.com.sample.spark.WindowFunctions.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

unix_timestamp(col(""next_interval"")) - unix_timestamp(col(""unique_utc_starttime"")) > rollupInMin * 60
"
"udf/spark_repos_2/1_geoHeil_sparkContrastCoding/..src.main.scala.MyEstimator.scala/udf/70.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

df(target) === 1
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/134.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""name000"", ""name001"", ""name002"", ""name003"", ""name004"") and !($""col0"" isin (""name001"", ""name002"", ""name003""))
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/114.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..core.src.test.scala.org.apache.spark.sql.PhoenixSuite.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""row005"", ""row001"", ""row002"") and !($""col0"" isin (""row001"", ""row002""))
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/64.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/88.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/100.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobAccessing2Clusters.scala/udf/79.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobAccessing2Clusters.scala/udf/83.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""5""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/59.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""40""
"
"udf/spark_repos_2/1_gjeevanm_SparkHBaseConnector/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""20"" && $""key"" >= ""1""
"
"udf/spark_repos_2/1_gridu_STREAMING-mcieczynski/..bot-detector.src.main.scala.pl.mcieszynski.gridu.detector.structured.DetectorServiceStructured.scala/udf/25.19.Dataset-Event.map","Type: org.apache.spark.sql.Dataset[pl.mcieszynski.gridu.detector.events.Event]
Call: map

convertToStructuredEvent
"
"udf/spark_repos_2/1_gridu_STREAMING-mcieczynski/..bot-detector.src.main.scala.pl.mcieszynski.gridu.detector.structured.DetectorServiceStructured.scala/udf/32.21.Dataset-AggregatedIpInformation).map","Type: org.apache.spark.sql.Dataset[(String, pl.mcieszynski.gridu.detector.events.AggregatedIpInformation)]
Call: map

aggregatedIpInformation => DetectedBot(aggregatedIpInformation._1, System.currentTimeMillis(), aggregatedIpInformation._2.botDetected.get)
"
"udf/spark_repos_2/1_gridu_STREAMING-mcieczynski/..bot-detector.src.main.scala.pl.mcieszynski.gridu.detector.structured.DetectorServiceStructured.scala/udf/42.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

func = recordTuple => eventConversion(recordTuple._1, recordTuple._2)
"
"udf/spark_repos_2/1_gridu_STREAMING-mcieczynski/..bot-detector.src.main.scala.pl.mcieszynski.gridu.detector.structured.DetectorServiceStructured.scala/udf/44.22.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[pl.mcieszynski.gridu.detector.events.Event]
Call: filter

event => event != null
"
"udf/spark_repos_2/1_gridu_STREAMING-mcieczynski/..bot-detector.src.main.scala.pl.mcieszynski.gridu.detector.structured.DetectorServiceStructured.scala/udf/64.22.Dataset-StructuredEvent.filter","Type: org.apache.spark.sql.Dataset[pl.mcieszynski.gridu.detector.events.StructuredEvent]
Call: filter

event => !previouslyDetectedBotIps.contains(event.ip)
"
"udf/spark_repos_2/1_gridu_STREAMING-mcieczynski/..bot-detector.src.main.scala.pl.mcieszynski.gridu.detector.structured.DetectorServiceStructured.scala/udf/76.22.Dataset-AggregatedIpInformation).filter","Type: org.apache.spark.sql.Dataset[(String, pl.mcieszynski.gridu.detector.events.AggregatedIpInformation)]
Call: filter

func = (aggregatedIpInformation: (String, AggregatedIpInformation)) => aggregatedIpInformation._2.botDetected.nonEmpty
"
"udf/spark_repos_2/1_haichao-zhao_sparksql-train/..src.main.scala.com.zhc.bigdata.chapter04.DatasetAPIApp.scala/udf/14.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/1_haichao-zhao_sparksql-train/..src.main.scala.com.zhc.bigdata.chapter04.InteroperatingRDDApp.scala/udf/37.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => ""Name:"" + x.getAs[String](""name"")
"
"udf/spark_repos_2/1_haichao-zhao_sparksql-train/..src.main.scala.com.zhc.bigdata.chapter05.DataSourseApp.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val strings = x.getString(0).split("","")
        (strings(0).trim, strings(1).trim.toInt)
      }
"
"udf/spark_repos_2/1_haichao-zhao_sparksql-train/..src.main.scala.com.zhc.bigdata.chapter05.DataSourseApp.scala/udf/73.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x(0) + ""|"" + x(1).toString
"
"udf/spark_repos_2/1_haichao-zhao_sparksql-train/..src.main.scala.com.zhc.bigdata.chapter06.HiveSourceApp.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""deptno"" === 3
"
"udf/spark_repos_2/1_haichao-zhao_sparksql-train/..src.main.scala.com.zhc.bigdata.chapter06.UDFApp.scala/udf/17.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => s.split("","").size
"
"udf/spark_repos_2/1_hardikgw_spark-analytics/..src.main.scala.IndexerMain.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getLong(1) > 2
"
"udf/spark_repos_2/1_hardikgw_spark-analytics/..src.main.scala.IndexerMain.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""ab.attr"".contains(""user_id"")
"
"udf/spark_repos_2/1_hardikgw_spark-analytics/..src.main.scala.IndexerMain.scala/udf/73.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getLong(1) > 2
"
"udf/spark_repos_2/1_hhj970815_dmp01/..src.main.scala.cn.itcast.pro.ParseIp.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

ip => {
        val lookupService = new LookupService(ConfigUtils.GEOLITECITY_DAT)
        val location: Location = lookupService.getLocation(ip)
        val longitude: Float = location.longitude
        val latitude: Float = location.latitude
        val iPAddressUtils = new IPAddressUtils
        val ipregion = iPAddressUtils.getregion(ip)
        val region = ipregion.getRegion
        val city = ipregion.getCity
        (ip, longitude, latitude, region, city)
      }
"
"udf/spark_repos_2/1_hhj970815_dmp01/..src.main.scala.cn.itcast.pro.TagPro.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""""""
        |(imei is not null and imei!='') or
        |(mac is not null and mac!='') or
        |(idfa is not null and idfa!='') or
        |(openudid is not null and openudid!='') or
        |(androidid is not null and androidid!='')
      """""".stripMargin
"
"udf/spark_repos_2/1_hhj970815_dmp01/..src.main.scala.cn.itcast.pro.TagPro.scala/udf/41.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr = line.split(""##"")
        val appId = arr.head
        val appName = arr.last
        (appId, appName)
      }
"
"udf/spark_repos_2/1_hhj970815_dmp01/..src.main.scala.cn.itcast.pro.TagPro.scala/udf/52.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr = line.split(""##"")
        val arrValue = arr.head
        val arrCode = arr.last
        (arrValue, arrCode)
      }
"
"udf/spark_repos_2/1_hhj970815_dmp01/..src.main.scala.cn.itcast.pro.TagPro.scala/udf/63.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

geoHashCode _
"
"udf/spark_repos_2/1_htmlandreza_twitter-bigdata/..src.main.scala.1_MostUsedHashtags.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isnull($""text"")
"
"udf/spark_repos_2/1_htmlandreza_twitter-bigdata/..src.main.scala.2_MostUsedHashtagsDate.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isnull($""text"")
"
"udf/spark_repos_2/1_htmlandreza_twitter-bigdata/..src.main.scala.3_MostUsedHashtagsDateJSON.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isnull($""text"")
"
"udf/spark_repos_2/1_htmlandreza_twitter-bigdata/..src.main.scala.4_MostUsedWords.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isnull($""text"")
"
"udf/spark_repos_2/1_Hu12138_spark2.x-mlib/..src.main.scala.ml.DataTest.scala/udf/12.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new Middle
"
"udf/spark_repos_2/1_ibm-cloud-streaming-retail-demo_spark-structured-streaming-on-iae-to-scylladb/..src.main.scala.kafka.KafkaSource.scala/udf/26.24.Dataset-InvoiceItemKafka.filter","Type: org.apache.spark.sql.Dataset[invoices.InvoiceItemKafka]
Call: filter

_.invoiceItem != null
"
"udf/spark_repos_2/1_ibm-cloud-streaming-retail-demo_spark-structured-streaming-on-iae-to-scylladb/..src.main.scala.kafka.KafkaSource.scala/udf/28.22.Dataset-InvoiceItemKafka.filter","Type: org.apache.spark.sql.Dataset[invoices.InvoiceItemKafka]
Call: filter

_.invoiceItem.InvoiceNo != null
"
"udf/spark_repos_2/1_inaccel_spark/..src.main.scala.org.apache.spark.mllib.util.MLUtils.scala/udf/71.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_2/1_jiayuasu_CSE512-Project-Phase2-Template/..src.main.scala.cse512.SpatialQuery.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => true
"
"udf/spark_repos_2/1_jiayuasu_CSE512-Project-Phase2-Template/..src.main.scala.cse512.SpatialQuery.scala/udf/32.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => true
"
"udf/spark_repos_2/1_jiayuasu_CSE512-Project-Phase2-Template/..src.main.scala.cse512.SpatialQuery.scala/udf/45.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => true
"
"udf/spark_repos_2/1_jiayuasu_CSE512-Project-Phase2-Template/..src.main.scala.cse512.SpatialQuery.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => true
"
"udf/spark_repos_2/1_JivenYi_Hello-Spark/..src.main.scala.sparksql.DataFrameOperationScala.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 19
"
"udf/spark_repos_2/1_JivenYi_Hello-Spark/..src.main.scala.sparksql.DataFrameRDDScala.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 30
"
"udf/spark_repos_2/1_JivenYi_Hello-Spark/..src.main.scala.sparksql.DataFrameRDDScala.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 30
"
"udf/spark_repos_2/1_jixuan1989_hadoop-spark-course/..src.main.scala.org.training.spark.sql.MovieUserAnalyzerWithDataFrame.scala/udf/50.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""movieid = $MOVIE_ID""
"
"udf/spark_repos_2/1_jixuan1989_hadoop-spark-course/..src.main.scala.org.training.spark.sql.SparkSQLSimpleExample.scala/udf/87.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          u => (u.getAs[String](""userID"").toLong, u.getAs[String](""age"").toInt + 1)
        }
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.dev.audit-release.sbt_app_sql.src.main.scala.SqlApp.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/40.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/92.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isGoodBucket($""count"")
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 0.0d
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 4.0d
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/113.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/51.23.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/98.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/553.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/557.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/585.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/589.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/896.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/905.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/78.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_2/1_joao-parana_spark-build/..2.0.1.spark-2.0.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/118.25.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerTaskEnd"")
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/120.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkTaskEnd](string)
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/125.20.Dataset-Stage).map","Type: org.apache.spark.sql.Dataset[(com.conversantmedia.sparkprofiler.SparkTaskEnd, com.conversantmedia.sparkprofiler.Stage)]
Call: map

tuple => {
        val applicationId = tuple._2.applicationId
        val applicationName = tuple._2.applicationName
        val taskInfo = tuple._1.Task$u0020Info
        val taskMetrics = tuple._1.Task$u0020Metrics
        val accumulableMemory = locally {
          val _t_m_p_18 = locally {
            val _t_m_p_19 = taskInfo.Accumulables
            _t_m_p_19.filter(_.Name == ""peakExecutionMemory"")
          }
          _t_m_p_18.map(i => i.Value.toLong)
        }
        val peakMemory = if (accumulableMemory.nonEmpty) accumulableMemory.head else 0
        val accumulableInputRows = locally {
          val _t_m_p_20 = locally {
            val _t_m_p_21 = taskInfo.Accumulables
            _t_m_p_21.filter(_.Name == ""number of input rows"")
          }
          _t_m_p_20.map(i => i.Value.toLong)
        }
        val inputRows = if (accumulableInputRows.nonEmpty) accumulableInputRows.head else 0
        val accumulableOutputRows = locally {
          val _t_m_p_22 = locally {
            val _t_m_p_23 = taskInfo.Accumulables
            _t_m_p_23.filter(_.Name == ""number of output rows"")
          }
          _t_m_p_22.map(i => i.Value.toLong)
        }
        val outputRows = if (accumulableOutputRows.nonEmpty) accumulableOutputRows.head else 0
        val jobId = tuple._2.jobId
        val stageId = tuple._2.stageId
        val stageAttemptId = tuple._2.stageAttemptId
        val taskId = taskInfo.Task$u0020ID
        val taskType = tuple._1.`Task Type`
        val attempt = taskInfo.Attempt
        val executorId = taskInfo.Executor$u0020ID
        val host = taskInfo.Host
        val stageName = tuple._2.stageName
        val locality = taskInfo.Locality
        val speculative = taskInfo.Speculative
        val applicationStartTime = tuple._2.applicationStartTime
        val applicationEndTime = tuple._2.applicationEndTime
        val jobStartTime = tuple._2.jobStartTime
        val jobEndTime = tuple._2.jobEndTime
        val stageStartTime = tuple._2.stageStartTime
        val stageEndTime = tuple._2.stageEndTime
        val taskStartTime = taskInfo.Launch$u0020Time
        val taskEndTime = taskInfo.Finish$u0020Time
        val failed = taskInfo.Failed
        val taskDuration = taskEndTime - taskStartTime
        val stageDuration = tuple._2.stageDuration
        val jobDuration = tuple._2.jobDuration
        val applicationDuration = tuple._2.applicationDuration
        val gettingResultTime = taskInfo.Getting$u0020Result$u0020Time
        val gcTime = if (taskMetrics.nonEmpty) taskMetrics.get.JVM$u0020GC$u0020Time else 0
        val resultSerializationTime = if (taskMetrics.nonEmpty) taskMetrics.get.Result$u0020Serialization$u0020Time else 0
        val resultSize = if (taskMetrics.nonEmpty) taskMetrics.get.Result$u0020Size else 0
        val memoryBytesSpilled = if (taskMetrics.nonEmpty) taskMetrics.get.Memory$u0020Bytes$u0020Spilled else 0
        val diskBytesSpilled = if (taskMetrics.nonEmpty) taskMetrics.get.Disk$u0020Bytes$u0020Spilled else 0
        val inputMetrics = if (taskMetrics.nonEmpty) taskMetrics.get.Input$u0020Metrics else None
        val shuffleWriteMetrics = if (taskMetrics.nonEmpty) taskMetrics.get.Shuffle$u0020Write$u0020Metrics else None
        val shuffleReadMetrics = if (taskMetrics.nonEmpty) taskMetrics.get.Shuffle$u0020Read$u0020Metrics else None
        val dataReadMethod = if (inputMetrics.nonEmpty) inputMetrics.get.Data$u0020Read$u0020Method else ""n/a""
        val bytesRead = if (inputMetrics.nonEmpty) inputMetrics.get.Bytes$u0020Read else 0L
        val recordsRead = inputMetrics match {
          case None =>
            0L
          case Some(_) =>
            inputMetrics.get.Records$u0020Read
        }
        val shuffleBytesWritten = shuffleWriteMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleWriteMetrics.get.Shuffle$u0020Bytes$u0020Written
        }
        val shuffleRecordsWritten = shuffleWriteMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleWriteMetrics.get.Shuffle$u0020Records$u0020Written
        }
        val shuffleWriteTime = shuffleWriteMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleWriteMetrics.get.Shuffle$u0020Write$u0020Time
        }
        val remoteBlocksFetched = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Remote$u0020Blocks$u0020Fetched
        }
        val localBlocksFetched = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Local$u0020Blocks$u0020Fetched
        }
        val fetchWaitTime = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Fetch$u0020Wait$u0020Time
        }
        val remoteBytesRead = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Remote$u0020Bytes$u0020Read
        }
        val localBytesRead = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Local$u0020Bytes$u0020Read
        }
        val totalRecordsRead = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Total$u0020Records$u0020Read
        }
        Task(applicationId, applicationName, jobId, stageId, stageName, stageAttemptId, taskId, attempt, taskType, executorId, host, peakMemory, inputRows, outputRows, locality, speculative, applicationStartTime, applicationEndTime, jobStartTime, jobEndTime, stageStartTime, stageEndTime, taskStartTime, taskEndTime, failed, taskDuration, stageDuration, jobDuration, applicationDuration, gcTime, gettingResultTime, resultSerializationTime, resultSize, dataReadMethod, bytesRead, recordsRead, memoryBytesSpilled, diskBytesSpilled, shuffleBytesWritten, shuffleRecordsWritten, shuffleWriteTime, remoteBlocksFetched, localBlocksFetched, fetchWaitTime, remoteBytesRead, localBytesRead, totalRecordsRead)
      }
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerApplicationStart"")
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/17.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerApplicationEnd"")
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/34.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerExecutorAdded"")
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/36.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkExecutorAdded](string)
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/38.19.Dataset-SparkExecutorAdded.map","Type: org.apache.spark.sql.Dataset[com.conversantmedia.sparkprofiler.SparkExecutorAdded]
Call: map

e => {
        val executorId = e.`Executor ID`
        val host = e.`Executor Info`.Host
        val cores = e.`Executor Info`.`Total Cores`
        val startTime = e.Timestamp
        Executor(sparkApplication.applicationName, sparkApplication.applicationId, executorId, host, cores, startTime)
      }
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/52.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerJobStart"")
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/54.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkJobStart](string)
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/59.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerJobEnd"")
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/61.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkJobEnd](string)
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/65.20.Dataset-SparkJobEnd).map","Type: org.apache.spark.sql.Dataset[(com.conversantmedia.sparkprofiler.SparkJobStart, com.conversantmedia.sparkprofiler.SparkJobEnd)]
Call: map

tuple => {
        val jobId = tuple._1.`Job ID`
        val jobStartTime = tuple._1.Submission$u0020Time
        val jobEndTime = tuple._2.Completion$u0020Time
        val stages = tuple._1.Stage$u0020IDs
        val jobDuration = jobEndTime - jobStartTime
        val result = tuple._2.Job$u0020Result.Result
        Job(sparkApplication.applicationId, sparkApplication.applicationName, jobId, sparkApplication.applicationStartTime, sparkApplication.applicationEndTime, jobStartTime, jobEndTime, jobDuration, sparkApplication.applicationDuration, stages, result)
      }
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/83.27.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerStageCompleted"")
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/85.22.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkStageComplete](string)
"
"udf/spark_repos_2/1_JThakrar_sparkprofiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/87.20.Dataset-SparkStageComplete.map","Type: org.apache.spark.sql.Dataset[com.conversantmedia.sparkprofiler.SparkStageComplete]
Call: map

stage => {
        val stageInfo = stage.Stage$u0020Info
        val stageId = stageInfo.Stage$u0020ID
        val job = locally {
          val _t_m_p_14 = jobs
          _t_m_p_14.filter(j => j.stages.contains(stageId))
        }.last
        val jobId = job.jobId
        val attempt = stageInfo.Stage$u0020Attempt$u0020ID
        val stageName = stageInfo.Stage$u0020Name
        val details = stageInfo.Details
        val taskCount = stageInfo.Number$u0020of$u0020Tasks
        val rddCount = stageInfo.RDD$u0020Info.size
        val applicationStartTime = job.applicationStartTime
        val applicationEndTime = job.applicationEndTime
        val jobStartTime = job.jobStartTime
        val jobEndTime = job.jobEndTime
        val stageStartTime = stageInfo.Submission$u0020Time
        val stageEndTime = stageInfo.Completion$u0020Time
        val stageDuration = stageEndTime - stageStartTime
        val jobDuration = job.jobDuration
        val applicationDuration = job.applicationDuration
        Stage(job.applicationId, job.applicationName, jobId, stageId, stageName, attempt, details, taskCount, rddCount, applicationStartTime, applicationEndTime, jobStartTime, jobEndTime, stageStartTime, stageEndTime, stageDuration, jobDuration, applicationDuration)
      }
"
"udf/spark_repos_2/1_junqueira_nasa-test/..src.main.scala.br.com.nasa.utils.Util.scala/udf/47.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""name='"" + base.toString + ""'""
"
"udf/spark_repos_2/1_kiakaku_model/..src.main.scala.com.vega.scorecard.model.Scoring.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""id = "" + model_id
"
"udf/spark_repos_2/1_kthristov_cubos-olap/..src.main.scala.utad.App.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.isNullAt(0)
"
"udf/spark_repos_2/1_kthristov_cubos-olap/..src.main.scala.utad.benchmark.CassandraBenchmark.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getDouble(2) > 0
"
"udf/spark_repos_2/1_kthristov_cubos-olap/..src.main.scala.utad.benchmark.HDFSBenchmark.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getDouble(2) > 0
"
"udf/spark_repos_2/1_leifblaese_spark-languagedetector/..src.main.scala.org.apache.spark.ml.feature.languagedetection.LanguageDetectorModel.scala/udf/93.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val textColInd = row.fieldIndex(getInputCol)
        val text = row.getString(textColInd)
        val detectedLanguage = LanguageDetectorModel.detect(text, bProbabilitiesMap.value, supportedLanguages, gramLenghts)
        Row.fromSeq(row.toSeq :+ detectedLanguage.toString)
      }
"
"udf/spark_repos_2/1_leifblaese_spark-languagedetector/..src.main.scala.org.apache.spark.ml.feature.languagedetection.LanguageDetector.scala/udf/135.22.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._1
"
"udf/spark_repos_2/1_leifblaese_spark-languagedetector/..src.main.scala.org.apache.spark.ml.feature.languagedetection.LanguageDetector.scala/udf/145.25.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

langTextPair => langTextPair._1 == lang
"
"udf/spark_repos_2/1_leifblaese_spark-languagedetector/..src.main.scala.org.apache.spark.ml.feature.languagedetection.preprocessing.LowerCasePreprocessor.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (row: Row) => 
        val textColInd = row.fieldIndex($(outputCol))
        val labelColInd = row.fieldIndex($(labelCol))
        val lang = row.getString(labelColInd)
        val text = row.getString(textColInd)
        val lcText = text.toLowerCase(Locale.forLanguageTag(lang))
        val rowseq = locally {
          val _t_m_p_3 = locally {
            val _t_m_p_4 = row.toSeq.toArray.zipWithIndex
            _t_m_p_4.filter(ai => ai._2 != textColInd)
          }
          _t_m_p_3.map(_._1)
        }
        val r: Row = new GenericRowWithSchema(rowseq ++ Array(lcText), transformSchema(row.schema))
        r
      }
"
"udf/spark_repos_2/1_leifblaese_spark-languagedetector/..src.main.scala.org.apache.spark.ml.feature.languagedetection.preprocessing.SpecialCharPreprocessor.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (row: Row) => 
        val textColInd = row.fieldIndex($(outputCol))
        val text = row.getString(textColInd)
        val replText = text.replaceAll(""/_[]*()%^&@$#:|{}<>~`\""\\"", """").replaceAll(""  *"", """")
        val rowseq = locally {
          val _t_m_p_3 = locally {
            val _t_m_p_4 = row.toSeq.toArray.zipWithIndex
            _t_m_p_4.filter(ai => ai._2 != textColInd)
          }
          _t_m_p_3.map(_._1)
        }
        val r: Row = new GenericRowWithSchema(rowseq ++ Array(replText), transformSchema(row.schema))
        r
      }
"
"udf/spark_repos_2/1_longhuaqiang_doit12_yiee/..dataware.src.main.scala.cn.doitedu.dw.dict.GeohashDict.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val lat: Double = row.getAs[Double](""lat"")
        val lng: Double = row.getAs[Double](""lng"")
        val province: String = row.getAs[String](""province"")
        val city: String = row.getAs[String](""city"")
        val district: String = row.getAs[String](""district"")
        val country: String = row.getAs[String](""country"")
        val geoCode: String = GeoHash.geoHashStringWithCharacterPrecision(lat, lng, 6)
        (geoCode, province, city, district, country)
      }
"
"udf/spark_repos_2/1_longhuaqiang_doit12_yiee/..dataware.src.main.scala.cn.doitedu.dw.idmap.Demo.scala/udf/35.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val bc_map = bc.value
        val name = locally {
          val _t_m_p_4 = line.split("","")
          _t_m_p_4.filter(StringUtils.isNotBlank(_))
        }(0)
        val gid = bc_map.get(name.hashCode.toLong).get
        gid + "","" + line
      }
"
"udf/spark_repos_2/1_longhuaqiang_doit12_yiee/..dataware.src.main.scala.cn.doitedu.dw.pre.AppLogDataPreprocess.scala/udf/35.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
              var bean: AppLogBean = null
              try {
                val jsonobj = JSON.parseObject(line)
                val eventid = jsonobj.getString(""eventid"")
                val timestamp = jsonobj.getString(""timestamp"").toLong
                val eventobj: JSONObject = jsonobj.getJSONObject(""event"")
                import scala.collection.JavaConversions._
                val javaMap: Map[String, String] = eventobj.getInnerMap.asInstanceOf[util.Map[String, String]].toMap
                val event: Map[String, String] = javaMap
                val userobj = jsonobj.getJSONObject(""user"")
                val uid = userobj.getString(""uid"")
                val sessionId = userobj.getString(""sessionId"")
                val phoneobj = userobj.getJSONObject(""phone"")
                val imei = phoneobj.getString(""imei"")
                val mac = phoneobj.getString(""mac"")
                val imsi = phoneobj.getString(""imsi"")
                val osName = phoneobj.getString(""osName"")
                val osVer = phoneobj.getString(""osVer"")
                val androidId = phoneobj.getString(""androidId"")
                val resolution = phoneobj.getString(""resolution"")
                val deviceType = phoneobj.getString(""deviceType"")
                val deviceId = phoneobj.getString(""deviceId"")
                val uuid = phoneobj.getString(""uuid"")
                val appobj = userobj.getJSONObject(""app"")
                val appid = appobj.getString(""appid"")
                val appVer = appobj.getString(""appVer"")
                val release_ch = appobj.getString(""release_ch"")
                val promotion_ch = appobj.getString(""promotion_ch"")
                val locobj = userobj.getJSONObject(""loc"")
                var lng = 0.0d
                var lat = -90.0d
                try {
                  lng = locobj.getDouble(""longtitude"")
                  lat = locobj.getDouble(""latitude"")
                } catch {
                  case e: Exception =>
                    e.printStackTrace()
                }
                val carrier = locobj.getString(""carrier"")
                val netType = locobj.getString(""netType"")
                val cid_sn = locobj.getString(""cid_sn"")
                val ip = locobj.getString(""ip"")
                val tmp = (imei + imsi + mac + uid + uuid + androidId).replaceAll(""null"", """")
                if (StringUtils.isNotBlank(tmp) && event != null && StringUtils.isNotBlank(eventid) && StringUtils.isNotBlank(sessionId)) {
                  bean = AppLogBean(Long.MinValue, eventid, event, uid, imei, mac, imsi, osName, osVer, androidId, resolution, deviceType, deviceId, uuid, appid, appVer, release_ch, promotion_ch, lng, lat, carrier, netType, cid_sn, ip, sessionId, timestamp)
                }
              } catch {
                case e: Exception =>
                  e.printStackTrace()
              }
              bean
            }
"
"udf/spark_repos_2/1_lpang36_BigDataScalaSpark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/142.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working.asInstanceOf[String], sex.asInstanceOf[String], age.asInstanceOf[String], primaryNeeds.asInstanceOf[Double], work.asInstanceOf[Double], other.asInstanceOf[Double])
      }
"
"udf/spark_repos_2/1_LUJUHUI_flume_kafka_sparkstreaming_hdfs_mysql/..src.main.scala.meituan.product.AreaTopNProductCountSpark.scala/udf/53.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getLong(0), row)
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.log.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.log.TopNStatJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.log.TopNStatJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.log.TopNStatJob.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.log.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.log.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.log.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.mylog_parse.AccessLogTopNJob.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""immoctype"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.mylog_parse.AccessLogTopNJob.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""immoctype"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.mylog_parse.AccessLogTopNJob.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""times_rnk"" < 4
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.mylog_parse.AccessLogTopNJob.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""immoctype"" === ""video""
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.spark.DataFrameRDDApp.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

studentDF.col(""age"") > 30
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.imooc.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.com.imooc.spark.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.imooc.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.spark_sql.DSL_demo.scala/udf/15.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(no: Int) => no match {
        case 1 => ""第一战力""
        case 2 => ""第二战力""
        case 3 => ""打酱油""
        case 4 => ""宠物""
        case _ => ""菜鸡""
      }
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.spark_sql.DSL_demo.scala/udf/25.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AVG_UDAF
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.spark_sql.UDF_UDAF.scala/udf/17.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: Double, num: Int) => {
        import java.math.BigDecimal
        val res = new BigDecimal(value)
        res.setScale(num, BigDecimal.ROUND_HALF_UP).doubleValue()
      }
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.spark_sql.UDF_UDAF.scala/udf/25.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AVG_UDAF
"
"udf/spark_repos_2/1_maketubu7_imooc.spark-sql/..src.main.scala.spark_sql.UDF_UDAF.scala/udf/9.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: Double) => {
        import java.math.BigDecimal
        val res = new BigDecimal(value)
        res.setScale(2, BigDecimal.ROUND_HALF_UP).doubleValue()
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.converter.Converter.scala/udf/21.19.Dataset-MainParsed.map","Type: org.apache.spark.sql.Dataset[entity.MainParsed]
Call: map

{ mainParsed => 
        (mainParsed.id, mainParsed.`type`, mainParsed.actor, mainParsed.publico, mainParsed.repo, mainParsed.created_at, mainParsed.payload)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/101.20.Dataset-Payload, String).map","Type: org.apache.spark.sql.Dataset[(String, entity.Payload, String)]
Call: map

{
        case (tempo: String, payload: Payload, time: String) =>
          (time, payload.commits.size)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/10.21.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
          case (id: String, tipo: String, actor: Actor, pubblico: Boolean, repo: Repo, created_at: String, payload: Payload) =>
            (payload.push_id, payload)
        }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/117.22.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
          case (id: String, tipo: String, actor: Actor, pubblico: Boolean, repo: Repo, created_at: String, payload: Payload) =>
            (repo, payload)
        }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/122.23.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[(entity.Repo, entity.Payload)]
Call: filter

x => x != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/126.23.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[(entity.Repo, entity.Payload)]
Call: filter

x => x._2 != null && x._2.commits != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/130.20.Dataset-Payload).map","Type: org.apache.spark.sql.Dataset[(entity.Repo, entity.Payload)]
Call: map

{
        case (repo: Repo, payload: Payload) =>
          (repo, payload.commits.size)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/15.22.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[(BigInt, entity.Payload)]
Call: filter

x => x != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/19.22.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[(BigInt, entity.Payload)]
Call: filter

x => x._2 != null && x._2.commits != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/23.19.Dataset-Payload).map","Type: org.apache.spark.sql.Dataset[(BigInt, entity.Payload)]
Call: map

{
        case (id: BigInt, payload: Payload) =>
          (id, payload.commits.size)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/39.21.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
          case (id: String, tipo: String, actor: Actor, pubblico: Boolean, repo: Repo, created_at: String, payload: Payload) =>
            (actor, payload)
        }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/44.22.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[(entity.Actor, entity.Payload)]
Call: filter

x => x != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/48.22.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[(entity.Actor, entity.Payload)]
Call: filter

x => x._2 != null && x._2.commits != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/52.19.Dataset-Payload).map","Type: org.apache.spark.sql.Dataset[(entity.Actor, entity.Payload)]
Call: map

{
        case (actor: Actor, payload: Payload) =>
          (actor, payload.commits.size)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/63.22.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
          case (id: String, tipo: String, actor: Actor, pubblico: Boolean, repo: Repo, created_at: String, payload: Payload) =>
            ((actor, tipo), payload)
        }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/68.23.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[((entity.Actor, String), entity.Payload)]
Call: filter

x => x != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/72.23.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[((entity.Actor, String), entity.Payload)]
Call: filter

x => x._2 != null && x._2.commits != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/76.20.Dataset-Payload).map","Type: org.apache.spark.sql.Dataset[((entity.Actor, String), entity.Payload)]
Call: map

{
        case ((actor: Actor, tipo: String), payload: Payload) =>
          ((actor, tipo), payload.commits.size)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/87.22.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
          case (id: String, tipo: String, actor: Actor, pubblico: Boolean, repo: Repo, created_at: String, payload: Payload) =>
            (created_at, payload)
        }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/92.23.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[(String, entity.Payload)]
Call: filter

x => x != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.CommitDataSetOperation.scala/udf/96.23.Dataset-Payload).filter","Type: org.apache.spark.sql.Dataset[(String, entity.Payload)]
Call: filter

x => x._2 != null && x._2.commits != null
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/10.19.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
        case (_, _, actor: Actor, _, _, _, _) =>
          ("""" + actor.id, actor)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/111.23.Dataset-Actor, BigInt).filter","Type: org.apache.spark.sql.Dataset[(entity.Actor, BigInt)]
Call: filter

""count = "" + actorWithMaxEvents._2
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/133.23.Dataset-Actor, BigInt).filter","Type: org.apache.spark.sql.Dataset[(entity.Actor, BigInt)]
Call: filter

""count = "" + actorWithMaxEvents._2
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/155.23.Dataset-Repo, BigInt), BigInt).filter","Type: org.apache.spark.sql.Dataset[((entity.Actor, entity.Repo, BigInt), BigInt)]
Call: filter

""_2 = "" + actorRepoAndHourWithMaxEvents._2
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/177.23.Dataset-Repo, BigInt), BigInt).filter","Type: org.apache.spark.sql.Dataset[((entity.Actor, entity.Repo, BigInt), BigInt)]
Call: filter

""_2 = "" + actorRepoAndHourWithMaxEvents._2
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/192.20.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
        case (id: String, tipo: String, actor: Actor, pubblico: Boolean, repo: Repo, created_at: String, payload: Payload) =>
          val input = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss'Z'"")
          val date = new DateTime(input.parse(created_at))
          (id, tipo, actor, pubblico, repo, date.getMinuteOfHour(), payload)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/20.19.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
        case (_, _, _, _, repo: Repo, _, _) =>
          ("""" + repo.id, repo)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/30.19.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{
        case (_, tipo: String, _, _, _, _, _) => tipo
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/49.19.Dataset-Commit.map","Type: org.apache.spark.sql.Dataset[entity.Commit]
Call: map

{
        case commit: Commit =>
          (commit.author.name, commit.author)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/71.19.Dataset-Actor, BigInt).map","Type: org.apache.spark.sql.Dataset[(String, entity.Actor, BigInt)]
Call: map

{
        case (tipo, actor: Actor, numero) =>
          ((tipo, actor), numero)
      }
"
"udf/spark_repos_2/1_matteo-bellinaso_GroupBigData/..src.main.scala.operations.DataSetOperations.EventDataSetOperation.scala/udf/95.19.Dataset-Repo, String, BigInt).map","Type: org.apache.spark.sql.Dataset[(entity.Actor, entity.Repo, String, BigInt)]
Call: map

{
        case (actor: Actor, repo: Repo, tipo, numero) =>
          ((actor, repo, tipo), numero)
      }
"
"udf/spark_repos_2/1_nkfranklin_loan_demand/..Xgboost_d30.scala/udf/51.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val row_feature = locally {
          val _t_m_p_5 = features
          _t_m_p_5.map(f => if (max_map.get(f).getOrElse(0) == 0 || min_map.get(f).getOrElse(0) == max_map.get(f).getOrElse(0)) 0.0d else {
            (row.getAs(f).toString.toDouble.formatted(""%.3f"").toFloat - min_map.get(f).get) / (max_map.get(f).get - min_map.get(f).get)
          })
        }
        LabeledPoint(row.get(1).toString.toDouble, Vectors.dense(row_feature))
      }
"
"udf/spark_repos_2/1_PlatinaBoy_Saprk_Recommended/..RecommendedDemo.src.main.testdmp.utiles.Business2Mysql.scala/udf/17.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val long = row.getAs[String](""long"")
          val lat = row.getAs[String](""lat"")
          val businfo: String = GetBusinessInfo.getBussinessInfo(lat, long)
          val geohash = GeoHash.withCharacterPrecision(TurnType.toDouble(lat), TurnType.toDouble(long), 12).toBase32
          (geohash, businfo)
        }
"
"udf/spark_repos_2/1_psy0703_headline_test/..headline_test.src.main.scala.com.dgmall.sparktest.dgmallTest.app.structced.StructedMain.scala/udf/17.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => {
        val records: String = x._2
        val json: CommenRecord = JSON.parseObject(records, classOf[CommenRecord])
        json
      }
"
"udf/spark_repos_2/1_psy0703_headline_test/..headline_test.src.main.scala.com.dgmall.sparktest.dgmallTest.app.structced.StructedMain.scala/udf/26.24.Dataset-CommenRecord.filter","Type: org.apache.spark.sql.Dataset[com.dgmall.sparktest.dgmallTest.bean.CommenRecord]
Call: filter

x => if (x.Event.contains(""play"")) true else false
"
"udf/spark_repos_2/1_psy0703_headline_test/..headline_test.src.main.scala.com.dgmall.sparktest.dgmallTest.app.structced.StructedMain.scala/udf/28.19.Dataset-CommenRecord.map","Type: org.apache.spark.sql.Dataset[com.dgmall.sparktest.dgmallTest.bean.CommenRecord]
Call: map

x => {
        val prop: String = x.Properties
        val watch: AppWatch = JSON.parseObject(prop, classOf[AppWatch])
        val wacthTable = WatchTable(x.distinct_id, x.Time, x.Event, x.Type, watch.getTrace_id, watch.getAlg_match, watch.getAlg_rank, watch.getRule, watch.getBhv_amt, watch.getUser_id, watch.getVideo_id, watch.getVideo_user_id, watch.getVideo_desc, watch.getVideo_tag, watch.getWatch_time_long, watch.getVideo_long, watch.getMusic_name, watch.getMusic_write, watch.getVideo_topic, watch.getVideo_address, watch.getIs_attention, watch.getIs_like, watch.getIs_comment, watch.getIs_share_weixin, watch.getIs_share_friendster, watch.getIs_share_qq, watch.getIs_save, watch.getIs_get_red_packets, watch.getRed_packets_sum, watch.getIs_copy_site, watch.getIs_report, watch.getReport_content, watch.getIs_not_interested, watch.getIs_go_shop, watch.getShop_id, watch.getShop_name)
        wacthTable
      }
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day04.src.main.scala.com.qs.movie.topN.scala/udf/13.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.split(""::"").length == 4
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day04.src.main.scala.com.qs.movie.topN.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""cnt"" > 2000
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day04.src.main.scala.com.qs.SQL.SparkSQLWordCount.scala/udf/12.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day04.src.main.scala.com.qs.SQL.SparkSQLWordCount.scala/udf/21.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day06.src.main.scala.com.qs.topN.SparkSQLEdu.scala/udf/13.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.split(""[/]"").length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day06.src.main.scala.com.qs.topN.SparkSQLEdu.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

value => {
        val arr: Array[String] = value.split(""[/]"")
        val subject = arr(2).split(""[.]"")(0)
        val teacher = arr(3)
        (subject, teacher)
      }
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.eventTime.StructuredWindow.scala/udf/15.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.IOT.IOTOffline.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""signal"".gt(10)
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.IOT.IOTOnline.scala/udf/13.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.split("" "").length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.IOT.IOTOnline.scala/udf/23.22.Dataset-DeviceData.filter","Type: org.apache.spark.sql.Dataset[com.qs.IOT.DeviceData]
Call: filter

device => device.signal > 10
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.ip.IPDSL.scala/udf/12.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.split(""\\|"").length == 15
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.ip.IPDSL.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.trim.split(""\\|"")
        (arr(2).toLong, arr(3).toLong, arr(arr.length - 2), arr(arr.length - 1))
      }
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.ip.IPDSL.scala/udf/22.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.split(""\\|"").length > 2
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.ip.IPDSL.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val ipString: String = line.trim.split(""\\|"")(1)
        ipString
      }
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.ip.IPSQL.scala/udf/10.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.split(""\\|"").length == 15
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.ip.IPSQL.scala/udf/12.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.trim.split(""\\|"")
        (arr(2).toLong, arr(3).toLong, arr(arr.length - 2), arr(arr.length - 1))
      }
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.ip.IPSQL.scala/udf/20.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.split(""\\|"").length > 2
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.ip.IPSQL.scala/udf/22.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val ipString: String = line.trim.split(""\\|"")(1)
        val ipLong: Long = IPUtils.ipToLong(ipString)
        ipLong
      }
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.json.StructSourceJson.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"".lt(25)
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.kafka.StructReadKafka.scala/udf/15.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.split("" "").length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.kafka.StructReadKafka.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""provinceId"".lt(10).and($""orderPrice"".geq(50))
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.mysql.StructSinkMysql.scala/udf/12.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.split("" "").length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.StructWordCount.scala/udf/15.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.split("" "").length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.watermark.StructuredWatermarkUpdate.scala/udf/14.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => null != line && line.trim.length > 0
"
"udf/spark_repos_2/1_q12138s_spark-learn/..spark_day07.src.main.scala.com.qs.watermark.StructuredWatermarkUpdate.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.trim.split("","")
        (arr(0), Timestamp.valueOf(arr(1)))
      }
"
"udf/spark_repos_2/1_Ravikiran357_Scala-project/..src.main.scala.cse512.HotcellAnalysis.scala/udf/31.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 0)
"
"udf/spark_repos_2/1_Ravikiran357_Scala-project/..src.main.scala.cse512.HotcellAnalysis.scala/udf/35.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 1)
"
"udf/spark_repos_2/1_Ravikiran357_Scala-project/..src.main.scala.cse512.HotcellAnalysis.scala/udf/39.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupTime: String) => HotcellUtils.CalculateCoordinate(pickupTime, 2)
"
"udf/spark_repos_2/1_Ravikiran357_Scala-project/..src.main.scala.cse512.HotcellAnalysis.scala/udf/55.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Int, y: Int, minX: Int, minY: Int, maxX: Int, maxY: Int) => HotcellUtils.ST_Contains(x, y, minX, minY, maxX, maxY)
"
"udf/spark_repos_2/1_Ravikiran357_Scala-project/..src.main.scala.cse512.HotzoneAnalysis.scala/udf/13.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(string: String) => string.replace(""("", """").replace("")"", """")
"
"udf/spark_repos_2/1_Ravikiran357_Scala-project/..src.main.scala.cse512.HotzoneAnalysis.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => HotzoneUtils.ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_2/1_santoshtechie9_spark-scala-examples/..src.main.scala.com.techknowera.spark.sql.Datasets.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""DEC"")
"
"udf/spark_repos_2/1_sh92_ScalaRecommenderSystem/..src.main.scala.com.myrecsys.spark.ALSRecSys.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x(0) == 1
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.airplane.AirplaneExample.scala/udf/109.21.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[com.stayrascal.spark.example.airplane.Flight]
Call: map

flight => flight.dest
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.airplane.AirplaneExample.scala/udf/124.22.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[com.stayrascal.spark.example.airplane.Flight]
Call: map

flight => flight.origin
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.airplane.AirplaneExample.scala/udf/139.22.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[com.stayrascal.spark.example.airplane.Flight]
Call: map

flight => flight.uniqueCarrier
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.airplane.AirplaneExample.scala/udf/151.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row(2).toString + "","" + row(3).toString + "","" + row(5).toString + "","" + row(7).toString + "","" + row(8).toString + "","" + row(12).toString + "","" + row(16).toString + "","" + row(17).toString + "","" + row(14).toString + "","" + row(15).toString
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.airplane.AirplaneExample.scala/udf/155.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseFields
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.airplane.AirplaneExample.scala/udf/64.22.Dataset-LabeledPoint.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.mllib.regression.LabeledPoint]
Call: filter

x => x.label == 0
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.airplane.AirplaneExample.scala/udf/68.22.Dataset-LabeledPoint.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.mllib.regression.LabeledPoint]
Call: filter

x => x.label == 1
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.airplane.AirplaneExample.scala/udf/88.19.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[com.stayrascal.spark.example.airplane.Flight]
Call: map

flight => {
        val vDayOfMonth = flight.dayOfMonth - 1
        val vDayOfWeek = flight.dayOfWeek - 1
        val vCRSDepTime = flight.crsDepTime
        val vCRSArrTime = flight.crsArrTime
        val vCarrierID = mCarrier(flight.uniqueCarrier)
        val vCRSElapsedTime = flight.crsElapsedTime
        val vOriginID = mOrigin(flight.origin)
        val vDestID = mDest(flight.dest)
        val vDelayFlag = flight.delayFlag
        Array(vDelayFlag.toDouble, vDayOfMonth.toDouble, vDayOfWeek.toDouble, vCRSDepTime.toDouble, vCRSArrTime.toDouble, vCarrierID.toDouble, vCRSElapsedTime.toDouble, vOriginID.toDouble, vDestID.toDouble)
      }
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.FPGrowthExample.scala/udf/14.28.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => !line.contains(""items"")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.FPGrowthExample.scala/udf/16.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(""\\{"")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.FPGrowthExample.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.MlExample.scala/udf/27.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseMovieData
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.MlExample.scala/udf/31.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseUserData
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.MlExample.scala/udf/35.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRatingData
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.MlExample.scala/udf/55.19.Dataset-Rating.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.mllib.recommendation.Rating]
Call: map

{
        case Rating(user, product, rating) =>
          (user, product)
      }
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.MlExample.scala/udf/62.19.Dataset-Rating.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.mllib.recommendation.Rating]
Call: map

{
        case Rating(user, product, rating) =>
          (user, product)
      }
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.MlExample.scala/udf/69.19.Dataset-Rating.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.mllib.recommendation.Rating]
Call: map

{
        case Rating(user, product, rating) =>
          ((user, product), rating)
      }
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.RascalExample.scala/udf/18.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

X => X.split("","")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.example.RascalExample.scala/udf/25.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

pas
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.pipeline.EmployeePicker.scala/udf/16.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""EmployeeId"") isin (ids: _*)
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.pipeline.EmployeePicker.scala/udf/18.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

to_date(col(""EndDate"")) === ""2015-10-25""
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.pipeline.EmployeePicker.scala/udf/23.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""EmployeeId""), row.getAs[DenseVector](""FEatures"").toArray.mkString("",""))
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-data.src.main.scala.com.stayrascal.spark.pipeline.RascalCombiner.scala/udf/100.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[DenseVector](""Features"").toArray.mkString("","")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.recommendation.FPGrowth.scala/udf/13.28.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => !line.contains(""items"")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.recommendation.FPGrowth.scala/udf/15.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(""\\{"")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.recommendation.FPGrowth.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/61.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/63.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/68.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/70.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/75.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/77.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/82.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.regression.LogisticRegressionApp.scala/udf/84.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_2/1_stayrascal_rascal-spark/..rascal-kafka.src.main.scala.com.stayrascal.spark.kafka.SQLExample.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != "" ""
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.active_day.ActiveDaysU.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

iter => {
        val user: String = iter.getAs[String](0)
        val start: String = iter.getAs[String](2)
        var end: String = iter.getAs[String](3)
        if (end == ""9999-12-31"") {
          end = current_date
        }
        (user, (start, end))
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_dic.biz.scala/udf/12.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

iter => {
        val fields: Array[String] = iter.split("", "")
        val province: String = fields(1).substring(1, fields(1).length - 1)
        val city: String = fields(2).substring(1, fields(2).length - 1)
        val distract: String = fields(3).substring(1, fields(3).length - 1)
        val biz: String = fields(4).substring(1, fields(4).length - 1)
        var bizGeo: String = null
        try {
          val lng = fields(5).substring(1, fields(5).length - 1).toDouble
          val lat = fields(6).substring(1, fields(6).length - 3).toDouble
          bizGeo = GeoHash.withCharacterPrecision(lat, lng, 6).toBase32
        } catch {
          case e: Exception =>
        }
        (bizGeo, province, city, distract, biz)
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_dic.GaoDeDic.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val url = ""https://restapi.amap.com/v3/geocode/regeo?""
        val key = ""a21304e65185618370cbc2ae9be3a03d""
        val client: CloseableHttpClient = HttpClientBuilder.create().build()
        val get: HttpGet = new HttpGet(url + ""key"" + ""="" + key + ""&"" + ""location"" + ""="" + line)
        val response: CloseableHttpResponse = client.execute(get)
        val content: InputStream = response.getEntity.getContent
        import scala.collection.JavaConversions._
        val str: String = IOUtils.readLines(content).mkString("""")
        val fields: Array[Double] = locally {
          val _t_m_p_2 = line.mkString("""").split("","")
          _t_m_p_2.map(_.toDouble)
        }
        var geoHash: String = GeoHash.withCharacterPrecision(fields(1), fields(0), 6).toBase32
        val json: JSONObject = JSON.parseObject(str)
        val status: String = json.getString(""status"")
        var province: String = """"
        var city: String = """"
        var district: String = """"
        var bizName: String = """"
        if (status.equals(""1"")) {
          val regeocode: JSONObject = json.getJSONObject(""regeocode"")
          val addressComponent: JSONObject = regeocode.getJSONObject(""addressComponent"")
          province = addressComponent.getString(""province"")
          city = addressComponent.getString(""city"")
          district = addressComponent.getString(""district"")
          val businessAreas: JSONArray = addressComponent.getJSONArray(""businessAreas"")
          if (businessAreas.size() > 0) {
            for (i <- 0 until businessAreas.size()) {
              val businessAreasJson = businessAreas.getJSONObject(i)
              val lng_lat: Array[Double] = locally {
                val _t_m_p_3 = businessAreasJson.getString(""location"").split("","")
                _t_m_p_3.map(_.toDouble)
              }
              bizName = businessAreasJson.getString(""name"")
              geoHash = GeoHash.withCharacterPrecision(lng_lat(1), lng_lat(0), 6).toBase32
              ((geoHash, province, city, district), bizName)
            }
          }
        }
        ((geoHash, province, city, district), bizName)
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_dic.LocalDic.scala/udf/12.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields: Array[String] = line.split("", "")
        val childer_code: String = fields(0).substring(fields(0).length - 6, fields(0).length)
        val name: String = fields(1).substring(1, fields(1).length - 1)
        val parent_code: String = fields(2)
        val level: String = fields(4)
        val lat: String = fields(fields.length - 1).substring(0, fields(fields.length - 1).length - 2)
        val lng: String = fields(fields.length - 2)
        (childer_code, name, parent_code, level, lng, lat)
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_dic.LocalDic.scala/udf/32.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val province: String = row.getAs[String](""province"")
        val city: String = row.getAs[String](""city"")
        val distract: String = row.getAs[String](""distract"")
        var lng: Double = 0
        var lat: Double = 0
        try {
          lng = row.getAs[String](""lng"").toDouble
          lat = row.getAs[String](""lat"").toDouble
        } catch {
          case e: NumberFormatException =>
        }
        val localDic: String = GeoHash.withCharacterPrecision(lat, lng, 5).toBase32
        (localDic, province, city, distract)
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.datapreprocess.LogPre.scala/udf/105.19.Dataset-LogBean.map","Type: org.apache.spark.sql.Dataset[datapreprocess.LogBean]
Call: map

bean => {
        val lat: Double = bean.latitude
        val lng: Double = bean.longtitude
        val geoHash2Biz: String = GeoHash.withCharacterPrecision(lat, lng, 6).toBase32
        val geoHash2Area: String = GeoHash.withCharacterPrecision(lat, lng, 5).toBase32
        val bizValue: Map[String, (String, String, String, String)] = bizBD.value
        val areaValue: Map[String, (String, String, String)] = areaBD.value
        val bizInfo: (String, String, String, String) = bizValue.getOrElse(geoHash2Biz, ("""", """", """", """"))
        bean.province = bizInfo._1
        bean.city = bizInfo._2
        bean.district = bizInfo._3
        bean.biz = bizInfo._4
        if (bizInfo._1.equals("""")) {
          val areaInfo: (String, String, String) = areaValue.getOrElse(geoHash2Area, ("""", """", """"))
          bean.province = areaInfo._1
          bean.city = areaInfo._2
          bean.district = areaInfo._3
        }
        bean
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.datapreprocess.LogPre.scala/udf/129.24.Dataset-LogBean.filter","Type: org.apache.spark.sql.Dataset[datapreprocess.LogBean]
Call: filter

bean => StringUtils.isBlank(bean.province)
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.datapreprocess.LogPre.scala/udf/131.19.Dataset-LogBean.map","Type: org.apache.spark.sql.Dataset[datapreprocess.LogBean]
Call: map

bean => bean.latitude + "","" + bean.longtitude
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.datapreprocess.LogPre.scala/udf/135.19.Dataset-LogBean.map","Type: org.apache.spark.sql.Dataset[datapreprocess.LogBean]
Call: map

bean => {
        val option = bean.event.get(""title"")
        if (option != None) {
          val terms: util.List[Term] = HanLP.segment(option.get)
          import scala.collection.JavaConversions._
          val keyword: String = locally {
            val _t_m_p_10 = locally {
              val _t_m_p_11 = terms
              _t_m_p_11.map(term => term.word)
            }
            _t_m_p_10.filter(_.size > 1)
          }.mkString("" "")
          bean.event.+=(""keyword"" -> keyword)
        }
        bean
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.datapreprocess.LogPre.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        var bean: LogBean = null
        try {
          val json: String = line.split("" --> "")(1)
          val jsonObject: JSONObject = JSON.parseObject(json)
          val uObject: JSONObject = jsonObject.getJSONObject(""u"")
          val cookieid: String = uObject.getString(""cookieid"")
          val account: String = uObject.getString(""account"")
          val phoneObject: JSONObject = uObject.getJSONObject(""phone"")
          val imei = phoneObject.getString(""imei"")
          val osName = phoneObject.getString(""osName"")
          val osVer = phoneObject.getString(""osVer"")
          val resolution = phoneObject.getString(""resolution"")
          val androidId = phoneObject.getString(""androidId"")
          val manufacture = phoneObject.getString(""manufacture"")
          val deviceId = phoneObject.getString(""deviceId"")
          val appObject: JSONObject = uObject.getJSONObject(""app"")
          val appid = appObject.getString(""appid"")
          val appVer = appObject.getString(""appVer"")
          val release_ch = appObject.getString(""release_ch"")
          val promotion_ch = appObject.getString(""promotion_ch"")
          val locObject: JSONObject = uObject.getJSONObject(""loc"")
          val areacode = locObject.getString(""areacode"")
          val longtitude = locObject.getDouble(""longtitude"")
          val latitude = locObject.getDouble(""latitude"")
          val carrier = locObject.getString(""carrier"")
          val netType = locObject.getString(""netType"")
          val sessionId = uObject.getString(""sessionId"")
          val logType = jsonObject.getString(""logType"")
          val commit_time = jsonObject.getLong(""commit_time"")
          val event: JSONObject = jsonObject.getJSONObject(""event"")
          val keys: util.Set[String] = event.keySet()
          import scala.collection.JavaConversions._
          val eventMap: Map[String, String] = locally {
            val _t_m_p_2 = keys
            _t_m_p_2.map(key => (key, event.getString(key)))
          }.toMap
          bean = LogBean(cookieid, account, imei, osName, osVer, resolution, androidId, manufacture, deviceId, appid, appVer, release_ch, promotion_ch, areacode, longtitude, latitude, carrier, netType, sessionId, logType, commit_time, eventMap)
        } catch {
          case e: Exception =>
        }
        bean
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.datapreprocess.LogPre.scala/udf/65.22.Dataset-LogBean.filter","Type: org.apache.spark.sql.Dataset[datapreprocess.LogBean]
Call: filter

bean => {
        val account: String = bean.account
        val cookieid: String = bean.cookieid
        val androidId: String = bean.androidId
        val deviceId: String = bean.deviceId
        val sessionId: String = bean.sessionId
        val imei: String = bean.imei
        val builder: StringBuilder = new StringBuilder()
        val stringBuilder: StringBuilder = builder.append(account).append(cookieid).append(androidId).append(deviceId).append(sessionId).append(imei)
        val str: String = stringBuilder.replaceAllLiterally(""null"", """")
        StringUtils.isNotBlank(str)
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.datapreprocess.LogPre.scala/udf/81.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

iter => {
        val geoHash: String = iter.getAs[String](""geoHash"")
        val province: String = iter.getAs[String](""province"")
        val city: String = iter.getAs[String](""city"")
        val distract: String = iter.getAs[String](""distract"")
        (geoHash, (province, city, distract))
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.datapreprocess.LogPre.scala/udf/93.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

iter => {
        val geoHash: String = iter.getAs[String](""geoHash"")
        val province: String = iter.getAs[String](""province"")
        val city: String = iter.getAs[String](""city"")
        val district: String = iter.getAs[String](""district"")
        val biz: String = iter.getAs[String](""biz"")
        (geoHash, (province, city, district, biz))
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_process.LogProcess.scala/udf/104.19.Dataset-LogBean.map","Type: org.apache.spark.sql.Dataset[data_process.LogBean]
Call: map

bean => {
        val lng: Double = bean.longtitude
        val lat: Double = bean.latitude
        val geo2Loc = GeoHash.withCharacterPrecision(lat, lng, 5).toBase32
        val geo2Biz = GeoHash.withCharacterPrecision(lat, lng, 6).toBase32
        val locDic: Map[String, (String, String, String)] = locBD.value
        val bizDic: Map[String, (String, String, String, String)] = bizBD.value
        val logBiz: (String, String, String, String) = bizDic.getOrElse(geo2Biz, ("""", """", """", """"))
        bean.province = logBiz._1
        bean.city = logBiz._2
        bean.district = logBiz._3
        bean.biz = logBiz._4
        if (logBiz._1.equals("""")) {
          val logLoc: (String, String, String) = locDic.getOrElse(geo2Loc, ("""", """", """"))
          bean.province = logLoc._1
          bean.city = logLoc._2
          bean.district = logLoc._3
        }
        bean
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_process.LogProcess.scala/udf/128.24.Dataset-LogBean.filter","Type: org.apache.spark.sql.Dataset[data_process.LogBean]
Call: filter

bean => bean.province.equals("""")
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_process.LogProcess.scala/udf/130.19.Dataset-LogBean.map","Type: org.apache.spark.sql.Dataset[data_process.LogBean]
Call: map

bean => bean.longtitude + "","" + bean.latitude
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_process.LogProcess.scala/udf/137.19.Dataset-LogBean.map","Type: org.apache.spark.sql.Dataset[data_process.LogBean]
Call: map

bean => {
        val maybeString: Option[String] = bean.event.get(""title"")
        if (maybeString != None) {
          val terms: util.List[Term] = HanLP.segment(maybeString.get)
          val stopW: Set[Row] = stWord.value
          import scala.collection.JavaConversions._
          val keyword: String = locally {
            val _t_m_p_10 = locally {
              val _t_m_p_11 = terms
              _t_m_p_11.map(term => term.word)
            }
            _t_m_p_10.filter(word => word.size > 1 && !stopW.contains(word))
          }.mkString("" "")
          bean.event.+=(""keyword"" -> keyword)
        }
        bean
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_process.LogProcess.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

iter => {
        var bean: LogBean = null
        try {
          val json: String = iter.split("" --> "")(1)
          val objectJson: JSONObject = JSON.parseObject(json)
          val uObject: JSONObject = objectJson.getJSONObject(""u"")
          val cookieid: String = uObject.getString(""cookieid"")
          val account: String = uObject.getString(""account"")
          val phoneObject: JSONObject = uObject.getJSONObject(""phone"")
          val imei = phoneObject.getString(""imei"")
          val osName = phoneObject.getString(""osName"")
          val osVer = phoneObject.getString(""osVer"")
          val resolution = phoneObject.getString(""resolution"")
          val androidId = phoneObject.getString(""androidId"")
          val manufacture = phoneObject.getString(""manufacture"")
          val deviceId = phoneObject.getString(""deviceId"")
          val appObject: JSONObject = uObject.getJSONObject(""app"")
          val appid: String = appObject.getString(""appid"")
          val appVer: String = appObject.getString(""appVer"")
          val release_ch: String = appObject.getString(""release_ch"")
          val promotion_ch: String = appObject.getString(""promotion_ch"")
          val locObject: JSONObject = uObject.getJSONObject(""loc"")
          val areacode: String = locObject.getString(""areacode"")
          val longtitude: Double = locObject.getDouble(""longtitude"")
          val latitude: Double = locObject.getDouble(""latitude"")
          val carrier: String = locObject.getString(""carrier"")
          val netType: String = locObject.getString(""netType"")
          val sessionId: String = uObject.getString(""sessionId"")
          val logType: String = objectJson.getString(""logType"")
          val commit_time: Long = objectJson.getLong(""commit_time"")
          val eventObject: JSONObject = objectJson.getJSONObject(""event"")
          val keys: util.Set[String] = eventObject.keySet()
          import scala.collection.JavaConversions._
          val eventMap: Map[String, String] = locally {
            val _t_m_p_2 = keys
            _t_m_p_2.map(key => (key, eventObject.getString(key)))
          }.toMap
          bean = LogBean(cookieid, account, imei, osName, osVer, resolution, androidId, manufacture, deviceId, appid, appVer, release_ch, promotion_ch, areacode, longtitude, latitude, carrier, netType, sessionId, logType, commit_time: Long, eventMap)
        } catch {
          case e: Exception =>
        }
        bean
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_process.LogProcess.scala/udf/64.22.Dataset-LogBean.filter","Type: org.apache.spark.sql.Dataset[data_process.LogBean]
Call: filter

bean => {
        val account: String = bean.account
        val cookieid: String = bean.cookieid
        val sessionId: String = bean.sessionId
        val imei: String = bean.imei
        val deviceId: String = bean.deviceId
        val androidId: String = bean.androidId
        val builder: StringBuilder = new StringBuilder()
        val stringBuilder: StringBuilder = builder.append(account).append(cookieid).append(sessionId).append(imei).append(deviceId).append(androidId)
        val str: String = stringBuilder.replaceAllLiterally(""null"", """")
        StringUtils.isNotBlank(str)
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_process.LogProcess.scala/udf/80.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val LocGeo: String = row.getAs[String](""_1"")
        val province: String = row.getAs[String](""_2"")
        val city: String = row.getAs[String](""_3"")
        val distract: String = row.getAs[String](""_4"")
        (LocGeo, (province, city, distract))
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.data_process.LogProcess.scala/udf/91.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val LocGeo: String = row.getAs[String](""_1"")
        val province: String = row.getAs[String](""_2"")
        val city: String = row.getAs[String](""_3"")
        val distract: String = row.getAs[String](""_4"")
        val biz: String = row.getAs[String](""_5"")
        (LocGeo, (province, city, distract, biz))
      }
"
"udf/spark_repos_2/1_sunnyspark_data_dragon/..data_warehouse.src.main.scala.dic.LocationParse.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val province: String = row.getAs[String](""province"")
        val city: String = row.getAs[String](""city"")
        val distract: String = row.getAs[String](""distract"")
        val lng = row.getAs[Double](""lng"")
        val lat = row.getAs[Double](""lat"")
        val geoHash: String = GeoHash.withCharacterPrecision(lat, lng, 5).toBase32
        (geoHash, province, city, distract)
      }
"
"udf/spark_repos_2/1_taanhtuan_shopee_ner/..src.main.scala.shopee.recognition.entity.TrainProcess.scala/udf/95.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size($""brands"") > 0
"
"udf/spark_repos_2/1_thesquelched_spark-lineage/..src.test.scala.org.chojin.spark.lineage.SparkSqlLineageListenerSpec.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'name =!= ""c""
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.log.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.log.TopNStatJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.log.TopNStatJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.log.TopNStatJob.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.log.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.log.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.log.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.spark.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.cti.sparksql.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_tianqinglei_SparkSQLProject/..src.main.scala.com.cti.sparksql.spark.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.cti.sparksql.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_tupol_spark-utils-demos/..src.main.scala.org.tupol.sparkutils.demos.sparksummit2019.noConfigOutput.SimpleApp.scala/udf/11.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_2/1_tupol_spark-utils-demos/..src.main.scala.org.tupol.sparkutils.demos.sparksummit2019.noConfigOutput.SimpleApp.scala/udf/15.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_2/1_tupol_spark-utils-demos/..src.main.scala.org.tupol.sparkutils.demos.sparksummit2019.noOutput.SimpleApp.scala/udf/18.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(context.filterA)
"
"udf/spark_repos_2/1_tupol_spark-utils-demos/..src.main.scala.org.tupol.sparkutils.demos.sparksummit2019.noOutput.SimpleApp.scala/udf/22.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(context.filterB)
"
"udf/spark_repos_2/1_tupol_spark-utils-demos/..src.main.scala.org.tupol.sparkutils.demos.sparksummit2019.quickstart.SimpleApp.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_2/1_tupol_spark-utils-demos/..src.main.scala.org.tupol.sparkutils.demos.sparksummit2019.quickstart.SimpleApp.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_2/1_tupol_spark-utils-demos/..src.main.scala.org.tupol.sparkutils.demos.sparksummit2019.withOutput.SimpleApp.scala/udf/19.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(context.filterA)
"
"udf/spark_repos_2/1_tupol_spark-utils-demos/..src.main.scala.org.tupol.sparkutils.demos.sparksummit2019.withOutput.SimpleApp.scala/udf/23.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(context.filterB)
"
"udf/spark_repos_2/1_unchartedsoftware_graph-mapping/..src.main.scala.software.uncharted.graphing.tiling.BasicOperations.scala/udf/41.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_2/1_vicotorz_SparkSQLDemoApp/..src.main.scala.com.demo.sparkapp.TopNStatJob.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""20161110"" && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_vicotorz_SparkSQLDemoApp/..src.main.scala.com.demo.spark.DataSetApp.scala/udf/13.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.demo.spark.DataSetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_watson1999_spark-examples/..src.main.scala.com.watson.trace.TraceLog.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

each => {
        var eachLine = """"
        try {
          eachLine = GzipUtils.decompress(each.getAs(""value""))
        } catch {
          case e: Exception =>
            e.printStackTrace()
        }
        println(eachLine)
        val json = new JSONObject()
        try {
          if (StringUtils.isNotBlank(eachLine)) {
            val trace = JSON.parseObject(eachLine)
            val tag = trace.getJSONObject(""spanTag"").getJSONObject(""tag"")
            json.put(""traceId"", trace.getString(""traceId""))
            json.put(""spanId"", trace.getString(""spanId""))
            json.put(""concurrent"", tag.getIntValue(""concurrent""))
            json.put(""elapsed"", tag.getLongValue(""elapsed""))
            json.put(""serverName"", tag.getString(""serverName""))
            json.put(""iface"", tag.getString(""iface""))
            json.put(""version"", tag.getString(""version""))
            json.put(""method"", tag.getString(""method""))
            json.put(""input"", tag.getString(""input""))
            json.put(""output"", tag.getString(""output""))
            json.put(""kind"", tag.getString(""kind""))
            json.put(""localAddress"", tag.getString(""localAddress""))
            json.put(""remoteAddress"", tag.getString(""remoteAddress""))
            json.put(""success"", tag.getBooleanValue(""success""))
            json.put(""timestamp"", tag.getLongValue(""timestamp""))
            json.put(""saveTime"", DateUtils.getISO8601Timestamp(new Date()))
          }
        } catch {
          case e: Exception =>
            System.err.println(eachLine)
            e.printStackTrace()
        }
        UnifiedTraceSchemaCase(if (json.isEmpty) """" else json.toJSONString)
      }
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.dataframe.Basic.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.dataframe.DatasetConversion.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.dataframe.UDF.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

myNameFilter($""name"")
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.dataframe.UDF.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

salesFilter($""sales"", lit(2000.0d))
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.dataframe.UDF.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

stateFilter($""state"", array(lit(""CA""), lit(""MA""), lit(""NY""), lit(""NJ"")))
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.dataframe.UDF.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

multipleFilter($""state"", $""discount"", struct(lit(""CA""), lit(100.0d)))
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/58.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

struct _
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/67.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAtomic _
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/71.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

arrayLength _
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.JSON.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => s.replaceAllLiterally(""$"", """")
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.UDAF2.scala/udf/48.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.UDAF_Multi.scala/udf/43.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mystats
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.UDAF.scala/udf/41.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.UDF.scala/udf/16.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

westernState _
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.UDF.scala/udf/27.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

manyCustomers _
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.UDF.scala/udf/47.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

stateRegion _
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.UDF.scala/udf/62.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

discountRatio _
"
"udf/spark_repos_2/1_wjl198435_SparkSteamingJanusGraph/..src.main.scala.sql.UDF.scala/udf/76.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeStruct _
"
"udf/spark_repos_2/1_yarson_garbage_classification/..index.src.main.scala.com.open.data.IndexBuilder.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(""\t"")
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.analysis.GroupTopN.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

row => {
        val words = row.split("","")
        (words(3), words(12), words(15).toLong)
      }
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.catalog.CatelogDemo.scala/udf/12.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(word: String) => word.split("","").length
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.catalog.CatelogDemo.scala/udf/16.22.Dataset-Function.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Function]
Call: filter

'name === ""udf_string_length""
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.dataframe.text2df.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val words = row.toString().split("","")
        words(0) + "","" + words(1)
      }
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.etl.OfflineETL.scala/udf/20.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => {
        val ip = IpParseUtil.IpParse(input)
        ip.getCountry
      }
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.etl.OfflineETL.scala/udf/27.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => {
        val ip = IpParseUtil.IpParse(input)
        ip.getProvince
      }
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.etl.OfflineETL.scala/udf/34.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => {
        val ip = IpParseUtil.IpParse(input)
        ip.getCity
      }
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.etl.OfflineETL.scala/udf/41.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => {
        val ip = IpParseUtil.IpParse(input)
        ip.getArea
      }
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.etl.OfflineETL.scala/udf/48.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => input.split("" "")(0)
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.join.JdbcJoinHive.scala/udf/14.19.Dataset-EmpDept.map","Type: org.apache.spark.sql.Dataset[com.tunan.spark.sql.join.JdbcJoinHive.EmpDept]
Call: map

_.ename
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.join.JdbcJoinHive.scala/udf/18.19.Dataset-EmpDept.map","Type: org.apache.spark.sql.Dataset[com.tunan.spark.sql.join.JdbcJoinHive.EmpDept]
Call: map

x => Result(x.empno, x.ename, x.deptno, x.dname, x.prize)
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.orc.text2orc.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

row => {
        val words = row.split("","")
        (words(0), words(1))
      }
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.udf.AvgUDAF.scala/udf/19.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AgeAvgUDAF
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.udf.StringLength.scala/udf/12.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

row => {
        val words = row.split(""\t"")
        (words(0), words(1))
      }
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.udf.StringLength.scala/udf/19.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(love: String) => love.split("","").length
"
"udf/spark_repos_2/1_yerias_tunan-spark/..tunan-spark-sql.src.main.scala.com.tunan.spark.sql.udf.StringLength.scala/udf/25.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(love: String) => love.split("","").length
"
"udf/spark_repos_2/1_zhenchao125_spark-1128/..spark-sql.src.main.scala.com.atguigu.spark.sql.day02.CreateDataSet.scala/udf/10.22.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[com.atguigu.spark.sql.day02.User]
Call: filter

_.age > 10
"
"udf/spark_repos_2/1_zhenchao125_spark-1128/..spark-sql.src.main.scala.com.atguigu.spark.sql.day02.DFMatch.scala/udf/9.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(age: Long, name: String) => age
      }
"
"udf/spark_repos_2/1_zhenchao125_spark-1128/..spark-sql.src.main.scala.com.atguigu.spark.sql.day02.udf.UDAFDemo_2.scala/udf/10.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAvg
"
"udf/spark_repos_2/1_zhenchao125_spark-1128/..spark-sql.src.main.scala.com.atguigu.spark.sql.day02.udf.UDAFDemo.scala/udf/9.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MySum
"
"udf/spark_repos_2/1_zhenchao125_spark-1128/..spark-sql.src.main.scala.com.atguigu.spark.sql.project.SqlApp.scala/udf/13.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityRemarkUDAF
"
"udf/spark_repos_2/1_zhichaochen_learn-spark/..src.main.scala.sparksql.MyTest2.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => ""name:"" + x(0)
"
"udf/spark_repos_2/1_zhichaochen_learn-spark/..src.main.scala.sparksql.MyTest2.scala/udf/26.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => ""name:"" + x.getAs[String](""name"")
"
"udf/spark_repos_2/1_zhichaochen_learn-spark/..src.main.scala.sparksql.MyTest.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 20
"
"udf/spark_repos_2/1_zhichaochen_learn-spark/..src.main.scala.SqlDataSourceExample.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""' and typed = 0""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""'""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/45.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, 1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/51.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, -1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""' and typed = 1""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/64.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, 1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/70.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, -1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/80.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => ConsumedOnce(log.logID, log.userID, log.consumed)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/86.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => ConsumedOnce(log.logID, log.userID, -log.consumed)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/92.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

userInfo(""registeredTime"") >= ""2016-10-01"" && userInfo(""registeredTime"") <= ""2016-10-14"" && userLog(""time"") >= userInfo(""registeredTime"") && userLog(""time"") <= date_add(userInfo(""registeredTime""), 14) && userLog(""typed"") === 0
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/97.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

userInfo(""registeredTime"") >= ""2016-10-01"" && userInfo(""registeredTime"") <= ""2016-10-14"" && userLog(""time"") >= userInfo(""registeredTime"") && userLog(""time"") <= date_add(userInfo(""registeredTime""), 14) && userLog(""typed"") === 1
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.Movie_Users_Analyzer_DateFrame.scala/udf/63.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.Movie_Users_Analyzer_DateSet.scala/udf/70.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.md.chapter_code.dt.spark.sparksql.Movie_Users_Analyzer_DateSet.scala/udf/75.23.Dataset-Rating.filter","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.Movie_Users_Analyzer_DateSet.Rating]
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_01_getting_started.n_03_spark_sql_untyped_dataset_operations.Run.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_01_getting_started.n_06_spark_sql_creating_dataset.Run.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_01_getting_started.n_06_spark_sql_creating_dataset.Run.scala/udf/16.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_02_spark_dataset.action.n_04_dataset_head_n.Run.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => {
        val tp = x.split("","")
        (tp(0).trim, tp(1).trim.toInt)
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_02_spark_dataset.action.n_07_dataset_foreach_class.Run.scala/udf/10.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => Person(line.split("","")(0), line.split("" "")(1).trim.toLong)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_02_spark_dataset.action.n_08_dataset_map.Run.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split("" "").size
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_02_spark_dataset.action.n_09_dataset_reduce.Run.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split("" "").size
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_02_spark_dataset.action.n_09_dataset_reduce_计算行中最多单词的个数.Run.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split("" "").size
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_02_spark_dataset.transformation.n_02_dataset_filter_func_boolean.Run.scala/udf/8.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""spark"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_02_spark_dataset.user_defined_untype_aggregation.n_01_uduta_MyAverage.Run.scala/udf/35.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_03_spark_dataframe.n_07_dataframe_filter_column.Run.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 20
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_03_spark_dataframe.n_23_dataframe_sql_map.Run.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

people => ""name:"" + people(0) + ""\tage:"" + people(1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_03_spark_dataframe.n_24_dataframe_sql_map_fieldName.Run.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

people => ""name:"" + people.getAs[String](""name"") + ""\tage:"" + people.getAs[Long](""age"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_03_spark_dataframe.n_25_dataframe_to_class.Run.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => Person(line.split("","")(0), line.split("","")(1).trim.toLong)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_03_spark_dataframe.n_26_dataframe_to_class_foreach.Run.scala/udf/11.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => Person(line.split("","")(0), line.split("","")(1).trim.toLong)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_04_spark_text.n_03_text_foreach_class.Run.scala/udf/10.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => Person(line.split("","")(0), line.split("" "")(1).trim.toLong)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_07_spark_udf.n_01_udf_str_length.Run.scala/udf/9.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length()
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_07_spark_udf.n_02_udf_str_length.Run.scala/udf/13.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getStrLength _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_08_spark_udaf.n_01_spark_udaf_count.Run2.scala/udf/32.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

CustomerCount
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_08_spark_udaf.n_01_spark_udaf_count.Run.scala/udf/32.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

CustomerCount
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_08_spark_udaf.n_02_spark_udaf_max.Run2.scala/udf/35.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

CustomerSum
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_08_spark_udaf.n_02_spark_udaf_max.RunGroupByMax.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

CustomerMax
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_08_spark_udaf.n_02_spark_udaf_max.Run.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

CustomerMax
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_08_spark_udaf.n_03_spark_udaf_sum.Run.scala/udf/32.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

CustomerSum
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_08_spark_udaf.n_04_spark_udaf_average.Run.scala/udf/35.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.bigdata.sql.n_08_spark_udaf.n_05_spark_udaf_groupby_max.Run.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

CustomerMax
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.imooc.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.imooc.DataFrameRDDApp.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.imooc.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.imooc.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.imooc.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.imooc.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.oreilly.scala.SparkSQLTwitter.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.oreilly.scala.SparkSQLTwitter.scala/udf/39.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: String).length
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sev7e0.spark.sql.A_1_DataFrameTest.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

person => ""name:"" + person(0)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sev7e0.spark.sql.A_1_DataFrameTest.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

person => ""name:"" + person.getAs[String](""name"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sev7e0.spark.sql.A_1_DataFrameTest.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

person => person.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sev7e0.spark.sql.A_2_DataSetTest.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sev7e0.spark.sql.A_6_HiveTables.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sev7e0.spark.sql.A_8_MyAverage.scala/udf/30.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

A_8_MyAverage
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sev7e0.spark.structured_streaming.A_1_BasicOperation.scala/udf/16.24.Dataset-DeviceData.filter","Type: org.apache.spark.sql.Dataset[com.sev7e0.spark.structured_streaming.A_1_BasicOperation.DeviceData]
Call: filter

_.signal > 10
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sev7e0.spark.structured_streaming.A_1_BasicOperation.scala/udf/18.19.Dataset-DeviceData.map","Type: org.apache.spark.sql.Dataset[com.sev7e0.spark.structured_streaming.A_1_BasicOperation.DeviceData]
Call: map

_.device
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.sha.sparkmall.offline.app.AreaTop3ProductApp.scala/udf/15.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

cityRemarkUDAF
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.struct.stream.QueryBasedStreamingNaiveBayes.scala/udf/17.22.Dataset-LabeledTokenCounts.filter","Type: org.apache.spark.sql.Dataset[com.struct.stream.LabeledTokenCounts]
Call: filter

r => tokens.contains(r.value)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.log.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.log.TopNStatJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.log.TopNStatJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.log.TopNStatJob.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.log.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.log.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.log.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.spark.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.xuzh.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.com.xuzh.spark.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.xuzh.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.Basic.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/107.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val r1 = row.getString(0) + "" "" * 2
        val r2 = row.getInt(1) * 2
        val r3 = "" "" + row.get(2).toString + "" ""
        (r1 + r3, r2)
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/121.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.length > 0
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/123.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

word => (word, 1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/213.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(df(""key2"") > 2).and(df(""key3"") === 5)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/217.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""key2"") > 2 and df(""key3"") === 5
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/221.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""key2"") > 2 || df(""key3"") === 5
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/226.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""key2 = ${max + 2}""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/230.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""key1"") > ""aaa""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/234.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key2"" === length($""key3"") - 1
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/240.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => {
        val flag = row.getInt(1) > 2
        flag
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/370.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Integer, y: Integer) => (x * y, 11)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/389.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAverage
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameDemo.scala/udf/74.19.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[dataframe.DataFrameDemo.Person]
Call: map

person => {
        val name = person.name
        val age = person.age + 10
        (name, age)
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DataFrameExamples.scala/udf/14.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") > 23
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DatasetConversion.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DatasetConversion.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.DateTimeOperations.scala/udf/17.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""date2"") > lit(""2019-12-12"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.UDF.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

myNameFilter($""name"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.UDF.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

salesFilter($""sales"", lit(2000.0d))
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.UDF.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

stateFilter($""state"", array(lit(""CA""), lit(""MA""), lit(""NY""), lit(""NJ"")))
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.UDF.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

multipleFilter($""state"", $""discount"", struct(lit(""CA""), lit(100.0d)))
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.dataframe.UDT.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

myMagnitude($""point"").lt(10)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.hbase.HBaseReadRowWriteStats.scala/udf/113.19.Dataset-SensorStatsRow.map","Type: org.apache.spark.sql.Dataset[hbase.HBaseReadRowWriteStats.SensorStatsRow]
Call: map

{
        case sensorStatsRow =>
          SensorStatsRow.convertToPutStats(sensorStatsRow)
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.hbase.HBaseReadRowWriteStats.scala/udf/98.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(rowkey: String, maxhz: Double, minhz: Double, avghz: Double, maxdisp: Double, mindisp: Double, avgdisp: Double, maxflo: Double, minflo: Double, avgflo: Double, maxsedPPM: Double, minsedPPM: Double, avgsedPPM: Double, maxpsi: Double, minpsi: Double, avgpsi: Double, maxchlPPM: Double, minchlPPM: Double, avgchlPPM: Double) =>
          SensorStatsRow(rowkey: String, maxhz: Double, minhz: Double, avghz: Double, maxdisp: Double, mindisp: Double, avgdisp: Double, maxflo: Double, minflo: Double, avgflo: Double, maxsedPPM: Double, minsedPPM: Double, avgsedPPM: Double, maxpsi: Double, minpsi: Double, avgpsi: Double, maxchlPPM: Double, minchlPPM: Double, avgchlPPM: Double)
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.ingestion.TopNStartJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.ingestion.TopNStartJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.ingestion.TopNStartJob.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.ingestion.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.ingestion.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.ingestion.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.ingestion.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.ingestion.transformations.ColumnsHandler.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCond
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.kafkaDemo.objectProject.dataImportKafkaPerformance.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

newRow => eventRow(newRow(0).toString, if (!newRow(1).toString.equals("""")) newRow(1).toString else ""0"", newRow(2).toString, if (!newRow(3).toString.equals("""")) newRow(3).toString else ""0"", newRow(4).toString, newRow(5).toString, if (!newRow(6).toString.equals("""")) newRow(6).toString else ""0"", newRow(7).toString, newRow(8).toString, newRow(9).toString, newRow(10).toString, newRow(11).toString, newRow(12).toString)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.kafkaDemo.textProject.dataImportKafkaPerformance.scala/udf/47.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

newRow => (newRow(0).toString, if (!newRow(1).toString.equals("""")) newRow(1).toString else ""0"", newRow(2).toString, if (!newRow(3).toString.equals("""")) newRow(3).toString else ""0"", newRow(4).toString, newRow(5).toString, if (!newRow(6).toString.equals("""")) newRow(6).toString else ""0"", newRow(7).toString, newRow(8).toString, newRow(9).toString, newRow(10).toString, newRow(11).toString, newRow(12).toString)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.bus.OrderJob.scala/udf/15.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

{
          case (line, timestamp) =>
            val orderInfo = line.split("","")
            if (orderInfo != null && orderInfo.size > 4) {
              val objEvent = NOEvent(orderInfo(0), orderInfo(1).toInt, orderInfo(2), orderInfo(3).toDouble, orderInfo(4), timestamp)
              objEvent
            } else {
              null
            }
        }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.bus.StructuredOrderStateListRturn.scala/udf/17.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

{
          case (line, timestamp) =>
            val orderInfo = line.split("","")
            if (orderInfo != null && orderInfo.size > 4) {
              val objEvent = NOEvent(orderInfo(0), orderInfo(1).toInt, orderInfo(2), orderInfo(3).toDouble, orderInfo(4), timestamp)
              objEvent
            } else {
              null
            }
        }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.bus.StructuredOrderStateListRturn.scala/udf/28.22.Dataset-NOEvent.filter","Type: org.apache.spark.sql.Dataset[org.roy.demo.streaming.bus.NOEvent]
Call: filter

obj => obj != null
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.bus.StructuredStoreOrderState.scala/udf/17.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

{
          case (line, timestamp) =>
            val orderInfo = line.split("","")
            if (orderInfo != null && orderInfo.size > 4) {
              val gId = orderInfo(2) + ""_"" + orderInfo(1).toInt + ""_"" + orderInfo(4)
              val objEvent = dataEvent(gId, orderInfo(0), orderInfo(1).toInt, orderInfo(2), orderInfo(3).toDouble, orderInfo(4), timestamp)
              objEvent
            } else {
              null
            }
        }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.bus.StructuredStoreOrderState.scala/udf/29.22.Dataset-dataEvent.filter","Type: org.apache.spark.sql.Dataset[org.roy.demo.streaming.bus.dataEvent]
Call: filter

obj => obj != null
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.orderStructured.OrderStructuredJob.scala/udf/18.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

{
          case (line, timestamp) =>
            val orderInfo = line.split("","")
            if (orderInfo != null && orderInfo.size > 4) {
              val gId = orderInfo(2) + ""_"" + orderInfo(1).toInt + ""_"" + orderInfo(4)
              val objEvent = orderEvent(gId, orderInfo(0), orderInfo(1).toInt, orderInfo(2), orderInfo(3).toDouble, orderInfo(4), timestamp)
              objEvent
            } else {
              null
            }
        }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.orderStructured.OrderStructuredJob.scala/udf/30.22.Dataset-orderEvent.filter","Type: org.apache.spark.sql.Dataset[org.roy.demo.streaming.orderStructured.orderEvent]
Call: filter

obj => obj != null
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.state.StructuredOrderExt.scala/udf/24.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

{
          case (line, timestamp) =>
            val orderInfo = line.split("","")
            if (orderInfo != null && orderInfo.size > 4) {
              val objEvent = OEvent(orderInfo(0), orderInfo(1).toInt, orderInfo(2), orderInfo(3).toDouble, orderInfo(4), timestamp)
              val sysDate = new SimpleDateFormat(""yyyy-MM-dd"").format(objEvent.timestamp)
              println(""sysDate=="" + sysDate)
              println(""sysDate=="" + sysDate.equals(objEvent.orderDate))
              if (sysDate.equals(objEvent.orderDate)) {
                objEvent
              } else {
                null
              }
            } else {
              null
            }
        }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.state.StructuredOrderExt.scala/udf/42.22.Dataset-OEvent.filter","Type: org.apache.spark.sql.Dataset[org.roy.demo.streaming.state.OEvent]
Call: filter

obj => obj != null
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.state.StructuredOrderStateListExt.scala/udf/24.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

{
          case (line, timestamp) =>
            val orderInfo = line.split("","")
            if (orderInfo != null && orderInfo.size > 4) {
              val objEvent = NOEvent(orderInfo(0), orderInfo(1).toInt, orderInfo(2), orderInfo(3).toDouble, orderInfo(4), timestamp)
              val sysDate = new SimpleDateFormat(""yyyy-MM-dd"").format(objEvent.timestamp)
              println(""sysDate=="" + sysDate)
              println(""sysDate=="" + sysDate.equals(objEvent.orderDate))
              if (sysDate.equals(objEvent.orderDate)) {
                objEvent
              } else {
                null
              }
            } else {
              null
            }
        }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.state.StructuredOrderStateListExt.scala/udf/42.22.Dataset-NOEvent.filter","Type: org.apache.spark.sql.Dataset[org.roy.demo.streaming.state.NOEvent]
Call: filter

obj => obj != null
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.structured.StructuredOperationsDFTest.scala/udf/20.24.Dataset-DeviceData.filter","Type: org.apache.spark.sql.Dataset[streaming.structured.StructuredOperationsDFTest.DeviceData]
Call: filter

_.signal > 10
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.org.roy.demo.streaming.structured.StructuredOperationsDFTest.scala/udf/22.19.Dataset-DeviceData.map","Type: org.apache.spark.sql.Dataset[streaming.structured.StructuredOperationsDFTest.DeviceData]
Call: map

_.device
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/107.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val r1 = row.getString(0) + "" "" * 2
        val r2 = row.getInt(1) * 2
        val r3 = "" "" + row.get(2).toString + "" ""
        (r1 + r3, r2)
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/121.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.length > 0
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/123.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

word => (word, 1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/213.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(df(""key2"") > 2).and(df(""key3"") === 5)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/217.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""key2"") > 2 and df(""key3"") === 5
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/221.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""key2"") > 2 || df(""key3"") === 5
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/226.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""key2 = ${max + 2}""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/230.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""key1"") > ""aaa""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/234.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key2"" === length($""key3"") - 1
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/240.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => {
        val flag = row.getInt(1) > 2
        flag
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/370.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Integer, y: Integer) => (x * y, 11)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/389.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAverage
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.DataFrameDemo.scala/udf/74.19.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[sparkdemo.DataFrameDemo.Person]
Call: map

person => {
        val name = person.name
        val age = person.age + 10
        (name, age)
      }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.JoinDemo.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""FLAG"") === ""1""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.JoinDemo.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""FLAG"").isNull
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.practice.Demo02.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split("","")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.practice.Demo03.scala/udf/21.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.practice.Demo04.scala/udf/11.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
      val fields = line.split("","")
      (fields(0), fields(1), fields(2), fields(3))
    }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.practice.Demo05.scala/udf/104.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(in: String) => {
      val fm = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"")
      val intime = fm.parse(in)
      val ms = intime.getTime
      val hour = intime.getHours
      val min = (intime.getMinutes.toDouble / 60).round
      val res = hour + min
      if (res >= 24) {
        res - 24
      } else {
        res
      }
    }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.practice.Demo05.scala/udf/120.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(in: String) => {
      val fm = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"")
      val intime = fm.parse(in)
      val calender = Calendar.getInstance()
      calender.setTime(intime)
      val hour = calender.get(Calendar.HOUR_OF_DAY)
      val min = (calender.get(Calendar.MINUTE).toDouble / 60).round
      val res = hour + min
      if (res >= 24) {
        res - 24
      } else {
        res
      }
    }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.practice.Demo05.scala/udf/21.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new Interval
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.practice.Demo05.scala/udf/91.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(in: String, out: String) => {
      val loc = new Locale(""en"")
      val fm = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"", loc)
      val intime = fm.parse(in)
      val outtime = fm.parse(out)
      val minusHour = (outtime.getTime - intime.getTime) / 3600000
      minusHour
    }
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.SemiStructuredUtilUDF.scala/udf/57.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

struct _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.SemiStructuredUtilUDF.scala/udf/66.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAtomic _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sparkdemo.SemiStructuredUtilUDF.scala/udf/70.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

arrayLength _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""' and typed = 0""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""'""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/45.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[sql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, 1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/51.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[sql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, -1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""' and typed = 1""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/64.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[sql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, 1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/70.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[sql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, -1)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/80.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[sql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => ConsumedOnce(log.logID, log.userID, log.consumed)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/86.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[sql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => ConsumedOnce(log.logID, log.userID, -log.consumed)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/92.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

userInfo(""registeredTime"") >= ""2016-10-01"" && userInfo(""registeredTime"") <= ""2016-10-14"" && userLog(""time"") >= userInfo(""registeredTime"") && userLog(""time"") <= date_add(userInfo(""registeredTime""), 14) && userLog(""typed"") === 0
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.EB_Users_Analyzer_DateSet.scala/udf/97.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

userInfo(""registeredTime"") >= ""2016-10-01"" && userInfo(""registeredTime"") <= ""2016-10-14"" && userLog(""time"") >= userInfo(""registeredTime"") && userLog(""time"") <= date_add(userInfo(""registeredTime""), 14) && userLog(""typed"") === 1
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.JSON.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => s.replaceAllLiterally(""$"", """")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.Movie_Users_Analyzer_DateFrame.scala/udf/63.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.Movie_Users_Analyzer_DateSet.scala/udf/70.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.Movie_Users_Analyzer_DateSet.scala/udf/75.23.Dataset-Rating.filter","Type: org.apache.spark.sql.Dataset[sql.Movie_Users_Analyzer_DateSet.Rating]
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.UDAF2.scala/udf/48.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.UDAF_Multi.scala/udf/43.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mystats
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.UDAF.scala/udf/41.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.UDF.scala/udf/16.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

westernState _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.UDF.scala/udf/27.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

manyCustomers _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.UDF.scala/udf/47.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

stateRegion _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.UDF.scala/udf/62.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

discountRatio _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.sql.UDF.scala/udf/76.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeStruct _
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.streamings.kafka.KafkaSource.scala/udf/14.22.Dataset-SimpleSongAggregationKafka.filter","Type: org.apache.spark.sql.Dataset[streamings.radio.SimpleSongAggregationKafka]
Call: filter

_.radioCount != null
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.streamings.SentimentAnalysisOnDemonetization.scala/udf/27.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""tweet_id"" =!= ""none""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.streamings.SentimentAnalysisOnDemonetization.scala/udf/29.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""tweet_id"".startsWith(""\"""")
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.streamings.SentimentAnalysisOnDemonetization.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rating"" >= 0
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.streamings.SentimentAnalysisOnDemonetization.scala/udf/42.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rating"" < 0
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.streamings.structured.quick_start.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"")(0)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.streamings.structured.StructuredStreamingOffset.scala/udf/16.19.Dataset-(String, Long).map","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: map

x => readLogs(x._1, x._2.toString)
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.utils.DataIntegration.DataUpdate.scala/udf/217.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""FLAG"") === ""1""
"
"udf/spark_repos_2/1_zixuedanxin_bigdata_learn/..spark-scala-learn.src.main.scala.utils.DataIntegration.DataUpdate.scala/udf/224.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""FLAG"").isNull
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.indexing.WikipediaIndexingExample.scala/udf/24.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.indexing.WikipediaIndexingExample.scala/udf/26.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replaceAll(""_"", "" "")
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.indexing.WikipediaIndexingExample.scala/udf/28.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replaceAll(""[^a-zA-Z0-9\\s]"", """")
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.indexing.WikipediaPagesIndexingExample.scala/udf/30.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replaceAll(""_"", "" "")
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.indexing.WikipediaPagesIndexingExample.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replaceAll(""[^a-zA-Z0-9\\s]"", """")
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.spatial.SpatialWorldCitiesSelfLinkage.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getString(1).toDouble, row.getString(0).toDouble), (row.getString(2), row.getString(3)))
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.utils.Utils.scala/udf/15.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.utils.Utils.scala/udf/17.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replaceAll(""_"", "" "")
"
"udf/spark_repos_2/1_zouzias_spark-lucenerdd-aws/..src.main.scala.org.zouzias.spark.lucenerdd.aws.utils.Utils.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replaceAll(""[^a-zA-Z0-9\\s]"", """")
"
"udf/spark_repos_2/20_prithvirajbose_spark-dev/..src.main.scala.examples.sql.TestDataset.scala/udf/25.22.Dataset-People.filter","Type: org.apache.spark.sql.Dataset[examples.sql.People]
Call: filter

partialFilterAge
"
"udf/spark_repos_2/20_prithvirajbose_spark-dev/..src.main.scala.examples.sql.TestParquet.scala/udf/17.22.Dataset-Purchase.filter","Type: org.apache.spark.sql.Dataset[examples.sql.Purchase]
Call: filter

data => data.method == ""Discover""
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/176.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeConstantTile
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/180.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileToArray[Int]
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/184.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileToArray[Double]
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/188.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

aggHistogram
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/192.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

aggStats
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/196.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileMean
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/200.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileMean
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/204.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileMean
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/208.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileSum
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/212.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileHistogram
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/216.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileStats
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/220.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

dataCells
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/224.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

dataCells
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/228.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggStats
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/232.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggMax
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/236.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggMin
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/240.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggMean
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/244.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggCount
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/248.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAdd
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/252.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localSubtract
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/256.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localMultiply
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/260.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localDivide
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/264.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

cellTypes
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.package.scala/udf/268.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

renderAscii
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/10.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileToArray[Int]
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/14.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileToArray[Double]
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/18.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

aggHistogram
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/22.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

aggStats
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/26.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileMean
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/30.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileMean
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileMean
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/38.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileSum
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/42.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileHistogram
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/46.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

tileStats
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/50.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

dataCells
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/54.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

dataCells
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/58.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggStats
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/62.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggMax
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/6.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeConstantTile
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/66.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggMin
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/70.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggMean
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/74.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAggCount
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/78.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localAdd
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/82.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localSubtract
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/86.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localMultiply
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/90.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

localDivide
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/94.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

cellTypes
"
"udf/spark_repos_2/21_s22s_pre-lt-raster-frames/..core.src.main.scala.astraea.spark.rasterframes.functions.Registrator.scala/udf/98.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

renderAscii
"
"udf/spark_repos_2/22_sparkbyexamples_spark-examples/..spark-sql-examples.src.main.scala.com.sparkbyexamples.spark.dataframe.DataFrameWithSimpleDSL.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""State"") === ""PR""
"
"udf/spark_repos_2/22_sparkbyexamples_spark-examples/..spark-sql-examples.src.main.scala.com.sparkbyexamples.spark.dataframe.UDFDataFrame.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

to_date(replaceUDF($""date1"")) > date_add(to_date(replaceUDF(lit(minDate))), 7)
"
"udf/spark_repos_2/22_sparkbyexamples_spark-examples/..spark-sql-examples.src.main.scala.com.sparkbyexamples.spark.dataset.xml.ReadBooksXML.scala/udf/10.19.Dataset-Books.map","Type: org.apache.spark.sql.Dataset[com.sparkbyexamples.spark.beans.Books]
Call: map

f => BooksDiscounted(f._id, f.author, f.description, f.price, f.publish_date, f.title, f.price - f.price * 20 / 100)
"
"udf/spark_repos_2/22_sparkbyexamples_spark-examples/..spark-sql-examples.src.main.scala.com.sparkbyexamples.spark.stackoverflow.AddingLiterral.scala/udf/17.19.Dataset-Employee.map","Type: org.apache.spark.sql.Dataset[com.sparkbyexamples.spark.stackoverflow.Employee]
Call: map

rec => (EmpData(""1"", rec.EmpId), EmpData(""2"", rec.Experience.toString), EmpData(""3"", rec.Salary.toString))
"
"udf/spark_repos_2/23_maropu_spark-sql-server/..sql.sql-server.src.test.scala.org.apache.spark.sql.server.service.postgresql.PgDialectSuite.scala/udf/36.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

() => ""test""
"
"udf/spark_repos_2/25_PacktPublishing_Scala-Programming-Projects/..Chapter10-11.bitcoin-analyser.src.main.scala.coinyser.BatchProducer.scala/udf/57.22.Dataset-Transaction.filter","Type: org.apache.spark.sql.Dataset[coinyser.Transaction]
Call: filter

$""timestamp"" >= lit(fromInstant.getEpochSecond).cast(TimestampType) && $""timestamp"" < lit(untilInstant.getEpochSecond).cast(TimestampType)
"
"udf/spark_repos_2/2_AndoneSZJ_dataMining/..src.main.scala.com.seven.spark.rdd.Volume.scala/udf/110.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

sale30 _
"
"udf/spark_repos_2/2_AndoneSZJ_dataMining/..src.main.scala.com.seven.spark.rdd.Volume.scala/udf/117.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

replaceById _
"
"udf/spark_repos_2/2_AndoneSZJ_dataMining/..src.main.scala.com.seven.spark.rdd.Volume.scala/udf/77.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

sale7 _
"
"udf/spark_repos_2/2_AndoneSZJ_dataMining/..src.main.scala.com.seven.spark.sparksql.ContrastCommunity.scala/udf/30.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

removeSymbol _
"
"udf/spark_repos_2/2_ethan0606_GraphEmbedding-Spark/..src.main.scala.com.github.ethan.util.DataLoader.scala/udf/8.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val line = x.getString(0).split(""\\s+"")
        val src = line(0).toLong
        val dst = line(1).toLong
        val weight = if (line.length == 2) {
          1d
        } else {
          line(2).toDouble
        }
        (src, dst, weight)
      }
"
"udf/spark_repos_2/2_ethan0606_GraphEmbedding-Spark/..src.main.scala.com.github.ethan.util.GraphUtil.scala/udf/13.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => {
        val src = x.getAs[Long](srcCol)
        val dst = x.getAs[Long](dstCol)
        val weight = x.getAs[Double](weightCol)
        val srcDst = Array(src, dst).sorted.mkString("","")
        (srcDst, weight)
      }
"
"udf/spark_repos_2/2_ethan0606_GraphEmbedding-Spark/..src.main.scala.com.github.ethan.util.GraphUtil.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(srcCol) =!= col(dstCol)
"
"udf/spark_repos_2/2_ethan0606_GraphEmbedding-Spark/..src.main.scala.com.github.ethan.util.Word2VecIndexGenerator.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val id = x.getAs[String](w2)
        val feature = x.getAs[Seq[Double]](v2).toArray
        val featureAsDV = new DenseVector(feature)
        val result = m.findSynonyms(featureAsDV, topK, Some(id))
        val simId = locally {
          val _t_m_p_4 = result
          _t_m_p_4.map(_._1)
        }
        val simScore = locally {
          val _t_m_p_5 = result
          _t_m_p_5.map(_._2)
        }
        (id, simId, simScore)
      }
"
"udf/spark_repos_2/2_ethan0606_GraphEmbedding-Spark/..src.main.scala.com.github.ethan.walker.RandomWalk.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val src = x.getAs[Long](srcCol)
        val dst = x.getAs[Long](dstCol)
        Edge(src, dst, EdgeAttr())
      }
"
"udf/spark_repos_2/2_ethan0606_GraphEmbedding-Spark/..src.main.scala.com.github.ethan.walker.RandomWalk.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val src = x.getLong(0)
        val dstSeq = x.getSeq[Long](1)
        var weightSeq = x.getSeq[Double](2)
        if (weightLog) weightSeq = locally {
          val _t_m_p_3 = weightSeq
          _t_m_p_3.map(x => math.log1p(x))
        }
        val dstWeight = dstSeq.zip(weightSeq).sortBy(_._2).reverse.take(bcMaxDegree)
        (src, NodeAttr(neighbors = locally {
          val _t_m_p_4 = dstWeight
          _t_m_p_4.map(x => (x._1, x._2))
        }.toArray))
      }
"
"udf/spark_repos_2/2_koiralo_kafka-spark-cassandra-pipeline/..SparkBatchJobs.src.main.scala.com.ultra.tendency.AnalysisFunctions.scala/udf/15.22.Dataset-SensorData.filter","Type: org.apache.spark.sql.Dataset[com.ultra.tendency.domain.SensorData]
Call: filter

$""time"".cast(DateType) === date
"
"udf/spark_repos_2/2_mahesh2492_learning-spark/..src.main.scala.dataframe.DataFrameExamples.scala/udf/14.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") > 23
"
"udf/spark_repos_2/2_mahesh2492_learning-spark/..src.main.scala.dataframe.DateTimeOperations.scala/udf/17.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""date2"") > lit(""2019-12-12"")
"
"udf/spark_repos_2/2_Minsub_ScalaMaven/..src.main.scala.com.minsub.spark.sql.DatasetExample.scala/udf/13.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/2_Minsub_ScalaMaven/..src.main.scala.com.minsub.spark.sql.DatasetInferringExample.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

p => ""Name: "" + p(0)
"
"udf/spark_repos_2/2_Minsub_ScalaMaven/..src.main.scala.com.minsub.spark.sql.DatasetInferringExample.scala/udf/23.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

p => ""Name: "" + p.getAs[String](""name"")
"
"udf/spark_repos_2/2_pcalpha_SparkDemo/..src.main.scala.cn.com.pcalpha.spark_demo.sql.dataframe.example.PeopleInfoCalculator2.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF(""gender"").equalTo(""M"")
"
"udf/spark_repos_2/2_pcalpha_SparkDemo/..src.main.scala.cn.com.pcalpha.spark_demo.sql.dataframe.example.PeopleInfoCalculator2.scala/udf/38.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

peopleDF(""height"") > 180
"
"udf/spark_repos_2/2_pcalpha_SparkDemo/..src.main.scala.cn.com.pcalpha.spark_demo.sql.dataframe.example.PeopleInfoCalculator2.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF(""gender"").equalTo(""M"")
"
"udf/spark_repos_2/2_pcalpha_SparkDemo/..src.main.scala.cn.com.pcalpha.spark_demo.sql.dataframe.example.PeopleInfoCalculator2.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF(""gender"").equalTo(""F"")
"
"udf/spark_repos_2/2_pcalpha_SparkDemo/..src.main.scala.cn.com.pcalpha.spark_demo.sql.dataset.example.PeopleInfoCalculator3.scala/udf/34.24.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[cn.com.pcalpha.spark_demo.sql.dataset.example.PeopleInfoCalculator3.Person]
Call: filter

peopleDS(""gender"").equalTo(""M"")
"
"udf/spark_repos_2/2_pcalpha_SparkDemo/..src.main.scala.cn.com.pcalpha.spark_demo.sql.dataset.example.PeopleInfoCalculator3.scala/udf/36.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[cn.com.pcalpha.spark_demo.sql.dataset.example.PeopleInfoCalculator3.Person]
Call: filter

peopleDS(""height"") > 180
"
"udf/spark_repos_2/2_pcalpha_SparkDemo/..src.main.scala.cn.com.pcalpha.spark_demo.sql.dataset.example.PeopleInfoCalculator3.scala/udf/46.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[cn.com.pcalpha.spark_demo.sql.dataset.example.PeopleInfoCalculator3.Person]
Call: filter

peopleDS(""gender"").equalTo(""M"")
"
"udf/spark_repos_2/2_pcalpha_SparkDemo/..src.main.scala.cn.com.pcalpha.spark_demo.sql.dataset.example.PeopleInfoCalculator3.scala/udf/50.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[cn.com.pcalpha.spark_demo.sql.dataset.example.PeopleInfoCalculator3.Person]
Call: filter

peopleDS(""gender"").equalTo(""F"")
"
"udf/spark_repos_2/2_phdata_retirement-age/..src.main.scala.io.phdata.retirementage.filters.DatedTableFilter.scala/udf/11.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

date_add(dateExpression, table.expiration_days) < current_date()
"
"udf/spark_repos_2/2_phdata_retirement-age/..src.main.scala.io.phdata.retirementage.filters.DatedTableFilter.scala/udf/15.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

date_add(dateExpression, table.expiration_days) > current_date()
"
"udf/spark_repos_2/2_phdata_retirement-age/..src.main.scala.io.phdata.retirementage.filters.DatedTableFilter.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

date_add(col(table.expiration_column), table.expiration_days) < current_date()
"
"udf/spark_repos_2/2_phdata_retirement-age/..src.main.scala.io.phdata.retirementage.loadgen.LoadGenerator.scala/udf/70.19.Dataset-LoadGenTemp.map","Type: org.apache.spark.sql.Dataset[io.phdata.retirementage.loadgen.LoadGenerator.LoadGenTemp]
Call: map

{
        case change =>
          change.copy(id = UUID.randomUUID().toString, dimension_id = UUID.randomUUID().toString, expiration_date = tempDateGenerator())
      }
"
"udf/spark_repos_2/2_phdata_retirement-age/..src.main.scala.io.phdata.retirementage.loadgen.LoadGenerator.scala/udf/96.19.Dataset-LoadGenTemp.map","Type: org.apache.spark.sql.Dataset[io.phdata.retirementage.loadgen.LoadGenerator.LoadGenTemp]
Call: map

{
        case change =>
          change.copy(dimension_id = UUID.randomUUID().toString)
      }
"
"udf/spark_repos_2/2_phdata_retirement-age/..src.main.scala.io.phdata.retirementage.storage.HdfsStorage.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""col_name"") === ""Location""
"
"udf/spark_repos_2/2_qzeng2490_Spark-The-Definitive-Guide/..project-templates.scala.src.main.scala.DataFrameExample.scala/udf/15.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.pointlessUDF(_: String): String
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!($""label"" === ($""prediction""))
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/38.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/43.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/45.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!($""label"" === ($""prediction""))
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/50.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/52.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!($""label"" === ($""prediction""))
"
"udf/spark_repos_2/2_rajatjaswal_csye7200-airbnb/..src.main.scala.spark.TrainModel.scala/udf/89.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => HouseAddress(Decision.parse(if (row.getDouble(13) == 0.0d) {
        ""T""
      } else {
        ""F""
      }).get, row.getDouble(1), row.getDouble(2), row.getLong(3), row.getLong(4), row.getString(5), row.getInt(6), row.getBoolean(7), row.getBoolean(8), row.getInt(9))
"
"udf/spark_repos_2/2_sgloutnikov_masters-writing-project/..spark.src.main.scala.edu.sjsu.cs298.corenlp.ReviewCounter.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'count > 50
"
"udf/spark_repos_2/2_sgloutnikov_masters-writing-project/..spark.src.main.scala.edu.sjsu.cs298.naivebayes.NaiveBayesPredict.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(reviewId: String, userId: String, sentence: String, filteredWords: Seq[String], position: Int) =>
          val sentiment = naiveBayesModel.predict(hashingTF.transform(filteredWords))
          (reviewId, userId, sentence, filteredWords, position, sentiment.toInt)
      }
"
"udf/spark_repos_2/2_superruzafa_scala-spark-big-data/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/131.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, primaryNeeds, work, other)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.NewProcess.scala/udf/153.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => cid_vid(line.get(0).asInstanceOf[String], line.get(1).asInstanceOf[String])
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.NewProcess.scala/udf/168.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => (line.getAs[String](0), line.getAs[mutable.WrappedArray[String]](1).toArray)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.NewProcess.scala/udf/239.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => xid_tags_new(line.getAs[String](0), line.getAs[mutable.WrappedArray[String]](1).toArray, line.getAs[String](2), line.getAs[Double](3), line.getAs[Int](4), line.getAs[Int](5))
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.NewProcess.scala/udf/273.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val xid = line.getAs[String](0)
        val features = line.getAs[SV](1)
        val tags = line.getAs[mutable.WrappedArray[String]](2)
        val title = line.getAs[String](3)
        if (tags == null) (xid, features, null, title) else (xid, features, tags.toArray, title)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.NewProcess.scala/udf/291.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!_.isNullAt(2)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.NewProcess.scala/udf/295.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val vid1 = line.getAs[String](0)
        val sv1 = line.getAs[SV](1)
        val vid_tags = line.getAs[mutable.WrappedArray[String]](2).toArray
        val vid_title = line.getAs[String](3)
        val bsv1 = new SparseVector[Double](sv1.indices, sv1.values, sv1.size)
        val ret = locally {
          val _t_m_p_45 = broadcast_cid_sv.value
          _t_m_p_45.map(line2 => {
            val cid = line2._1
            val sv2 = line2._2
            val cid_tags = line2._3
            val cid_title = line2._4
            val bsv2 = new SparseVector[Double](sv2.indices, sv2.values, sv2.size)
            val cosSim = bsv1.dot(bsv2) / (norm(bsv1) * norm(bsv2))
            (cid, cosSim, cid_tags, sv2, cid_title)
          })
        }.maxBy(_._2)
        cos_result(vid1, ret._1, ret._2, vid_title, ret._5, vid_tags, ret._3, sv1, ret._4)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.ProcessVideoInfo.scala/udf/34.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""tag"".equalTo("""")
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/135.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => cid_vid(line.get(0).asInstanceOf[String], line.get(1).asInstanceOf[String])
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/150.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => (line.getAs[String](0), line.getAs[mutable.WrappedArray[String]](1).toArray)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/247.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val title_xid = line.getAs[String](0)
        val title_features = line.getAs[SV](1)
        val tag_features = line.getAs[SV](2)
        val leaderactor_features = line.getAs[SV](3)
        val cidtags_features = line.getAs[SV](4)
        val title_tags = line.getAs[mutable.WrappedArray[String]](5)
        val tag_tags = line.getAs[mutable.WrappedArray[String]](6)
        val leaderactor_tags = line.getAs[mutable.WrappedArray[String]](7)
        val cidtags_tags = line.getAs[mutable.WrappedArray[String]](8)
        (title_xid, title_features, tag_features, leaderactor_features, cidtags_features, title_tags, tag_tags, leaderactor_tags, cidtags_tags)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/316.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!_.isNullAt(2)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/320.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val vid1 = line.getAs[String](0)
        val sv1 = line.getAs[SV](1)
        val vid_tags = line.getAs[mutable.WrappedArray[String]](2)
        val bsv1 = new SparseVector[Double](sv1.indices, sv1.values, sv1.size)
        val ret = locally {
          val _t_m_p_39 = broadcast_cid_sv.value
          _t_m_p_39.map(line2 => {
            val cid = line2._1
            val bsv2 = line2._2
            val cid_tags = line2._3
            val cosSim = bsv1.dot(bsv2) / (norm(bsv1) * norm(bsv2))
            (cid, cosSim, cid_tags)
          })
        }.maxBy(_._2)
        (vid1, ret, vid_tags)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/424.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val cid = line.get(0).asInstanceOf[String]
        val sv = line.get(1).asInstanceOf[SV]
        (cid, Array(sv))
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/488.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => cid_vid(line.get(0).asInstanceOf[String], line.get(1).asInstanceOf[String])
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/498.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => (line.get(0).asInstanceOf[String], line.get(1).asInstanceOf[SV])
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Algorithm.TagProcess.scala/udf/541.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => (line.get(0).asInstanceOf[String], line.get(1).asInstanceOf[SV])
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.ColdTagVids.scala/udf/100.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val tag = line.getAs[String](""tag"")
          val tagid = line.getAs[Long](""tagid"")
          val tag_type = line.getAs[String](""tag_type"")
          val vid_weight = locally {
            val _t_m_p_18 = line.getAs[Seq[Row]](""vid_weight"")
            _t_m_p_18.map(r => (r.getString(0), r.getDouble(1)))
          }.sortBy(_._2)(Ordering[Double].reverse)
          (tag, tagid, tag_type, vid_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.ColdTagVids.scala/udf/127.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val tagid = line.getAs[Long](""tagid"").toString
          val vid_weight = locally {
            val _t_m_p_20 = line.getAs[Seq[Row]](""vid_weight"")
            _t_m_p_20.map(r => (r.getString(0), r.getDouble(1)))
          }
          KeyValueWeight(tagid, value_weight = vid_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.ColdTagVids.scala/udf/30.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

arr => arr.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.ColdTagVids.scala/udf/38.22.Dataset-vid_ztid_click_exposure.filter","Type: org.apache.spark.sql.Dataset[BasicData.ColdTagVids.vid_ztid_click_exposure]
Call: filter

_.exposure > exposure_max
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.ColdTagVids.scala/udf/51.28.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.ColdTagVids.scala/udf/65.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""ts"" > now - expire_sec
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.ColdTagVids.scala/udf/74.20.Dataset-vid_ztid_click_exposure.map","Type: org.apache.spark.sql.Dataset[BasicData.ColdTagVids.vid_ztid_click_exposure]
Call: map

_.vid
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.UserPlayPercentFrom4499.scala/udf/13.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: Array[Byte]) => new String(Base64.decodeBase64(s))
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidCTR.scala/udf/29.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

arr => arr.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidCTR.scala/udf/45.26.Dataset-vid_ztid_click_exposure.filter","Type: org.apache.spark.sql.Dataset[BasicData.VidCTR.vid_ztid_click_exposure]
Call: filter

data => ztid_filter_set.contains(data.ztid)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidCTR.scala/udf/56.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""click"" >= click_min || $""exposure"" >= exposure_min
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidCTR.scala/udf/58.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""ctr"" <= 1.0d
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/142.32.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/187.25.Dataset-DsVidLine.filter","Type: org.apache.spark.sql.Dataset[BasicData.VidTagsWeight.DsVidLine]
Call: filter

_.tags.length != 0
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/189.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getString(0)
        val tags_id = line.getAs[Seq[Seq[Long]]](4).flatten.distinct
        val tags_source = line.getAs[Seq[Seq[String]]](5).flatten.distinct
        DsVidLine(vid, 0, tags_id.toArray, tags_source.toArray, 1.0d, is_distinct = false)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/271.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val vid = line.getString(0)
          val duration = line.getInt(1)
          val tags_id = line.getSeq[Seq[Long]](2).flatten.distinct.toArray
          val tags_source = line.getSeq[Seq[String]](3).flatten.distinct.toArray
          var spv: SparseVector[Double] = locally {
            val _t_m_p_40 = locally {
              val _t_m_p_41 = line.getSeq[SV](4)
              _t_m_p_41.map(sv => new SparseVector[Double](sv.indices, sv.values, sv.size))
            }
            _t_m_p_40.reduce(_ + _)
          }
          spv = spv / norm(spv)
          vid_idf_line(vid, duration, tags_id, tags_source, locally {
            val _t_m_p_42 = spv.index
            _t_m_p_42.filter(_ != 0)
          }, locally {
            val _t_m_p_43 = spv.data
            _t_m_p_43.filter(_ != 0.0d)
          })
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/340.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => filter_data(line)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/349.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vid: String, weight: Double) => (vid, weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/355.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val vid = line.getString(0)
          val value_weight = locally {
            val _t_m_p_48 = line.getAs[Seq[Row]](2)
            _t_m_p_48.map(v => (v.getLong(1).toString, v.getDouble(2)))
          }
          KeyValueWeight(vid, value_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/376.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val tag_id = line.getString(0)
        val vid_weight = locally {
          val _t_m_p_51 = line.getAs[Seq[Row]](1)
          _t_m_p_51.map(d => (d.getString(0), d.getDouble(1)))
        }.sortWith(_._2 > _._2)
        val vid_weight_res = vid_weight.take(vid_length)
        KeyValueWeight(tag_id, vid_weight_res)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/399.28.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/407.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        var s = line
        for (r <- Defines.pattern_list) {
          s = r.replaceAllIn(s, """")
        }
        s
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/468.30.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/495.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0).trim().toLowerCase
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/511.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(2).length >= 2
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/528.27.Dataset-(String, String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, String, Long)]
Call: filter

_._1 != """"
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/530.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 20
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeight.scala/udf/625.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getString(0)
        val value_weight = locally {
          val _t_m_p_95 = line.getAs[Seq[Row]](2)
          _t_m_p_95.map(v => (v.getLong(1).toString, v.getDouble(2)))
        }.distinct.sortWith(_._2 > _._2).take(100)
        KeyValueWeight(vid, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/142.32.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/187.25.Dataset-DsVidLine.filter","Type: org.apache.spark.sql.Dataset[BasicData.VidTagsWeightV2.DsVidLine]
Call: filter

_.tags.length != 0
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/189.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getString(0)
        val tags_id = line.getAs[Seq[Seq[Long]]](4).flatten.distinct
        val tags_source = line.getAs[Seq[Seq[String]]](5).flatten.distinct
        DsVidLine(vid, 0, tags_id.toArray, tags_source.toArray, 1.0d, is_distinct = false)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/271.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val vid = line.getString(0)
          val duration = line.getInt(1)
          val tags_id = line.getSeq[Seq[Long]](2).flatten.distinct.toArray
          val tags_source = line.getSeq[Seq[String]](3).flatten.distinct.toArray
          var spv: SparseVector[Double] = locally {
            val _t_m_p_40 = locally {
              val _t_m_p_41 = line.getSeq[SV](4)
              _t_m_p_41.map(sv => new SparseVector[Double](sv.indices, sv.values, sv.size))
            }
            _t_m_p_40.reduce(_ + _)
          }
          spv = spv / norm(spv)
          vid_idf_line(vid, duration, tags_id, tags_source, locally {
            val _t_m_p_42 = spv.index
            _t_m_p_42.filter(_ != 0)
          }, locally {
            val _t_m_p_43 = spv.data
            _t_m_p_43.filter(_ != 0.0d)
          })
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/340.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => filter_data(line)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/349.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vid: String, weight: Double) => (vid, weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/355.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val vid = line.getString(0)
          val value_weight = locally {
            val _t_m_p_48 = line.getAs[Seq[Row]](2)
            _t_m_p_48.map(v => (v.getLong(1).toString, v.getDouble(2)))
          }
          KeyValueWeight(vid, value_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/379.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val tag_id = line.getString(0)
        val vid_weight = locally {
          val _t_m_p_51 = line.getAs[Seq[Row]](1)
          _t_m_p_51.map(d => (d.getString(0), d.getDouble(1)))
        }.sortWith(_._2 > _._2)
        val vid_weight_res = vid_weight.take(vid_length)
        KeyValueWeight(tag_id, vid_weight_res)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/402.28.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/410.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        var s = line
        for (r <- Defines.pattern_list) {
          s = r.replaceAllIn(s, """")
        }
        s
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/471.30.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/498.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0).trim().toLowerCase
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/511.28.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/553.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(2).length >= 2
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/570.27.Dataset-(String, String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, String, Long)]
Call: filter

_._1 != """"
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/572.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 20
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/706.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getString(0)
        val value_weight = locally {
          val _t_m_p_112 = line.getAs[Seq[Row]](2)
          _t_m_p_112.map(v => (v.getLong(1).toString, v.getDouble(2)))
        }.distinct.sortWith(_._2 > _._2).take(100)
        KeyValueWeight(vid, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/718.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = if (line.isNullAt(0)) {
          line.getString(2)
        } else {
          line.getString(0)
        }
        val value_weight = if (line.isNullAt(0)) {
          locally {
            val _t_m_p_114 = line.getSeq[Row](3)
            _t_m_p_114.map(row => (row.getString(0), row.getDouble(1)))
          }.sortBy(_._2)(Ordering[Double].reverse).take(100)
        } else if (line.isNullAt(2)) {
          locally {
            val _t_m_p_115 = line.getSeq[Row](1)
            _t_m_p_115.map(row => (row.getString(0), row.getDouble(1)))
          }.sortBy(_._2)(Ordering[Double].reverse).take(100)
        } else {
          val vtags = locally {
            val _t_m_p_116 = line.getSeq[Row](1)
            _t_m_p_116.map(row => (row.getString(0), row.getDouble(1)))
          }.sortBy(_._2)(Ordering[Double].reverse).take(100).toMap
          val qtags = locally {
            val _t_m_p_117 = line.getSeq[Row](3)
            _t_m_p_117.map(row => (row.getString(0), row.getDouble(1)))
          }.sortBy(_._2)(Ordering[Double].reverse).take(100).toMap
          val tags_result = (vtags /: qtags)({
            case (map, (k, v: Double)) =>
              map + (k -> (v + map.getOrElse(k, 0.0d)))
          })
          tags_result.toSeq.sortBy(_._2)(Ordering[Double].reverse).take(100)
        }
        KeyValueWeight(vid, locally {
          val _t_m_p_118 = value_weight
          _t_m_p_118.map(vw => (vw._1, Tools.normalize(vw._2)))
        })
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/794.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val tagid = line.getString(0)
        val vid_weight = locally {
          val _t_m_p_124 = line.getSeq[Row](1)
          _t_m_p_124.map(r => (r.getString(0), r.getDouble(1)))
        }.sortBy(_._2).take(200)
        KeyValueWeight(tagid, vid_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV2.scala/udf/820.23.Dataset-KeyValueWeight.map","Type: org.apache.spark.sql.Dataset[Utils.Tools.KeyValueWeight]
Call: map

line => {
          val vid = line.key
          val value_weight = line.value_weight.distinct.sortBy(_._2)(Ordering[Double].reverse).take(100)
          KeyValueWeight(vid, value_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/113.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val vid = line.getString(0)
          val duration = line.getInt(1)
          val tag = line.getString(2)
          val tagid = if (line.isNullAt(3)) Hash.hash_own(tag) else line.getLong(3)
          val tag_type = if (line.isNullAt(3)) ""inner_tag"" else ""pioneer_tag""
          (vid, duration, tag, tagid, tag_type)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/137.34.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/184.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getString(0)
        val tag = line.getString(1)
        val tagid = if (line.isNullAt(2)) Hash.hash_own(tag) else line.getLong(2)
        val tag_type = if (line.isNullAt(2)) ""inner_tag"" else ""pioneer_tag""
        (vid, tag, tagid, tag_type)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/234.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val vid = line.getAs[String](""vid"")
          val duration = line.getAs[Int](""duration"")
          val tag_tagid = locally {
            val _t_m_p_37 = line.getAs[Seq[Row]](""tag_tagid"")
            _t_m_p_37.map(row => (row.getString(0), row.getLong(1), row.getString(2)))
          }
          val tag_id = line.getAs[Seq[Long]](""tag_id"")
          val weight = line.getAs[Double](""weight"")
          val features = line.getAs[SV](""features"")
          val tagidhash_weight = features.indices.zip(features.values).toMap
          val tag_info_res = locally {
            val _t_m_p_38 = tag_tagid
            _t_m_p_38.map(tp => {
              val tag = tp._1
              val tagid = tp._2
              val tag_type = tp._3
              val tagid_hash = hash_tf_idf(tagid)
              val tagid_weight = tagidhash_weight.getOrElse(tagid_hash, 0.0d)
              TagWeightInfo(tag, tagid, tagid_weight * weight, tag_type)
            })
          }
          (vid, duration, tag_info_res)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/286.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getAs[String](""vid"")
        val duration = line.getAs[Int](""duration"")
        val tag_infos = locally {
          val _t_m_p_41 = locally {
            val _t_m_p_42 = line.getAs[Seq[Seq[Row]]](""tag_info"").flatten
            _t_m_p_42.map(info => TagWeightInfo(info.getString(0), info.getLong(1), info.getDouble(2), info.getString(3)))
          }.groupBy(x => (x.tag, x.tagid, x.tag_type))
          _t_m_p_41.map(s => TagWeightInfo(s._1._1, s._1._2, locally {
            val _t_m_p_43 = s._2
            _t_m_p_43.map(_.weight)
          }.sum, s._1._3))
        }.toSeq.sortBy(_.weight)(Ordering[Double].reverse).take(30)
        val weight_max = locally {
          val _t_m_p_44 = tag_infos
          _t_m_p_44.map(_.weight)
        }.max
        (vid, duration, locally {
          val _t_m_p_45 = tag_infos
          _t_m_p_45.map(tg => TagWeightInfo(tg.tag, tg.tagid, tg.weight / weight_max, tg.tag_type))
        })
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/319.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vid: String, weight: Double) => (vid, weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/353.28.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/361.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        var s = line
        for (r <- Defines.pattern_list) {
          s = r.replaceAllIn(s, """")
        }
        s
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/422.30.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/44.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/449.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0).trim().toLowerCase
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/466.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(2).length >= 2
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/483.27.Dataset-(String, String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, String, Long)]
Call: filter

_._1 != """"
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/485.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 20
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""tagid_number"" === 1
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/655.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val vid = if (line.isNullAt(0)) {
          line.getString(2)
        } else {
          line.getString(0)
        }
        val value_weight = if (line.isNullAt(0)) {
          locally {
            val _t_m_p_103 = line.getSeq[Row](3)
            _t_m_p_103.map(r => TagWeightInfo(r.getString(0), r.getLong(1), r.getDouble(2), r.getString(3)))
          }.sortBy(_.weight)(Ordering[Double].reverse).take(30)
        } else if (line.isNullAt(2)) {
          locally {
            val _t_m_p_104 = line.getSeq[Row](1)
            _t_m_p_104.map(r => TagWeightInfo(r.getString(0), r.getLong(1), r.getDouble(2), r.getString(3)))
          }.sortBy(_.weight)(Ordering[Double].reverse).take(30)
        } else {
          val vtags = locally {
            val _t_m_p_105 = locally {
              val _t_m_p_106 = line.getSeq[Row](1)
              _t_m_p_106.map(r => TagWeightInfo(r.getString(0), r.getLong(1), r.getDouble(2), r.getString(3)))
            }.sortBy(_.weight)(Ordering[Double].reverse).take(30)
            _t_m_p_105.map(t => (t.tag, t.tagid, t.tag_type) -> t.weight)
          }.toMap
          val qtags = locally {
            val _t_m_p_107 = locally {
              val _t_m_p_108 = line.getSeq[Row](3)
              _t_m_p_108.map(r => TagWeightInfo(r.getString(0), r.getLong(1), r.getDouble(2), r.getString(3)))
            }.sortBy(_.weight)(Ordering[Double].reverse).take(30)
            _t_m_p_107.map(t => (t.tag, t.tagid, t.tag_type) -> t.weight)
          }.toMap
          val tags_result = (vtags /: qtags)({
            case (map, (k, v: Double)) =>
              map + (k -> (v + map.getOrElse(k, 0.0d)))
          })
          locally {
            val _t_m_p_109 = tags_result
            _t_m_p_109.map(kv => TagWeightInfo(kv._1._1, kv._1._2, kv._2, kv._1._3))
          }.toSeq.sortBy(_.weight)(Ordering[Double].reverse).take(30)
        }
        val value_weight_normal = locally {
          val _t_m_p_110 = value_weight
          _t_m_p_110.map(v => TagWeightInfo(v.tag, v.tagid, Tools.normalize(v.weight), v.tag_type))
        }
        VidTagInfoList(vid, value_weight_normal)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/721.21.Dataset-VidTagInfoList.map","Type: org.apache.spark.sql.Dataset[BasicData.VidTagsWeightV3.VidTagInfoList]
Call: map

line => {
        val vid = line.vid
        val tag_info = locally {
          val _t_m_p_113 = line.tag_info_list
          _t_m_p_113.map(t => TagInfo(t.tag, t.tagid, t.weight))
        }
        (vid, tag_info.sortBy(_.weight)(Ordering[Double].reverse))
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/736.29.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/752.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => filter_vid_vtags_line(row)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/801.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val tag = line.getAs[String](""tag"")
        val tagid = line.getAs[Long](""tagid"")
        val tag_type = line.getAs[String](""tag_type"")
        val vid_weight = locally {
          val _t_m_p_127 = line.getAs[Seq[Row]](""vid_weight"")
          _t_m_p_127.map(r => (r.getString(0), Tools.normalize(r.getDouble(1))))
        }.sortBy(_._2)(Ordering[Double].reverse).take(1000)
        (tag, tagid, tag_type, vid_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidTagsWeightV3.scala/udf/867.23.Dataset-VidTagsWeightWithControl.map","Type: org.apache.spark.sql.Dataset[BasicData.VidTagsWeightV3.VidTagsWeightWithControl]
Call: map

line => {
          val vid = line.vid
          val is_normal = line.is_normal
          val ts = line.timestamp
          val tag_weight = line.tag_info_list.distinct.sortBy(_.weight)(Ordering[Double].reverse).take(100)
          VidTagsWeightWithControl(vid, is_normal, ts, tag_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidVidRecommander.scala/udf/31.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(index: String, weight: Double) => (index, weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidVidRecommander.scala/udf/87.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""number"" === 1
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidVidRecommander.scala/udf/89.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid1 = line.getString(0)
        val vid2 = line.getString(1)
        val similarity = line.getDouble(2)
        val value_weight = Array((vid2, similarity))
        KeyValueWeight(vid1, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidVidRecommanderV2.scala/udf/107.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val vid1 = line.getString(0)
          val vid2_sim = locally {
            val _t_m_p_7 = line.getSeq[Row](1)
            _t_m_p_7.map(vid_sim_tp => (vid_sim_tp.getString(0), vid_sim_tp.getDouble(1)))
          }.sortBy(_._2)(Ordering[Double].reverse).take(40).toArray
          KeyValueWeight(vid1, vid2_sim)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidVidRecommanderV2.scala/udf/30.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(index: String, weight: Double) => (index, weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.BasicData.VidVidRecommanderV2.scala/udf/76.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid1 = line.getString(0)
        val vid2 = line.getString(1)
        val weight = if (line.isNullAt(2) || line.getString(2).isEmpty || line.getString(2) == """") {
          line.getDouble(3) * 0.1d
        } else {
          line.getDouble(4)
        }
        val weight_type = if (line.isNullAt(2) || line.getString(2).isEmpty || line.getString(2) == """") {
          ""sim""
        } else {
          ""ctr""
        }
        (vid1, vid2, weight, weight_type)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.DataAdapter.ItemMeta.scala/udf/140.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        var s: String = """"
        for (i <- cols_pos_broadcast.value.indices) {
          s += ""\t"" * col_indice_diff_broadcast.value(i)
          val field_name = cols_pos_broadcast.value(i)._2
          val field_value = line.getAs[String](field_name)
          if (field_value != null && !field_value.isEmpty) s += field_value
        }
        s += ""\t"" * (col_format_num - cols_pos_broadcast.value.last._1 - 1)
        ""-1"" + s
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.DataAdapter.ItemMeta.scala/udf/57.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val vid = row.getAs[String](""vid"")
        val duration = row.getAs[Int](""duration"")
        val tag_info = locally {
          val _t_m_p_7 = row.getAs[Seq[Row]](""tag_info"")
          _t_m_p_7.map(tg => TagWeightInfo(tg.getString(0), tg.getLong(1), tg.getDouble(2), tg.getString(3)))
        }
        (vid, duration, tag_info)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.DataAdapter.ItemMeta.scala/udf/94.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getAs[String](""vid"")
        val qtags_info = locally {
          val _t_m_p_13 = line.getAs[Seq[Row]](""tag_weight"")
          _t_m_p_13.map(row => TagWeightInfo(line.getString(0), row.getLong(1), row.getDouble(2), row.getString(3)))
        }.sortBy(_.weight)(Ordering[Double].reverse).take(30)
        (vid, qtags_info)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.DataAdapter.UserInterest.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val value_weight = locally {
          val _t_m_p_2 = locally {
            val _t_m_p_3 = locally {
              val _t_m_p_4 = line.getAs[Seq[Row]](1)
              _t_m_p_4.map(v => (v.getString(0), v.getDouble(1)))
            }.sortBy(_._2)(Ordering[Double].reverse).take(30).distinct
            _t_m_p_3.map(tp => (tp._1, Tools.normalize(tp._2)))
          }
          _t_m_p_2.map(tp => s""${tp._1}:${tp._2.formatted(""%.4f"")}"")
        }.mkString(raw""|"")
        (guid, ""TAG:"" + value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.DataAdapter.UserInterest.scala/udf/33.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getString(0)
          val first = line.getInt(1)
          val second = line.getInt(2)
          val weight = line.getDouble(3)
          val cat_weight = (first, second, weight)
          (guid, cat_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.DataAdapter.UserInterest.scala/udf/42.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val cat_weight = locally {
          val _t_m_p_7 = locally {
            val _t_m_p_8 = line.getSeq[Row](1)
            _t_m_p_8.map(tp => (tp.getInt(0), tp.getInt(1), tp.getDouble(2)))
          }.sortBy(_._3)(Ordering[Double].reverse).take(30).distinct
          _t_m_p_7.map(tp => s""${tp._1}.${tp._2}:${tp._3.formatted(""%.4f"")}"")
        }.mkString(raw""|"")
        (guid, ""CAT_S2:"" + cat_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.DataAdapter.UserInterest.scala/udf/57.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = if (line.isNullAt(0)) line.getString(1) else line.getString(0)
        val tag_weight = if (!line.isNullAt(2)) line.getString(2) else null
        val cat_weight = if (!line.isNullAt(3)) line.getString(3) else null
        val res_str = {
          if (tag_weight == null) cat_weight else if (cat_weight == null) tag_weight else {
            cat_weight + (s""$$"") + tag_weight
          }
        }
        ""guid="" + guid + ""\t"" + res_str
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidRecomType.scala/udf/107.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getString(0)
          val value_weight = locally {
            val _t_m_p_11 = line.getAs[Seq[Row]](1)
            _t_m_p_11.map(v => (s""${v.getInt(0)}.${v.getInt(1)}"", v.getDouble(2)))
          }.sortBy(_._2)(Ordering[Double].reverse).take(30)
          KeyValueWeight(guid, value_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidRecomType.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""percent"" > 0.1d
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidRecomType.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""vid"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/105.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[Row]]) => {
        val data = locally {
          val _t_m_p_10 = xs
          _t_m_p_10.map(line => locally {
            val _t_m_p_11 = line
            _t_m_p_11.map(arr => (arr.getLong(0), arr.getDouble(1)))
          })
        }
        val ret = locally {
          val _t_m_p_12 = data.flatten.groupBy(_._1)
          _t_m_p_12.map(line => {
            val tagid = line._1
            val weight = locally {
              val _t_m_p_13 = line._2
              _t_m_p_13.map(_._2)
            }.sum
            (tagid, weight)
          })
        }
        ret.toSeq.sortBy(_._2)(Ordering[Double].reverse).take(200)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/129.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[String]]) => xs.flatten.distinct
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/159.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getString(0)
          val tag_tagid_weight = locally {
            val _t_m_p_18 = line.getSeq[Row](1)
            _t_m_p_18.map(kv => (kv.getLong(1), kv.getDouble(2) * time_weight_res))
          }
          (guid, tag_tagid_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/191.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val value_weight = locally {
          val _t_m_p_20 = locally {
            val _t_m_p_21 = line.getAs[Seq[Row]](1)
            _t_m_p_21.map(v => (v.getLong(0).toString, v.getDouble(1)))
          }.sortBy(_._2)(Ordering[Double].reverse).take(30).distinct
          _t_m_p_20.map(tp => (tp._1, Tools.normalize(tp._2)))
        }
        KeyValueWeight(guid, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/25.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[Long]]) => xs.flatten.distinct
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/29.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[String]]) => xs.flatten.distinct
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(features_data: Seq[Double], weight: Double) => locally {
        val _t_m_p_5 = features_data
        _t_m_p_5.map(data => data * weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/40.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(features_indice: Seq[Seq[Int]], features_datas: Seq[Seq[Double]]) => if (features_datas.length != features_indice.length) {
        null
      } else {
        val sv_ret = new SparseVector[Double](new Array[Int](0), new Array[Double](0), TAG_HASH_LENGTH)
        for (i <- features_indice.indices) {
          sv_ret += (new SparseVector[Double](features_indice(i).toArray, features_datas(i).toArray, TAG_HASH_LENGTH))
        }
        (sv_ret.index, sv_ret.data)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtags.scala/udf/60.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val guid = line.getString(0)
        val vids = line.getAs[Seq[String]](1)
        val tag_ids = line.getAs[Seq[Long]](2)
        val tags_source = line.getAs[Seq[String]](3)
        val features_index = line.getAs[Seq[Int]](4)
        val features_data = line.getAs[Seq[Double]](5)
        val ret_arr = new ArrayBuffer[(String, Long, Double)]
        for (tag <- tags_source.distinct) {
          if (tag.contains(""_inner"")) {
            val tag_pure = tag.replace(""_inner"", """")
            val tag_id = Hash.hash_own(tag_pure)
            val hash_value = hash_tf_idf(Hash.hash_own(tag_pure))
            val index = if (features_index == null || features_index.isEmpty) -1 else features_index.indexOf(hash_value)
            if (index == -1) {} else {
              val weight = features_data(index)
              ret_arr += ((tag_pure, tag_id, weight))
            }
          } else {
            val tag_data = tag.split(""##"")
            val tag_pure = tag_data(0)
            val tag_id = tag_data(tag_data.length - 1).toLong
            val hash_value = hash_tf_idf(tag_id)
            val index = if (features_index == null || features_index.isEmpty) -1 else features_index.indexOf(hash_value)
            if (index != -1) {
              val weight = features_data(index)
              ret_arr += ((tag_pure, tag_id, weight))
            }
          }
        }
        val arr_normal = locally {
          val _t_m_p_8 = ret_arr
          _t_m_p_8.map(tp => (tp._1, tp._2, Tools.normalize(tp._3)))
        }
        (guid, vids, ret_arr, arr_normal)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtagsV2.scala/udf/109.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getAs[String](""guid"")
        val tag_weight = locally {
          val _t_m_p_17 = line.getAs[Seq[Seq[Row]]](""tagid_weight"").flatten
          _t_m_p_17.map(info => (info.getString(0), info.getDouble(1)))
        }.foldLeft(Map[String, Double]())({
          case (map, (k, v)) =>
            map + (k -> (v + map.getOrElse(k, 0.0d)))
        }).toSeq.sortBy(_._2)(Ordering[Double].reverse).take(100)
        (guid, tag_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtagsV2.scala/udf/136.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
            val guid = line.getString(0)
            val value_weight = locally {
              val _t_m_p_20 = locally {
                val _t_m_p_21 = line.getAs[Seq[Row]](1)
                _t_m_p_21.map(v => (v.getString(0), v.getDouble(1)))
              }.sortBy(_._2)(Ordering[Double].reverse).take(30).distinct
              _t_m_p_20.map(tp => (tp._1, Tools.normalize_guid_weight(tp._2)))
            }
            val weight_max = locally {
              val _t_m_p_22 = value_weight
              _t_m_p_22.map(_._2)
            }.max
            val value_weight_res = locally {
              val _t_m_p_23 = value_weight
              _t_m_p_23.map(tp => (tp._1, tp._2 * 2 / weight_max))
            }
            KeyValueWeight(guid, value_weight_res)
          }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtagsV2.scala/udf/159.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
            val guid = line.getString(0)
            val value_weight = locally {
              val _t_m_p_25 = locally {
                val _t_m_p_26 = line.getAs[Seq[Row]](1)
                _t_m_p_26.map(v => (v.getString(0), v.getDouble(1)))
              }.sortBy(_._2)(Ordering[Double].reverse).take(30).distinct
              _t_m_p_25.map(tp => (tp._1, tp._2))
            }
            KeyValueWeight(guid, value_weight)
          }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtagsV2.scala/udf/24.19.Dataset-VidTagInfoList.map","Type: org.apache.spark.sql.Dataset[BasicData.VidTagsWeightV3.VidTagInfoList]
Call: map

v => KeyValueWeight(v.vid, locally {
        val _t_m_p_3 = v.tag_info_list
        _t_m_p_3.map(taginfo => (taginfo.tagid.toString, taginfo.weight))
      })
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtagsV2.scala/udf/33.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""tag_weight"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtagsV2.scala/udf/35.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getAs[String](""guid"")
          val tag_weight = locally {
            val _t_m_p_7 = locally {
              val _t_m_p_8 = line.getAs[Seq[Seq[Row]]](""tag_weight"").flatten
              _t_m_p_8.map(row => (row.getString(0), row.getDouble(1)))
            }.groupBy(_._1)
            _t_m_p_7.map(kv => (kv._1, kv._2.foldLeft(0.0d)(_ + _._2)))
          }.toSeq
          (guid, locally {
            val _t_m_p_9 = tag_weight
            _t_m_p_9.filter(_._2 > 0.01d)
          }, locally {
            val _t_m_p_10 = tag_weight
            _t_m_p_10.map(tw => (tw._1, Tools.normalize(tw._2)))
          })
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtagsV2.scala/udf/90.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size($""tagid_weight"") > 0
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.GuidVtagsV2.scala/udf/92.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
          val guid = line.getAs[String](0)
          val tag_weight = locally {
            val _t_m_p_15 = line.getAs[Seq[Row]](1)
            _t_m_p_15.map(row => (row.getString(0), row.getDouble(1) * time_weight_res))
          }
          (guid, tag_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/103.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(data: Seq[Row]) => {
        val d = locally {
          val _t_m_p_17 = data
          _t_m_p_17.map(line => (line.getString(0), line.getDouble(1)))
        }
        d.sortBy(_._2)(Ordering[Double].reverse).take(200)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/113.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(cid: String, cid_weight: Double) => (cid, cid_weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/11.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vid_num: Long, current_update: Long) => if (vid_num == 0 || current_update == 0 || vid_num > current_update) {
        0.2d
      } else {
        vid_num.toDouble / current_update.toDouble
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/119.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""vid"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/125.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val ts = line.getLong(1)
        val vid = line.getString(2)
        val cid = line.getString(3)
        val vid_weight = line.getDouble(4)
        val percent = line.getDouble(5)
        (guid, vid, cid, vid_weight * percent)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/139.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val vid_tuple2 = locally {
          val _t_m_p_22 = line.getAs[Seq[Row]](1)
          _t_m_p_22.map(line => (line.getString(0), line.getDouble(1)))
        }.sortBy(_._2)(Ordering[Double].reverse).take(100)
        (guid, vid_tuple2, locally {
          val _t_m_p_23 = vid_tuple2
          _t_m_p_23.map(tp => (tp._1, Tools.normalize(tp._2)))
        })
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/156.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val cid_tuple2 = locally {
          val _t_m_p_25 = line.getAs[Seq[Row]](1)
          _t_m_p_25.map(line => (line.getString(0), line.getDouble(1)))
        }.sortBy(_._2)(Ordering[Double].reverse).take(100)
        (guid, cid_tuple2, locally {
          val _t_m_p_26 = cid_tuple2
          _t_m_p_26.map(tp => (tp._1, Tools.normalize(tp._2)))
        })
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/178.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[Row]]) => {
        val data = locally {
          val _t_m_p_28 = xs
          _t_m_p_28.map(line => locally {
            val _t_m_p_29 = line
            _t_m_p_29.map(arr => (arr.getString(0), arr.getDouble(1)))
          })
        }
        val ret = locally {
          val _t_m_p_30 = data.flatten.groupBy(_._1)
          _t_m_p_30.map(line => {
            val vid = line._1
            val weight = locally {
              val _t_m_p_31 = line._2
              _t_m_p_31.map(line => line._2)
            }.sum
            (vid, weight)
          })
        }
        ret.toSeq.sortBy(_._2)(Ordering[Double].reverse).take(100)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/229.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getString(0)
          val cid_weight = locally {
            val _t_m_p_35 = line.getSeq[Row](1)
            _t_m_p_35.map(kv => (kv.getString(0), kv.getDouble(1) * time_weight_res))
          }
          (guid, cid_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/254.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[Row]]) => {
        val data = locally {
          val _t_m_p_37 = xs
          _t_m_p_37.map(line => locally {
            val _t_m_p_38 = line
            _t_m_p_38.map(arr => (arr.getString(0), arr.getDouble(1)))
          })
        }
        val ret = locally {
          val _t_m_p_39 = data.flatten.groupBy(_._1)
          _t_m_p_39.map(line => {
            val vid = line._1
            val weight = locally {
              val _t_m_p_40 = line._2
              _t_m_p_40.map(_._2)
            }.sum
            (vid, weight)
          })
        }
        ret.toSeq.sortBy(_._2)(Ordering[Double].reverse).take(100)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/305.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getString(0)
          val vid_weight = locally {
            val _t_m_p_44 = line.getSeq[Row](1)
            _t_m_p_44.map(kv => (kv.getString(0), kv.getDouble(1) * time_weight_res))
          }
          (guid, vid_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/335.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val value_weight = locally {
          val _t_m_p_46 = line.getAs[Seq[Row]](1).take(30)
          _t_m_p_46.map(v => (v.getString(0), Tools.normalize(v.getDouble(1))))
        }
        KeyValueWeight(guid, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/364.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val value_weight = locally {
          val _t_m_p_48 = line.getAs[Seq[Row]](1).take(30)
          _t_m_p_48.map(v => (v.getString(0), Tools.normalize(v.getDouble(1))))
        }
        KeyValueWeight(guid, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/80.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""vid_t"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.GuidData.Jmtags.scala/udf/99.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vid: String, vid_weight: Double) => (vid, vid_weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.TagVids.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.isNullAt(2)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.TagVids.scala/udf/32.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val vid = line.getAs[String](0)
        val tag_name = line.getAs[String](6)
        val tag_id = line.getAs[String](5).toLong
        val hashval = hash_tf_idf(tag_id)
        val indice = line.getAs[Seq[Int]](2)
        val tag_index = indice.indexOf(hashval)
        val weights = line.getAs[Seq[Double]](3)
        val weight = weights(tag_index)
        (tag_id, tag_name, vid, weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/105.27.Dataset-(Int, Int).filter","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: filter

_._1 > i_mul
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/107.25.Dataset-(Int, Int).filter","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: filter

_._1 <= i_mul_plus
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/112.27.Dataset-(Int, Int).filter","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: filter

_._2 > i_mul
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/114.25.Dataset-(Int, Int).filter","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: filter

_._2 <= i_mul_plus
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/122.23.Dataset-(Int, Int).filter","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: filter

_._1 > 100
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/125.23.Dataset-(Int, Int).filter","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: filter

_._2 > 100
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/132.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val data = line.split(""###"", -1)
        (data(0), data(1))
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/141.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => (line.getString(0), line.getString(2))
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/164.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val tag = line.getString(0)
        val tagid = line.getString(1)
        val vid = line.getString(2)
        val title = line.getString(3)
        val weight = line.getDouble(4)
        (tag, tagid, vid + ""###"" + title + ""###"" + weight.formatted(""%.4f""))
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/195.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val key = line.getString(0)
        val tag_info = locally {
          val _t_m_p_33 = line.getAs[Seq[Row]](1)
          _t_m_p_33.map(row => (row.getString(0), row.getDouble(1)))
        }
        val count = tag_info.length
        (key, count)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/207.23.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

line => line._1.toLong > 0 && line._1.toLong < 100000000
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/211.23.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

line => !(line._1.toLong > 0 && line._1.toLong < 100000000)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/217.25.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 == i
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/220.25.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 == i
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/229.27.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 > i_mul
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/231.25.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 <= i_mul_plus
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/236.27.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 > i_mul
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/238.25.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 <= i_mul_plus
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/249.27.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 > i_mul
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/251.25.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 <= i_mul_plus
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/256.27.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 > i_mul
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/258.25.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 <= i_mul_plus
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/266.23.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 > 1000
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/269.23.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._2 > 1000
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/32.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""title"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/34.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""duration"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/36.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val vid = line.getString(0)
        val title = line.getString(1)
        val map_name = line.getString(2)
        val duration = line.getInt(3)
        val tag_info = locally {
          val _t_m_p_7 = locally {
            val _t_m_p_8 = line.getAs[Seq[Row]](4)
            _t_m_p_8.map(row => (row.getString(0), row.getLong(1), row.getDouble(2)))
          }.sortBy(_._3)(Ordering[Double].reverse)
          _t_m_p_7.map(row => row._1 + ""###"" + row._2 + ""###"" + row._3.formatted(""%.04f""))
        }.mkString("";"")
        val checkup_time = line.getString(5)
        val url = ""http://v.qq.com/x/page/"" + vid + "".html""
        (vid, title, map_name, tag_info, checkup_time, duration, url)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/57.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""vid_info"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getString(0)
        val title = line.getString(1)
        val map_name = line.getString(2)
        val duration = line.getInt(3)
        val tag_info = locally {
          val _t_m_p_11 = locally {
            val _t_m_p_12 = line.getSeq[Row](4)
            _t_m_p_12.map(arr => (arr.getString(0), arr.getLong(1), arr.getDouble(2)))
          }.sortBy(_._3)(Ordering[Double].reverse)
          _t_m_p_11.map(arr => arr._1 + ""###"" + arr._2 + ""###"" + arr._3.formatted(""%.4f""))
        }.mkString("";"")
        val checkup_time = line.getString(5)
        (vid, title, map_name, duration, tag_info, checkup_time)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/78.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val vid = line.getString(0)
        val tag_info = locally {
          val _t_m_p_14 = line.getAs[Seq[Row]](2)
          _t_m_p_14.map(row => (row.getString(0), row.getLong(1), row.getDouble(2)))
        }
        val poineer_count = tag_info.count(info => 0 < info._2 && info._2 < 100000000)
        val inner_count = tag_info.length - poineer_count
        (poineer_count, inner_count)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/93.25.Dataset-(Int, Int).filter","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: filter

_._1 == i
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTagsRank.scala/udf/96.25.Dataset-(Int, Int).filter","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: filter

_._2 == i
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTags.scala/udf/10.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(tags_id: Seq[Long], tags_source: Seq[String], indice: Seq[Int], datas: Seq[Double]) => if (tags_id == null || tags_id.isEmpty) null else {
        val ret_tags = new ArrayBuffer[(String, Long, Double)]
        for (tag <- tags_source.distinct) {
          if (tag.contains(""_inner"")) {
            val tag_pure = tag.replace(""_inner"", """")
            val hash_value = Hash.hash_own(tag_pure)
            val hash_tf = Hash.hash_tf_idf(hash_value)
            val index = indice.indexOf(hash_tf)
            val weight = datas(index)
            ret_tags append ((tag_pure, hash_value, weight))
          } else {
            val tag_data = tag.split(""##"", -1)
            val tag_pure = tag_data(0)
            val hash_value = if (tag_data.length == 1) tag_data(0).toLong else tag_data(1).toLong
            val hash_tf = Hash.hash_tf_idf(hash_value)
            val index = indice.indexOf(hash_tf)
            val weight = datas(index)
            ret_tags append ((tag_pure, hash_value, weight))
          }
        }
        locally {
          val _t_m_p_2 = ret_tags.sortWith(_._3 > _._3)
          _t_m_p_2.map(line => line._1 + ""#"" + line._2 + ""#"" + line._3.formatted(""%.4f""))
        }.mkString("";"")
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Pm.VidTags.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""vid"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.RedisManager.VidTag.scala/udf/48.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val line_data = line.split(""\t"", -1)
        val guid = line_data(0)
        val cps = line_data(1)
        val value_weight = locally {
          val _t_m_p_5 = locally {
            val _t_m_p_6 = cps.split("";"", -1)
            _t_m_p_6.map(line => {
              val line_data = line.split("","", -1)
              val cp = line_data(0)
              val weight = if (line_data.length == 2) line_data(1).toDouble else -1
              (cp, weight)
            })
          }
          _t_m_p_5.filter(_._2 != -1)
        }
        KeyValueWeight(guid, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.RedisManager.VidTag.scala/udf/87.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val line_data = line.split(""\t"", -1)
        val guid = line_data(0)
        val cps = line_data(1)
        val value_weight = locally {
          val _t_m_p_8 = locally {
            val _t_m_p_9 = cps.split("";"", -1)
            _t_m_p_9.map(line => {
              val line_data = line.split("","", -1)
              val cp = line_data(0)
              val weight = if (line_data.length == 2) line_data(1).toDouble else -1
              (cp, weight)
            })
          }
          _t_m_p_8.filter(_._2 != -1)
        }
        KeyValueWeight(guid, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/105.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(cid: String, cid_weight: Double) => (cid, cid_weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/111.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""vid"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/118.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val ts = line.getLong(1)
        val vid = line.getString(2)
        val cid = line.getString(3)
        val vid_weight = line.getDouble(4)
        val percent = line.getDouble(5)
        val now_time = now.value
        val time_weight = Math.pow((now_time - ts).toDouble / (24 * 60 * 60), -0.35d)
        val time_weight_res = if (time_weight > 1) 1 else time_weight
        (guid, vid, cid, time_weight_res * vid_weight * percent)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/137.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val cid_tuple2 = locally {
          val _t_m_p_22 = line.getAs[Seq[Row]](1)
          _t_m_p_22.map(line => (line.getString(0), line.getDouble(2)))
        }.sortWith(_._2 > _._2)
        (guid, cid_tuple2)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/164.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getString(0)
          val value_weight = locally {
            val _t_m_p_25 = line.getAs[Seq[Row]](1).take(100)
            _t_m_p_25.map(v => (v.getString(0), v.getDouble(2)))
          }
          KeyValueWeight(guid, value_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/173.23.Dataset-KeyValueWeight.filter","Type: org.apache.spark.sql.Dataset[Utils.Tools.KeyValueWeight]
Call: filter

d => Tools.boss_guid.contains(d.key)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/194.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getString(0)
          val value_weight = locally {
            val _t_m_p_29 = line.getAs[Seq[Row]](1).take(100)
            _t_m_p_29.map(v => (v.getString(0), v.getDouble(2)))
          }
          KeyValueWeight(guid, value_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/203.23.Dataset-KeyValueWeight.filter","Type: org.apache.spark.sql.Dataset[Utils.Tools.KeyValueWeight]
Call: filter

d => Tools.boss_guid.contains(d.key)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/78.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""vid_t"".isNotNull
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/91.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vid: String, vid_weight: Double) => (vid, vid_weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/9.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vid_num: Long, current_update: Long) => if (vid_num == 0 || current_update == 0 || vid_num > current_update) {
        0.2d
      } else {
        vid_num.toDouble / current_update.toDouble
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.CidVidIndexWash.scala/udf/95.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(data: Seq[Row]) => {
        val d = locally {
          val _t_m_p_17 = data
          _t_m_p_17.map(line => (line.getString(0), line.getString(1)))
        }
        d.sortWith(_._2 > _._2)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/181.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getString(0)
        val duration = line.getInt(1)
        val tags_id = new ArrayBuffer[Long]
        val tags_source = new ArrayBuffer[String]
        var spv: SparseVector[Double] = new SparseVector[Double](new Array[Int](0), new Array[Double](0), TAG_HASH_LENGTH)
        for (i <- 0 until vid_useful_col.size) {
          if (!line.isNullAt(6 * i) && !line.getAs[String](6 * i).equals("""")) {
            tags_id ++= line.getAs[Seq[Long]](6 * i + 2)
            tags_source ++= line.getAs[Seq[String]](6 * i + 3)
            val weight = line.getAs[Double](6 * i + 4)
            val sv = line.getAs[SV](6 * i + 5)
            spv += new SparseVector[Double](sv.indices, sv.values, sv.size) * weight
          }
        }
        spv = spv / norm(spv)
        vid_idf_line(vid, duration, tags_id.distinct.toArray, tags_source.distinct.toArray, spv.index, spv.data)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/248.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => filter_data(line)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/275.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[Long]]) => xs.flatten.distinct
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/279.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[String]]) => xs.flatten.distinct
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/283.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(features_data: Seq[Double], weight: Double) => locally {
        val _t_m_p_27 = features_data
        _t_m_p_27.map(data => data * weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/290.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(features_indice: Seq[Seq[Int]], features_datas: Seq[Seq[Double]]) => if (features_datas.length != features_indice.length) {
        null
      } else {
        val sv_ret = new SparseVector[Double](new Array[Int](0), new Array[Double](0), TAG_HASH_LENGTH)
        for (i <- features_indice.indices) {
          sv_ret += (new SparseVector[Double](features_indice(i).toArray, features_datas(i).toArray, TAG_HASH_LENGTH))
        }
        (sv_ret.index, sv_ret.data)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/305.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""vid2"".isNotNull && $""vid2"" =!= """"
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/314.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val vids = line.getAs[Seq[String]](1)
        val tag_ids = line.getAs[Seq[Long]](2)
        val tags_source = line.getAs[Seq[String]](3)
        val features_index = line.getAs[Seq[Int]](4)
        val features_data = line.getAs[Seq[Double]](5)
        val ret_arr = new ArrayBuffer[(String, Long, Double)]
        for (tag <- tags_source.distinct) {
          if (tag.contains(""_inner"")) {
            val tag_pure = tag.replace(""_inner"", """")
            val tag_id = Hash.hash_own(tag_pure)
            val hash_value = hash_tf_idf(Hash.hash_own(tag_pure))
            val index = features_index.indexOf(hash_value)
            if (index == -1) {} else {
              val weight = features_data(index)
              ret_arr += ((tag_pure, tag_id, weight))
            }
          } else {
            val tag_data = tag.split(""##"")
            val tag_pure = tag_data(0)
            val tag_id = tag_data(tag_data.length - 1).toLong
            val hash_value = hash_tf_idf(tag_id)
            val index = features_index.indexOf(hash_value)
            if (index == -1) {} else {
              val weight = features_data(index)
              ret_arr += ((tag_pure, tag_id, weight))
            }
          }
        }
        (guid, vids, ret_arr)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/388.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(xs: Seq[Seq[String]]) => xs.flatten.distinct
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/392.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(features_data: Seq[Double], weight: Double) => locally {
        val _t_m_p_34 = features_data
        _t_m_p_34.map(data => data * weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/399.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(guid_tag_weight: Seq[Seq[Row]]) => {
        val tag_map = new mutable.HashMap[String, Long]()
        val temp = new SparseVector[Double](new Array[Int](0), new Array[Double](0), TAG_HASH_LENGTH)
        for (tag_id_weight <- guid_tag_weight) {
          val feature_index = locally {
            val _t_m_p_36 = tag_id_weight
            _t_m_p_36.map(line => Hash.hash_tf_idf(line.getLong(1)))
          }.toArray
          val feature_data = locally {
            val _t_m_p_37 = tag_id_weight
            _t_m_p_37.map(_.getDouble(2))
          }.toArray
          tag_map ++= locally {
            val _t_m_p_38 = tag_id_weight
            _t_m_p_38.map(line => (line.getString(0), line.getLong(1)))
          }
          temp += (new SparseVector[Double](feature_index, feature_data, TAG_HASH_LENGTH))
        }
        locally {
          val _t_m_p_39 = tag_map
          _t_m_p_39.map(kv => {
            val hash_id = Hash.hash_tf_idf(kv._2, TAG_HASH_LENGTH)
            val index = temp.index.indexOf(hash_id)
            val weight = temp.data(index)
            (kv._1, kv._2, weight)
          })
        }.toSeq.sortWith(_._3 > _._3)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/430.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(tagid_weight: Seq[Row], weight: Double) => locally {
        val _t_m_p_41 = tagid_weight
        _t_m_p_41.map(line => (line.getString(0), line.getLong(1), line.getDouble(2) / weight))
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/471.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val guid = line.getString(0)
          val value_weight = locally {
            val _t_m_p_44 = line.getAs[Seq[Row]](2).take(100)
            _t_m_p_44.map(v => (v.getLong(1).toString, v.getDouble(2)))
          }.distinct
          KeyValueWeight(guid, value_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/480.23.Dataset-KeyValueWeight.filter","Type: org.apache.spark.sql.Dataset[Utils.Tools.KeyValueWeight]
Call: filter

d => Tools.boss_guid.contains(d.key)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/498.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val vid = line.getString(0)
        val value_weight = locally {
          val _t_m_p_46 = line.getAs[Seq[Row]](2)
          _t_m_p_46.map(v => (v.getLong(1).toString, v.getDouble(2)))
        }.distinct.sortWith(_._2 > _._2).take(100)
        KeyValueWeight(vid, value_weight)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/516.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vid: String, weight: Double) => (vid, weight)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/528.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
          val vid = line.getString(0)
          val value_weight = locally {
            val _t_m_p_50 = line.getAs[Seq[Row]](2)
            _t_m_p_50.map(v => (v.getLong(1).toString, v.getDouble(2)))
          }
          KeyValueWeight(vid, value_weight)
        }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.TagRecommend.scala/udf/549.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val tag_id = line.getString(0)
        val vid_weight = locally {
          val _t_m_p_53 = line.getAs[Seq[Row]](1)
          _t_m_p_53.map(d => (d.getString(0), d.getDouble(1)))
        }.sortWith(_._2 > _._2)
        val vid_weight_res = vid_weight.take(5000)
        KeyValueWeight(tag_id, vid_weight_res)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.TagRecommender.VidFilter.scala/udf/88.22.Dataset-vid_play_times.filter","Type: org.apache.spark.sql.Dataset[TagRecommender.VidFilter.vid_play_times]
Call: filter

$""play_times"" >= threshold
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Test.LpTest.scala/udf/30.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"", -1)
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Test.LpTest.scala/udf/38.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        var s = line
        for (r <- pattern_list) {
          s = r.replaceAllIn(s, """")
        }
        s
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Test.Test.scala/udf/49.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val guid = line.getString(0)
        val value_weight = line.getAs[Seq[Row]](2)
        val value_weight_res = locally {
          val _t_m_p_5 = {
            if (value_weight.length > 100) value_weight.take(100) else value_weight
          }
          _t_m_p_5.map(v => (v.getLong(1).toString, v.getDouble(2)))
        }.distinct
        KeyValueWeight(guid, value_weight_res)
      }
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Utils.Tools.scala/udf/67.19.Dataset-KeyValueWeight.map","Type: org.apache.spark.sql.Dataset[Utils.Tools.KeyValueWeight]
Call: map

line => line.value_weight.length
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Utils.Tools.scala/udf/76.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""length"" >= i * interval && $""length"" < (i + 1) * interval
"
"udf/spark_repos_2/2_uuunic_ProcessVideoInfo/..src.main.scala.Utils.Tools.scala/udf/81.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""length"" >= (LOOPS - 1) * interval
"
"udf/spark_repos_2/2_WeiWenda_SimpleTpin/..src.scala.lite.main.InfluenceMeasure.scala/udf/328.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""wtbz"" === 1
"
"udf/spark_repos_2/2_WeiWenda_SimpleTpin/..src.scala.lite.main.InfluenceMeasure.scala/udf/332.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""wtbz"" === 0
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.Feature.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""glad"".isin(trueValues: _*)
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.Feature.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""tcl"".isin(trueValues: _*)
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.GadmFeature.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

substring($""gid_0"", 0, 1) === i(0)
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.GadmFeature.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""gid_0"" >= i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.GadmFeature.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""gid_0"" < i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.GadmFeature.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""gid_0"" === i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.GadmFeature.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""gid_1"" === i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.GadmFeature.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""gid_2"" === i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.SimpleFeature.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""fid"" >= i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.WdpaFeature.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

substring($""iso"", 0, 1) === i(0)
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.WdpaFeature.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""iso"" >= i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.WdpaFeature.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""iso"" < i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.WdpaFeature.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""iso"" === i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.WdpaFeature.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""wdpaid"" >= i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.features.WdpaFeature.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""status"" === i
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.annualupdate.AnnualUpdateExport.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""umd_tree_cover_loss__year"".isNotNull && $""umd_tree_cover_loss__ha"" > 0
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.annualupdate_minimal.AnnualUpdateMinimalExport.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""umd_tree_cover_loss__year"".isNotNull && $""umd_tree_cover_loss__ha"" > 0
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.annualupdate_minimal.AnnualUpdateMinimalExport.scala/udf/71.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""umd_tree_cover_loss__year"".isNotNull && $""umd_tree_cover_loss__ha"" > 0
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.annualupdate_minimal.AnnualUpdateMinimalExport.scala/udf/88.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""umd_tree_cover_loss__year"".isNotNull && $""umd_tree_cover_loss__ha"" > 0
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.carbonflux.CarbonFluxExport.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""umd_tree_cover_loss__year"".isNotNull && $""umd_tree_cover_loss__ha"" > 0
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.carbon_sensitivity.CarbonSensitivityExport.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""umd_tree_cover_loss__year"".isNotNull && $""umd_tree_cover_loss__ha"" > 0
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.firealerts.FireAlertsDF.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""alert__date"".isNotNull
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.firealerts.FireAlertsDF.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""alert__date"".isNotNull
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.gladalerts.GladAlertsDF.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""data_group.tile.z"" === minZoom
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.gladalerts.GladAlertsDF.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""alert__date"".isNotNull
"
"udf/spark_repos_2/2_wri_gfw_forest_loss_geotrellis/..src.main.scala.org.globalforestwatch.summarystats.gladalerts.GladAlertsDF.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""alert__date"".isNotNull
"
"udf/spark_repos_2/30_unchartedsoftware_sparkpipe-core/..src.main.scala.software.uncharted.sparkpipe.ops.core.dataframe.numeric.package.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

test
"
"udf/spark_repos_2/30_unchartedsoftware_sparkpipe-core/..src.main.scala.software.uncharted.sparkpipe.ops.core.dataframe.temporal.package.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterFcn(new Column(timeCol))
"
"udf/spark_repos_2/30_unchartedsoftware_sparkpipe-core/..src.main.scala.software.uncharted.sparkpipe.ops.core.dataframe.temporal.package.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterFcn(new Column(timeCol))
"
"udf/spark_repos_2/30_unchartedsoftware_sparkpipe-core/..src.main.scala.software.uncharted.sparkpipe.ops.core.dataframe.text.package.scala/udf/112.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterFunc(new Column(stringCol))
"
"udf/spark_repos_2/30_unchartedsoftware_sparkpipe-core/..src.main.scala.software.uncharted.sparkpipe.ops.core.dataframe.text.package.scala/udf/117.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!filterFunc(new Column(stringCol))
"
"udf/spark_repos_2/36_dbis-ilm_stark/..src.main.scala.dbis.stark.sql.Functions.scala/udf/26.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CalcRasterHistogram
"
"udf/spark_repos_2/3_charleso_fuse/..src.main.scala.fuse.Data.scala/udf/13.17.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

f
"
"udf/spark_repos_2/3_charleso_fuse/..src.main.scala.fuse.Data.scala/udf/27.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

r => f(E.fromRowUnsafe(r))
"
"udf/spark_repos_2/3_charleso_fuse/..src.main.scala.fuse.Data.scala/udf/50.19.Dataset-Row).map","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, org.apache.spark.sql.Row)]
Call: map

r => Row(r._1, r._2)
"
"udf/spark_repos_2/3_charleso_fuse/..src.main.scala.fuse.Data.scala/udf/64.19.Dataset-Row).map","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, org.apache.spark.sql.Row)]
Call: map

r => Row(r._1, r._2)
"
"udf/spark_repos_2/3_huashishaojie_Sparkstreaming_News/..sparkS.src.main.scala.com.spark.test.StructuredStreamingKafka.scala/udf/13.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.Cohort.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        x => !x.isNullAt(_idIdx) && !x.isNullAt(_apiIdx) && (x.getAs[String](_apiIdx) == ""https://www.classting.com/api/users"" || x.getAs[String](_apiIdx) == ""https://oauth.classting.com/v1/oauth2/sign_up"" || x.getAs[String](_apiIdx) == ""https://oauth.classting.com/api/users"") && x.getAs[String](_methodIdx) == ""POST"" && x.getAs[Any](_codeIdx) + """" == ""200""
      }
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.Cohort.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

rawlogsDF(""api"").isNotNull && rawlogsDF(""api"").notEqual(""page_move"") && rawlogsDF(""api"").notEqual(""_null"") && rawlogsDF(""id"").isNotNull
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.StatsAccumUser.scala/udf/123.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        x => !x.isNullAt(apiIdx) && x.getAs[String](apiIdx).contains(""https://www.classting.com/api/users"") && x.getAs[String](methodIdx) == ""DELETE"" && x.getAs[Any](codeIdx) + """" == ""200"" && x.getAs[String](apiIdx).split(""/"").length == 6
      }
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.StatsAccumUser.scala/udf/130.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ x => 
        if (x.getAs[String](roleIdx) != """") {
          role2 = x.getAs[String](roleIdx)
        }
        if (langIdx < 0) {
          if (deviceIdx > 0) {
            val device = x.getAs[String](deviceIdx) + """"
            StatsAccum(role2, device.replace("".event"", """"), x.getAs[String](countryIdx), todayDir, ""_null"", x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          } else if (tagIdx > 0) {
            val tag = x.getAs[String](tagIdx) + """"
            StatsAccum(role2, tag.replace("".event"", """"), x.getAs[String](countryIdx), todayDir, ""_null"", x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          } else {
            StatsAccum(role2, ""_null"", x.getAs[String](countryIdx), todayDir, ""_null"", x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          }
        } else {
          if (deviceIdx > 0) {
            val device = x.getAs[String](deviceIdx) + """"
            StatsAccum(role2, device.replace("".event"", """"), x.getAs[String](countryIdx), todayDir, x.getAs[String](langIdx), x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          } else if (tagIdx > 0) {
            val tag = x.getAs[String](tagIdx) + """"
            StatsAccum(role2, tag.replace("".event"", """"), x.getAs[String](countryIdx), todayDir, x.getAs[String](langIdx), x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          } else {
            StatsAccum(role2, ""_null"", x.getAs[String](countryIdx), todayDir, x.getAs[String](langIdx), x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          }
        }
      }
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.StatsAccumUser.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        x => !x.isNullAt(apiIdx) && (x.getAs[String](apiIdx) == ""https://www.classting.com/api/users"" || x.getAs[String](apiIdx) == ""https://oauth.classting.com/v1/oauth2/sign_up"" || x.getAs[String](apiIdx) == ""https://oauth.classting.com/api/users"") && x.getAs[String](methodIdx) == ""POST"" && x.getAs[Any](codeIdx) + """" == ""200""
      }
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.StatsAccumUser.scala/udf/94.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ x => 
        if (x.getAs[String](roleIdx) != """") {
          role = x.getAs[String](roleIdx)
        }
        if (langIdx < 0) {
          if (deviceIdx > 0) {
            val device = x.getAs[String](deviceIdx) + """"
            StatsAccum(role, device.replace("".event"", """"), x.getAs[String](countryIdx), todayDir, ""_null"", x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          } else if (tagIdx > 0) {
            val tag = x.getAs[String](tagIdx) + """"
            StatsAccum(role, tag.replace("".event"", """"), x.getAs[String](countryIdx), todayDir, ""_null"", x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          } else {
            StatsAccum(role, ""_null"", x.getAs[String](countryIdx), todayDir, ""_null"", x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          }
        } else {
          if (deviceIdx > 0) {
            val device = x.getAs[String](deviceIdx) + """"
            StatsAccum(role, device.replace("".event"", """"), x.getAs[String](countryIdx), todayDir, x.getAs[String](langIdx), x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          } else if (tagIdx > 0) {
            val tag = x.getAs[String](tagIdx) + """"
            StatsAccum(role, tag.replace("".event"", """"), x.getAs[String](countryIdx), todayDir, x.getAs[String](langIdx), x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          } else {
            StatsAccum(role, ""_null"", x.getAs[String](countryIdx), todayDir, x.getAs[String](langIdx), x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
          }
        }
      }
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.StatsUniqueClass.scala/udf/102.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        x => UniqueStats(x.getAs[String](""target_id""), x.getAs[String](""role""), x.getAs[String](""device""), x.getAs[String](""country""), x.getAs[Long](""unique_cnt""), todayDir, x.getAs[String](""language""), x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeNameND)
      }
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.StatsUniqueClass.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ x => 
        val targetType = """" + x.getAs[String](targetTypeIdx)
        val resType = """" + x.getAs[String](resourceTypeIdx)
        targetType.equals(""class"") && (resType.equals(""texthome"") || resType.equals(""filehome"") || resType.equals(""photohome"") || resType.equals(""videohome"") || resType.equals(""sharehome"") || resType.equals(""photos"") || resType.equals(""notice"")) && x.getAs[Any](""code"") + """" != ""400""
      }
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.StatsUniqueUser.scala/udf/101.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        x => UniqueStats(x.getAs[String](""id""), x.getAs[String](""role""), x.getAs[String](""device""), x.getAs[String](""country""), x.getAs[Long](""unique_cnt""), todayDir, x.getAs[String](""language""), x.getAs[Any](""grade"").toString.toLong, timeStamp.toString, indexName, typeName)
      }
"
"udf/spark_repos_2/3_hyonaldo_spark-submit-examples/..src.main.scala.com.classting.StatsUniqueUser.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        x => !x.isNullAt(apiIdx) && !x.getAs[String](apiIdx).equals(""page_move"") && !x.getAs[String](apiIdx).equals(""_null"") && !x.isNullAt(idIdx) && !x.getAs[String](idIdx).isEmpty() && x.getAs[Any](codeIdx) + """" != ""400""
      }
"
"udf/spark_repos_2/3_JorisTruong_youtube-setl/..src.main.scala.com.github.joristruong.transformer.videofactorytransformer.AddCountryFactory.scala/udf/13.22.Dataset-Video.filter","Type: org.apache.spark.sql.Dataset[com.github.joristruong.entity.Video]
Call: filter

video => !video.removed
"
"udf/spark_repos_2/3_martinmatak_monero-linkability/..src.main.scala.Main.scala/udf/26.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row(0).asInstanceOf[Long], row(1).asInstanceOf[Long])
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.sparksql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.sparksql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.sparksql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.sparksql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.sparksql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.sparksql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.sparksql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.sparksql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.test.WindowFunctionTest.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.test.WindowFunctionTest.scala/udf/37.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..module1.src.main.scala.test.WindowFunctionTest.scala/udf/42.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.examples-scala.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_2/3_mengzhongtian_spark2.2-maven/..SparkRepoExample.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_2/3_yintaoxue_SenseML/..feature.src.main.scala.org.senseml.feature.features.TimeSeriesFeature.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(windFieldName) =!= ""ts_-1_-1_-1""
"
"udf/spark_repos_2/3_yintaoxue_SenseML/..feature.src.test.scala.test.feature.App.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val f = row.getString(0).split("","")
        (f(0).toInt, f(1).toInt, f(2).toInt, f(3).toInt, f(4).toDouble, f(5).toInt, f(6))
      }
"
"udf/spark_repos_2/3_zqhxuyuan_spark-connectors/..core.src.test.scala.com.zqh.spark.connectors.test.TestJsonSchema.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""event"")
"
"udf/spark_repos_2/3_zqhxuyuan_spark-connectors/..core.src.test.scala.com.zqh.spark.connectors.test.TestJsonSchema.scala/udf/75.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](0)
"
"udf/spark_repos_2/3_zqhxuyuan_spark-connectors/..ml.src.main.scala.org.apache.spark.sql.mllib.QueryBasedStreamingNaiveBayes.scala/udf/17.22.Dataset-LabeledTokenCounts.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.mllib.LabeledTokenCounts]
Call: filter

r => tokens.contains(r.value)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/11.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.exp)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/15.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeTwoParamOp(math.pow)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/19.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.sqrt)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/23.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.sin)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/27.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.cos)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/31.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.tan)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/35.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.log)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/39.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeTwoParamOp(math.min)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/43.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeTwoParamOp(math.max)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/47.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.floor)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/51.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.ceil)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/55.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.signum)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..commons.src.main.scala.io.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/7.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.abs)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..deeplang.src.main.scala.io.deepsense.deeplang.doperables.MissingValuesHandler.scala/udf/105.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

CommonQueries.isMissingInColumnPredicate(df, columnName, declaredAsMissingValues)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..deeplang.src.main.scala.io.deepsense.deeplang.doperables.MissingValuesHandler.scala/udf/151.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!CommonQueries.isMissingInColumnPredicate(sparkDataFrame, column, declaredAsMissing)
"
"udf/spark_repos_2/41_deepsense-ai_seahorse-workflow-executor/..deeplang.src.main.scala.io.deepsense.deeplang.doperables.MissingValuesHandler.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!CommonQueries.isMissingInRowPredicate(df, columns, declaredAsMissingValues)
"
"udf/spark_repos_2/46_curtishoward_sparkudfexamples/..scala-udaf-from-python.src.main.scala.com.cloudera.fce.curtis.sparkudfexamples.scalaudaffrompython.ScalaUDAFFromPythonExample.scala/udf/33.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new SumProductAggregateFunction
"
"udf/spark_repos_2/46_curtishoward_sparkudfexamples/..scala-udaf.src.main.scala.com.cloudera.fce.curtis.sparkudfexamples.scalaudaf.ScalaUDAFExample.scala/udf/35.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new SumProductAggregateFunction
"
"udf/spark_repos_2/46_curtishoward_sparkudfexamples/..scala-udf.src.main.scala.com.cloudera.fce.curtis.sparkudfexamples.scalaudf.ScalaUDFExample.scala/udf/11.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(degreesCelcius: Double) => degreesCelcius * 9.0d / 5.0d + 32.0d
"
"udf/spark_repos_2/4_cj1128_udemy-spark-scala/..src.main.scala.me.cjting.spark.DataFrames.scala/udf/24.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[me.cjting.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_2/4_geerdink_fast-data/..lambda.src.main.scala.etl.BatchEtl.scala/udf/19.24.Dataset-Customer.filter","Type: org.apache.spark.sql.Dataset[etl.domain.Customer]
Call: filter

_.premium
"
"udf/spark_repos_2/4_geerdink_fast-data/..lambda.src.main.scala.etl.BatchEtl.scala/udf/21.22.Dataset-Customer.filter","Type: org.apache.spark.sql.Dataset[etl.domain.Customer]
Call: filter

_.age > 18
"
"udf/spark_repos_2/4_geerdink_fast-data/..lambda.src.main.scala.etl.BatchEtl.scala/udf/26.21.Dataset-Order).map","Type: org.apache.spark.sql.Dataset[(etl.domain.Customer, etl.domain.Order)]
Call: map

r => r._2.amount * (_._2.product.price)
"
"udf/spark_repos_2/4_geerdink_fast-data/..lambda.src.main.scala.etl.BatchEtl.scala/udf/32.19.Dataset-Order).map","Type: org.apache.spark.sql.Dataset[(etl.domain.Customer, etl.domain.Order)]
Call: map

r => s""${r._1.name} has ordered ${r._2.amount} units of ${r._2.product.name}s, for a total price of $total""
"
"udf/spark_repos_2/4_poteman_spark-tutorial/..src.main.scala.part2.AggregateFunctions.scala/udf/35.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_2/4_poteman_spark-tutorial/..src.main.scala.part2.SqlDataFrameAndDataset.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(1)
"
"udf/spark_repos_2/4_poteman_spark-tutorial/..src.main.scala.part2.SqlDataFrameAndDataset.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_2/4_poteman_spark-tutorial/..src.main.scala.part2.SqlDataFrameAndDataset.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/4_poteman_spark-tutorial/..src.main.scala.part2.SqlDataFrameAndDataset.scala/udf/40.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/4_poteman_spark-tutorial/..src.main.scala.part2.SqlDataFrameAndDataset.scala/udf/56.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_2/4_RamkSwamy_sparkmlpipeline/..src.main.scala.com.ram.spark.ml.Utils.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""trim($columnName)"" + "" <> '"" + pattern + ""'""
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.functions.LabelKeyFunction.scala/udf/22.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

column === lit(l)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.io.impl.csv.CsvMetaDataSource.scala/udf/20.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(TYPE_FIELD) === lit(elementType)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.io.impl.csv.CsvMetaDataSource.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

rowToElementMetaData
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.io.impl.csv.indexed.IndexedCsvDataSource.scala/udf/50.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

FilterExpressions.hasLabel(metaData.label)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.io.impl.csv.indexed.IndexedCsvDataSource.scala/udf/52.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

FilterExpressions.hasLabel(metaData.label)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.io.impl.metadata.MetaData.scala/udf/16.22.Dataset-ElementMetaData.filter","Type: org.apache.spark.sql.Dataset[org.gradoop.spark.io.impl.metadata.ElementMetaData]
Call: filter

FilterExpressions.hasLabel(label)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.io.impl.metadata.MetaData.scala/udf/22.22.Dataset-ElementMetaData.filter","Type: org.apache.spark.sql.Dataset[org.gradoop.spark.io.impl.metadata.ElementMetaData]
Call: filter

FilterExpressions.hasLabel(label)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.io.impl.metadata.MetaData.scala/udf/28.22.Dataset-ElementMetaData.filter","Type: org.apache.spark.sql.Dataset[org.gradoop.spark.io.impl.metadata.ElementMetaData]
Call: filter

FilterExpressions.hasLabel(label)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.api.layouts.gve.GveLayout.scala/udf/13.20.Dataset-L#V.filter","Type: org.apache.spark.sql.Dataset[L#V]
Call: filter

FilterExpressions.hasLabel(label)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.api.layouts.gve.GveLayout.scala/udf/17.20.Dataset-L#E.filter","Type: org.apache.spark.sql.Dataset[L#E]
Call: filter

FilterExpressions.hasLabel(label)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.api.layouts.gve.GveLayout.scala/udf/9.20.Dataset-L#G.filter","Type: org.apache.spark.sql.Dataset[L#G]
Call: filter

FilterExpressions.hasLabel(label)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.changelayout.GveToTfl.scala/udf/38.24.Dataset-A.filter","Type: org.apache.spark.sql.Dataset[A]
Call: filter

FilterExpressions.hasLabel(l)
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.changelayout.GveToTfl.scala/udf/51.19.Dataset-ElementMetaData.map","Type: org.apache.spark.sql.Dataset[org.gradoop.spark.io.impl.metadata.ElementMetaData]
Call: map

_.label
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.changelayout.GveToTfl.scala/udf/55.19.Dataset-ElementMetaData.map","Type: org.apache.spark.sql.Dataset[org.gradoop.spark.io.impl.metadata.ElementMetaData]
Call: map

_.label
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.changelayout.GveToTfl.scala/udf/59.19.Dataset-ElementMetaData.map","Type: org.apache.spark.sql.Dataset[org.gradoop.spark.io.impl.metadata.ElementMetaData]
Call: map

_.label
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.subgraph.gve.GveSubgraph.scala/udf/17.26.Dataset-L#V.filter","Type: org.apache.spark.sql.Dataset[L#V]
Call: filter

vertexFilterExpression
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.subgraph.gve.GveSubgraph.scala/udf/21.26.Dataset-L#E.filter","Type: org.apache.spark.sql.Dataset[L#E]
Call: filter

edgeFilterExpression
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.subgraph.gve.GveSubgraph.scala/udf/27.26.Dataset-L#V.filter","Type: org.apache.spark.sql.Dataset[L#V]
Call: filter

vertexFilterExpression
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.subgraph.gve.GveSubgraph.scala/udf/33.26.Dataset-L#E.filter","Type: org.apache.spark.sql.Dataset[L#E]
Call: filter

edgeFilterExpression
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.model.impl.operators.tostring.gve.CanonicalAdjacencyMatrixBuilder.scala/udf/21.19.Dataset-L#G.map","Type: org.apache.spark.sql.Dataset[L#G]
Call: map

graphHeadToString
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.transformation.TransformationFunctions.scala/udf/24.21.Dataset-A.map","Type: org.apache.spark.sql.Dataset[A]
Call: map

{ (e: A) => {
          e.graphIds = e.graphIds ++ graphIds
          e
        } }
"
"udf/spark_repos_2/4_timo95_gradoop-spark/..gradoop-spark.src.main.scala.org.gradoop.spark.util.TflFunctions.scala/udf/93.26.Dataset-EL.filter","Type: org.apache.spark.sql.Dataset[EL]
Call: filter

not(FilterExpressions.hasLabel(e._1))
"
"udf/spark_repos_2/4_yennanliu_spark-etl-pipeline/..src.main.scala.sparkhelloworld.SparkProcessGameRDD.scala/udf/87.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""usd_cost"".isNotNull
"
"udf/spark_repos_2/4_yennanliu_spark-etl-pipeline/..src.main.scala.StructuredStreaming.StructuredStreamingOperations.scala/udf/16.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2.split("":"")
"
"udf/spark_repos_2/4_yennanliu_spark-etl-pipeline/..src.main.scala.StructuredStreaming.WindowOperationV2.scala/udf/25.23.Dataset-(BigInt, String, String, String, String, BigInt, String).map","Type: org.apache.spark.sql.Dataset[(BigInt, String, String, String, String, BigInt, String)]
Call: map

r => TweetData(r._1, r._2, r._3, r._4, r._5, r._6, r._7)
"
"udf/spark_repos_2/4_yennanliu_spark-etl-pipeline/..src.main.scala.StructuredStreaming.WindowOperationV2.scala/udf/27.24.Dataset-TweetData.filter","Type: org.apache.spark.sql.Dataset[TweetData]
Call: filter

_.replyToScreenName != null
"
"udf/spark_repos_2/4_yennanliu_spark-etl-pipeline/..src.main.scala.StructuredStreaming.WindowOperationV2.scala/udf/29.19.Dataset-TweetData.map","Type: org.apache.spark.sql.Dataset[TweetData]
Call: map

t => (t.replyToScreenName, new Timestamp(t.createdAt.toLong), t.id, if (t.firstHashtag == null) 0 else t.firstHashtag.length)
"
"udf/spark_repos_2/5_AbsaOSS_spark-hats/..src.test.scala.za.co.absa.spark.hats.transformations.samples.SampleErrorUDFs.scala/udf/12.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arr: mutable.WrappedArray[ErrorMessage]) => if (arr != null) {
      locally {
        val _t_m_p_3 = arr.distinct
        _t_m_p_3.filter { (a: AnyRef) => a != null }
      }
    } else {
      Seq[ErrorMessage]()
    }
"
"udf/spark_repos_2/5_AbsaOSS_spark-hats/..src.test.scala.za.co.absa.spark.hats.transformations.samples.SampleErrorUDFs.scala/udf/6.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (errCol: String, rawValue: String) => ErrorMessage.confCastErr(errCol, rawValue)
    }
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.AvroSource.scala/udf/102.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.AvroSource.scala/udf/106.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/103.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/75.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.DataType.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.HBaseSource.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.HBaseSource.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_2/5_agile-lab-dev_wasp/..plugin-hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.example.datasources.HBaseSource.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_2/5_hortonworks_fieldeng-scythe/..src.main.scala.com.hortonworks.scythe.cronus.Helper.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df.col(colSigName) === s1
"
"udf/spark_repos_2/5_hortonworks_fieldeng-scythe/..src.main.scala.com.hortonworks.scythe.cronus.Helper.scala/udf/34.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$colSigName == '$sigName'""
"
"udf/spark_repos_2/5_jongwook_collective-als/..src.test.scala.com.github.jongwook.cmf.MovieLensALS.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => ((row.getAs[Int](""userId""), row.getAs[Int](""movieId"")), row.getAs[Float](""prediction"").toDouble)
      }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.common.mergers.OntologyUtil.scala/udf/13.19.Dataset-OntologyTermBasic).map","Type: org.apache.spark.sql.Dataset[(T, io.kf.etl.models.ontology.OntologyTerm, io.kf.etl.models.ontology.OntologyTermBasic)]
Call: map

{
        case (observable, ontologyTerm, ontologyTermBasic) =>
          (observable, ontologyTerm, if (ontologyTermBasic != null) {
            OntologicalTermWithParents_ES(name = ontologyTermBasic.toString, parents = if (ontologyTermBasic != null) ontologyTermBasic.parents else Nil, age_at_event_days = observable.age_at_event_days.toSet)
          } else null)
      }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.common.mergers.OntologyUtil.scala/udf/26.19.Dataset-OntologyTermBasic).map","Type: org.apache.spark.sql.Dataset[(T, io.kf.etl.models.ontology.OntologyTerm, io.kf.etl.models.ontology.OntologyTermBasic)]
Call: map

{
        case (observable, ontologyTerm, ontologyTermBasic) =>
          (observable, ontologyTerm, if (ontologyTermBasic != null) {
            DiagnosisTermWithParents_ES(name = ontologyTermBasic.toString, parents = if (ontologyTermBasic != null) ontologyTermBasic.parents else Nil)
          } else null)
      }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.common.mergers.OntologyUtil.scala/udf/42.19.Dataset-OntologyTerm).map","Type: org.apache.spark.sql.Dataset[(T, io.kf.etl.models.ontology.OntologyTerm)]
Call: map

{
        case (eObservable, ontologyTerm) if ontologyTerm != null =>
          (eObservable, ontologyTerm, ontologyTerm.ancestors)
        case (eObservable, _) =>
          (eObservable, null, Nil)
      }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.featurecentric.transform.FeatureCentricTransformer.scala/udf/57.21.Dataset-ESequencingExperimentGenomicFile).map","Type: org.apache.spark.sql.Dataset[(io.kf.etl.models.dataservice.ESequencingExperiment, io.kf.etl.models.dataservice.ESequencingExperimentGenomicFile)]
Call: map

{
          case (sequencingExperiment, sequencingExperimentGenomicFile) =>
            SequencingExperimentES_GenomicFileId(sequencing_experiment = EntityConverter.ESequencingExperimentToSequencingExperimentES(sequencingExperiment), genomic_file_id = if (sequencingExperimentGenomicFile != null) sequencingExperimentGenomicFile.genomic_file else None)
        }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.featurecentric.transform.FeatureCentricTransformer.scala/udf/77.19.Dataset-SequencingExperimentsES_GenomicFileId).map","Type: org.apache.spark.sql.Dataset[(io.kf.etl.models.dataservice.EGenomicFile, io.kf.etl.models.internal.SequencingExperimentsES_GenomicFileId)]
Call: map

tuple => Option(tuple._2) match {
        case Some(_) =>
          EntityConverter.EGenomicFileToGenomicFileES(tuple._1, tuple._2.sequencing_experiments)
        case None =>
          EntityConverter.EGenomicFileToGenomicFileES(tuple._1, Seq.empty)
      }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.participantcommon.transform.step.MergeBiospecimenPerParticipant.scala/udf/35.21.Dataset-OntologyTermBasic).map","Type: org.apache.spark.sql.Dataset[(io.kf.etl.models.dataservice.EBiospecimen, io.kf.etl.models.ontology.OntologyTermBasic)]
Call: map

{
          case (biospeciem, term) if term != null =>
            biospeciem.copy(ncit_id_anatomical_site = formatTerm(term))
          case (biospecimen, _) =>
            biospecimen
        }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.participantcommon.transform.step.MergeBiospecimenPerParticipant.scala/udf/42.19.Dataset-OntologyTermBasic).map","Type: org.apache.spark.sql.Dataset[(io.kf.etl.models.dataservice.EBiospecimen, io.kf.etl.models.ontology.OntologyTermBasic)]
Call: map

{
        case (biospeciem, term) if term != null =>
          biospeciem.copy(ncit_id_tissue_type = formatTerm(term))
        case (biospeciem, _) =>
          biospeciem
      }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.participantcommon.transform.step.MergeBiospecimenPerParticipant.scala/udf/54.19.Dataset-EDiagnosis).map","Type: org.apache.spark.sql.Dataset[((io.kf.etl.models.dataservice.EBiospecimen, io.kf.etl.models.dataservice.EBiospecimenDiagnosis), io.kf.etl.models.dataservice.EDiagnosis)]
Call: map

{
        case ((b, _), d) =>
          (b, d)
      }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.participantcommon.transform.step.MergeBiospecimenPerParticipant.scala/udf/69.19.Dataset-DuoCode.map","Type: org.apache.spark.sql.Dataset[io.kf.etl.models.duocode.DuoCode]
Call: map

d => (d.id, d.toString)
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.participantcommon.transform.step.MergeDiagnosis.scala/udf/14.22.Dataset-EDiagnosis.filter","Type: org.apache.spark.sql.Dataset[io.kf.etl.models.dataservice.EDiagnosis]
Call: filter

_.participant_id.isDefined
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.participantcommon.transform.step.MergePhenotype.scala/udf/12.22.Dataset-EPhenotype.filter","Type: org.apache.spark.sql.Dataset[io.kf.etl.models.dataservice.EPhenotype]
Call: filter

{
        p => p.observed match {
          case Some(o) =>
            Seq(""positive"", ""negative"").contains(o.trim.toLowerCase)
          case _ =>
            false
        }
      }
"
"udf/spark_repos_2/5_kids-first_kf-portal-etl/..src.main.scala.io.kf.etl.processors.participantcommon.transform.step.MergeStudy.scala/udf/10.19.Dataset-EStudy).map","Type: org.apache.spark.sql.Dataset[(io.kf.etl.models.dataservice.EParticipant, io.kf.etl.models.dataservice.EStudy)]
Call: map

tuple => {
        val study = EntityConverter.EStudyToStudyES(tuple._2)
        EntityConverter.EParticipantToParticipantES(tuple._1).copy(study = Some(study))
      }
"
"udf/spark_repos_2/5_ViralTexts_vt-passim/..src.main.scala.APSMeta.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        ('startdate.isNull || 'startdate <= 'date) && ('enddate.isNull || 'enddate >= 'date)
      }
"
"udf/spark_repos_2/5_ViralTexts_vt-passim/..src.main.scala.APS.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        ('startdate.isNull || 'startdate <= 'date) && ('enddate.isNull || 'enddate >= 'date)
      }
"
"udf/spark_repos_2/5_ViralTexts_vt-passim/..src.main.scala.LCMerge.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'defcover =!= """"
"
"udf/spark_repos_2/5_ViralTexts_vt-passim/..src.main.scala.MetsAltoDDD.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'issue.isNotNull && 'series.isNotNull
"
"udf/spark_repos_2/5_ViralTexts_vt-passim/..src.main.scala.MetsMetaDDD.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'series.isNotNull
"
"udf/spark_repos_2/6_BlancRay_PUAdapter/..pu4spark.src.main.scala.com.zzy.GradualReductionPULearner.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(prevLabel) === GradualReductionPULearner.undefLabel && curDF(curLabel) === GradualReductionPULearner.relNegLabel
"
"udf/spark_repos_2/6_BlancRay_PUAdapter/..pu4spark.src.main.scala.com.zzy.GradualReductionPULearner.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) === GradualReductionPULearner.posLabel
"
"udf/spark_repos_2/6_BlancRay_PUAdapter/..pu4spark.src.main.scala.com.zzy.GradualReductionPULearner.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) === GradualReductionPULearner.relNegLabel
"
"udf/spark_repos_2/6_BlancRay_PUAdapter/..pu4spark.src.main.scala.com.zzy.GradualReductionPULearner.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) =!= GradualReductionPULearner.undefLabel
"
"udf/spark_repos_2/6_BlancRay_PUAdapter/..pu4spark.src.main.scala.com.zzy.GradualReductionPULearner.scala/udf/52.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) === GradualReductionPULearner.relNegLabel
"
"udf/spark_repos_2/6_BlancRay_PUAdapter/..pu4spark.src.main.scala.com.zzy.GradualReductionPULearner.scala/udf/56.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) === GradualReductionPULearner.relNegLabel
"
"udf/spark_repos_2/6_BlancRay_PUAdapter/..pu4spark.src.main.scala.com.zzy.TraditionalPULearner.scala/udf/21.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(prevLabel) === TraditionalPULearner.undefLabel && curDF(curLabel) === TraditionalPULearner.relNegLabel
"
"udf/spark_repos_2/6_BlancRay_PUAdapter/..pu4spark.src.main.scala.com.zzy.TraditionalPULearner.scala/udf/29.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

curDF(curLabel) !== TraditionalPULearner.undefLabel
"
"udf/spark_repos_2/6_PacktPublishing_Advanced-Machine-Learning-with-Spark-2.x/..src.main.scala.com.example.CreatingDatasets.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_2/6_PacktPublishing_Advanced-Machine-Learning-with-Spark-2.x/..src.main.scala.com.example.ProgrammingGuideSQL.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_2/6_PacktPublishing_Advanced-Machine-Learning-with-Spark-2.x/..src.main.scala.com.tomekl007.anomalydetection.RunKMeans.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
        val cluster = row.getAs[Int](""cluster"")
        val vec = row.getAs[Vector](""scaledFeatureVector"")
        Vectors.sqdist(centroids(cluster), vec) >= farthestDistanceBetweenTwoNormalClusters
      }
"
"udf/spark_repos_2/6_PacktPublishing_Advanced-Machine-Learning-with-Spark-2.x/..src.main.scala.com.tomekl007.anomalydetection.RunKMeans.scala/udf/62.19.Dataset-Vector).map","Type: org.apache.spark.sql.Dataset[(Int, org.apache.spark.ml.linalg.Vector)]
Call: map

{
        case (cluster, vec) =>
          Vectors.sqdist(centroids(cluster), vec)
      }
"
"udf/spark_repos_2/6_PacktPublishing_Advanced-Machine-Learning-with-Spark-2.x/..src.test.scala.com.tomekl007.SparkApisTests.scala/udf/28.22.Dataset-UserData.filter","Type: org.apache.spark.sql.Dataset[com.tomekl007.UserData]
Call: filter

_.userId == ""a""
"
"udf/spark_repos_2/6_SANSA-Stack_SANSA-DataLake/..sansa-datalake-spark.src.main.scala.net.sansa_stack.datalake.spark.SparkExecutor.scala/udf/135.32.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

whereString
"
"udf/spark_repos_2/6_SANSA-Stack_SANSA-DataLake/..sansa-datalake-spark.src.main.scala.net.sansa_stack.datalake.spark.SparkExecutor.scala/udf/145.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

finalDF(column).like(operand_value._2.replace(""\"""", """"))
"
"udf/spark_repos_2/6_SANSA-Stack_SANSA-DataLake/..sansa-datalake-spark.src.main.scala.net.sansa_stack.datalake.spark.SparkExecutor.scala/udf/179.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!ndf(column).equalTo(skipValue)
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/1029.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getSeq[Double](0).toArray
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/1128.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val vals = row.getSeq[Double](0)
        vals(1) - vals(0)
      }
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/1137.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getSeq[Double](0).size
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/1321.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/315.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => IndexedDistance(x.getLong(0), x.getLong(2), dist(x.getSeq[A](1), x.getSeq[A](3)))
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/371.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getSeq[Double](1).toArray, x.getLong(0))
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/473.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getLong(0)
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/507.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => {
        val d = dist(x.getSeq[A](1), x.getSeq[A](3))
        IndexedDistance(x.getLong(0), x.getLong(2), -0.5d * d * d)
      }
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/530.22.Dataset-IndexedDistance.map","Type: org.apache.spark.sql.Dataset[com.github.unsupervise.spark.tss.core.IndexedDistance]
Call: map

x => x.copy(distance = x.distance + offset)
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/688.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => scalarScalarTuples(x.getInt(0), x.getInt(1))
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/711.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getSeq[Double](1).toArray, x.getLong(0))
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/743.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getLong(0)
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/881.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_2/6_unsupervise_spark-tss/..src.main.scala.com.github.unsupervise.spark.tss.core.TSS.scala/udf/932.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.feature.Imputer.scala/udf/49.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

ic.isNotNull && ic =!= $(missingValue) && !ic.isNaN
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.feature.LSH.scala/udf/100.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.feature.LSH.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.feature.LSH.scala/udf/59.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.util.MLUtils.scala/udf/105.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.util.MLUtils.scala/udf/116.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.util.MLUtils.scala/udf/139.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.util.MLUtils.scala/udf/150.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.util.MLUtils.scala/udf/266.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.main.scala.com.tencent.angel.sona.ml.util.MLUtils.scala/udf/277.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_2/71_Angel-ML_sona/..angelml.src.test.scala.com.tencent.angel.sona.ml.feature.LSHTest.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distUDF(col(s""a.$inputCol""), col(s""b.$inputCol"")) < threshold
"
"udf/spark_repos_2/7_hy-2013_FEpipeline/..src.main.scala.org.fepipeline.feature.evaluation.StatisticsEvaluator.scala/udf/70.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

s""$inputCol != -1""
"
"udf/spark_repos_2/7_hy-2013_FEpipeline/..src.main.scala.org.fepipeline.feature.evaluation.StatisticsEvaluator.scala/udf/78.24.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

s""$inputCol != -1""
"
"udf/spark_repos_2/7_hy-2013_FEpipeline/..src.main.scala.org.fepipeline.feature.subsample.dataframe.DataFrameSubsampler.scala/udf/23.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

subsampleUDF(struct(locally {
        val _t_m_p_2 = inputCols
        _t_m_p_2.map(col)
      }: _*))
"
"udf/spark_repos_2/7_JiyangM_taobao-behavior/..userbehavior.src.main.scala.com.userbehavior.analysis.example.rddToDataFrame.CreateDataFrame.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""categroyId:"" + t(0) + "",triggerNum:"" + t(1) + "",dayId:"" + t(2)
"
"udf/spark_repos_2/7_TomLous_coursera-scala-capstone/..src.main.scala.observatory.Extraction.scala/udf/18.19.Dataset-Joined.map","Type: org.apache.spark.sql.Dataset[observatory.Joined]
Call: map

j => (StationDate(j.day, j.month, j.year), Location(j.latitude, j.longitude), j.temperature)
"
"udf/spark_repos_2/7_windyzj_sparkmall0808/..sparkmall-offline.src.main.scala.com.atguigu.sparkmall0808.offline.app.AreaTop3ClickCountApp.scala/udf/7.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityClickCountUDAF
"
"udf/spark_repos_2/80_knockdata_spark-highcharts/..src.main.scala.com.knockdata.spark.highcharts.convert.scala/udf/155.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

column === aSeriesValue
"
"udf/spark_repos_2/80_knockdata_spark-highcharts/..src.main.scala.com.knockdata.spark.highcharts.convert.scala/udf/68.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(colName) === row.getAs[Any](colName)
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.cluster.MainTestLinkage.scala/udf/45.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toSeq.asInstanceOf[Seq[Double]]
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.cluster.MainTestLinkage.scala/udf/50.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toSeq.asInstanceOf[Seq[Double]]
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.cluster.MainTestLinkage.scala/udf/55.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toSeq.asInstanceOf[Seq[Double]]
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.cluster.MainTestLinkage.scala/udf/66.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getLong(1), row.getSeq[Double](0).toList)
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.cluster.MainTestLinkage.scala/udf/85.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.getLong(1) < r.getLong(3)
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.cluster.MainTestLinkage.scala/udf/87.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ r => 
        val dist = distanceMethod match {
          case ""Euclidean"" =>
            distEuclidean(r.getSeq[Double](0), r.getSeq[Double](2))
        }
        (r.getLong(1), r.getLong(3), dist)
      }
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.linkage.LinkageModel.scala/udf/179.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getInt(1)
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.linkage.Linkage.scala/udf/332.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""(idW1Points1 == idW1) or (idW1Points1 == idW2) "" + ""or (idW2Points1 == idW1) or (idW2Points1 == idW2)""
"
"udf/spark_repos_2/8_josemarialuna_ClusterIndices/..src.main.scala.es.us.linkage.Linkage.scala/udf/339.28.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => (newIndex.toInt, filterDF(r.getInt(0), r.getInt(1), point1, point2), math.min(r.getFloat(2), r.getFloat(5)))
"
"udf/spark_repos_2/8_zhenchao125_spark0830/..spark-sql-project.src.main.scala.com.atguigu.sql.project.SqlApp.scala/udf/18.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RemarkUDAF
"
"udf/spark_repos_2/8_zhenchao125_spark0830/..spar-ksql.src.main.scala.com.atguigu.sql.day02.udf.MySumDemo1.scala/udf/12.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MySum
"
"udf/spark_repos_2/8_zhenchao125_spark0830/..spar-ksql.src.main.scala.com.atguigu.sql.day02.udf.MySumDemo1.scala/udf/16.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAvg
"
"udf/spark_repos_2/8_zhenchao125_spark0830/..spar-ksql.src.main.scala.com.atguigu.sql.day02.udf.MySumDemo2.scala/udf/10.22.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[com.atguigu.sql.day02.udf.User]
Call: filter

_.age != null
"
"udf/spark_repos_3/100_paypal_yurita/..src.main.scala.com.paypal.risk.platform.veda.analytics.anomaly.AnomalyExampleApp.scala/udf/32.19.Dataset-WindowRange).map","Type: org.apache.spark.sql.Dataset[(com.paypal.risk.platform.veda.analytics.anomaly.ColumnKey, com.paypal.risk.platform.veda.analytics.anomaly.Report, com.paypal.risk.platform.veda.analytics.anomaly.WindowRange)]
Call: map

_.toString
"
"udf/spark_repos_3/100_paypal_yurita/..src.main.scala.com.paypal.risk.platform.veda.analytics.clustering.ClusteringExampleApp.scala/udf/27.19.Dataset-WindowRange).map","Type: org.apache.spark.sql.Dataset[(com.paypal.risk.platform.veda.analytics.anomaly.ColumnKey, com.paypal.risk.platform.veda.analytics.anomaly.Report, com.paypal.risk.platform.veda.analytics.anomaly.WindowRange)]
Call: map

_.toString
"
"udf/spark_repos_3/107_aliyun_aliyun-emapreduce-demo/..src.main.scala.com.aliyun.emr.example.spark.sql.streaming.ContinuousStructuredLoghubSample.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

e => (e, e.length)
"
"udf/spark_repos_3/108_jleetutorial_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(""country"").===(""Afghanistan"")
"
"udf/spark_repos_3/108_jleetutorial_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(AGE_MIDPOINT) < 20
"
"udf/spark_repos_3/108_jleetutorial_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/22.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.country == ""Afghanistan""
"
"udf/spark_repos_3/108_jleetutorial_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/29.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.age_midpoint.isDefined && response.age_midpoint.get < 20.0d
"
"udf/spark_repos_3/108_jleetutorial_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/36.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.salary_midpoint.isDefined
"
"udf/spark_repos_3/108_jleetutorial_scala-spark-tutorial/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/41.19.Dataset-Response.map","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: map

response => locally {
        val _t_m_p_5 = response.salary_midpoint
        _t_m_p_5.map(point => Math.round(point / 20000) * 20000)
      }.orElse(None)
"
"udf/spark_repos_3/110_aliyun_aliyun-emapreduce-sdk/..examples.src.main.scala.com.aliyun.emr.examples.sql.streaming.ContinuousStructuredDatahubSample.scala/udf/19.19.Dataset-Value.map","Type: org.apache.spark.sql.Dataset[com.aliyun.emr.examples.sql.streaming.ContinuousStructuredDatahubSample.Value]
Call: map

r => (r.value0, r.value0.length, r.value1, r.value1.length)
"
"udf/spark_repos_3/110_aliyun_aliyun-emapreduce-sdk/..examples.src.main.scala.com.aliyun.emr.examples.sql.streaming.ContinuousStructuredLoghubSample.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

e => (e, e.length)
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.clean.FlightClean2.scala/udf/25.19.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[clean.FlightClean2.Flight]
Call: map

createFlightwId
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/103.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/110.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/112.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/117.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/119.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/124.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/126.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/131.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/133.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/91.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/95.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_3/11_caroljmcdonald_spark-ml-flightdelay/..src.main.scala.ml.Flight.scala/udf/99.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_3/11_ldbc_gcore-spark/..src.main.scala.spark.sql.operators.PathSearch.scala/udf/143.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""cost"") > 0
"
"udf/spark_repos_3/11_ldbc_gcore-spark/..src.main.scala.spark.sql.SqlPlanner.scala/udf/544.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

entityDf(labelColSelect) <=> lbl
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/118.25.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerTaskEnd"")
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/120.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkTaskEnd](string)
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/125.20.Dataset-Stage).map","Type: org.apache.spark.sql.Dataset[(com.conversantmedia.sparkprofiler.SparkTaskEnd, com.conversantmedia.sparkprofiler.Stage)]
Call: map

tuple => {
        val applicationId = tuple._2.applicationId
        val applicationName = tuple._2.applicationName
        val taskInfo = tuple._1.Task$u0020Info
        val taskMetrics = tuple._1.Task$u0020Metrics
        val accumulableMemory = locally {
          val _t_m_p_18 = locally {
            val _t_m_p_19 = taskInfo.Accumulables
            _t_m_p_19.filter(_.Name == ""peakExecutionMemory"")
          }
          _t_m_p_18.map(i => i.Value.toLong)
        }
        val peakMemory = if (accumulableMemory.nonEmpty) accumulableMemory.head else 0
        val accumulableInputRows = locally {
          val _t_m_p_20 = locally {
            val _t_m_p_21 = taskInfo.Accumulables
            _t_m_p_21.filter(_.Name == ""number of input rows"")
          }
          _t_m_p_20.map(i => i.Value.toLong)
        }
        val inputRows = if (accumulableInputRows.nonEmpty) accumulableInputRows.head else 0
        val accumulableOutputRows = locally {
          val _t_m_p_22 = locally {
            val _t_m_p_23 = taskInfo.Accumulables
            _t_m_p_23.filter(_.Name == ""number of output rows"")
          }
          _t_m_p_22.map(i => i.Value.toLong)
        }
        val outputRows = if (accumulableOutputRows.nonEmpty) accumulableOutputRows.head else 0
        val jobId = tuple._2.jobId
        val stageId = tuple._2.stageId
        val stageAttemptId = tuple._2.stageAttemptId
        val taskId = taskInfo.Task$u0020ID
        val taskType = tuple._1.`Task Type`
        val attempt = taskInfo.Attempt
        val executorId = taskInfo.Executor$u0020ID
        val host = taskInfo.Host
        val stageName = tuple._2.stageName
        val locality = taskInfo.Locality
        val speculative = taskInfo.Speculative
        val applicationStartTime = tuple._2.applicationStartTime
        val applicationEndTime = tuple._2.applicationEndTime
        val jobStartTime = tuple._2.jobStartTime
        val jobEndTime = tuple._2.jobEndTime
        val stageStartTime = tuple._2.stageStartTime
        val stageEndTime = tuple._2.stageEndTime
        val taskStartTime = taskInfo.Launch$u0020Time
        val taskEndTime = taskInfo.Finish$u0020Time
        val failed = taskInfo.Failed
        val taskDuration = taskEndTime - taskStartTime
        val stageDuration = tuple._2.stageDuration
        val jobDuration = tuple._2.jobDuration
        val applicationDuration = tuple._2.applicationDuration
        val gettingResultTime = taskInfo.Getting$u0020Result$u0020Time
        val gcTime = if (taskMetrics.nonEmpty) taskMetrics.get.JVM$u0020GC$u0020Time else 0
        val resultSerializationTime = if (taskMetrics.nonEmpty) taskMetrics.get.Result$u0020Serialization$u0020Time else 0
        val resultSize = if (taskMetrics.nonEmpty) taskMetrics.get.Result$u0020Size else 0
        val memoryBytesSpilled = if (taskMetrics.nonEmpty) taskMetrics.get.Memory$u0020Bytes$u0020Spilled else 0
        val diskBytesSpilled = if (taskMetrics.nonEmpty) taskMetrics.get.Disk$u0020Bytes$u0020Spilled else 0
        val inputMetrics = if (taskMetrics.nonEmpty) taskMetrics.get.Input$u0020Metrics else None
        val shuffleWriteMetrics = if (taskMetrics.nonEmpty) taskMetrics.get.Shuffle$u0020Write$u0020Metrics else None
        val shuffleReadMetrics = if (taskMetrics.nonEmpty) taskMetrics.get.Shuffle$u0020Read$u0020Metrics else None
        val dataReadMethod = if (inputMetrics.nonEmpty) inputMetrics.get.Data$u0020Read$u0020Method else ""n/a""
        val bytesRead = if (inputMetrics.nonEmpty) inputMetrics.get.Bytes$u0020Read else 0L
        val recordsRead = inputMetrics match {
          case None =>
            0L
          case Some(_) =>
            inputMetrics.get.Records$u0020Read
        }
        val shuffleBytesWritten = shuffleWriteMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleWriteMetrics.get.Shuffle$u0020Bytes$u0020Written
        }
        val shuffleRecordsWritten = shuffleWriteMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleWriteMetrics.get.Shuffle$u0020Records$u0020Written
        }
        val shuffleWriteTime = shuffleWriteMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleWriteMetrics.get.Shuffle$u0020Write$u0020Time
        }
        val remoteBlocksFetched = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Remote$u0020Blocks$u0020Fetched
        }
        val localBlocksFetched = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Local$u0020Blocks$u0020Fetched
        }
        val fetchWaitTime = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Fetch$u0020Wait$u0020Time
        }
        val remoteBytesRead = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Remote$u0020Bytes$u0020Read
        }
        val localBytesRead = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Local$u0020Bytes$u0020Read
        }
        val totalRecordsRead = shuffleReadMetrics match {
          case None =>
            0L
          case Some(_) =>
            shuffleReadMetrics.get.Total$u0020Records$u0020Read
        }
        Task(applicationId, applicationName, jobId, stageId, stageName, stageAttemptId, taskId, attempt, taskType, executorId, host, peakMemory, inputRows, outputRows, locality, speculative, applicationStartTime, applicationEndTime, jobStartTime, jobEndTime, stageStartTime, stageEndTime, taskStartTime, taskEndTime, failed, taskDuration, stageDuration, jobDuration, applicationDuration, gcTime, gettingResultTime, resultSerializationTime, resultSize, dataReadMethod, bytesRead, recordsRead, memoryBytesSpilled, diskBytesSpilled, shuffleBytesWritten, shuffleRecordsWritten, shuffleWriteTime, remoteBlocksFetched, localBlocksFetched, fetchWaitTime, remoteBytesRead, localBytesRead, totalRecordsRead)
      }
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerApplicationStart"")
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/17.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerApplicationEnd"")
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/34.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerExecutorAdded"")
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/36.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkExecutorAdded](string)
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/38.19.Dataset-SparkExecutorAdded.map","Type: org.apache.spark.sql.Dataset[com.conversantmedia.sparkprofiler.SparkExecutorAdded]
Call: map

e => {
        val executorId = e.`Executor ID`
        val host = e.`Executor Info`.Host
        val cores = e.`Executor Info`.`Total Cores`
        val startTime = e.Timestamp
        Executor(sparkApplication.applicationName, sparkApplication.applicationId, executorId, host, cores, startTime)
      }
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/52.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerJobStart"")
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/54.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkJobStart](string)
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/59.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerJobEnd"")
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/61.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkJobEnd](string)
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/65.20.Dataset-SparkJobEnd).map","Type: org.apache.spark.sql.Dataset[(com.conversantmedia.sparkprofiler.SparkJobStart, com.conversantmedia.sparkprofiler.SparkJobEnd)]
Call: map

tuple => {
        val jobId = tuple._1.`Job ID`
        val jobStartTime = tuple._1.Submission$u0020Time
        val jobEndTime = tuple._2.Completion$u0020Time
        val stages = tuple._1.Stage$u0020IDs
        val jobDuration = jobEndTime - jobStartTime
        val result = tuple._2.Job$u0020Result.Result
        Job(sparkApplication.applicationId, sparkApplication.applicationName, jobId, sparkApplication.applicationStartTime, sparkApplication.applicationEndTime, jobStartTime, jobEndTime, jobDuration, sparkApplication.applicationDuration, stages, result)
      }
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/83.27.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""SparkListenerStageCompleted"")
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/85.22.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => parseJSON[SparkStageComplete](string)
"
"udf/spark_repos_3/12_conversant_spark-profiler/..src.main.scala.com.conversantmedia.sparkprofiler.Parser.scala/udf/87.20.Dataset-SparkStageComplete.map","Type: org.apache.spark.sql.Dataset[com.conversantmedia.sparkprofiler.SparkStageComplete]
Call: map

stage => {
        val stageInfo = stage.Stage$u0020Info
        val stageId = stageInfo.Stage$u0020ID
        val job = locally {
          val _t_m_p_14 = jobs
          _t_m_p_14.filter(j => j.stages.contains(stageId))
        }.last
        val jobId = job.jobId
        val attempt = stageInfo.Stage$u0020Attempt$u0020ID
        val stageName = stageInfo.Stage$u0020Name
        val details = stageInfo.Details
        val taskCount = stageInfo.Number$u0020of$u0020Tasks
        val rddCount = stageInfo.RDD$u0020Info.size
        val applicationStartTime = job.applicationStartTime
        val applicationEndTime = job.applicationEndTime
        val jobStartTime = job.jobStartTime
        val jobEndTime = job.jobEndTime
        val stageStartTime = stageInfo.Submission$u0020Time
        val stageEndTime = stageInfo.Completion$u0020Time
        val stageDuration = stageEndTime - stageStartTime
        val jobDuration = job.jobDuration
        val applicationDuration = job.applicationDuration
        Stage(job.applicationId, job.applicationName, jobId, stageId, stageName, attempt, details, taskCount, rddCount, applicationStartTime, applicationEndTime, jobStartTime, jobEndTime, stageStartTime, stageEndTime, stageDuration, jobDuration, applicationDuration)
      }
"
"udf/spark_repos_3/12_LuckyZXL2016_Spark-Example/..src.main.scala.com.zxl.spark2_2.structured.StructuredStreamingKafka.scala/udf/13.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_3/12_pkeropen_BigData-News/..spark_news.src.main.scala.com.vita.spark.StructuredStreamingKafka.scala/udf/15.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_3/12_pkeropen_BigData-News/..structured-streaming-demo.src.main.scala.com.vita.spark.streaming.StructuredStreamingOffset.scala/udf/24.19.Dataset-(String, Long).map","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: map

x => readLogs(x._1, x._2.toString)
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.main.scala.org.apache.spark.sql.execution.command.AnalyzePartitionCommand.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(filter)
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/10.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

length(trim($""value"")) > 0
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/15.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!$""value"".startsWith(options.comment.toString)
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/102.24.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/141.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % 2L === 0L
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/156.27.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

func
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/203.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/218.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/49.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

func
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.DatasetSuite.scala/udf/1346.20.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.DataSourceReadBenchmark.scala/udf/214.24.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.DataSourceReadBenchmark.scala/udf/255.24.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.DataSourceReadBenchmark.scala/udf/296.24.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.DataSourceReadBenchmark.scala/udf/462.24.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.DataSourceReadBenchmark.scala/udf/87.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.FilterPushdownBenchmark.scala/udf/87.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.csv.CSVBenchmarks.scala/udf/25.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => str
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.csv.CSVBenchmarks.scala/udf/32.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (_:Row) => true }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.csv.CSVBenchmarks.scala/udf/54.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (row: Row) => true }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.csv.CSVBenchmarks.scala/udf/64.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (row: Row) => true }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.csv.CSVBenchmarks.scala/udf/70.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (row: Row) => true }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.csv.CSVBenchmarks.scala/udf/90.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (_:Row) => true }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.csv.CSVBenchmarks.scala/udf/96.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (_:Row) => true }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.csv.TestCsvData.scala/udf/7.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ index => 
        val predefinedSample = Set[Long](2, 8, 15, 27, 30, 34, 35, 37, 44, 46, 57, 62, 68, 72)
        if (predefinedSample.contains(index)) {
          index.toString
        } else {
          (index.toDouble + 0.1d).toString
        }
      }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.json.JsonBenchmarks.scala/udf/84.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (_:Row) => true }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.json.JsonBenchmarks.scala/udf/90.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (_:Row) => true }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.datasources.json.TestJsonData.scala/udf/107.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ index => 
        val predefinedSample = Set[Long](2, 8, 15, 27, 30, 34, 35, 37, 44, 46, 57, 62, 68, 72)
        if (predefinedSample.contains(index)) {
          s""""""{""f1"":${index.toString}}""""""
        } else {
          s""""""{""f1"":${(index.toDouble + 0.1d).toString}}""""""
        }
      }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.execution.SQLExecutionSuite.scala/udf/80.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ x => 
        while (!SQLExecutionSuite.canProgress) {
          Thread.sleep(1)
        }
        x
      }
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.SessionStateSuite.scala/udf/45.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: String).length + (_: Int)
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.SessionStateSuite.scala/udf/55.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: Int) + 1
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.core.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/98.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/542.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/546.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/574.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/578.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/892.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/902.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.orc.OrcReadBenchmark.scala/udf/125.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.orc.OrcReadBenchmark.scala/udf/244.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.orc.OrcReadBenchmark.scala/udf/65.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.hive.orc.OrcReadBenchmark.scala/udf/95.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => Random.nextLong
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/130_Qihoo360_XSQL/..sql.xsql.src.main.scala.org.apache.spark.sql.xsql.execution.command.XSQLAnalyzePartitionCommand.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(filter)
"
"udf/spark_repos_3/14_Acxiom_metalus/..metalus-core.src.test.scala.com.acxiom.pipeline.applications.ApplicationTests.scala/udf/487.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

func
"
"udf/spark_repos_3/14_bigdataguide_AuraSparkTraining/..src.main.scala.org.training.spark.ml.ALSExample.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/14_bigdataguide_AuraSparkTraining/..src.main.scala.org.training.spark.sql.MovieUserAnalyzerWithDataFrame.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""movieid = $MOVIE_ID""
"
"udf/spark_repos_3/14_bigdataguide_AuraSparkTraining/..src.main.scala.org.training.spark.sql.SparkSQLSimpleExample.scala/udf/87.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          u => (u.getAs[String](""userID"").toLong, u.getAs[String](""age"").toInt + 1)
        }
"
"udf/spark_repos_3/14_srowen_cdsw-simple-serving/..acme-dataeng.src.main.scala.com.cloudera.datascience.cdsw.acme.ACMEData.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{
        line => if (line.startsWith(""\""date\"""")) {
          line
        } else {
          line.substring(line.indexOf(',') + 1)
        }
      }
"
"udf/spark_repos_3/15_hortonworks-spark_spark-hive-streaming-sink/..example.src.main.scala.com.hortonworks.spark.hive.example.HiveStreamingExample.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ s => 
        val records = s.split("","")
        assert(records.length >= 4)
        (records(0).toInt, records(1), records(2), records(3))
      }
"
"udf/spark_repos_3/15_mozilla_telemetry-streaming/..src.main.scala.com.mozilla.telemetry.streaming.EventsToAmplitude.scala/udf/142.22.Dataset-KeyedAmplitudePayload.map","Type: org.apache.spark.sql.Dataset[com.mozilla.telemetry.streaming.EventsToAmplitude.KeyedAmplitudePayload]
Call: map

_.events
"
"udf/spark_repos_3/15_mozilla_telemetry-streaming/..src.main.scala.com.mozilla.telemetry.streaming.ExperimentEnrollmentsToTestTube.scala/udf/70.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
        val m = locally {
          val _t_m_p_6 = r.getValuesMap[Any](r.schema.fieldNames)
          _t_m_p_6.map({
            case (""object"", o) =>
              ""type"" -> o
            case (""window_start"", t: Timestamp) =>
              ""window_start"" -> t.getTime
            case (""window_end"", t: Timestamp) =>
              ""window_end"" -> t.getTime
            case (k: String, v) =>
              k -> v
          })
        }
        implicit val formats = org.json4s.DefaultFormats
        Serialization.write(m)
      }
"
"udf/spark_repos_3/15_uhh-lt_josimtext/..src.main.scala.de.uhh.lt.jst.dt.CoNLL2DepTermContext.scala/udf/21.19.Dataset-TermContext.map","Type: org.apache.spark.sql.Dataset[de.uhh.lt.jst.dt.entities.TermContext]
Call: map

tc => s""${tc.term}\t${tc.context}""
"
"udf/spark_repos_3/15_uhh-lt_josimtext/..src.main.scala.de.uhh.lt.jst.dt.Text2TrigramTermContext.scala/udf/19.19.Dataset-TermContext.map","Type: org.apache.spark.sql.Dataset[de.uhh.lt.jst.dt.entities.TermContext]
Call: map

tc => s""${tc.term}\t${tc.context}""
"
"udf/spark_repos_3/17_rphes_SBD-tudelft/..example.src.main.scala.example.scala/udf/16.22.Dataset-SensorData.filter","Type: org.apache.spark.sql.Dataset[example.ExampleSpark.SensorData]
Call: filter

a => a.timestamp == (new Timestamp(2014 - 1900, 2, 10, 1, 1, 0, 0))
"
"udf/spark_repos_3/19_YunKillerE_sparkStreamingKafkaPerformance/..src.main.scala.objectProject.dataImportKafkaPerformance.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

newRow => eventRow(newRow(0).toString, if (!newRow(1).toString.equals("""")) newRow(1).toString else ""0"", newRow(2).toString, if (!newRow(3).toString.equals("""")) newRow(3).toString else ""0"", newRow(4).toString, newRow(5).toString, if (!newRow(6).toString.equals("""")) newRow(6).toString else ""0"", newRow(7).toString, newRow(8).toString, newRow(9).toString, newRow(10).toString, newRow(11).toString, newRow(12).toString)
"
"udf/spark_repos_3/19_YunKillerE_sparkStreamingKafkaPerformance/..src.main.scala.textProject.dataImportKafkaPerformance.scala/udf/47.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

newRow => (newRow(0).toString, if (!newRow(1).toString.equals("""")) newRow(1).toString else ""0"", newRow(2).toString, if (!newRow(3).toString.equals("""")) newRow(3).toString else ""0"", newRow(4).toString, newRow(5).toString, if (!newRow(6).toString.equals("""")) newRow(6).toString else ""0"", newRow(7).toString, newRow(8).toString, newRow(9).toString, newRow(10).toString, newRow(11).toString, newRow(12).toString)
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/43.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/66.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/71.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/95.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/1_aduta_apache-spark/..spark-2-examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_3/1_anand27krishn_SparkScala/..FrankSupplies.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_3/1_andrewrgoss_udemy-spark-scala/..advanced_examples.src.main.scala.com.andrewrgoss.spark.DataFrames.scala/udf/26.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.andrewrgoss.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_3/1_anhp58_CoDe/..src.main.scala.main.Utilities.scala/udf/139.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataset(""label"") === 0
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Hotspot-Analysis-Template-master.src.main.scala.cse512.HotcellAnalysis.scala/udf/16.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 0)
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Hotspot-Analysis-Template-master.src.main.scala.cse512.HotcellAnalysis.scala/udf/20.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 1)
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Hotspot-Analysis-Template-master.src.main.scala.cse512.HotcellAnalysis.scala/udf/24.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupTime: String) => HotcellUtils.CalculateCoordinate(pickupTime, 2)
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Hotspot-Analysis-Template-master.src.main.scala.cse512.HotcellAnalysis.scala/udf/49.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Int, y: Int, z: Int) => SpaceTimeCube.computeGetisOrdStat(x, y, z)
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Hotspot-Analysis-Template-master.src.main.scala.cse512.HotzoneAnalysis.scala/udf/13.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(string: String) => string.replace(""("", """").replace("")"", """")
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Hotspot-Analysis-Template-master.src.main.scala.cse512.HotzoneAnalysis.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => HotzoneUtils.ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Phase2-Template-master.src.main.scala.cse512.SpatialQuery.scala/udf/44.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Phase2-Template-master.src.main.scala.cse512.SpatialQuery.scala/udf/57.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Phase2-Template-master.src.main.scala.cse512.SpatialQuery.scala/udf/68.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => ST_Within(pointString1, pointString2, distance)
"
"udf/spark_repos_3/1_apantea01_CSE511Project/..CSE512-Project-Phase2-Template-master.src.main.scala.cse512.SpatialQuery.scala/udf/81.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => ST_Within(pointString1, pointString2, distance)
"
"udf/spark_repos_3/1_ArnavDhiman_CSE-511Phase2/..src.main.scala.cse512.HotcellAnalysis.scala/udf/16.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 0)
"
"udf/spark_repos_3/1_ArnavDhiman_CSE-511Phase2/..src.main.scala.cse512.HotcellAnalysis.scala/udf/20.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 1)
"
"udf/spark_repos_3/1_ArnavDhiman_CSE-511Phase2/..src.main.scala.cse512.HotcellAnalysis.scala/udf/24.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupTime: String) => HotcellUtils.CalculateCoordinate(pickupTime, 2)
"
"udf/spark_repos_3/1_ArnavDhiman_CSE-511Phase2/..src.main.scala.cse512.HotcellAnalysis.scala/udf/53.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Int, y: Int, z: Int, sum: Double, neighbors: Int) => HotcellUtils.score(x, y, z, minX, maxX, minY, maxY, minZ, maxZ, numCells.toDouble, sum, neighbors, mean, stdDev)
"
"udf/spark_repos_3/1_ArnavDhiman_CSE-511Phase2/..src.main.scala.cse512.HotzoneAnalysis.scala/udf/13.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(string: String) => string.replace(""("", """").replace("")"", """")
"
"udf/spark_repos_3/1_ArnavDhiman_CSE-511Phase2/..src.main.scala.cse512.HotzoneAnalysis.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => HotzoneUtils.ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_3/1_bearmug_functional-principles-scala/..course-4.week-4.src.main.scala.timeusage.TimeUsage.scala/udf/110.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
"
"udf/spark_repos_3/1_bearmug_functional-principles-scala/..course-4.week-4.src.main.scala.timeusage.TimeUsage.scala/udf/115.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, primaryNeeds, work, other)
      }
"
"udf/spark_repos_3/1_BiswadeepPatra_Spark/..sparkcore.SparkSQLWithCSVFile1.scala/udf/28.22.Dataset-FakeFriend.filter","Type: org.apache.spark.sql.Dataset[com.elearningpoint.sparkcore.SparkSQLWithCSVFile1.FakeFriend]
Call: filter

fakeFriend(""age"") < 21
"
"udf/spark_repos_3/1_BiswadeepPatra_Spark/..sparkcore.SparkSQLWithJsonData1.scala/udf/22.22.Dataset-Employee.filter","Type: org.apache.spark.sql.Dataset[com.elearningpoint.sparkcore.SparkSQLWithJsonData1.Employee]
Call: filter

employeeDataSet(""age"") > 23
"
"udf/spark_repos_3/1_BiswadeepPatra_Spark/..SparkSql.SparkSQLWithCSVFile1.scala/udf/28.22.Dataset-FakeFriend.filter","Type: org.apache.spark.sql.Dataset[com.elearningpoint.sparkcore.SparkSQLWithCSVFile1.FakeFriend]
Call: filter

fakeFriend(""age"") < 21
"
"udf/spark_repos_3/1_BiswadeepPatra_Spark/..SparkSql.SparkSQLWithJsonData1.scala/udf/22.22.Dataset-Employee.filter","Type: org.apache.spark.sql.Dataset[com.elearningpoint.sparkcore.SparkSQLWithJsonData1.Employee]
Call: filter

employeeDataSet(""age"") > 23
"
"udf/spark_repos_3/1_BiyuHuang_CodePrototypesDemo/..demo.SparkDemo.src.main.scala.com.wallace.spark.sparkdemo.dataframedemo.DataFrameDemo.scala/udf/115.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getInt(1)
"
"udf/spark_repos_3/1_BiyuHuang_CodePrototypesDemo/..demo.SparkDemo.src.main.scala.com.wallace.spark.sparkdemo.dataframedemo.DataFrameDemo.scala/udf/136.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

res3.col(""Time"") >= ""2016-05-23 09:00:00""
"
"udf/spark_repos_3/1_BiyuHuang_CodePrototypesDemo/..demo.SparkDemo.src.main.scala.com.wallace.spark.sparkdemo.dataframedemo.DataFrameDemo.scala/udf/148.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

resDF.col(""col_name"") === ""Location:""
"
"udf/spark_repos_3/1_BiyuHuang_CodePrototypesDemo/..demo.SparkDemo.src.main.scala.com.wallace.spark.sparkdemo.dataframedemo.DataFrameDemo.scala/udf/156.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

srcDF.col(""num"") > 0
"
"udf/spark_repos_3/1_BiyuHuang_CodePrototypesDemo/..demo.SparkDemo.src.main.scala.com.wallace.spark.sparkdemo.dataframedemo.WindowExprDemo.scala/udf/16.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_3/1_BiyuHuang_CodePrototypesDemo/..demo.SparkDemo.src.main.scala.com.wallace.spark.sparkdemo.udfdemo.UdfDemo.scala/udf/16.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

squared
"
"udf/spark_repos_3/1_BiyuHuang_CodePrototypesDemo/..demo.SparkDemo.src.main.scala.com.wallace.spark.sparkdemo.udfdemo.UdfDemo.scala/udf/23.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeDT(_: String, _: String, _: String)
"
"udf/spark_repos_3/1_bogdannb_scala_spark_course/..3_Spark_Streaming_Concepts.code.SparkStreamingExamples.src.com.sundogsoftware.sparksql.SparkSqlStart.scala/udf/12.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/1_bogdannb_scala_spark_course/..3_Spark_Streaming_Concepts.Solution.SparkStreamingExamples.src.com.sundogsoftware.sparksql.SparkSqlStart.scala/udf/12.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/1_brunopacheco1_machinelearning/..src.main.scala.com.dev.bruno.ml.deeplearning.lstm.ProcessCSV.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filter
"
"udf/spark_repos_3/1_compasses_elastic-spark/..spark-test.src.main.scala.spark.aas.Chap2Intro.scala/udf/32.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_match"" === lit(false)
"
"udf/spark_repos_3/1_compasses_elastic-spark/..spark-test.src.main.scala.spark.aas.Chap2Intro.scala/udf/66.17.Dataset-MatchData.map","Type: org.apache.spark.sql.Dataset[spark.aas.MatchData]
Call: map

md => (scoreMatchData(md), md.is_match)
"
"udf/spark_repos_3/1_compasses_elastic-spark/..spark-test.src.main.scala.spark.aas.Chap3Recommand.scala/udf/21.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
      val Array(user, artist, _*) = line.split("" "")
      (user.toInt, artist.toInt)
    }
"
"udf/spark_repos_3/1_compasses_elastic-spark/..spark-test.src.main.scala.spark.aas.Chap3Recommand.scala/udf/32.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (badID, goodID)
"
"udf/spark_repos_3/1_compasses_elastic-spark/..spark-test.src.main.scala.spark.aas.Chap3Recommand.scala/udf/75.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""user"" === userID
"
"udf/spark_repos_3/1_compasses_elastic-spark/..spark-test.src.main.scala.spark.aas.Chap3Recommand.scala/udf/80.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (existingArtistIDs: _*)
"
"udf/spark_repos_3/1_compasses_elastic-spark/..spark-test.src.main.scala.spark.aas.Chap3Recommand.scala/udf/87.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (recommendedArtistIDs: _*)
"
"udf/spark_repos_3/1_compasses_elastic-spark/..spark-test.src.main.scala.spark.aas.Chap3Recommand.scala/udf/99.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
          val Array(userID, artistID, count) = locally {
            val _t_m_p_9 = line.split(' ')
            _t_m_p_9.map(_.toInt)
          }
          val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)
          (userID, finalArtistID, count)
        }
"
"udf/spark_repos_3/1_crawler-plus_hadoop-family/..src.main.scala.site.it4u.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 10
"
"udf/spark_repos_3/1_crawler-plus_hadoop-family/..src.main.scala.site.it4u.spark.DataFrameRDDApp.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_3/1_crawler-plus_hadoop-family/..src.main.scala.site.it4u.spark.DatasetApp.scala/udf/11.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[site.it4u.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_3/1_DITAS-Project_ehealth-spark-vdc-with-dal/..DAL.src.main.scala.com.ditas.DataMovementServer.scala/udf/141.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => UtilFunctions.anyNotNull(row)
"
"udf/spark_repos_3/1_DITAS-Project_ehealth-spark-vdc-with-dal/..DAL.src.main.scala.com.ditas.EhealthServer.scala/udf/193.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => UtilFunctions.anyNotNull(row)
"
"udf/spark_repos_3/1_DITAS-Project_ehealth-spark-vdc-with-dal/..DAL.src.main.scala.com.ditas.EhealthServer.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => UtilFunctions.anyNotNull(row)
"
"udf/spark_repos_3/1_DITAS-Project_ehealth-spark-vdc-with-dal/..DAL.src.main.scala.com.ditas.EnforcementEngineResponseProcessor.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => DataFrameUtils.anyNotNull(row)
"
"udf/spark_repos_3/1_forchard_demy/..geo.src.main.scala.AddressLocalizator.scala/udf/20.31.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => ParsedAddress(r.getAs[Int](idColumn), tableName, r.getAs[String](addressColumn))
"
"udf/spark_repos_3/1_forchard_demy/..geo.src.main.scala.AddressLocalizator.scala/udf/53.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.getAs[String](postcodeColumn) != null && r.getAs[String](postcodeColumn).length > 0
"
"udf/spark_repos_3/1_forchard_demy/..geo.src.main.scala.AddressLocalizator.scala/udf/59.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => Locality(r.getAs[String](""_locality""), r.getAs[String](postcodeColumn).toInt, r.getAs[String](inseeCodeColumn), r.getAs[String](localityNameColumn), r.getAs[Double](""_meanLon""), r.getAs[Double](""_meanLat""), r.getAs[Int](""_count""))
"
"udf/spark_repos_3/1_forchard_demy/..geo.src.main.scala.AddressLocalizator.scala/udf/69.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => Street(r.getAs[String](streetColumn), r.getAs[String](""_locality""), r.getAs[String](postcodeColumn).toInt, r.getAs[Seq[String]](""_number""), r.getAs[Seq[Double]](""_lon""), r.getAs[Seq[Double]](""_lat""), r.getAs[Double](""_meanLon""), r.getAs[Double](""_meanLat""), r.getAs[Int](""_count""))
"
"udf/spark_repos_3/1_forchard_demy/..geo.src.main.scala.Execute.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.getAs[String](""postcode"") != ""PLURI""
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.linalg.performance.scala/udf/139.24.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ => locally {
            val _t_m_p_16 = locally {
              val _t_m_p_17 = generateV(n / nPart)
              _t_m_p_17.map({
                case (v1, v2) =>
                  (v1.asBreeze, v2.asBreeze) match {
                    case (b1, b2) =>
                      b1.dot(b2) / (norm(b1) * norm(b2))
                  }
              })
            }
            _t_m_p_16.reduce(_ + _)
          }
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.linalg.performance.scala/udf/161.24.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ => locally {
            val _t_m_p_20 = locally {
              val _t_m_p_21 = generateV(n / nPart)
              _t_m_p_21.map({
                case (v1, v2) =>
                  v1.cosineSimilarity(v2)
              })
            }
            _t_m_p_20.reduce(_ + _)
          }
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.linalg.performance.scala/udf/180.24.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

{ _ => 
            val bls = com.github.fommil.netlib.BLAS.getInstance()
            locally {
              val _t_m_p_24 = locally {
                val _t_m_p_25 = generateV(n / nPart)
                _t_m_p_25.map({
                  case (v1, v2) =>
                    (v1.values, v2.values, vSize) match {
                      case (a1, a2, l) =>
                        bls.ddot(l, a1, 1, a2, 1) / (bls.dnrm2(l, a1, 1) * bls.dnrm2(l, a2, 1))
                    }
                })
              }
              _t_m_p_24.reduce(_ + _)
            }
          }
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.SemanticPhrase.scala/udf/108.19.Dataset-TaggedPhrase, Int)).map","Type: org.apache.spark.sql.Dataset[((Int, Int), (demy.mllib.text.TaggedPhrase, Int))]
Call: map

p => p._2._1
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.SemanticPhrase.scala/udf/92.25.Dataset-TaggedPhrase).map","Type: org.apache.spark.sql.Dataset[((Int, Int), demy.mllib.text.TaggedPhrase)]
Call: map

p => (p._2, index)
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TermlLikelyhoodEvaluator.scala/udf/113.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getAs[MLVector](""rawPrediction"")(0)
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TermlLikelyhoodEvaluator.scala/udf/116.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getAs[Double](""prediction"") match {
        case 0.0d => -1
        case 1.0d => 1
      }
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TermlLikelyhoodEvaluator.scala/udf/122.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""prediction"") === 1.0d
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TermlLikelyhoodEvaluator.scala/udf/125.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""prediction"") === 0.0d
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TermlLikelyhoodEvaluator.scala/udf/140.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getAs[MLVector](rawScoresColumnName)(0)
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TermlLikelyhoodEvaluator.scala/udf/143.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getAs[Double](scoresColumnName) match {
          case 0.0d => -1
          case 1.0d => 1
        }
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TermlLikelyhoodEvaluator.scala/udf/149.25.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(scoresColumnName) === 1.0d
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TermlLikelyhoodEvaluator.scala/udf/152.25.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(scoresColumnName) === 0.0d
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TweetCleaner.scala/udf/36.25.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

p => (p._1, if (p._2 > 0) getOrDefault(positiveCharsTo) else if (p._2 < 0) getOrDefault(negativeCharsTo) else """")
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.TweetCleaner.scala/udf/38.23.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

p => (Pattern.quote(p._1), "" "" + p._2 + "" "")
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.Word2VecApplier.scala/udf/106.23.Dataset-Vector).filter","Type: org.apache.spark.sql.Dataset[(String, org.apache.spark.ml.linalg.Vector)]
Call: filter

p => p match {
        case (token, vector) =>
          !stop.value.contains(token)
      }
"
"udf/spark_repos_3/1_forchard_demy/..mllib.src.main.scala.text.Word2VecApplier.scala/udf/94.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => s.split("" "")
"
"udf/spark_repos_3/1_forchard_demy/..twitter.src.main.scala.Execute.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getString(0)
"
"udf/spark_repos_3/1_GnaneshKunal_scala-hadoop/..src.main.scala.gk.hadoop.book.io.MaxTemperatureWithCompression.scala/udf/11.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => {
      val year = x.substring(15, 19)
      val quality = x.substring(92, 93)
      val airTemp = if (x.charAt(87) == '+') x.substring(88, 92).toInt else x.substring(87, 92).toInt
      val temp = if (airTemp != 9999 && quality.matches(""[01459]"")) airTemp else 0
      Temperature(year.toInt, temp)
    }
"
"udf/spark_repos_3/1_ibm-cloud-streaming-retail-demo_spark-structured-streaming-on-iae-to-elasticsearch/..src.main.scala.Main.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""InvoiceNo"".isNotNull
"
"udf/spark_repos_3/1_imranece59_poc/..src.main.scala.demo.helper.ProcessDataHelper.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""skuName =='$skuName'""
"
"udf/spark_repos_3/1_imranece59_poc/..src.main.scala.demo.helper.ProcessDataHelper.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""skuName !='$skuName'""
"
"udf/spark_repos_3/1_jerryzhu2007_dateformatting/..dateformatting.src.test.scala.com.kyleong.utils.DateFormattingTest.scala/udf/36.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Year"") >= 1970 && col(""Year"") <= 2039
"
"udf/spark_repos_3/1_jerryzhu2007_dateformatting/..dateformatting.src.test.scala.com.kyleong.utils.DateFormattingTest.scala/udf/38.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""Month"") <= 12
"
"udf/spark_repos_3/1_jerryzhu2007_dateformatting/..dateformatting.src.test.scala.com.kyleong.utils.DateFormattingTest.scala/udf/40.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""Day"") <= 31
"
"udf/spark_repos_3/1_joao-parana_CPD-02/..covtype.src.main.scala.eic.randomforest.RDFRunner.scala/udf/134.19.Dataset-Double.map","Type: org.apache.spark.sql.Dataset[Double]
Call: map

_ / total
"
"udf/spark_repos_3/1_joscani_predict_binary_h2o/..src.main.scala.com.h2ospark.BinaryModels.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ x => 
        val r = new RowData
        locally {
          val _t_m_p_3 = header.indices
          _t_m_p_3.foreach(idx => r.put(header(idx), x.getAs[String](idx)))
        }
        val score = easyModel.predictBinomial(r).classProbabilities
        (x.getAs[String](0), score(1))
      }
"
"udf/spark_repos_3/1_kaplanbora_spark-talk/..src.main.scala.com.kaplanbora.App.scala/udf/27.20.Dataset-Scrobble.filter","Type: org.apache.spark.sql.Dataset[com.kaplanbora.models.Scrobble]
Call: filter

scrobble => scrobble.utc_time.contains(""2018"")
"
"udf/spark_repos_3/1_kaplanbora_spark-talk/..src.main.scala.com.kaplanbora.App.scala/udf/32.17.Dataset-Artist.map","Type: org.apache.spark.sql.Dataset[com.kaplanbora.models.Artist]
Call: map

{
      artist => MinifiedArtist(artist.name, artist.stats.listeners.toLong, locally {
        val _t_m_p_6 = artist.tags
        _t_m_p_6.map(_.name)
      }, locally {
        val _t_m_p_7 = artist.similarArtists
        _t_m_p_7.map(_.name)
      })
    }
"
"udf/spark_repos_3/1_kaplanbora_spark-talk/..src.main.scala.com.kaplanbora.App.scala/udf/57.21.Dataset-EnrichedArtist.map","Type: org.apache.spark.sql.Dataset[com.kaplanbora.models.EnrichedArtist]
Call: map

artist => (artist.name, artist.similarArtists.head)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..HotcellAnalysis.scala/udf/15.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 0)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..HotcellAnalysis.scala/udf/19.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 1)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..HotcellAnalysis.scala/udf/23.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupTime: String) => HotcellUtils.CalculateCoordinate(pickupTime, 2)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..HotcellAnalysis.scala/udf/42.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(inputX: Int) => HotcellUtils.squared_val(inputX)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..HotcellAnalysis.scala/udf/53.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(minX: Int, minY: Int, minZ: Int, maxX: Int, maxY: Int, maxZ: Int, inputX: Int, inputY: Int, inputZ: Int) => HotcellUtils.countNeighbours(minX, minY, minZ, maxX, maxY, maxZ, inputX, inputY, inputZ)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..HotcellAnalysis.scala/udf/59.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Int, y: Int, z: Int, mean: Double, sd: Double, countn: Int, sumn: Int, numcells: Int) => HotcellUtils.gettisordstatistic(x, y, z, mean, sd, countn, sumn, numcells)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..HotzoneAnalysis.scala/udf/14.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(string: String) => string.replace(""("", """").replace("")"", """")
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..HotzoneAnalysis.scala/udf/22.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => HotzoneUtils.ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..src.main.scala.cse512.SpatialQuery.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => Contains_function(queryRectangle, pointString)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..src.main.scala.cse512.SpatialQuery.scala/udf/32.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => Within_function(pointString1, pointString2, distance)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..src.main.scala.cse512.SpatialQuery.scala/udf/45.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => Within_function(pointString1, pointString2, distance)
"
"udf/spark_repos_3/1_krithika-n_NYC_Taxi_data_Challenge_Phase1/..src.main.scala.cse512.SpatialQuery.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => Contains_function(queryRectangle, pointString)
"
"udf/spark_repos_3/1_krohit-scala_MSStreamingStack/..MoneySmart.msConsumerApis.src.main.scala.com.consumer.StructuredStreaming.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""user_id"").isNotNull && col(""u_id"").isNull
"
"udf/spark_repos_3/1_krohit-scala_MSStreamingStack/..MoneySmart.msConsumerApis.src.main.scala.com.consumer.StructuredStreaming.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""user_id"") === col(""u_id"")
"
"udf/spark_repos_3/1_lianyt1994_spark-project/..src.main.scala.com.pubinfo.sparksql.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_3/1_lianyt1994_spark-project/..src.main.scala.com.pubinfo.sparksql.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_3/1_lianyt1994_spark-project/..src.main.scala.com.pubinfo.sparksql.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_3/1_likemezhoujie_SparkStreaming_splits/..spark.src.main.scala.com.yida.sparkSql.ReflectSchema.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 30
"
"udf/spark_repos_3/1_lttoto_SparkSQLProject/..src.main.scala.com.lt.spark.log.TopNStatJob.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""20170511"" && $""cmsType"" === ""video""
"
"udf/spark_repos_3/1_lttoto_SparkSQLProject/..src.main.scala.com.lt.spark.log.TopNStatJob.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""20170511"" && $""cmsType"" === ""video""
"
"udf/spark_repos_3/1_lttoto_SparkSQLProject/..src.main.scala.com.lt.spark.log.TopNStatJob.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""20170511"" && $""cmsType"" === ""video""
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.dataset.Flight.scala/udf/27.22.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[dataset.Flight.Flight]
Call: filter

flight => flight.crsdephour == 10
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.dataset.Flight.scala/udf/34.22.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[dataset.Flight.Flight]
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.dataset.Flight.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.dataset.Flight.scala/udf/49.22.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[dataset.Flight.Flight]
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.dataset.Flight.scala/udf/57.22.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[dataset.Flight.Flight]
Call: filter

$""depdelay"" > 40
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.machinelearning.Flight.scala/udf/100.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.machinelearning.Flight.scala/udf/102.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.machinelearning.Flight.scala/udf/79.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.machinelearning.Flight.scala/udf/81.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.machinelearning.Flight.scala/udf/86.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.machinelearning.Flight.scala/udf/88.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.machinelearning.Flight.scala/udf/93.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_3/1_mapr-demos_flightdelayhol/..src.main.scala.machinelearning.Flight.scala/udf/95.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_3/1_margara_spark-tutorial/..src.main.scala.batch.bank.Bank2.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""total"" === maxTotal
"
"udf/spark_repos_3/1_margara_spark-tutorial/..src.main.scala.batch.bank.Bank2.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""totalDeposits.sum(amount)"".isNull && $""totalWithdrawals.sum(amount)"" > 0 || $""totalWithdrawals.sum(amount)"" > ($""totalDeposits.sum(amount)"")
"
"udf/spark_repos_3/1_margara_spark-tutorial/..src.main.scala.batch.bank.Bank.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sumWithdrawals(""sum(amount)"") === maxTotal
"
"udf/spark_repos_3/1_margara_spark-tutorial/..src.main.scala.batch.bank.Bank.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

totalDepositsPerAccount(""sum(amount)"").isNull.and(totalWithdrawalsPerAccount(""sum(amount)"") > 0).or(totalWithdrawalsPerAccount(""sum(amount)"") > totalDepositsPerAccount(""sum(amount)""))
"
"udf/spark_repos_3/1_micho10_Big-Data-Analysis/..week_03.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/102.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs(""working""), row.getAs(""sex""), row.getAs(""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
"
"udf/spark_repos_3/1_micho10_Big-Data-Analysis/..week_03.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/108.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

key => TimeUsageRow(key._1._1, key._1._2, key._1._3, key._2, key._3, key._4)
"
"udf/spark_repos_3/1_Moussi_spark-sql-cassandra-aggregator/..src.main.scala.dataframes.SparkSQLUDFunctionsManip.scala/udf/18.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(tags: String) => ""&lt;"".r.findAllMatchIn(tags).length
"
"udf/spark_repos_3/1_Moussi_spark-sql-cassandra-aggregator/..src.main.scala.statistic.StatisticalDataExplorationFiveNumberSummary.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

value => value.getDouble(0)
"
"udf/spark_repos_3/1_Moussi_spark-sql-cassandra-aggregator/..src.main.scala.statistic.StatisticalDataExplorationOutlier.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""value < $lowerRange and value > $upperRange""
"
"udf/spark_repos_3/1_Nemchinovrp_Fintech-Trading/..spark-processor2.src.main.scala.me.sandbox.sql.streaming.spark.logic.StructureWindowUdf.scala/udf/25.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""total_events"") > minEvents
"
"udf/spark_repos_3/1_Nemchinovrp_Fintech-Trading/..spark-processor2.src.main.scala.me.sandbox.sql.streaming.spark.logic.StructureWindowUdf.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""incident"").isNotNull
"
"udf/spark_repos_3/1_nikkatalnikov_datakeeper/..src.main.scala.datakeeper.dirtypartitioner.DirtyPartitioner.scala/udf/27.30.Dataset-Column.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Column]
Call: filter

_.isPartition
"
"udf/spark_repos_3/1_nikkatalnikov_datakeeper/..src.main.scala.datakeeper.dirtypartitioner.DirtyPartitioner.scala/udf/29.25.Dataset-Column.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Column]
Call: map

_.name
"
"udf/spark_repos_3/1_nikkatalnikov_datakeeper/..src.main.scala.datakeeper.dirtypartitioner.DirtyPartitioner.scala/udf/78.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => newPartitions.exists(hasKey(_, row))
"
"udf/spark_repos_3/1_nikkatalnikov_datakeeper/..src.main.scala.datakeeper.dirtypartitioner.DirtyPartitioner.scala/udf/99.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hasKey(part.key, _)
"
"udf/spark_repos_3/1_ollik1_spark-clipboard/..src.main.scala.com.github.ollik1.clipboard.SparkShowRelation.scala/udf/17.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{
          line => line.trim.stripPrefix(""|"").stripSuffix(""|"")
        }
"
"udf/spark_repos_3/1_pankajmahato_spark-scala/..src.main.scala.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(""country"").===(""Afghanistan"")
"
"udf/spark_repos_3/1_pankajmahato_spark-scala/..src.main.scala.com.sparkTutorial.sparkSql.StackOverFlowSurvey.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(AGE_MIDPOINT) < 20
"
"udf/spark_repos_3/1_pankajmahato_spark-scala/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/22.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.country == ""Afghanistan""
"
"udf/spark_repos_3/1_pankajmahato_spark-scala/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/29.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.age_midpoint.isDefined && response.age_midpoint.get < 20.0d
"
"udf/spark_repos_3/1_pankajmahato_spark-scala/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/36.22.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: filter

response => response.salary_midpoint.isDefined
"
"udf/spark_repos_3/1_pankajmahato_spark-scala/..src.main.scala.com.sparkTutorial.sparkSql.TypedDataset.scala/udf/41.19.Dataset-Response.map","Type: org.apache.spark.sql.Dataset[com.sparkTutorial.sparkSql.Response]
Call: map

response => locally {
        val _t_m_p_5 = response.salary_midpoint
        _t_m_p_5.map(point => Math.round(point / 20000) * 20000)
      }.orElse(None)
"
"udf/spark_repos_3/1_pankajmahato_spark-scala/..src.main.scala.yorbit.sql.question2.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(""Primary Type"").===(""NARCOTICS"") && responseWithSelectedColumns.col(""Year"").===(""2015"")
"
"udf/spark_repos_3/1_pankajmahato_spark-scala/..src.main.scala.yorbit.sql.question3.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(""Primary Type"").===(""THEFT"")
"
"udf/spark_repos_3/1_pascals-ager_spark-deets-service/..src.main.scala.io.pascals.spark.transformer.package.scala/udf/24.21.Dataset-PageAccessEvent.map","Type: org.apache.spark.sql.Dataset[io.pascals.spark.models.PageAccessEvent]
Call: map

{
          case PageAccessEvent(user_ident, Some(""DOUBLE_PAGE_MODE""), event) =>
            PageAccessEventCount(user_ident, Some(2), event)
          case PageAccessEvent(user_ident, Some(""SINGLE_PAGE_MODE""), event) =>
            PageAccessEventCount(user_ident, Some(1), event)
          case PageAccessEvent(user_ident, Some(_), event) =>
            PageAccessEventCount(user_ident, Some(0), event)
          case PageAccessEvent(user_ident, None, event) =>
            PageAccessEventCount(user_ident, Some(0), event)
        }
"
"udf/spark_repos_3/1_pascals-ager_spark-deets-service/..src.main.scala.io.pascals.spark.transformer.package.scala/udf/44.26.Dataset-PageAccessEventCount.filter","Type: org.apache.spark.sql.Dataset[io.pascals.spark.models.PageAccessEventCount]
Call: filter

!($""event"" === typedLit(UNDEFINED.event))
"
"udf/spark_repos_3/1_PerfectZQ_parquet2dbs/..src.main.scala.com.zq.elasticsearch.FPSParquet2ES.scala/udf/138.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

formatTimestamp
"
"udf/spark_repos_3/1_pixipanda_sparksql/..src.main.scala.com.pixipanda.sparksql.test.SkewGrocery.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'category === ""grocery_pos""
"
"udf/spark_repos_3/1_postBG_spark2_practice/..scala.lab.ml.twitter.classifier.language.ExamineAndTrain.scala/udf/56.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toString
"
"udf/spark_repos_3/1_postBG_spark2_practice/..scala.lab.ml.twitter.classifier.language.ExamineAndTrain.scala/udf/60.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

Utils.featurize
"
"udf/spark_repos_3/1_postBG_spark2_practice/..scala.lab.sql.loganalyzer.LogAnalyzerSQL.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getLong(1))
"
"udf/spark_repos_3/1_postBG_spark2_practice/..scala.lab.sql.loganalyzer.LogAnalyzerSQL.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_3/1_postBG_spark2_practice/..scala.lab.sql.loganalyzer.LogAnalyzerSQL.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_3/1_rajrohith_spark-guide-databrick/..project-templates.scala.src.main.scala.DataFrameExample.scala/udf/15.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.pointlessUDF(_: String): String
"
"udf/spark_repos_3/1_recipegrace_BigLibrary/..electric.src.main.scala.com.recipegrace.biglibrary.electric.ElectricSession.scala/udf/7.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => f.getAs[String](0)
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.com.emergency.calls.analysis.uc1.EmergencyCallsAnlysis.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x(0) + "" -> "" + x(2)
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.com.emergency.calls.analysis.uc1.EmergencyCallsAnlysis.scala/udf/23.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

item => item
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.com.emergency.calls.analysis.uc1.EmergencyCallsAnlysis.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x(0) + "" -> "" + (x(1), x(2))
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.com.emergency.calls.analysis.uc1.EmergencyCallsAnlysis.scala/udf/41.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

item => item
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.recommandationModel.ml.MovieRecommander.scala/udf/48.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

ratingDF(""userId"") === userId
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.recommandationModel.ml.MovieRecommander.scala/udf/53.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

ratingDF(""userId"") === userId
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.recommandationModel.ml.MovieRecommander.scala/udf/57.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

ratingDF(""userId"") !== userId
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.recommandationModel.ml.MovieRecommander.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

ratingDF(""userId"") === userId
"
"udf/spark_repos_3/1_reddy279_SparkMachineLearningRef/..src.main.scala-2.11.recommandationModel.ml.MovieRecommander.scala/udf/64.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a.rating"".isNull
"
"udf/spark_repos_3/1_RRRluxoft_COURSERA-timeusage/..src.main.scala.timeusage.TimeUsage.scala/udf/103.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

e => TimeUsageRow(e._1._1, e._1._2, e._1._3, (e._2 * 10.0d).round / 10.0d, (e._3 * 10.0d).round / 10.0d, (e._4 * 10.0d).round / 10.0d)
"
"udf/spark_repos_3/1_RRRluxoft_COURSERA-timeusage/..src.main.scala.timeusage.TimeUsage.scala/udf/97.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

e => TimeUsageRow(e.getAs[String](""working""), e.getAs[String](""sex""), e.getAs[String](""age""), e.getAs[Double](""primaryNeedsProjection""), e.getAs[Double](""workProjection""), e.getAs[Double](""otherProjection""))
"
"udf/spark_repos_3/1_rsomepalli_mapzen-tools/..extractors.src.main.scala.org.lakumbra.mapzen.BaseIndexer.scala/udf/25.21.Dataset-Place.map","Type: org.apache.spark.sql.Dataset[org.lakumbra.mapzen.PlaceIndexer.Place]
Call: map

p => PlaceFlat(p.id, p.name, p.lat_lon.lat, p.lat_lon.lon, p.parent_id, p.layer, p.label, p.country, p.country_a, p.region, p.locality, p.neighbourhood_borough, p.population_rank)
"
"udf/spark_repos_3/1_rsomepalli_mapzen-tools/..extractors.src.main.scala.org.lakumbra.mapzen.LocalityIndexer.scala/udf/35.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => LocationDoc(r.getAs[Int](""con_id""), r.getAs[String](""con_name""), Location(r.getAs[Double](""con_latitude""), r.getAs[Double](""con_longitude"")), r.getAs[Int](""cou_id""), r.getAs[String](""cou_name""), Location(r.getAs[Double](""cou_latitude""), r.getAs[Double](""cou_longitude"")), r.getAs[Int](""reg_id""), r.getAs[String](""reg_name""), Location(r.getAs[Double](""reg_latitude""), r.getAs[Double](""reg_longitude"")), r.getAs[Int](""loc_id""), r.getAs[String](""loc_name""), Location(r.getAs[Double](""loc_latitude""), r.getAs[Double](""loc_longitude"")))
"
"udf/spark_repos_3/1_rsomepalli_mapzen-tools/..extractors.src.main.scala.org.lakumbra.mapzen.PlaceIndexer.scala/udf/104.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => {
        val localityName = r.getAs[String](""name"")
        val countryName = r.getAs[String](""country_name"")
        val regionName = r.getAs[String](""reg_name"")
        Place(r.getAs[Int](""id""), r.getAs[String](""name""), buildLocation(r), r.getAs[Int](""country_id""), ""locality"", s""$localityName, $regionName, $countryName "", countryName, r.getAs[String](""iso_country""), regionName, localityName, """", r.getAs[Int](""population_rank""))
      }
"
"udf/spark_repos_3/1_rsomepalli_mapzen-tools/..extractors.src.main.scala.org.lakumbra.mapzen.PlaceIndexer.scala/udf/131.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => {
        val countryName = r.getAs[String](""country_name"")
        val regionName = r.getAs[String](""name"")
        Place(r.getAs[Int](""id""), regionName, buildLocation(r), r.getAs[Int](""country_id""), ""region"", s""$regionName, $countryName "", countryName, r.getAs[String](""iso_country""), regionName, """", """", r.getAs[Int](""population_rank""))
      }
"
"udf/spark_repos_3/1_rsomepalli_mapzen-tools/..extractors.src.main.scala.org.lakumbra.mapzen.PlaceIndexer.scala/udf/155.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => {
        val countryName = r.getAs[String](""name"")
        Place(r.getAs[Int](""id""), countryName, buildLocation(r), r.getAs[Int](""parent_id""), ""country"", countryName, countryName, r.getAs[String](""iso_country""), """", """", """", r.getAs[Int](""population_rank""))
      }
"
"udf/spark_repos_3/1_rsomepalli_mapzen-tools/..extractors.src.main.scala.org.lakumbra.mapzen.PlaceIndexer.scala/udf/37.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => {
        val boroughName = r.getAs[String](""name"")
        val localityName = r.getAs[String](""locality_name"")
        val countryName = r.getAs[String](""country_name"")
        val regionName = r.getAs[String](""reg_name"")
        Place(r.getAs[Int](""id""), boroughName, buildLocation(r), r.getAs[Int](""locality_id""), ""borough"", s""$boroughName $localityName, $regionName, $countryName"", r.getAs[String](""country_name""), r.getAs[String](""iso_country""), r.getAs[String](""reg_name""), localityName, boroughName, r.getAs[Int](""population_rank""))
      }
"
"udf/spark_repos_3/1_rsomepalli_mapzen-tools/..extractors.src.main.scala.org.lakumbra.mapzen.PlaceIndexer.scala/udf/72.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => {
        val neighbourhoodName = r.getAs[String](""name"")
        val localityName = r.getAs[String](""locality_name"")
        val countryName = r.getAs[String](""country_name"")
        val regionName = r.getAs[String](""reg_name"")
        Place(r.getAs[Int](""id""), neighbourhoodName, buildLocation(r), r.getAs[Int](""locality_id""), ""neighbourhood"", s""$neighbourhoodName $localityName, $regionName, $countryName"", r.getAs[String](""country_name""), r.getAs[String](""iso_country""), r.getAs[String](""reg_name""), localityName, neighbourhoodName, r.getAs[Int](""population_rank""))
      }
"
"udf/spark_repos_3/1_sezerp_spark-cov-19-kaggle/..src.main.scala.com.pawelzabczynski.Main.scala/udf/30.22.Dataset-CovidDataSources.filter","Type: org.apache.spark.sql.Dataset[com.pawelzabczynski.covid.model.CovidDataSources]
Call: filter

r => r.age == 0
"
"udf/spark_repos_3/1_shihabuddinbuet_apache-spark-abc/..src.main.scala.com.su.apache.spark.basic.JoinDataframe.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s_id"").isNotNull
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/43.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/66.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/71.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/95.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""same_bucket"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""same_bucket"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/29.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") > distFP
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/33.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") < distFN
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distUDF(col(s""a.$inputCol""), col(s""b.$inputCol"")) < threshold
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/63.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""distCol"") < threshold
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isGoodBucket($""count"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 0.0d
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 4.0d
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/113.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/51.23.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/98.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/553.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/557.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/585.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/589.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/897.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/906.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/80.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/1_shiwendai_SparkSourceCodeLearning/..spark-2.1.1.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/1_soonraah_fm_spark/..src.main.scala.org.apache.spark.ml.fm.FactorizationMachinesSample.scala/udf/71.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val label = row.getAs[Double](""label"")
          (label, label)
        }
"
"udf/spark_repos_3/1_soonraah_fm_spark/..src.main.scala.org.apache.spark.ml.fm.FactorizationMachinesSGD.scala/udf/106.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
        row => row.getAs[Int](0)
      }
"
"udf/spark_repos_3/1_soonraah_fm_spark/..src.main.scala.org.apache.spark.ml.fm.FactorizationMachinesSGD.scala/udf/112.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

Strength(_, Random.nextGaussian() * initialSd)
"
"udf/spark_repos_3/1_soonraah_fm_spark/..src.main.scala.org.apache.spark.ml.fm.FactorizationMachinesSGD.scala/udf/116.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

FactorizedInteraction(_, Vectors.dense(locally {
        val _t_m_p_9 = 0 until dimFactorization
        _t_m_p_9.map {
          _ => Random.nextGaussian() * initialSd
        }
      }.toArray).toDense)
"
"udf/spark_repos_3/1_soonraah_fm_spark/..src.main.scala.org.apache.spark.ml.fm.FactorizationMachinesSGD.scala/udf/72.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[Double](""loss"")
"
"udf/spark_repos_3/1_soonraah_fm_spark/..src.main.scala.org.apache.spark.ml.fm.FactorizationMachinesSGD.scala/udf/81.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
            row => Strength(row.getAs[Int](""featureId""), row.getAs[Double](""strength""))
          }
"
"udf/spark_repos_3/1_soonraah_fm_spark/..src.main.scala.org.apache.spark.ml.fm.FactorizationMachinesSGD.scala/udf/86.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
            row => FactorizedInteraction(row.getAs[Int](""featureId""), row.getAs[DenseVector](""factorizedInteraction""))
          }
"
"udf/spark_repos_3/1_sparkaochong_spark-nba-analysis/..src.main.scala.com.ac.AgeAndExpThrendAnalysis.scala/udf/20.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => AgeOrExpStats(row.getAs[String](""name""), row.getAs[Int](""year""), row.getAs[Int](""age""), row.getAs[Int](""exp""), row.getAs[Double](""zToT""), row.getAs[Double](""nToT""))
"
"udf/spark_repos_3/1_steelxiang_spark-ftp/..src.main.scala.loaddata.ftp2hdfs_dpi.scala/udf/82.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => t.getString(0).split(""\\|"")
"
"udf/spark_repos_3/1_steelxiang_spark-ftp/..src.main.scala.loaddata.ftp2hdfs_it.scala/udf/80.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => {
        val line: Array[String] = t.getString(0).split(""\t"")
        val dataSource: Int = dataType
        var URL: String = line(0)
        var URL_Time: Int = 1
        if (filename.startsWith(""lte_cdpi_url"") || filename.startsWith(""3g_cdpi_url"")) {
          URL = line(1)
        } else {
          URL_Time = line(1).toInt
        }
        val Id: String = """"
        val dt: String = d
        YHtable(dataSource, URL, Id, URL_Time, dt)
      }
"
"udf/spark_repos_3/1_steelxiang_spark-ftp/..src.main.scala.loaddata.ftp2hdfs_js.scala/udf/76.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => {
          val dataSource: Int = 12
          val URL: String = t.getString(0)
          val Id: String = """"
          val URL_Time: Int = 1
          YHtable(dataSource, URL, Id, URL_Time, dt)
        }
"
"udf/spark_repos_3/1_steelxiang_spark-ftp/..src.main.scala.loaddata.ftp2hdfs_yh.scala/udf/44.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => t.getString(0).split(""\t"")
"
"udf/spark_repos_3/1_steelxiang_spark-ftp/..src.main.scala.loaddata.load_dpi.scala/udf/47.29.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => t.getString(0).split(""\\|"")
"
"udf/spark_repos_3/1_SteveFrank_quick-start-scala/..quick-spark-scala.src.main.scala.com.spark.demo.scala.spark.demo.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_3/1_SteveFrank_quick-start-scala/..quick-spark-scala.src.main.scala.com.spark.demo.scala.spark.demo.DataFrameRDDApp.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_3/1_SteveFrank_quick-start-scala/..quick-spark-scala.src.main.scala.com.spark.demo.scala.spark.demo.DatasetApp.scala/udf/13.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.spark.demo.scala.spark.demo.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_3/1_SteveFrank_quick-start-scala/..quick-spark-scala.src.main.scala.com.spark.demo.scala.spark.project.TopNStatJob.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_3/1_SteveFrank_quick-start-scala/..quick-spark-scala.src.main.scala.com.spark.demo.scala.spark.project.TopNStatJob.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_3/1_strohs_SparkEarningsAnalysis/..src.main.scala.com.sae.EarningsTrendDS.scala/udf/25.22.Dataset-EarningsData.filter","Type: org.apache.spark.sql.Dataset[com.sae.datasource.EarningsData]
Call: filter

s""releaseDate BETWEEN '$fourYearsAgo' AND '$endOfLastYear'""
"
"udf/spark_repos_3/1_strohs_SparkEarningsAnalysis/..src.main.scala.com.sae.EarningsTrendDS.scala/udf/30.21.Dataset-EarningsData.map","Type: org.apache.spark.sql.Dataset[com.sae.datasource.EarningsData]
Call: map

ed => ed.qtr -> ed
"
"udf/spark_repos_3/1_strohs_SparkEarningsAnalysis/..src.main.scala.com.sae.EarningsTrendDS.scala/udf/45.24.Dataset-PriceData.filter","Type: org.apache.spark.sql.Dataset[com.sae.datasource.PriceData]
Call: filter

s""date BETWEEN '$begin' AND '${earningsData.releaseDate.minusDays(1)}'""
"
"udf/spark_repos_3/1_strohs_SparkEarningsAnalysis/..src.main.scala.com.sae.EarningsTrendDS.scala/udf/49.24.Dataset-PriceData.filter","Type: org.apache.spark.sql.Dataset[com.sae.datasource.PriceData]
Call: filter

s""date BETWEEN '${earningsData.releaseDate.plusDays(1)}' AND '$end'""
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""same_bucket"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""same_bucket"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/29.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") > distFP
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/33.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") < distFN
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distUDF(col(s""a.$inputCol""), col(s""b.$inputCol"")) < threshold
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/63.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""distCol"") < threshold
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isGoodBucket($""count"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 0.0d
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 4.0d
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/14.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!$""value"".startsWith(options.comment.toString)
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

length(trim($""value"")) > 0
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/113.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/51.23.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/98.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/554.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/558.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/586.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/590.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/904.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/913.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/80.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/1_Syrux_MasterThesis/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/1_tangjun5555_recsys-spark/..src.main.scala.com.github.tangjun5555.recsys.spark.match.DeepWalk.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$ratingColumnName>0.0""
"
"udf/spark_repos_3/1_tangjun5555_recsys-spark/..src.main.scala.com.github.tangjun5555.recsys.spark.match.Node2Vec.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$ratingColumnName>0.0""
"
"udf/spark_repos_3/1_tangjun5555_recsys-spark/..src.main.scala.com.github.tangjun5555.recsys.spark.match.TagPreference.scala/udf/49.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ConcatIdByRank
"
"udf/spark_repos_3/1_tashoyan_sc/..04-big-data-analysis-with-scala-and-spark.04-timeusage.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/127.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, primaryNeeds, work, other)
      }
"
"udf/spark_repos_3/1_tashoyan_sc/..04-big-data-analysis-with-scala-and-spark.04-timeusage.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/137.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, primaryNeeds, work, other)
      }
"
"udf/spark_repos_3/1_vitamin-software_apache-spark-with-scala/..SparkWithScala.src.io.vitamin.spark.DataFrames.scala/udf/26.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[io.vitamin.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_3/1_wyq9894_SZ1901DMP/..src.main.scala.com.tags.TagsContext.scala/udf/56.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

TagsUtils.oneUseId
"
"udf/spark_repos_3/1_Yanbuc_spark-sql-ucerCF-itemCf/..spark.sparkmllib.ItemCf.scala/udf/34.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(item: Seq[String], item2: Seq[String]) => {
        val map = locally {
          val _t_m_p_7 = item
          _t_m_p_7.map(line => {
            val l = line.split(""_"")
            (l(0).toString, l(1).toString.toDouble)
          })
        }.toMap
        val retn = locally {
          val _t_m_p_8 = item2
          _t_m_p_8.filter(line => {
            val l = line.split(""_"")
            map.getOrElse(l(0), -1) == -1
          })
        }
        retn
      }
"
"udf/spark_repos_3/1_Yanbuc_spark-sql-ucerCF-itemCf/..spark.sparkmllib.UserCf.scala/udf/44.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(item1: Seq[String], item2: Seq[String]) => {
        val map = locally {
          val _t_m_p_6 = item1
          _t_m_p_6.map(x => {
            val l = x.split(""_"")
            (l(0).toString, l(1).toDouble)
          })
        }.toMap
        val retn = locally {
          val _t_m_p_7 = item2
          _t_m_p_7.filter(line => {
            val l = line.split(""_"")
            map.getOrElse(l(0).toString, -1) == -1
          })
        }
        retn
      }
"
"udf/spark_repos_3/1_Yanbuc_spark-sql-ucerCF-itemCf/..spark.sparkmllib.UserCf.scala/udf/75.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(cos: Double, items: Seq[String]) => {
        val tu = locally {
          val _t_m_p_10 = items
          _t_m_p_10.map(line => {
            val l = line.split(""_"")
            (l(0).toString, l(1).toString.toDouble * cos)
          })
        }
        tu
      }
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.eg.spark.sales.HashBillAndFindDuplicatesApp.scala/udf/14.29.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

md5HashString
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.eg.spark.sales.HashBillAndFindDuplicatesApp.scala/udf/18.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isEven
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.eg.spark.sales.HashBillAndFindDuplicatesApp.scala/udf/22.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

sha256HashString
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.eg.spark.sales.HashBillAndFindDuplicatesApp.scala/udf/26.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

hash
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.eg.spark.wordcount.WordCountDFApp.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""word"" =!= """"
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.eg.spark.wordcount.WordCountDSApp.scala/udf/34.24.Dataset-LineAndWord.filter","Type: org.apache.spark.sql.Dataset[com.aravind.oss.eg.spark.wordcount.WordCountDSApp.LineAndWord]
Call: filter

_.word != null
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.eg.spark.wordcount.WordCountDSApp.scala/udf/36.22.Dataset-LineAndWord.filter","Type: org.apache.spark.sql.Dataset[com.aravind.oss.eg.spark.wordcount.WordCountDSApp.LineAndWord]
Call: filter

!_.word.isEmpty
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.eg.spark.wordcount.WordCountDSAppV2.scala/udf/35.22.Dataset-Line.filter","Type: org.apache.spark.sql.Dataset[com.aravind.oss.eg.spark.wordcount.WordCountDSAppV2.Line]
Call: filter

!_.word.getOrElse("""").isEmpty
"
"udf/spark_repos_3/1_yaravind_learn-spark/..stackoverflow-complete.src.main.scala.com.aravind.oss.SOApp.scala/udf/18.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/1_yaravind_learn-spark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/119.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, round1(primaryNeeds), round1(work), round1(other))
      }
"
"udf/spark_repos_3/1_YogenRaii_scala-examples/..scala-spark-big-data.assignments.week4.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/90.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

row => TimeUsageRow(row._1._1, row._1._2, row._1._3, round(row._2, 1), round(row._3, 1), round(row._4, 1))
"
"udf/spark_repos_3/1_YogenRaii_scala-examples/..spark-examples.course-examples.udemy-spark.src.main.scala.com.eprogrammerz.examples.spark.DataSetExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => (row(0).asInstanceOf[Int], row(1).asInstanceOf[Double])
      }
"
"udf/spark_repos_3/1_YogenRaii_scala-examples/..spark-examples.course-examples.udemy-spark.src.main.scala.com.eprogrammerz.examples.spark.MovieRecommendationsALS.scala/udf/35.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split('\t')
"
"udf/spark_repos_3/1_YogenRaii_scala-examples/..spark-examples.course-examples.udemy-spark.src.main.scala.com.eprogrammerz.examples.spark.SparkSQLDS.scala/udf/23.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.eprogrammerz.examples.spark.SparkSQLDS.Person]
Call: filter

people(""age"") >= 13 && people(""age"") <= 19
"
"udf/spark_repos_3/1_YogenRaii_scala-examples/..spark-examples.course-examples.wikipedia.src.main.scala.sqlExamples.SparkSQLExample.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/1_YogenRaii_scala-examples/..spark-examples.course-examples.wikipedia.src.main.scala.sqlExamples.SparkSQLExample.scala/udf/47.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager(0).toString
"
"udf/spark_repos_3/1_YogenRaii_scala-examples/..spark-examples.course-examples.wikipedia.src.main.scala.sqlExamples.SparkSQLExample.scala/udf/51.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getAs[String](""name"")
"
"udf/spark_repos_3/1_YogenRaii_scala-examples/..spark-examples.course-examples.wikipedia.src.main.scala.sqlExamples.SparkSQLExample.scala/udf/75.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => attributes(0).toString
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.SQLDataSourceExample.scala/udf/37.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/1_yugecxy_spark-example/..src.main.scala.cn.xiaoyu.example.spark.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_3/1_zhanghuijie0916_SparkProject/..src.main.scala.org.sunny.sparkSqlApp.scala/udf/44.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ""Name:"" + row.getAs(""name"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ArticleAcc.scala/udf/23.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getTimestamp(2).getTime())
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ArticleAcc.scala/udf/25.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getString(0), row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ArticleAcc.scala/udf/45.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getString(1).startsWith(""https://m.dragonpass.com.cn/institute/"") && !row.isNullAt(0) && isYestoday(row.getTimestamp(2).getTime())
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ArticleAcc.scala/udf/47.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getString(0), row.getString(1).split(""/"")(4).split(""\\?"")(0)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ArticleAcc.scala/udf/49.19.Dataset-((String, String), Int).map","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: map

each => Article(each._1._1 + each._1._2, each._1._1, each._1._2, each._2)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.BannerAcc.scala/udf/22.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !getBannerId(row.getString(1)).equals("""") && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.BannerAcc.scala/udf/24.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getString(0), getBannerId(row.getString(1))), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.CIPAcc.scala/udf/25.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getDate(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.CIPAcc.scala/udf/27.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/106.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/108.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/24.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getDate(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/54.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/56.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/58.24.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

row => row._1._2.startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/81.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => isYestoday(row.getTimestamp(7).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/83.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(6).toString, row.getString(17)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.LoungeAcc.scala/udf/85.25.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ParkingAcc.scala/udf/25.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ParkingAcc.scala/udf/27.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ParkingAcc.scala/udf/29.26.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

each => !each._1.equals("""") && !each._2.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ParkingAcc.scala/udf/31.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ParkingAcc.scala/udf/52.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getDate(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ParkingAcc.scala/udf/54.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/101.25.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""R"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/121.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(1) && !row.isNullAt(5) && isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/123.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getInt(1).toString, row.getString(5)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/125.25.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""R"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/22.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/24.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/45.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/47.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/69.33.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-en/restaurant/detail' or url = '/VirtualCard-v5/restaurant/detail' "" + ""or url = '/VirtualCard-v6/restaurant/detail'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/71.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/73.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/75.26.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

each => !each._1.equals("""") && !each._2.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/77.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/97.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.RestAcc.scala/udf/99.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ShopAcc.scala/udf/24.32.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""url = '/VirtualCard-v5/shop/detail' or "" + ""url = '/VirtualCard-v5/shop/getNearbyShare'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ShopAcc.scala/udf/26.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ShopAcc.scala/udf/28.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ShopAcc.scala/udf/30.26.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

each => !each._1.equals("""") && !each._2.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ShopAcc.scala/udf/32.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ShopAcc.scala/udf/52.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ShopAcc.scala/udf/54.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getAs(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.ShopAcc.scala/udf/56.24.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

row => row._1._2.startsWith(""F"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/24.32.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v6/orderCip/listCip' or "" + ""url = '/VirtualCard-v6/trafficsite/list/cip' "" + ""or url = '/VirtualCard-v5/orderCip/getCips' or "" + ""url = '/VirtualCard-v5/orderCip/getCips'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/26.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/28.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/30.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/32.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/53.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2) && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/55.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/79.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsCIPAcc.scala/udf/81.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/113.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/115.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/23.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getDate(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/56.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/58.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/60.26.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

row => row._1._2.startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/62.21.Dataset-((String, String), Int).map","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: map

each => (each._1._1, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/86.31.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => isYestoday(row.getTimestamp(7).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/88.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(6).toString, row.getString(17)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/90.27.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsLoungeAcc.scala/udf/92.22.Dataset-((String, String), Int).map","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: map

each => (each._1._1, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/119.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/121.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/22.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getString(1).startsWith(""https://m.dragonpass.com.cn/institute/"") && !row.isNullAt(0) && isYestoday(row.getTimestamp(2).getTime())
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/24.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(0), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/44.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getTimestamp(2).getTime())
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/46.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(0), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/65.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && getBannerId(row.getString(1)).equals("""") && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/67.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(0), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/89.33.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v6/limousine/carTypeList' or "" + ""url = '/VirtualCard-v6/trafficsite/listHot/limousine' "" + ""or url = '/VirtualCard-v6/trafficsite/list/limousine'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/91.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/93.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/95.27.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsOneAcc.scala/udf/97.22.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsParkingAcc.scala/udf/25.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsParkingAcc.scala/udf/27.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsParkingAcc.scala/udf/29.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsParkingAcc.scala/udf/31.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsParkingAcc.scala/udf/52.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2) && isYestoday(row.getDate(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsParkingAcc.scala/udf/54.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/100.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/102.27.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""R"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/125.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(1) && !row.isNullAt(5) && isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/127.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getInt(1).toString, row.getString(5)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/129.27.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""R"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/22.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/24.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/45.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/47.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/69.33.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-en/restaurant/detail' or url = '/VirtualCard-v5/restaurant/detail' "" + ""or url = '/VirtualCard-v6/restaurant/detail'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/71.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/73.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/75.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/77.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsRestAcc.scala/udf/98.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsShopAcc.scala/udf/23.32.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v5/shop/detail' or "" + ""url = '/VirtualCard-v5/shop/getNearbyShare'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsShopAcc.scala/udf/25.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsShopAcc.scala/udf/27.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsShopAcc.scala/udf/29.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsShopAcc.scala/udf/31.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsShopAcc.scala/udf/51.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsShopAcc.scala/udf/53.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getAs(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsShopAcc.scala/udf/55.26.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

row => row._1._2.startsWith(""F"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/23.32.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v6/share/count/vvip' or "" + ""url = '/VirtualCard-v6/vvip/index' "" + ""or url = '/VirtualCard-v5/vvip/index' or "" + ""url = '/VirtualCard-v5/vvip/setTypes' or "" + ""url = '/VirtualCard-v5/share/getVvipShare'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/25.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(1).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/27.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/29.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

row => !row.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/31.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/51.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/53.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getAs(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/55.26.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""M"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/78.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => isYestoday(row.getTimestamp(2).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.UserTagsVVIPAcc.scala/udf/80.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs(0).toString, (row.getDouble(1), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.VVIPAcc.scala/udf/21.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => isYestoday(row.getTimestamp(4).getTime)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.VVIPAcc.scala/udf/23.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getAs(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.accumulate.VVIPAcc.scala/udf/25.24.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""M"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.article.Article.scala/udf/16.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getString(1).startsWith(""https://m.dragonpass.com.cn/institute/"") && !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.article.Article.scala/udf/18.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getString(0), row.getString(1).split(""/"")(4).split(""\\?"")(0)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.article.Article.scala/udf/29.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.article.Article.scala/udf/31.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getString(0), row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.banner.Banner.scala/udf/26.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !getBannerId(row.getString(1)).equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.banner.Banner.scala/udf/28.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getString(0), getBannerId(row.getString(1))), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.CIP.CIP.scala/udf/18.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.CIP.CIP.scala/udf/20.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), (row.getDouble(2), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.CIP.CIP.scala/udf/34.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.CIP.CIP.scala/udf/36.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/109.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(6).toString, row.getString(17)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/111.25.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/119.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/121.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/33.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/35.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), (row.getLong(2), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/48.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/66.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && row.getString(1).startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/68.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), row.getLong(2))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/80.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-en/lounge/detail' or "" + ""url = '/VirtualCard-v5/lounge/detail' "" + ""or url = '/VirtualCard-v6/lounge/detail' "" + ""or url = '/VirtualCard-v6/lounge/buttonShow'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/82.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/84.27.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

each => !each._1.equals("""") && !each._2.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/86.22.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/95.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getLong(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.lounge.Lounge.scala/udf/97.25.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

row => row._1._2.startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.parking.Parking.scala/udf/28.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.parking.Parking.scala/udf/30.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), (row.getDouble(2), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.parking.Parking.scala/udf/44.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.parking.Parking.scala/udf/46.26.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

each => !each._1.equals("""") && !each._2.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.parking.Parking.scala/udf/48.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.parking.Parking.scala/udf/56.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.parking.Parking.scala/udf/58.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.restaurant.RestaurantPriority.scala/udf/19.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getString(1) == ""zh-cn""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.restaurant.RestaurantPriority.scala/udf/21.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

parseRestaurantPriority
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/109.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(1) && !row.isNullAt(5)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/111.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getInt(1).toString, row.getString(5)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/113.25.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""R"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/33.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/35.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), (row.getDouble(2), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/43.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/45.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/54.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/56.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/68.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/70.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/82.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-en/restaurant/detail' or url = '/VirtualCard-v5/restaurant/detail' "" + ""or url = '/VirtualCard-v6/restaurant/detail'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/84.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/86.27.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

each => !each._1.equals("""") && !each._2.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/88.22.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/97.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getLong(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.rest.Rest.scala/udf/99.25.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""R"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.shop.Shop.scala/udf/26.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v5/shop/detail' or "" + ""url = '/VirtualCard-v5/shop/getNearbyShare'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.shop.Shop.scala/udf/28.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.shop.Shop.scala/udf/30.26.Dataset-(String, String).filter","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: filter

each => !each._1.equals("""") && !each._2.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.shop.Shop.scala/udf/32.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.shop.Shop.scala/udf/42.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getAs(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.shop.Shop.scala/udf/44.24.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

row => row._1._2.startsWith(""F"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserArticleLabels.scala/udf/15.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getString(1).startsWith(""https://m.dragonpass.com.cn/institute/"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserArticleLabels.scala/udf/17.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(1).split(""/"")(4).split(""\\?"")(0), (row.getString(0), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserArticleLabels.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getString(1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserRestaurantPreferences.scala/udf/21.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserRestaurantPreferences.scala/udf/23.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs(0).toString, (row.getString(1).toDouble, 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/20.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/22.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, (row.getDouble(2), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/34.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v6/orderCip/listCip' or "" + ""url = '/VirtualCard-v6/trafficsite/list/cip' "" + ""or url = '/VirtualCard-v5/orderCip/getCips' or "" + ""url = '/VirtualCard-v5/orderCip/getCips'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/36.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/38.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/40.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/48.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/50.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsCIP.scala/udf/61.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/118.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && row.getString(1).startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/120.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(0).toString, row.getString(1)), row.getLong(2))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/130.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/132.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/32.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/34.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, (row.getLong(2), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/48.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-en/lounge/detail' or "" + ""url = '/VirtualCard-v5/lounge/detail' "" + ""or url = '/VirtualCard-v6/lounge/detail' "" + ""or url = '/VirtualCard-v6/lounge/buttonShow'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/50.25.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/52.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/54.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/64.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getLong(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/66.27.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

row => row._1._2.startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/68.22.Dataset-((String, String), Int).map","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: map

each => (each._1._1, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/81.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getLong(6).toString, row.getString(17)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/83.27.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""N"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/85.22.Dataset-((String, String), Int).map","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: map

each => (each._1._1, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsLounge.scala/udf/97.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/30.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getString(1).startsWith(""https://m.dragonpass.com.cn/institute/"") && !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/32.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(0), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/43.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/45.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(0), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/54.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && getBannerId(row.getString(1)).equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/56.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(0), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/67.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs(0).toString, (row.getDouble(1), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/81.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v6/limousine/carTypeList' or "" + ""url = '/VirtualCard-v6/trafficsite/listHot/limousine' "" + ""or url = '/VirtualCard-v6/trafficsite/list/limousine'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/83.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/85.27.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/87.22.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsOne.scala/udf/95.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.userTagsParking.scala/udf/27.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.userTagsParking.scala/udf/29.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, (row.getDouble(2), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.userTagsParking.scala/udf/43.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.userTagsParking.scala/udf/45.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.userTagsParking.scala/udf/47.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.userTagsParking.scala/udf/55.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0) && !row.isNullAt(1) && !row.isNullAt(2)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.userTagsParking.scala/udf/57.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/112.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(1) && !row.isNullAt(5)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/114.26.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => ((row.getInt(1).toString, row.getString(5)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/116.27.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""R"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/32.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/34.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, (row.getDouble(2), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/42.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/44.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/53.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/55.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/67.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.isNullAt(0)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/69.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getLong(0).toString, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/81.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-en/restaurant/detail' or url = '/VirtualCard-v5/restaurant/detail' "" + ""or url = '/VirtualCard-v6/restaurant/detail'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/83.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/85.27.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/87.22.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/97.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getLong(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsRest.scala/udf/99.27.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""R"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsShop.scala/udf/26.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v5/shop/detail' or "" + ""url = '/VirtualCard-v5/shop/getNearbyShare'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsShop.scala/udf/28.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsShop.scala/udf/30.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

each => !each.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsShop.scala/udf/32.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsShop.scala/udf/43.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getAs(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsShop.scala/udf/45.26.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

row => row._1._2.startsWith(""F"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsVVIP.scala/udf/28.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs(0).toString, (row.getDouble(1), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsVVIP.scala/udf/42.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""url = '/VirtualCard-v6/share/count/vvip' or "" + ""url = '/VirtualCard-v6/vvip/index' "" + ""or url = '/VirtualCard-v5/vvip/index' or "" + ""url = '/VirtualCard-v5/vvip/setTypes' or "" + ""url = '/VirtualCard-v5/share/getVvipShare'""
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsVVIP.scala/udf/44.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => parseJson(row.getString(0))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsVVIP.scala/udf/46.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

row => !row.equals("""")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsVVIP.scala/udf/48.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsVVIP.scala/udf/58.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getAs(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsVVIP.scala/udf/60.27.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""M"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.usertags.UserTagsVVIP.scala/udf/70.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs(0).toString, (row.getDouble(1), 1))
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.VVIP.VVIP.scala/udf/14.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ((row.getAs(1).toString, row.getString(3)), 1)
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.cn.adatafun.dataprocess.VVIP.VVIP.scala/udf/16.24.Dataset-((String, String), Int).filter","Type: org.apache.spark.sql.Dataset[((String, String), Int)]
Call: filter

each => each._1._2.startsWith(""M"")
"
"udf/spark_repos_3/1_zhentaowang_data-processing-scala/..src.main.scala.ElasticReader.scala/udf/44.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""userID:"" + attributes(0) + ""|"" + ""isStop:"" + attributes(1)
"
"udf/spark_repos_3/1_zhigang0529_SparkLearn/..src.main.org.apache.spark.examples.SimpleApp.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""and"")
"
"udf/spark_repos_3/1_zhigang0529_SparkLearn/..src.main.org.apache.spark.examples.SimpleApp.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""home"")
"
"udf/spark_repos_3/1_zhoulu312_mlengine/..mlengine-spark.src.main.scala.com.lz.mlengine.spark.Pipeline.scala/udf/57.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

l => (l.getString(0), l.getString(1))
"
"udf/spark_repos_3/1_zhoulu312_mlengine/..mlengine-spark.src.main.scala.com.lz.mlengine.spark.Pipeline.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

l => (l.getString(0), l.getString(1).toDouble)
"
"udf/spark_repos_3/1_zhoulu312_mlengine/..mlengine-spark.src.main.scala.com.lz.mlengine.spark.Trainer.scala/udf/41.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_3/1_zhoulu312_mlengine/..mlengine-spark.src.test.scala.com.lz.mlengine.spark.ModelTestBase.scala/udf/41.21.Dataset-SVMData.map","Type: org.apache.spark.sql.Dataset[com.lz.mlengine.spark.SVMData]
Call: map

row => new TestRawPredictionVector(row.id, model.predictImpl(row.features))
"
"udf/spark_repos_3/1_zhoulu312_mlengine/..mlengine-spark.src.test.scala.com.lz.mlengine.spark.ModelTestBase.scala/udf/55.21.Dataset-SVMData.map","Type: org.apache.spark.sql.Dataset[com.lz.mlengine.spark.SVMData]
Call: map

row => new TestRawPredictionVector(row.id, model.predictImpl(row.features))
"
"udf/spark_repos_3/1_zhoulu312_mlengine/..mlengine-spark.src.test.scala.com.lz.mlengine.spark.ModelTestBase.scala/udf/69.21.Dataset-SVMData.map","Type: org.apache.spark.sql.Dataset[com.lz.mlengine.spark.SVMData]
Call: map

row => new TestProbabilityVector(row.id, model.predictImpl(row.features))
"
"udf/spark_repos_3/1_zhoulu312_mlengine/..mlengine-spark.src.test.scala.com.lz.mlengine.spark.ModelTestBase.scala/udf/83.22.Dataset-SVMData.map","Type: org.apache.spark.sql.Dataset[com.lz.mlengine.spark.SVMData]
Call: map

row => new TestProbabilityVector(row.id, model.predictImpl(row.features))
"
"udf/spark_repos_3/1_zhoulu312_mlengine/..mlengine-spark.src.test.scala.com.lz.mlengine.spark.ModelTestBase.scala/udf/93.20.Dataset-SVMData.map","Type: org.apache.spark.sql.Dataset[com.lz.mlengine.spark.SVMData]
Call: map

row => new TestPredictionScalar(row.id, model.predictImpl(row.features)(0))
"
"udf/spark_repos_3/1_zws0513_bigdata-note/..spark.spark-demo.src.main.scala.com.zw.bigdata.spark.ml.classify.ApplyModel.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: String, features: Vector) =>
          LabeledPoint(label.toDouble, features.toDense)
      }
"
"udf/spark_repos_3/1_zws0513_bigdata-note/..spark.spark-demo.src.main.scala.com.zw.bigdata.spark.ml.classify.NaiveBayesApp.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: String, features: Vector) =>
          LabeledPoint(label.toDouble, features.toDense)
      }
"
"udf/spark_repos_3/1_zws0513_bigdata-note/..spark.spark-demo.src.main.scala.com.zw.bigdata.spark.sql.udaf.ChannelDayDurationUDAF.scala/udf/64.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new ChannelDayDurationUDAF[Int](IntegerType)
"
"udf/spark_repos_3/1_zws0513_bigdata-note/..spark.spark-demo.src.main.scala.com.zw.bigdata.spark.sql.udaf.ChannelMonthDurationUDAF.scala/udf/67.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new ChannelDayDurationUDAF[Int](IntegerType)
"
"udf/spark_repos_3/1_zws0513_bigdata-note/..spark.spark-demo.src.main.scala.com.zw.bigdata.spark.sql.udaf.ChannelMonthDurationUDAF.scala/udf/71.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new ChannelMonthDurationUDAF()
"
"udf/spark_repos_3/1_zws0513_bigdata-note/..spark.spark-demo.src.main.scala.com.zw.bigdata.spark.sql.udaf.NotSpecWordUDAF.scala/udf/51.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new NotSpecWordUDAF()
"
"udf/spark_repos_3/1_zws0513_bigdata-note/..spark.spark-demo.src.main.scala.com.zw.bigdata.spark.sql.udf.MaxUDF.scala/udf/13.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(v1: Int, v2: Int, v3: Int, v4: Int) => v1.max(v2).max(v3).max(v4)
"
"udf/spark_repos_3/20_indix_sparkplug/..src.main.scala.sparkplug.SparkPlug.scala/udf/46.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getAs[Seq[GenericRowWithSchema]](plugDetails.get.column).nonEmpty
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.main.scala.org.apache.spark.ml.feature.Imputer.scala/udf/51.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

ic.isNotNull && ic =!= $(missingValue) && !ic.isNaN
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""same_bucket"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""same_bucket"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/30.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") > distFP
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/34.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") < distFN
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distUDF(col(s""a.$inputCol""), col(s""b.$inputCol"")) < threshold
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/64.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""distCol"") < threshold
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isGoodBucket($""count"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 0.0d
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 4.0d
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.mllib.src.test.scala.org.apache.spark.ml.fpm.FPGrowthSuite.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""freq"") === col(""expectedFreq"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/14.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!$""value"".startsWith(options.comment.toString)
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

length(trim($""value"")) > 0
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/102.24.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/141.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % 2L === 0L
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/156.27.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

func
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/203.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/218.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/49.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

func
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.SQLExecutionSuite.scala/udf/90.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ x => 
        while (!SQLExecutionSuite.canProgress) {
          Thread.sleep(1)
        }
        x
      }
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.SessionStateSuite.scala/udf/43.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: String).length + (_: Int)
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.SessionStateSuite.scala/udf/53.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: Int) + 1
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.core.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/87.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/537.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/541.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/569.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/573.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/887.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/896.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/20_TsinghuaDatabaseGroup_DITA/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/21_alvsanand_spark_recommender/..src.main.scala.es.alvsanand.spark_recommender.parser.DatasetIngestion.scala/udf/42.19.Dataset-ProductReview.map","Type: org.apache.spark.sql.Dataset[es.alvsanand.spark_recommender.model.ProductReview]
Call: map

_.product
"
"udf/spark_repos_3/21_alvsanand_spark_recommender/..src.main.scala.es.alvsanand.spark_recommender.trainer.ALSTrainer.scala/udf/27.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => r.getAs[Int](""id"")
"
"udf/spark_repos_3/21_alvsanand_spark_recommender/..src.main.scala.es.alvsanand.spark_recommender.trainer.ALSTrainer.scala/udf/31.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => r.getAs[Int](""id"")
"
"udf/spark_repos_3/21_alvsanand_spark_recommender/..src.main.scala.es.alvsanand.spark_recommender.trainer.ALSTrainer.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => Rating(row.getAs[Int](0), row.getAs[Int](1), 0)
      }
"
"udf/spark_repos_3/21_alvsanand_spark_recommender/..src.main.scala.es.alvsanand.spark_recommender.trainer.ALSTrainer.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(model.getPredictionCol) > 0 && !col(model.getPredictionCol).isNaN
"
"udf/spark_repos_3/21_alvsanand_spark_recommender/..src.main.scala.es.alvsanand.spark_recommender.trainer.ALSTrainer.scala/udf/73.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.getAs[Int](0) != r.getAs[Int](2)
"
"udf/spark_repos_3/21_alvsanand_spark_recommender/..src.main.scala.es.alvsanand.spark_recommender.trainer.ALSTrainer.scala/udf/75.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ r => 
          val idA = r.getAs[Int](0)
          val idB = r.getAs[Int](2)
          val featuresA = locally {
            val _t_m_p_9 = r.getAs[mutable.WrappedArray[Float]](1)
            _t_m_p_9.map(_.toDouble)
          }.toArray
          val featuresB = locally {
            val _t_m_p_10 = r.getAs[mutable.WrappedArray[Float]](3)
            _t_m_p_10.map(_.toDouble)
          }.toArray
          (idA, idB, cosineSimilarity(new DoubleMatrix(featuresA), new DoubleMatrix(featuresB)))
        }
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.entries.reports.ReporterMain.scala/udf/90.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ModelConverter.toModel(row)
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.executors.EnrichExecutor.scala/udf/41.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val driver = SharedDriver.get
        attemptedCount.add(1)
        Try {
          ModelConverter.toModel(row)
        } match {
          case Success(dplaMapData) =>
            enrich(dplaMapData, driver, improvedCount, successCount)
          case Failure(err) =>
            (null, s""Error parsing mapped data: ${err.getMessage}\n"" + (s""${err.getStackTrace.mkString(""""""
"""""")}""))
        }
      }
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.executors.EnrichExecutor.scala/udf/58.24.Dataset-Row, String).filter","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, String)]
Call: filter

tuple => Option(tuple._1).isDefined
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.executors.EnrichExecutor.scala/udf/60.19.Dataset-Row, String).map","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.Row, String)]
Call: map

tuple => tuple._1
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.executors.JsonlExecutor.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val record = ModelConverter.toModel(row)
        jsonlRecord(record)
      }
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.executors.MappingExecutor.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

oreAggRow => !locally {
        val _t_m_p_6 = oreAggRow.getAs[mutable.WrappedArray[Row]](""messages"")
        _t_m_p_6.map(msg => msg.getString(1))
      }.contains(IngestLogLevel.error)
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.messages.MessageProcessor.scala/udf/40.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
        case Row(msg: String, field: String, count: Long) =>
          MessageFieldRpt(msg, field, count)
      }
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.MetadataCompletenessReport.scala/udf/49.19.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => CompletenessTally(title = tally(dplaMapData.sourceResource.title), collection = tally(dplaMapData.sourceResource.title), description = tally(dplaMapData.sourceResource.description), creator = tally(dplaMapData.sourceResource.creator), publisher = tally(dplaMapData.sourceResource.publisher), contributor = tally(dplaMapData.sourceResource.creator), `type` = tally(dplaMapData.sourceResource.`type`), identifier = tally(dplaMapData.sourceResource.identifier), language = tally(dplaMapData.sourceResource.language), temporal = tally(dplaMapData.sourceResource.temporal), place = tally(dplaMapData.sourceResource.place), subject = tally(dplaMapData.sourceResource.subject), date = tally(dplaMapData.sourceResource.date), extent = tally(dplaMapData.sourceResource.extent), format = tally(dplaMapData.sourceResource.format), relation = tally(dplaMapData.sourceResource.relation), id = tally(Seq(dplaMapData.dplaUri)), dataProvider = tally(Seq(dplaMapData.dataProvider)), provider = tally(Seq(dplaMapData.provider)), preview = tally(Seq(dplaMapData.preview)), rights = {
        val sourceResourceRights = dplaMapData.sourceResource.rights
        val edmRights = dplaMapData.edmRights
        if (sourceResourceRights.nonEmpty || edmRights.nonEmpty) 1 else 0
      }, isShownAt = 0)
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/107.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.place))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/112.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.publisher))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/117.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.relation))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/122.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.replacedBy))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/127.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.replaces))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/132.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.rights))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/137.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.rightsHolder))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/142.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.subject))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/147.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.temporal))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/152.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.title))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/157.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.`type`))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/24.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(Seq(dplaMapData.dataProvider)))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/29.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.edmRights.toSeq))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/34.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.intermediateProvider.toSeq))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/39.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(Seq(dplaMapData.isShownAt.uri)))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/44.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.alternateTitle))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/49.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.collection))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/54.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.contributor))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/59.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.creator))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/64.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.date))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/69.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.description))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/74.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.extent))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/79.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(value = extractValue(dplaMapData.sourceResource.format))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/84.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.genre))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/89.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.identifier))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/94.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(extractValue(dplaMapData.sourceResource.language))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyDistinctValueReport.scala/udf/99.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => PropertyDistinctValueRpt(locally {
            val _t_m_p_17 = dplaMapData.sourceResource.language
            _t_m_p_17.map(l => l.concept.getOrElse(""__MISSING SkosConcept.concept__""))
          })
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/100.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.genre))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/105.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.identifier))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/110.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.language))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/115.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = locally {
            val _t_m_p_19 = oreAggregation.sourceResource.language
            _t_m_p_19.map(l => l.concept.getOrElse(""__MISSING SkosConcept.concept__""))
          })
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/123.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.place))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/128.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.publisher))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/133.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.relation))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/138.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.replacedBy))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/143.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.replaces))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/148.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.rights))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/153.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.rightsHolder))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/158.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.subject))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/163.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.temporal))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/168.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.title))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/173.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.`type`))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/32.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(Seq(oreAggregation.dataProvider)))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/37.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.edmRights.toSeq))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/42.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(Seq(oreAggregation.intermediateProvider)))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/47.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(Seq(oreAggregation.isShownAt.uri.toString)))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/52.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(locally {
            val _t_m_p_6 = oreAggregation.preview
            _t_m_p_6.map(_.uri.toString)
          }.toSeq))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/60.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.alternateTitle))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/65.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.contributor))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/70.23.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.collection))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/75.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.creator))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/80.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.date))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/85.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.description))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/90.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.extent))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.PropertyValueReport.scala/udf/95.24.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

oreAggregation => PropertyValueRpt(dplaUri = oreAggregation.dplaUri.toString, localUri = oreAggregation.isShownAt.uri.toString, value = extractValue(oreAggregation.sourceResource.format))
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.ThumbnailReport.scala/udf/41.19.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => {
        val hasImageType = dplaMapData.sourceResource.`type`.exists(_.toLowerCase() == ""image"")
        val hasPreview = previewUri(dplaMapData).nonEmpty
        MissingThumbnail(localUri(dplaMapData), dplaUri(dplaMapData), hasImageType, hasPreview)
      }
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.ThumbnailReport.scala/udf/59.19.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => {
        val hasPreview = previewUri(dplaMapData).nonEmpty
        val preview: Option[String] = locally {
          val _t_m_p_5 = previewUri(dplaMapData)
          _t_m_p_5.map(_.toString)
        }
        Thumbnail(localUri(dplaMapData), dplaUri(dplaMapData), preview.getOrElse(""""), hasPreview)
      }
"
"udf/spark_repos_3/23_dpla_ingestion3/..src.main.scala.dpla.ingestion3.reports.ThumbnailReport.scala/udf/77.19.Dataset-OreAggregation.map","Type: org.apache.spark.sql.Dataset[dpla.ingestion3.model.OreAggregation]
Call: map

dplaMapData => {
        val uri: Option[URI] = previewUri(dplaMapData)
        val preview: Option[String] = locally {
          val _t_m_p_8 = uri
          _t_m_p_8.map(_.toString)
        }
        val dimensions: Option[Dimensions] = locally {
          val _t_m_p_9 = uri
          _t_m_p_9.map(getDimensions)
        }
        ThumbnailDimensions(localUri(dplaMapData), dplaUri(dplaMapData), preview, dimensions)
      }
"
"udf/spark_repos_3/24_jongwook_spark-ranking-metrics/..src.main.scala.com.github.jongwook.SparkRankingMetrics.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(user: Int, item: Int, prediction: Double) =>
          (user, (item, prediction))
        case Row(user: Int, item: Int, prediction: Float) =>
          (user, (item, prediction.toDouble))
      }
"
"udf/spark_repos_3/24_jongwook_spark-ranking-metrics/..src.main.scala.com.github.jongwook.SparkRankingMetrics.scala/udf/28.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
        case Row(user: Int, item: Int, rating: Double) =>
          (user, (item, rating))
        case Row(user: Int, item: Int, rating: Float) =>
          (user, (item, rating.toDouble))
      }
"
"udf/spark_repos_3/26_jerryshao_spark-hive-streaming-sink/..example.src.main.scala.com.hortonworks.spark.hive.example.HiveStreamingExample.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ s => 
        val records = s.split("","")
        assert(records.length >= 4)
        (records(0).toInt, records(1), records(2), records(3))
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.sparkml.GBTTask.scala/udf/192.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[DenseVector].toArray(1), row.get(3).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.sparkml.GBTTask.scala/udf/200.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(AlgoUtils.FIELD_SEP)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.sparkml.GBTTask.scala/udf/232.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[Int], row.get(3).asInstanceOf[DenseVector].toArray(1), row.get(4).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.sparkml.GBTTask.scala/udf/240.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(AlgoUtils.FIELD_SEP)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.sparkml.RandomForestTask.scala/udf/153.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[DenseVector].toArray(1), row.get(3).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.sparkml.RandomForestTask.scala/udf/161.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(AlgoUtils.FIELD_SEP)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.sparkml.RandomForestTask.scala/udf/193.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[Int], row.get(3).asInstanceOf[DenseVector].toArray(1), row.get(4).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.sparkml.RandomForestTask.scala/udf/201.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(AlgoUtils.FIELD_SEP)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.AlgoUtils.scala/udf/125.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val prediction = row.get(2).asInstanceOf[DenseVector]
        var prob: Double = prediction.values(1)
        var newprob = (0.0d, 0.0d)
        if (prob > 0.0d) {
          newprob = (0.5d - prob / maxPos * 0.5d, prob / maxPos * 0.5d + 0.5d)
        } else {
          prob = prob.abs
          newprob = (prob / maxNeg * 0.5d + 0.5d, 0.5d - prob / maxNeg * 0.5d)
        }
        row.get(0).asInstanceOf[String] + FIELD_SEP + (row.get(1).asInstanceOf[String]) + FIELD_SEP + newprob._1 + FIELD_SEP + newprob._2 + FIELD_SEP + (row.get(3).asInstanceOf[Double])
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.AlgoUtils.scala/udf/175.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val prediction = row.get(3).asInstanceOf[DenseVector]
        var prob: Double = prediction.values(1)
        var newprob = (0.0d, 0.0d)
        if (prob > 0.0d) {
          newprob = (0.5d - prob / maxPos * 0.5d, prob / maxPos * 0.5d + 0.5d)
        } else {
          prob = prob.abs
          newprob = (prob / maxNeg * 0.5d + 0.5d, 0.5d - prob / maxNeg * 0.5d)
        }
        row.get(0).asInstanceOf[String] + FIELD_SEP + (row.get(1).asInstanceOf[String]) + FIELD_SEP + (row.get(2).asInstanceOf[Int]) + FIELD_SEP + newprob._1 + FIELD_SEP + newprob._2 + FIELD_SEP + (row.get(3).asInstanceOf[Double])
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.AlgoUtils.scala/udf/72.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[DenseVector].toArray(1), row.get(3).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.AlgoUtils.scala/udf/88.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[Int], row.get(3).asInstanceOf[DenseVector].toArray(1), row.get(4).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.DataLoadUtils.scala/udf/116.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val tokens = line.mkString(sep).split(sep, -1)
        val features = Vectors.dense(locally {
          val _t_m_p_4 = tokens.drop(1)
          _t_m_p_4.map(toDouble(_))
        })
        (tokens(0), features)
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.DataLoadUtils.scala/udf/136.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val tokens = line.mkString(sep).split(sep, -1)
        val features = Vectors.dense(locally {
          val _t_m_p_6 = tokens.drop(3)
          _t_m_p_6.map(toDouble(_))
        })
        (tokens(0), tokens(1), features)
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.DataLoadUtils.scala/udf/180.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val tokens = line.mkString(sep).split(sep, -1)
        val features = Vectors.dense(locally {
          val _t_m_p_8 = tokens.drop(4)
          _t_m_p_8.map(toDouble(_))
        })
        if (tokens(3).equalsIgnoreCase(""1"")) {
          (tokens(0), tokens(1), 1, features)
        } else {
          (tokens(0), tokens(1), 0, features)
        }
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.DataLoadUtils.scala/udf/204.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val tokens = line.mkString(sep).split(sep, -1)
        val features = Vectors.dense(locally {
          val _t_m_p_10 = tokens.drop(4)
          _t_m_p_10.map(toDouble(_))
        })
        if (tokens(3).equalsIgnoreCase(""1"")) {
          (tokens(0), tokens(1), 1, features)
        } else {
          (tokens(0), tokens(1), 0, features)
        }
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.DataLoadUtils.scala/udf/228.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val tokens = line.mkString(sep).split(sep, -1)
        val features = Vectors.dense(locally {
          val _t_m_p_12 = tokens.drop(2)
          _t_m_p_12.map(toDouble(_))
        })
        if (tokens(1).equalsIgnoreCase(""1"")) {
          (tokens(0), 1, features)
        } else {
          (tokens(0), 0, features)
        }
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.DataLoadUtils.scala/udf/252.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val tokens = line.mkString(sep).split(sep, -1)
        val features = Vectors.dense(locally {
          val _t_m_p_14 = tokens.drop(4)
          _t_m_p_14.map(toDouble(_))
        })
        if (tokens(3).equalsIgnoreCase(""1"")) {
          (tokens(0), tokens(1), 1, features)
        } else {
          (tokens(0), tokens(1), 0, features)
        }
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.utils.DataLoadUtils.scala/udf/96.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

line => {
        val tokens = line.mkString(sep).split(sep, -1)
        val features = Vectors.dense(locally {
          val _t_m_p_2 = tokens.drop(3)
          _t_m_p_2.map(toDouble(_))
        })
        (tokens(0), tokens(1), features)
      }
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkModelStackingMain.scala/udf/110.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[DenseVector].toArray(1), row.get(3).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkModelStackingMain.scala/udf/119.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(sep)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkModelStackingMain.scala/udf/199.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[Int], row.get(3).asInstanceOf[DenseVector].toArray(1), row.get(4).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkUserModelStackingMain.scala/udf/108.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[DenseVector].toArray(1), row.get(2).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkUserModelStackingMain.scala/udf/115.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(sep)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkUserModelStackingMain.scala/udf/192.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[Int], row.get(2).asInstanceOf[DenseVector].toArray(1), row.get(3).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostUserModel.scala/udf/101.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[DenseVector].toArray(1), row.get(2).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostUserModel.scala/udf/108.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(sep)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostUserModel.scala/udf/132.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[Int], row.get(2).asInstanceOf[DenseVector].toArray(1), row.get(3).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostUserModel.scala/udf/140.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(sep)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostUserModel.scala/udf/96.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[DenseVector].toArray(1), row.get(2).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostWithDataFrame.scala/udf/136.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[DenseVector].toArray(1), row.get(3).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostWithDataFrame.scala/udf/143.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[DenseVector].toArray(1), row.get(3).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostWithDataFrame.scala/udf/150.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(sep)
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostWithDataFrame.scala/udf/260.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[Int], row.get(3).asInstanceOf[DenseVector].toArray(1), row.get(4).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostWithDataFrame.scala/udf/290.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).asInstanceOf[String], row.get(1).asInstanceOf[String], row.get(2).asInstanceOf[Int], row.get(3).asInstanceOf[DenseVector].toArray(1), row.get(4).asInstanceOf[Double])
"
"udf/spark_repos_3/27_msjbear_jdata-spark/..src.main.scala.com.sjmei.jdata.xgboost.SparkXgboostWithDataFrame.scala/udf/300.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(sep)
"
"udf/spark_repos_3/29_cloudera_kafka-examples/..StructuredStreamingRefApp.advancedApp.src.main.scala.com.cloudera.streaming.refapp.DataFlows.scala/udf/28.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!'valid_record
"
"udf/spark_repos_3/29_cloudera_kafka-examples/..StructuredStreamingRefApp.advancedApp.src.main.scala.com.cloudera.streaming.refapp.DataFlows.scala/udf/32.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'valid_record
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise11.MachineLearning.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Informativeness"".isNotNull
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise5.ReadAndWrite.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[String](""Platform"") == ""GB""
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise6.Movies.scala/udf/10.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val name = row.getAs[String](""movie_title"")
        val directorName = row.getAs[String](""director_name"")
        val duration = row.getAs[Integer](""duration"")
        val facebookLikes = row.getAs[Integer](""cast_total_facebook_likes"")
        Movie(name, directorName, duration, facebookLikes)
      }
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise6.Movies.scala/udf/29.19.Dataset-Movie.map","Type: org.apache.spark.sql.Dataset[com.github.pedrovgs.sparkplayground.exercise6.Movie]
Call: map

_.facebookLikes
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise6.Movies.scala/udf/58.22.Dataset-Movie.filter","Type: org.apache.spark.sql.Dataset[com.github.pedrovgs.sparkplayground.exercise6.Movie]
Call: filter

movie => broadcastDirectors.value.contains(movie.directorName)
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise6.Movies.scala/udf/68.19.Dataset-Movie.map","Type: org.apache.spark.sql.Dataset[com.github.pedrovgs.sparkplayground.exercise6.Movie]
Call: map

_.facebookLikes.toDouble
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise9.Fifa.scala/udf/19.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!_.getAs[String](""Preffered_Position"").equalsIgnoreCase(""GK"")
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise9.Fifa.scala/udf/21.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getAs[String](""Name"")
"
"udf/spark_repos_3/29_pedrovgs_SparkPlayground/..src.main.scala.com.github.pedrovgs.sparkplayground.exercise9.Fifa.scala/udf/26.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => {
      val club = row.getAs[String](""Club"")
      !club.equalsIgnoreCase(""Free Agent"") && !club.equalsIgnoreCase(""Free Agents"")
    }
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionLoader.scala/udf/42.24.Dataset-EnrichedWithDimension.filter","Type: org.apache.spark.sql.Dataset[DimensionLoader.this.EnrichedWithDimension]
Call: filter

d => !hasType2Changes(d)
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionLoader.scala/udf/44.19.Dataset-EnrichedWithDimension.map","Type: org.apache.spark.sql.Dataset[DimensionLoader.this.EnrichedWithDimension]
Call: map

type1Change
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionLoader.scala/udf/50.24.Dataset-EnrichedWithDimension.filter","Type: org.apache.spark.sql.Dataset[DimensionLoader.this.EnrichedWithDimension]
Call: filter

hasType2Changes _
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionLoader.scala/udf/52.19.Dataset-EnrichedWithDimension.map","Type: org.apache.spark.sql.Dataset[DimensionLoader.this.EnrichedWithDimension]
Call: map

type2Changes
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionLoader.scala/udf/56.19.Dataset-EnrichedWithDimension.map","Type: org.apache.spark.sql.Dataset[DimensionLoader.this.EnrichedWithDimension]
Call: map

_._2
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionLoader.scala/udf/60.19.Dataset-EnrichedWithDimension.map","Type: org.apache.spark.sql.Dataset[DimensionLoader.this.EnrichedWithDimension]
Call: map

_._1
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionOps.scala/udf/24.17.Dataset-EnrichedWithDimension.map","Type: org.apache.spark.sql.Dataset[DimensionOps.this.EnrichedWithDimension]
Call: map

_._2
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionOps.scala/udf/31.17.Dataset-DimensionWithEnriched.map","Type: org.apache.spark.sql.Dataset[DimensionOps.this.DimensionWithEnriched]
Call: map

_._1
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionOps.scala/udf/38.17.Dataset-EnrichedWithDimension.map","Type: org.apache.spark.sql.Dataset[DimensionOps.this.EnrichedWithDimension]
Call: map

_._1
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionOps.scala/udf/45.17.Dataset-DimensionWithEnriched.map","Type: org.apache.spark.sql.Dataset[DimensionOps.this.DimensionWithEnriched]
Call: map

_._2
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionTableProvider.scala/udf/17.20.Dataset-DIM.filter","Type: org.apache.spark.sql.Dataset[DIM]
Call: filter

s""$isCurrentColumnName""
"
"udf/spark_repos_3/2_alghimo_spark-dimensional-modelling/..src.main.scala.org.alghimo.spark.dimensionalModelling.DimensionTableProvider.scala/udf/21.20.Dataset-DIM.filter","Type: org.apache.spark.sql.Dataset[DIM]
Call: filter

s""NOT $isCurrentColumnName""
"
"udf/spark_repos_3/2_CoderLee2014_rec-algos/..src.main.robertlee.rec.ranking.feature.FeatureInspector.scala/udf/123.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

pos(col).isNull || pos(col) === """" || pos(col).isNaN
"
"udf/spark_repos_3/2_CoderLee2014_rec-algos/..src.main.robertlee.rec.ranking.feature.FeatureInspector.scala/udf/127.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

neg_train(col).isNull || neg_train(col) === """" || neg_train(col).isNaN
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/21.22.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""protocol"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/24.22.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""protocol"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/28.22.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""protocol"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/31.22.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""protocol"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/36.24.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""protocol"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/38.22.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""informative""
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/42.22.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""concentration"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/45.22.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""concentration"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/49.22.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""protocol"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/52.23.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""protocol"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/56.23.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""compoundType"" !== """"
"
"udf/spark_repos_3/2_data-intuitive_LuciusAPI/..src.main.scala.com.dataintuitive.luciusapi.functions.StatisticsFunctions.scala/udf/59.23.Dataset-FlatDbRow.filter","Type: org.apache.spark.sql.Dataset[com.dataintuitive.luciusapi.Model.FlatDbRow]
Call: filter

$""compoundType"" !== """"
"
"udf/spark_repos_3/2_DITAS-Project_ehealth-sample-spark-vdc/..app.controllers.EHealthVDCController.scala/udf/137.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => DataFrameUtils.anyNotNull(row, Constants.DATE)
"
"udf/spark_repos_3/2_DITAS-Project_ehealth-sample-spark-vdc/..app.controllers.EHealthVDCController.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => DataFrameUtils.anyNotNull(row)
"
"udf/spark_repos_3/2_DITAS-Project_ehealth-sample-spark-vdc/..app.controllers.EnforcementEngineResponseProcessor.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => DataFrameUtils.anyNotNull(row)
"
"udf/spark_repos_3/2_EnzoBnl_flexible-memoization/..src.test.scala.com.enzobnl.flexiblememoization.GeneralSuite.scala/udf/21.29.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

igniteMemoFibo
"
"udf/spark_repos_3/2_EnzoBnl_flexible-memoization/..src.test.scala.com.enzobnl.flexiblememoization.GeneralSuite.scala/udf/39.29.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mapMemoFibo
"
"udf/spark_repos_3/2_EnzoBnl_flexible-memoization/..src.test.scala.com.enzobnl.flexiblememoization.GeneralSuite.scala/udf/54.29.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mf1
"
"udf/spark_repos_3/2_EnzoBnl_flexible-memoization/..src.test.scala.com.enzobnl.flexiblememoization.GeneralSuite.scala/udf/59.29.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mf2
"
"udf/spark_repos_3/2_itechseeker_Lambda-Architecture-using-SMACK-Stack/..src.main.scala.batch_layer.BatchProcessingActor.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""created_date"" > current_time - tweetDuration.toMillis && $""hashtag"".notEqual(""null"")
"
"udf/spark_repos_3/2_jacopocav_spark-ifs/..src.main.scala.ifs.ml.feature.RowSelector.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

newDS($(outputCol)) === true
"
"udf/spark_repos_3/2_javrasya_yelp-data/..processor.src.test.scala.com.dal.ahmet.yelpdata.processor.utils.YelpDataSampling.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""business_id"".isNotNull && $""user_id"".isNotNull
"
"udf/spark_repos_3/2_javrasya_yelp-data/..processor.src.test.scala.com.dal.ahmet.yelpdata.processor.utils.YelpDataSampling.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""user_id"".isNotNull
"
"udf/spark_repos_3/2_javrasya_yelp-data/..processor.src.test.scala.com.dal.ahmet.yelpdata.processor.utils.YelpDataSampling.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""business_id"".isNotNull
"
"udf/spark_repos_3/2_javrasya_yelp-data/..processor.src.test.scala.com.dal.ahmet.yelpdata.processor.utils.YelpDataSampling.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""business_id"".isNotNull && $""user_id"".isNotNull
"
"udf/spark_repos_3/2_javrasya_yelp-data/..processor.src.test.scala.com.dal.ahmet.yelpdata.processor.utils.YelpDataSampling.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank"" < 1000
"
"udf/spark_repos_3/2_javrasya_yelp-data/..processor.src.test.scala.com.dal.ahmet.yelpdata.processor.utils.YelpDataSampling.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank"" < 10000
"
"udf/spark_repos_3/2_javrasya_yelp-data/..processor.src.test.scala.com.dal.ahmet.yelpdata.processor.utils.YelpDataSampling.scala/udf/66.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank"" < 1000
"
"udf/spark_repos_3/2_javrasya_yelp-data/..processor.src.test.scala.com.dal.ahmet.yelpdata.processor.utils.YelpDataSampling.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""business_id"".isNotNull
"
"udf/spark_repos_3/2_JerryLead_SparkBench/..src.main.scala.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/2_JerryLead_SparkBench/..src.main.scala.sql.SparkSQLExample.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/2_JerryLead_SparkBench/..src.main.scala.sql.SparkSQLExample.scala/udf/39.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/2_JerryLead_SparkBench/..src.main.scala.sql.SparkSQLExample.scala/udf/58.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_3/2_JerryLead_SparkBench/..src.main.scala.sql.SparkSQLExample.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_3/2_JerryLead_SparkBench/..src.main.scala.sql.SparkSQLExample.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_3/2_JerryLead_SparkBench/..src.main.scala.sql.SparkSQLExample.scala/udf/92.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/2_JerryLead_SparkBench/..src.main.scala.sql.SQLDataSourceExample.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/40.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/92.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/2_joost-de-vries_spark-sbt-seed/..src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/2_Justxiaobu_tbl-bigdata/..tbl-bigdata-offline.src.main.scala.com.tbl.offline.process.AnalysisLog.scala/udf/15.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => NoahLogUtil.isValidateLogLine(line)
"
"udf/spark_repos_3/2_Justxiaobu_tbl-bigdata/..tbl-bigdata-offline.src.main.scala.com.tbl.offline.process.AnalysisLog.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val noahlog = NoahLogUtil.parseLogLine(line)
        val protocols = new Gson().fromJson(noahlog.msg.toString, classOf[Protocols])
        logInfo(noahlog.time, noahlog.level, noahlog.msgType, protocols.getPackageNo, protocols.getPathData, protocols.getCommand)
      }
"
"udf/spark_repos_3/2_Justxiaobu_tbl-bigdata/..tbl-bigdata-offline.src.main.scala.com.tbl.offline.process.AnalysisLog.scala/udf/26.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => {
        val pack = s""${r.getString(0)},${r.getString(1)},${r.getString(2)}""
        val command = r.getString(3)
        (pack, command)
      }
"
"udf/spark_repos_3/2_Justxiaobu_tbl-bigdata/..tbl-bigdata-offline.src.main.scala.com.tbl.offline.process.AnalysisLog.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => {
        val time = r.getString(0)
        val msgType = r.getString(1)
        val pack = r.getString(2) + ""_"" + r.getString(3)
        val command = r.getString(4)
        logAnalyse(time, msgType, pack, command)
      }
"
"udf/spark_repos_3/2_Justxiaobu_tbl-bigdata/..tbl-bigdata-offline.src.main.scala.com.tbl.offline.process.AnalysisModuleLog.scala/udf/12.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => NoahModuleLog.isValidateModuleLog(line)
"
"udf/spark_repos_3/2_Justxiaobu_tbl-bigdata/..tbl-bigdata-offline.src.main.scala.com.tbl.offline.process.AnalysisModuleLog.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val modulelog = NoahModuleLog.parseModuleLog(line)
        moduleInfo(modulelog.time, modulelog.moduleId, modulelog.moduleName)
      }
"
"udf/spark_repos_3/2_Justxiaobu_tbl-bigdata/..tbl-bigdata-offline.src.main.scala.com.tbl.offline.process.AnalysisModuleLog.scala/udf/23.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => moduleInfo(r.getString(0), r.getString(1), r.getString(2))
"
"udf/spark_repos_3/2_LeoIV_Germeval_fhdo/..src.main.scala.de.fhdo.germeval.modelling.transformers.RowPruner.scala/udf/11.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

retainCondition
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.dev.audit-release.sbt_app_sql.src.main.scala.SqlApp.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isGoodBucket($""count"")
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 0.0d
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 4.0d
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/113.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/51.23.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/98.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/558.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/562.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/590.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/594.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/78.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..utils4s.spark-dataframe-demo.src.main.scala.cn.thinkjoy.utils4s.spark.dataframe.SparkDataFrameUDFApp.scala/udf/16.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getSourceType(_: String)
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..utils4s.spark-dataframe-demo.src.main.scala.cn.thinkjoy.utils4s.spark.dataframe.SparkDataFrameUDFApp.scala/udf/21.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

lengthLongerThan _
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..utils4s.spark-dataframe-demo.src.main.scala.cn.thinkjoy.utils4s.spark.dataframe.SparkDataFrameUDFApp.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

longLength($""name"", lit(10))
"
"udf/spark_repos_3/2_LukeInkster_ScalaCorpus/..utils4s.spark-dataframe-demo.src.main.scala.cn.thinkjoy.utils4s.spark.dataframe.SparkDataFrameUDFApp.scala/udf/43.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

yearOnYear
"
"udf/spark_repos_3/2_ntent_cc-scala/..src.main.scala.com.ntent.commoncrawl.HostLinksToGraph.scala/udf/59.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

isValidUDF($""name"")
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.FraudDetectionTraining.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.FraudDetectionTraining.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.IntialImportToCassandra.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.IntialImportToCassandra.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.RealTimeFraudDetection.StructuredStreamingFraudDetection.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1.0d
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.RealTimeFraudDetection.StructuredStreamingFraudDetection.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" =!= 1.0d
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.testing.SamplePipeline.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.testing.SamplePipeline.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_3/2_pixipanda_FraudDetection/..src.main.scala.com.datamantra.testing.Streaming.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""partition"".isNotNull
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.spark.jobs.FraudDetectionTraining.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.spark.jobs.FraudDetectionTraining.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.spark.jobs.IntialImportToCassandra.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.spark.jobs.IntialImportToCassandra.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.spark.jobs.RealTimeFraudDetection.StructuredStreamingFraudDetection.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1.0d
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.spark.jobs.RealTimeFraudDetection.StructuredStreamingFraudDetection.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" =!= 1.0d
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.testing.SamplePipeline.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.testing.SamplePipeline.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_3/2_SainathDutkar_Fraud_Transaction_Monitor/..scr.FraudDetection.src.main.scala.com.fraudDetection.testing.Streaming.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""partition"".isNotNull
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Rpt.AppRpt.scala/udf/26.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
              val requestmode: Int = row.getAs[Int](""requestmode"")
              val processnode: Int = row.getAs[Int](""processnode"")
              val iseffective: Int = row.getAs[Int](""iseffective"")
              val isbilling: Int = row.getAs[Int](""isbilling"")
              val isbid: Int = row.getAs[Int](""isbid"")
              val iswin: Int = row.getAs[Int](""iswin"")
              val adorderid: Int = row.getAs[Int](""adorderid"")
              val winprice: Double = row.getAs[Double](""winprice"")
              val adpayment: Double = row.getAs[Double](""adpayment"")
              val appid: String = row.getAs[String](""appid"")
              val appname: String = row.getAs[String](""appname"")
              val request = com.utils.RptUtils2.request(requestmode, processnode)
              val click = com.utils.RptUtils2.click(requestmode, iseffective)
              val ad = com.utils.RptUtils2.AD(iseffective, isbilling, isbid, iswin, adorderid, winprice, adpayment)
              (appid, appname, request ++ click ++ ad)
            }
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Rpt.ClientRpt.scala/udf/13.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val requestmode: Int = row.getAs[Int](""requestmode"")
          val processnode: Int = row.getAs[Int](""processnode"")
          val iseffective: Int = row.getAs[Int](""iseffective"")
          val isbilling: Int = row.getAs[Int](""isbilling"")
          val isbid: Int = row.getAs[Int](""isbid"")
          val iswin: Int = row.getAs[Int](""iswin"")
          val adorderid: Int = row.getAs[Int](""adorderid"")
          val winprice: Double = row.getAs[Double](""winprice"")
          val adpayment: Double = row.getAs[Double](""adpayment"")
          val client: Int = row.getAs[Int](""client"")
          val request = com.utils.RptUtils2.request(requestmode, processnode)
          val click = com.utils.RptUtils2.click(requestmode, iseffective)
          val ad = com.utils.RptUtils2.AD(iseffective, isbilling, isbid, iswin, adorderid, winprice, adpayment)
          (client, request ++ click ++ ad)
        }
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Rpt.DeviceRpt.scala/udf/13.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val requestmode: Int = row.getAs[Int](""requestmode"")
          val processnode: Int = row.getAs[Int](""processnode"")
          val iseffective: Int = row.getAs[Int](""iseffective"")
          val isbilling: Int = row.getAs[Int](""isbilling"")
          val isbid: Int = row.getAs[Int](""isbid"")
          val iswin: Int = row.getAs[Int](""iswin"")
          val adorderid: Int = row.getAs[Int](""adorderid"")
          val winprice: Double = row.getAs[Double](""winprice"")
          val adpayment: Double = row.getAs[Double](""adpayment"")
          val device: String = row.getAs[String](""device"")
          val request = com.utils.RptUtils2.request(requestmode, processnode)
          val click = com.utils.RptUtils2.click(requestmode, iseffective)
          val ad = com.utils.RptUtils2.AD(iseffective, isbilling, isbid, iswin, adorderid, winprice, adpayment)
          (device, request ++ click ++ ad)
        }
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Rpt.LocationRpt.scala/udf/24.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
            val requestmode: Int = row.getAs[Int](""requestmode"")
            val processnode: Int = row.getAs[Int](""processnode"")
            val iseffective: Int = row.getAs[Int](""iseffective"")
            val isbilling: Int = row.getAs[Int](""isbilling"")
            val isbid: Int = row.getAs[Int](""isbid"")
            val iswin: Int = row.getAs[Int](""iswin"")
            val adorderid: Int = row.getAs[Int](""adorderid"")
            val winprice: Double = row.getAs[Double](""winprice"")
            val adpayment: Double = row.getAs[Double](""adpayment"")
            val provincename: String = row.getAs[String](""provincename"")
            val cityname: String = row.getAs[String](""cityname"")
            val request = com.utils.RptUtils.request(requestmode, processnode)
            val click = com.utils.RptUtils.click(requestmode, iseffective)
            val ad = com.utils.RptUtils.AD(iseffective, isbilling, isbid, iswin, adorderid, winprice, adpayment)
            ((provincename, cityname), request, click, ad)
          }
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Rpt.NetworkRpt.scala/udf/13.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val requestmode: Int = row.getAs[Int](""requestmode"")
          val processnode: Int = row.getAs[Int](""processnode"")
          val iseffective: Int = row.getAs[Int](""iseffective"")
          val isbilling: Int = row.getAs[Int](""isbilling"")
          val isbid: Int = row.getAs[Int](""isbid"")
          val iswin: Int = row.getAs[Int](""iswin"")
          val adorderid: Int = row.getAs[Int](""adorderid"")
          val winprice: Double = row.getAs[Double](""winprice"")
          val adpayment: Double = row.getAs[Double](""adpayment"")
          val networkmannername: String = row.getAs[String](""networkmannername"")
          val request = com.utils.RptUtils2.request(requestmode, processnode)
          val click = com.utils.RptUtils2.click(requestmode, iseffective)
          val ad = com.utils.RptUtils2.AD(iseffective, isbilling, isbid, iswin, adorderid, winprice, adpayment)
          (networkmannername, request ++ click ++ ad)
        }
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Rpt.OperatorRpt.scala/udf/20.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val requestmode: Int = row.getAs[Int](""requestmode"")
          val processnode: Int = row.getAs[Int](""processnode"")
          val iseffective: Int = row.getAs[Int](""iseffective"")
          val isbilling: Int = row.getAs[Int](""isbilling"")
          val isbid: Int = row.getAs[Int](""isbid"")
          val iswin: Int = row.getAs[Int](""iswin"")
          val adorderid: Int = row.getAs[Int](""adorderid"")
          val winprice: Double = row.getAs[Double](""winprice"")
          val adpayment: Double = row.getAs[Double](""adpayment"")
          val ispname: String = row.getAs[String](""ispname"")
          val request = com.utils.RptUtils2.request(requestmode, processnode)
          val click = com.utils.RptUtils2.click(requestmode, iseffective)
          val ad = com.utils.RptUtils2.AD(iseffective, isbilling, isbid, iswin, adorderid, winprice, adpayment)
          (ispname, request ++ click ++ ad)
        }
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Tags.TagsContext_2.scala/udf/35.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

TagUtils.OneUserId
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Tags.TagsContext.scala/udf/62.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

TagUtils.OneUserId
"
"udf/spark_repos_3/2_SmorSmor_DMP/..src.main.scala.com.Tags.TagsContext.scala/udf/64.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val userId = TagUtils.getOneUserId(row)
        val adList: List[(String, Int)] = TagsAD.makeTags(row)
        val appList: List[(String, Int)] = TagsApp.makeTags(row, appInfo.value)
        val channelList: List[(String, Int)] = TagsChannel.makeTags(row)
        val deviceList: List[(String, Int)] = TagsDevice.makeTags(row)
        val keywordsList: List[(String, Int)] = TagsKeywords.makeTags(row, stopInfo.value)
        val proAndCityList: List[(String, Int)] = TagsProAndCity.makeTags(row)
        (userId, adList ++ appList ++ channelList ++ deviceList ++ keywordsList ++ proAndCityList)
      }
"
"udf/spark_repos_3/2_thbaymet_simple-datalake/..src.main.scala.thbaymet.github.io.metadata.FollowJsonMetadata.scala/udf/20.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDFs.columnSplit
"
"udf/spark_repos_3/2_thbaymet_simple-datalake/..src.main.scala.thbaymet.github.io.SimpleDatalake.scala/udf/17.20.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[thbaymet.github.io.models.User]
Call: filter

_.isAmerican
"
"udf/spark_repos_3/2_thbaymet_simple-datalake/..src.main.scala.thbaymet.github.io.SimpleDatalake.scala/udf/23.20.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[thbaymet.github.io.models.User]
Call: filter

!_.isMale
"
"udf/spark_repos_3/2_tmoerman_brassica/..src.main.scala.org.aertslab.grnboost.GRNBoost.scala/udf/126.22.Dataset-ExpressionByGene.filter","Type: org.apache.spark.sql.Dataset[org.aertslab.grnboost.ExpressionByGene]
Call: filter

e => targets contains e.gene
"
"udf/spark_repos_3/2_tmoerman_brassica/..src.main.scala.org.aertslab.grnboost.GRNBoost.scala/udf/138.22.Dataset-Regulation.filter","Type: org.apache.spark.sql.Dataset[org.aertslab.grnboost.Regulation]
Call: filter

$""include"" === 1
"
"udf/spark_repos_3/2_tmoerman_brassica/..src.main.scala.org.aertslab.grnboost.GRNBoost.scala/udf/98.24.Dataset-ExpressionByGene.filter","Type: org.apache.spark.sql.Dataset[org.aertslab.grnboost.ExpressionByGene]
Call: filter

e => estimationTargets contains e.gene
"
"udf/spark_repos_3/2_tmoerman_brassica/..src.main.scala.org.aertslab.grnboost.util.RankUtils.scala/udf/41.24.Dataset-ExpressionByGene.filter","Type: org.apache.spark.sql.Dataset[org.aertslab.grnboost.ExpressionByGene]
Call: filter

e => tfs.contains(e.gene)
"
"udf/spark_repos_3/2_vaishalilambe_Team7_Santander_Product_Recommendation/..data-cleaning-app.src.main.scala.edu.neu.coe.csye7200.prodrec.dataclean.pipeline.Pipeline.scala/udf/14.28.Dataset-SantanderRecord.filter","Type: org.apache.spark.sql.Dataset[edu.neu.coe.csye7200.prodrec.dataclean.model.SantanderRecord]
Call: filter

d => d.customerInfo.code != None
"
"udf/spark_repos_3/2_vaishalilambe_Team7_Santander_Product_Recommendation/..data-cleaning-app.src.main.scala.edu.neu.coe.csye7200.prodrec.dataclean.pipeline.Pipeline.scala/udf/16.26.Dataset-SantanderRecord.filter","Type: org.apache.spark.sql.Dataset[edu.neu.coe.csye7200.prodrec.dataclean.model.SantanderRecord]
Call: filter

d => d.productInfo.product != ""[]""
"
"udf/spark_repos_3/2_vaishalilambe_Team7_Santander_Product_Recommendation/..data-cleaning-app.src.main.scala.edu.neu.coe.csye7200.prodrec.dataclean.pipeline.Pipeline.scala/udf/18.24.Dataset-SantanderRecord.filter","Type: org.apache.spark.sql.Dataset[edu.neu.coe.csye7200.prodrec.dataclean.model.SantanderRecord]
Call: filter

d => d.accountInfo.seniority.getOrElse(0) >= 0
"
"udf/spark_repos_3/2_vaishalilambe_Team7_Santander_Product_Recommendation/..data-cleaning-app.src.main.scala.edu.neu.coe.csye7200.prodrec.dataclean.pipeline.Pipeline.scala/udf/20.19.Dataset-SantanderRecord.map","Type: org.apache.spark.sql.Dataset[edu.neu.coe.csye7200.prodrec.dataclean.model.SantanderRecord]
Call: map

d => if (d.customerInfo.age.getOrElse(0) > 100) {
        new SantanderRecord(Customer(d.customerInfo.code, d.customerInfo.employmentStatus, d.customerInfo.countryOfResidence, d.customerInfo.gender, None, d.customerInfo.income), d.accountInfo, d.productInfo)
      } else d
"
"udf/spark_repos_3/2_woniu183_hive_table_monitor/..hive_table_monitor-master.src.main.scala.service.CollectDataMarketHiveTableSize.scala/udf/146.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""asrc"".contains($""src"")
"
"udf/spark_repos_3/2_woniu183_hive_table_monitor/..hive_table_monitor-master.src.main.scala.service.CollectHiveTableSize.scala/udf/145.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""asrc"".contains($""src"")
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw3.Top100Author.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""year"") >= 2000 && (df(""url"").contains(FILTER_FIRST) || df(""url"").contains(FILTER_SECOND) || df(""url"").contains(FILTER_THIRD))
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw3.Top100AuthorV2.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""year"") >= 2000 && (df(""url"").contains(FILTER_FIRST) || df(""url"").contains(FILTER_SECOND) || df(""url"").contains(FILTER_THIRD))
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw3.Top100AuthorV3.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""year"") >= 2000 && (df(""url"").contains(FILTER_FIRST) || df(""url"").contains(FILTER_SECOND) || df(""url"").contains(FILTER_THIRD))
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw3.Top100Keywords.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""year"") >= 2000 && locally {
        val _t_m_p_2 = conferences
        _t_m_p_2.map(x => df(""url"").contains(x))
      }.reduceLeft(_ || _)
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw3.Top100KeywordsV2.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""year"") >= 2000 && (df(""url"").contains(FILTER_FIRST) || df(""url"").contains(FILTER_SECOND) || df(""url"").contains(FILTER_THIRD))
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.FirstSection.scala/udf/23.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""author"").cast(StringType).contains(AUTHOR)
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.FirstSection.scala/udf/25.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(""year"").isNotNull
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.FirstSectionV3.scala/udf/37.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
            row(1).toString.contains(AUTHOR)
          } else false
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.FirstSectionV4.scala/udf/41.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
            row(1).toString.toLowerCase.contains(AUTHOR.toLowerCase)
          } else false
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.HW4.scala/udf/112.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
          row(1).toString.toLowerCase.contains(str.trim.toLowerCase())
        } else false
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.HW4.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
        row(1).toString.toLowerCase.contains(AUTHOR.toLowerCase)
      } else false
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.HW4.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
        row(1).toString.toLowerCase.contains(AUTHOR.toLowerCase)
      } else false
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.SecondSection.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""year"").isNotNull && df(""author"").cast(StringType).contains(AUTHOR)
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.SecondSectionV3.scala/udf/38.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
            row(1).toString.contains(AUTHOR)
          } else false
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.SecondSectionV4.scala/udf/42.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
            row(1).toString.toLowerCase.contains(AUTHOR.toLowerCase)
          } else false
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.ThirdSection.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""year"").isNotNull
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.ThirdSection.scala/udf/31.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(""author"").cast(StringType).contains(str.trim)
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.ThirdSectionV3.scala/udf/43.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
              row(1).toString.contains(str.trim)
            } else false
"
"udf/spark_repos_3/2_wzes_Distributed-system-project/..src.main.scala.com.distributed.application.hw4.ThirdSectionV4.scala/udf/47.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => if (row(1) != null) {
              row(1).toString.toLowerCase.contains(str.trim.toLowerCase())
            } else false
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..SparkScala.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.SparkBasic.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[SparkBasic.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.SparkDemo.SimpleApp.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.SparkDemo.SimpleApp.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.WarDataDigest.BattleDigest.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""battle_id"".isNotNull
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.WarDataDigest.SessionDigest.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""session_id"".isNotNull
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.WarDataDigest.WarDataOverview.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

gameLogDF(""user_server_region"") === ""US""
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.WarDataDigest.WarDataOverview.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

gameLogDF(""platform"") === ""IOS_TABLET""
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.WarDataDigest.WarDataOverview.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""outcome"".isNull
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.WarDataDigest.WarDataOverview.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

gameLogDF(""user_server_region"") === ""US""
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.WarDataDigest.WarDataOverview.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

gameLogDF(""user_server_region"") === ""EU""
"
"udf/spark_repos_3/2_yennanliu_utility_Scala/..src.main.scala.WarDataDigest.WarDataOverview.scala/udf/48.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

gameLogDF(""event_name"") === event
"
"udf/spark_repos_3/2_zcdJason_ML/..sparkML.src.main.scala.com.niuniuzcd.demo.ml.transformer.WOEIVEncoder.scala/udf/23.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(s""$col"") > t._1 && df(s""$col"") <= t._2
"
"udf/spark_repos_3/2_zcdJason_ML/..sparkML.src.main.scala.com.niuniuzcd.demo.ml.transformer.WOEIVEncoder.scala/udf/28.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(s""$col"") > t._1 && df(s""$col"") <= t._2
"
"udf/spark_repos_3/2_zcdJason_ML/..sparkML.src.test.scala.ml.feature.encoder.BaseEncoderTest.scala/udf/60.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

timeFormat
"
"udf/spark_repos_3/2_zcdJason_ML/..sparkML.src.test.scala.sta.StaTestDFGroupyByKey.scala/udf/40.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ContactRowsUDF
"
"udf/spark_repos_3/2_zcdJason_ML/..sparkML.src.test.scala.sta.TestKS.scala/udf/35.28.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AggKs
"
"udf/spark_repos_3/2_zcdJason_ML/..sparkML.src.test.scala.sta.TestKS.scala/udf/81.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(c).isNotNull
"
"udf/spark_repos_3/2_zcdJason_ML/..sparkML.src.test.scala.sta.TestKS.scala/udf/83.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => x.getDouble(0)
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/15.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

replaceEmptyStr _
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x == 1900
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x != 1900
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.convertYear _
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieData.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.convertYear _
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.RatingData.scala/udf/49.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getCurrentHour _
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.RatingData.scala/udf/55.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

assignTod _
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter05.2.0.0.scala-spark-app.src.main.scala.com.spark.recommendation.FeatureExtraction.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter08.scala.2.0.0.src.main.scala.org.sparksamples.kmeans.BisectingKMeans.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""prediction == "" + i
"
"udf/spark_repos_3/3_clojure-spark_Machine-Learning-with-Spark-Second-Edition/..Chapter08.scala.2.0.0.src.main.scala.org.sparksamples.kmeans.MovieLensKMeans.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""prediction == "" + i
"
"udf/spark_repos_3/3_tmcgrath_scala-for-spark/..src.main.scala.com.supergloo.Skeleton.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

i => i.getInt(0) + 1
"
"udf/spark_repos_3/42_qubole_spark-acid/..src.main.scala.com.qubole.spark.hiveacid.HiveAcidTable.scala/udf/134.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

resolvedExpr.sql
"
"udf/spark_repos_3/42_qubole_spark-acid/..src.main.scala.com.qubole.spark.hiveacid.HiveAcidTable.scala/udf/194.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

resolvedExpr.sql
"
"udf/spark_repos_3/43_YCG09_xgbspark-text-classification/..src.main.scala.com.lenovo.ml.DataPreprocess.scala/udf/21.29.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.toString
"
"udf/spark_repos_3/43_YCG09_xgbspark-text-classification/..src.main.scala.com.lenovo.ml.DataPreprocess.scala/udf/23.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.substring(1, x.length - 1).toLowerCase
"
"udf/spark_repos_3/43_YCG09_xgbspark-text-classification/..src.main.scala.com.lenovo.ml.DataPreprocess.scala/udf/25.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => regex1.replaceAllIn(x, """")
"
"udf/spark_repos_3/43_YCG09_xgbspark-text-classification/..src.main.scala.com.lenovo.ml.DataPreprocess.scala/udf/27.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => regex2.replaceAllIn(x, """")
"
"udf/spark_repos_3/43_YCG09_xgbspark-text-classification/..src.main.scala.com.lenovo.ml.DataPreprocess.scala/udf/29.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => regex3.replaceAllIn(x, """")
"
"udf/spark_repos_3/43_YCG09_xgbspark-text-classification/..src.main.scala.com.lenovo.ml.DataPreprocess.scala/udf/31.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => regex4.replaceAllIn(x, """")
"
"udf/spark_repos_3/43_YCG09_xgbspark-text-classification/..src.main.scala.com.lenovo.ml.DataPreprocess.scala/udf/81.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ x => 
        val parse = DicAnalysis.parse(x, dic.value).recognition(stop.value)
        val words = for (i <- Range(0, parse.size())) yield parse.get(i).getName
        val filterWords = locally {
          val _t_m_p_13 = locally {
            val _t_m_p_14 = words
            _t_m_p_14.map(_.trim)
          }
          _t_m_p_13.filter(x => x.length > 1 || single.value.contains(x))
        }
        locally {
          val _t_m_p_15 = filterWords
          _t_m_p_15.map(x => if (synonym.value.contains(x)) synonym.value(x) else x)
        }.mkString("" "")
      }
"
"udf/spark_repos_3/45_godatadriven_iterative-broadcast-join/..src.main.scala.com.godatadriven.generator.SkewedDataGenerator.scala/udf/18.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

Key
"
"udf/spark_repos_3/45_godatadriven_iterative-broadcast-join/..src.main.scala.com.godatadriven.generator.UniformDataGenerator.scala/udf/24.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

Key
"
"udf/spark_repos_3/45_godatadriven_iterative-broadcast-join/..src.main.scala.com.godatadriven.join.IterativeBroadcastJoin.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""pass"") === lit(iteration)
"
"udf/spark_repos_3/46_CoxAutomotiveDataSolutions_waimak/..waimak-rdbm-ingestion.src.main.scala.com.coxautodata.waimak.rdbm.ingestion.RDBMIngestionUtils.scala/udf/50.26.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""${temporalTableMetadata.startColName.get}"" <= snapshotTimestamp
"
"udf/spark_repos_3/46_CoxAutomotiveDataSolutions_waimak/..waimak-rdbm-ingestion.src.main.scala.com.coxautodata.waimak.rdbm.ingestion.RDBMIngestionUtils.scala/udf/52.24.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""${temporalTableMetadata.startColName.get}"" =!= ($""${temporalTableMetadata.endColName.get}"")
"
"udf/spark_repos_3/46_CoxAutomotiveDataSolutions_waimak/..waimak-rdbm-ingestion.src.main.scala.com.coxautodata.waimak.rdbm.ingestion.RDBMIngestionUtils.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""_row_num"" === 1 && lit(snapshotTimestamp) >= ($""${temporalTableMetadata.startColName.get}"") && lit(snapshotTimestamp) < ($""${temporalTableMetadata.endColName.get}"")
"
"udf/spark_repos_3/46_CoxAutomotiveDataSolutions_waimak/..waimak-storage.src.main.scala.com.coxautodata.waimak.storage.AuditTableFile.scala/udf/118.31.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

rows(STORE_TYPE_COLUMN).isin(HOT_PARTITION, COLD_PARTITION) && rows(STORE_REGION_COLUMN).isin(ids: _*)
"
"udf/spark_repos_3/46_CoxAutomotiveDataSolutions_waimak/..waimak-storage.src.main.scala.com.coxautodata.waimak.storage.AuditTableFile.scala/udf/122.31.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

auditRows(DE_LAST_UPDATED_COLUMN).between(from.getOrElse(lowTimestamp), to.getOrElse(highTimestamp))
"
"udf/spark_repos_3/46_CoxAutomotiveDataSolutions_waimak/..waimak-storage.src.main.scala.com.coxautodata.waimak.storage.AuditTableFile.scala/udf/333.39.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

df(STORE_TYPE_COLUMN).isin(HOT_PARTITION, COLD_PARTITION)
"
"udf/spark_repos_3/46_CoxAutomotiveDataSolutions_waimak/..waimak-storage.src.main.scala.com.coxautodata.waimak.storage.AuditTableFile.scala/udf/336.39.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

df(STORE_TYPE_COLUMN).isin(COLD_PARTITION)
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.api.ExtendedDataframe.scala/udf/20.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => s""${r(0)}""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.api.ExtendedDataframe.scala/udf/38.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => s""${r(0)}""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.dataflow.ComputeService.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" mode in ('derived')""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.dataflow.ComputeService.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""object_name = '$referenceObject' and storage_type = 'database' and mode = 'derived'""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.dataflow.Runner.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""mode = 'base'""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/117.23.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.id == id
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/120.23.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.id == id
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/124.23.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.id == id
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/130.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""id = '"" + idLookup + ""'""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/137.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""name = '"" + name + ""'""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/147.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""id = '$schemaId' ""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""name = '$groupName'""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/27.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      r => s""${r(0)}_${r(1)}""
    }
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""object_name = '${dsAlternateKey._1}' and data_persistence_src_id = '${dsAlternateKey._2}""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/48.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      r => s""${r(0)}_${r(1)}_${r(2)}""
    }
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""object_name = '$objectName' and storage_type = '$storageType'""
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/57.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      r => s""${r(0)}""
    }
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/64.22.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.object_ref == objectRef
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/68.22.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.object_ref == objectRef
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/71.22.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.object_ref == objectRef
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/81.23.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.id == id
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/85.23.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.id == id
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/95.23.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.object_name == objectName && r.data_persistence_src_id == DataPersistenceId
"
"udf/spark_repos_3/4_nhachem_stitchr/..core.src.main.scala.com.stitchr.core.registry.RegistryService.scala/udf/99.23.Dataset-DataSet.filter","Type: org.apache.spark.sql.Dataset[com.stitchr.core.common.Encoders.DataSet]
Call: filter

r => r.object_name == objectName
"
"udf/spark_repos_3/4_organ-nm_SZ1901DMP/..src.main.scala.com.tags.TagsContext.scala/udf/54.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

TagsUtils.oneUserId
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.completeness.ProfilePerDataProvider.scala/udf/30.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""provider == $pid""
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.completeness.ProfilePerDataProvider.scala/udf/32.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ r => 
          var fields = new ListBuffer[Int]()
          for (i <- range) {
            var value = r.getInt(i)
            if (value == 1) {
              fields += i
            }
          }
          fields.toList.mkString("","")
        }
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.completeness.ProfilePerDataProvider.scala/udf/62.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => simplenames(row.getInt(0))
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.completeness.ProfilePerDataProviderWithMapping.scala/udf/61.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        var id = row.getAs[String](""id"")
        var count = row.getAs[Long](""count"")
        var total = row.getAs[Long](""total"")
        var percent = row.getAs[Double](""percent"")
        var pattern = resolveFieldAbbreviation(row.getAs[String](""pattern""))
        (id, pattern, pattern.split("";"").length, count, total, percent)
      }
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.completeness.ProxyBasedCompletenessFromParquet.scala/udf/110.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""value"" > 0.0d
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.completeness.ProxyBasedCompletenessFromParquet.scala/udf/119.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""value"" > 0.0d
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.profiles.ProfilesForAllIds.scala/udf/107.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => {
        var fieldInts = x.getAs[String](""fields"").split("","")
        var fields = new ListBuffer[String]()
        for (i <- fieldInts) {
          fields += fieldMap.getOrElse(i.toInt, """")
        }
        var id = x.getAs[String](""id"")
        var total = collectionSizeMap.getOrElse(id, 0).asInstanceOf[Number].longValue
        var count = x.getAs[Long](""count"")
        var percent = count * 100.0d / total
        (id, fields.mkString("";""), fields.length, count, percent)
      }
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.profiles.ProfilesForAllIds.scala/udf/134.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => (x.getAs[String](""id""), fieldMap.getOrElse(x.getAs[Int](""field""), """") + ""="" + x.getAs[Long](""count"").toString)
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.profiles.ProfilesForAllIds.scala/udf/136.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getAs[String](""id""), x.getSeq(1).mkString("",""))
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.profiles.ProfilesForAllIds.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getAs[String](0).replace(""PROVIDER_Proxy_"", """").replace(""_"", "":""), x.getAs[Int](1))
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.profiles.ProfilesForAllIds.scala/udf/94.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getAs[String](""value"").split(""@""), x.getAs[Long](""count""))
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.profiles.Profiles.scala/udf/29.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""provider == $pid""
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.profiles.Profiles.scala/udf/31.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ r => 
          var fields = new ListBuffer[Int]()
          for (i <- range) {
            var value = r.getInt(i)
            if (value == 1) {
              fields += i
            }
          }
          fields.toList.mkString("","")
        }
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.profiles.Profiles.scala/udf/61.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => simplenames(row.getInt(0))
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.MultilingualityWithHistogramWithMapReduce.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.getAs[Double](""value"") != -1.0d
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationFieldContribution.scala/udf/142.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""field"") =!= ""fake""
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationFieldContribution.scala/udf/48.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
        var fieldNames = r.schema.fieldNames
        var prTagged = 0
        var prTotal = 0
        var euTagged = 0
        var euTotal = 0
        for (i <- 0 until r.size) {
          val isPr = fieldNames(i).startsWith(""provider_"")
          val value = r.getInt(i)
          if (value > -1) {
            if (isPr) {
              prTotal = prTotal + 1
            } else {
              euTotal = euTotal + 1
            }
            if (value > 0) {
              if (isPr) prTagged = prTagged + 1 else euTagged = euTagged + 1
            }
          }
        }
        val prPerc = if (prTotal == 0) 0.0d else prTagged / prTotal
        val euPerc = if (euTotal == 0) 0.0d else euTagged / euTotal
        (prTagged, prTotal, prPerc, euTagged, euTotal, euPerc)
      }
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationFieldContribution.scala/udf/97.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""start"" <= l && $""end"" >= l
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationStat.scala/udf/46.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.getDouble(0) > 0
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationStat.scala/udf/59.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.getInt(0) > 0
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForAllCollections.scala/udf/114.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(filterField) > -1
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForAllCollections.scala/udf/138.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => row.getInt(0)
      }
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForAllCollections.scala/udf/146.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""dataset"" === id
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForAllCollections.scala/udf/69.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""start"" <= l && $""end"" >= l
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForAllCollections.scala/udf/96.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""field"") =!= ""fake""
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForAll.scala/udf/121.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(filterField) > -1
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForAll.scala/udf/160.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""field"") =!= ""fake""
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForAll.scala/udf/76.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""start"" <= l && $""end"" >= l
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForLight.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""field"") =!= ""fake""
"
"udf/spark_repos_3/4_pkiraly_europeana-qa-spark/..scala.src.main.scala.de.gwdg.europeanaqa.spark.saturation.SaturationWithHistogramForLight.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""start"" <= l && $""end"" >= l
"
"udf/spark_repos_3/4_rockthejvm_spark-optimization/..src.main.scala.part1recap.SparkRecap.scala/udf/20.17.Dataset-GuitarPlayer.map","Type: org.apache.spark.sql.Dataset[part1recap.SparkRecap.GuitarPlayer]
Call: map

_.name
"
"udf/spark_repos_3/4_rockthejvm_spark-optimization/..src.main.scala.part2foundations.SparkAPIs.scala/udf/28.17.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ * 5
"
"udf/spark_repos_3/4_rockthejvm_spark-optimization/..src.main.scala.part3dfjoins.Bucketing.scala/udf/25.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" === 10
"
"udf/spark_repos_3/4_rockthejvm_spark-optimization/..src.main.scala.part3dfjoins.SkewedJoins.scala/udf/11.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

abs(laptopOffers.col(""procSpeed"") - laptops.col(""procSpeed"")) <= 0.1d
"
"udf/spark_repos_3/4_Skycrab_spark-ext/..src.main.scala.org.apache.spark.example.Test.scala/udf/11.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_3/4_Skycrab_spark-ext/..src.main.scala.org.apache.spark.example.Test.scala/udf/15.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_3/4_Skycrab_spark-ext/..src.main.scala.org.apache.spark.ml.feature.WoeBinning.scala/udf/134.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(inputColName).isNull
"
"udf/spark_repos_3/4_Skycrab_spark-ext/..src.main.scala.org.apache.spark.ml.feature.WoeBinning.scala/udf/139.23.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(inputColName).isNotNull
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.main.scala.org.apache.spark.ml.feature.Imputer.scala/udf/51.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

ic.isNotNull && ic =!= $(missingValue) && !ic.isNaN
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""same_bucket"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""same_bucket"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/30.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") > distFP
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/34.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""distance"") < distFN
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distUDF(col(s""a.$inputCol""), col(s""b.$inputCol"")) < threshold
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.LSHTest.scala/udf/64.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""distCol"") < threshold
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isGoodBucket($""count"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 0.0d
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 4.0d
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.mllib.src.test.scala.org.apache.spark.ml.fpm.FPGrowthSuite.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""freq"") === col(""expectedFreq"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/14.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!$""value"".startsWith(options.comment.toString)
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.main.scala.org.apache.spark.sql.execution.datasources.csv.CSVUtils.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

length(trim($""value"")) > 0
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/102.24.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/141.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % 2L === 0L
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/156.27.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

func
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/203.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/218.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/49.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

func
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.execution.SQLExecutionSuite.scala/udf/90.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ x => 
        while (!SQLExecutionSuite.canProgress) {
          Thread.sleep(1)
        }
        x
      }
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.SessionStateSuite.scala/udf/43.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: String).length + (_: Int)
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.SessionStateSuite.scala/udf/53.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: Int) + 1
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.core.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/87.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/537.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/541.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/569.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/573.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/887.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/896.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/4_ZhuXS_Spark-SourceCode-Reading/..spark-2.2.0.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_3/5_anlei-cdh_AlAnalysis/..AlSpark.src.main.scala.com.al.spark.examples.CollaborativeFilteringShow.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/5_anlei-cdh_AlAnalysis/..AlSpark.src.main.scala.com.al.spark.ml.Clustering.scala/udf/61.19.Dataset-Users.map","Type: org.apache.spark.sql.Dataset[com.al.spark.ml.Clustering.Users]
Call: map

user => {
        user.text = user.textlist.mkString("" "")
        user
      }
"
"udf/spark_repos_3/5_anlei-cdh_AlAnalysis/..AlSpark.src.main.scala.com.al.spark.ml.CollaborativeFiltering.scala/udf/28.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_3/5_anlei-cdh_AlAnalysis/..AlSpark.src.main.scala.com.al.spark.ml.DecisionTree.scala/udf/31.21.Dataset-Lr.map","Type: org.apache.spark.sql.Dataset[com.al.spark.ml.LogisticRegression.Lr]
Call: map

{ lr => 
          lr.text = WordSplitUtil.getWordSplit(lr.title)
          lr
        }
"
"udf/spark_repos_3/5_anlei-cdh_AlAnalysis/..AlSpark.src.main.scala.com.al.spark.ml.DecisionTree.scala/udf/36.22.Dataset-Lr.filter","Type: org.apache.spark.sql.Dataset[com.al.spark.ml.LogisticRegression.Lr]
Call: filter

lr => lr.text != null
"
"udf/spark_repos_3/5_anlei-cdh_AlAnalysis/..AlSpark.src.main.scala.com.al.spark.ml.LogisticRegression.scala/udf/57.21.Dataset-Lr.map","Type: org.apache.spark.sql.Dataset[com.al.spark.ml.LogisticRegression.Lr]
Call: map

{ lr => 
          lr.text = WordSplitUtil.getWordSplit(lr.title)
          lr
        }
"
"udf/spark_repos_3/5_anlei-cdh_AlAnalysis/..AlSpark.src.main.scala.com.al.spark.ml.LogisticRegression.scala/udf/62.22.Dataset-Lr.filter","Type: org.apache.spark.sql.Dataset[com.al.spark.ml.LogisticRegression.Lr]
Call: filter

lr => lr.text != null
"
"udf/spark_repos_3/5_caroljmcdonald_mapr-sparkml-sentiment-classification/..src.main.scala.machinelearning.Review.scala/udf/102.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_3/5_caroljmcdonald_mapr-sparkml-sentiment-classification/..src.main.scala.machinelearning.Review.scala/udf/104.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_3/5_caroljmcdonald_mapr-sparkml-sentiment-classification/..src.main.scala.machinelearning.Review.scala/udf/109.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_3/5_caroljmcdonald_mapr-sparkml-sentiment-classification/..src.main.scala.machinelearning.Review.scala/udf/111.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_3/5_caroljmcdonald_mapr-sparkml-sentiment-classification/..src.main.scala.machinelearning.Review.scala/udf/88.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_3/5_caroljmcdonald_mapr-sparkml-sentiment-classification/..src.main.scala.machinelearning.Review.scala/udf/90.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_3/5_caroljmcdonald_mapr-sparkml-sentiment-classification/..src.main.scala.machinelearning.Review.scala/udf/95.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_3/5_caroljmcdonald_mapr-sparkml-sentiment-classification/..src.main.scala.machinelearning.Review.scala/udf/97.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/110.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          val pi = totalRow / bcTotalElements.value
          val pij = row.getLong(i) / bcTotalElements.value
          if (pij != 0) 2 * (pij / pi * (pij / bc_pj.value)) / (pij / pi + pij / bc_pj.value) else 0.0d
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/155.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ fila => 
        var total = 0L
        var indice = """"
        for (i <- 0 until fila.length - 1) {
          var dato = fila.getLong(i)
          total += dato
        }
        (fila.getLong(fila.length - 1), total)
      }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/173.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ fila => 
        var max = 0L
        var indice = """"
        for (i <- 0 until fila.length) {
          var dato = fila.getLong(i)
          if (dato > max) {
            max = dato
            indice = colNames(i)
          }
        }
        (max, indice)
      }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/194.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ fila => 
        var max = -1L
        var indice = """"
        for (i <- 0 until fila.length - 1) {
          var dato = fila.getLong(i)
          if (dato > max) {
            max = dato
            indice = colNames(i)
          }
        }
        (fila.getLong(fila.length - 1), max, indice)
      }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/243.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ fila => 
          val colValue = fila.getLong(0)
          val colname = fila.getString(1)
          val valor = bc_mapPerfect.value(colname)
          if (colValue > valor) {
            valor
          } else {
            colValue
          }
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/356.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          val pi = totalRow / bcTotalElements.value
          pi * log2(pi)
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/36.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          val pi = totalRow.doubleValue() / bcTotalElements.value
          val rowEntropySeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i) / bcTotalElements.value.doubleValue()
            if (cellValue != 0) cellValue / pi * log2(cellValue / pi) else 0.0d
          }
          val rowEntropy = -rowEntropySeq.sum
          pi * rowEntropy
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/372.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          val variationOfInformationSeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i) / bcTotalElements.value
            val pi = totalRow / bcTotalElements.value
            val pj = bcColumnsSum.value.apply(i) / bcTotalElements.value
            if (cellValue != 0) cellValue * log2(cellValue / (pi * pj)) else 0.0d
          }
          variationOfInformationSeq.sum
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/396.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row).doubleValue()
          val rowPuritySeq = for (i <- 0 until row.size) yield {
            row.getLong(i) / totalRow
          }
          val rowPurity = rowPuritySeq.max
          totalRow / bcTotalElements.value * (1 - rowPurity)
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/418.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          combina2(totalRow)
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/432.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val variationOfInformationSeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i)
            combina2(cellValue)
          }
          variationOfInformationSeq.sum
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/454.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val variationOfInformationSeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i)
            if (cellValue != 0) combina2(cellValue) else 0.0d
          }
          variationOfInformationSeq.sum
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/467.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          if (totalRow != 0) combina2(totalRow) else 0.0d
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/491.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val jaccardSeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i)
            combina2(cellValue)
          }
          jaccardSeq.sum
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/504.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          combina2(totalRow)
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/526.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val fowlkesSeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i)
            combina2(cellValue)
          }
          fowlkesSeq.sum
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/539.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          combina2(totalRow)
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/562.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val variationOfInformationSeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i)
            combina2(cellValue)
          }
          variationOfInformationSeq.sum
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/575.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          combina2(totalRow)
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/597.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val minkowskiSeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i)
            combina2(cellValue)
          }
          minkowskiSeq.sum
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/60.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          val rowPuritySeq = for (i <- 0 until row.size) yield {
            row.getLong(i) / totalRow.doubleValue()
          }
          val rowPurity = rowPuritySeq.max
          totalRow / bcTotalElements.value * rowPurity
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/610.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          combina2(totalRow)
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/653.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => getSumRow(row)
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.ExternalValidation.scala/udf/83.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val totalRow = getSumRow(row)
          val variationOfInformationSeq = for (i <- 0 until row.size) yield {
            val cellValue = row.getLong(i) / bcTotalElements.value
            val pi = totalRow / bcTotalElements.value
            val pj = bcColumnsSum.value.apply(i) / bcTotalElements.value
            if (cellValue != 0) cellValue * log2(cellValue / (pi * pj)) else 0.0d
          }
          variationOfInformationSeq.sum
        }
"
"udf/spark_repos_3/5_josemarialuna_ExternalValidity/..src.main.scala.es.us.spark.mllib.clustering.validation.FeatureStatistics.scala/udf/48.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ this_row => 
          val totalRow = getTotalRow(this_row)
          val x2 = for (i <- 0 until this_row.size) yield {
            val realValue = this_row.getDouble(i)
            val expected = totalRow * bc_totalColumn.value.apply(i) / bc_totalDF.value
            val x2aux = (expected - realValue) * (expected - realValue) / expected
            x2aux
          }
          x2.sum
        }
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/48.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(airQualityId) =!= target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/52.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityData.col(airQualityId) === target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/56.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityTimeSeries.col(airQualityId) =!= target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/60.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

geoFeatures.col(geoFeatureId) =!= target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/64.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

geoFeatures.col(geoFeatureId) === target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/80.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

testingAirQuality.col(timeColumn) >= startTime and testingAirQuality.col(timeColumn) <= endTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/88.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

trainingAirQuality.col(timeColumn) === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.FishnetPrediction.scala/udf/49.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(timeColumn) === maxTimestamp
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.FishnetPrediction.scala/udf/63.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(timeColumn) >= startTime and airQualityCleaned.col(timeColumn) <= endTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.FishnetPrediction.scala/udf/71.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(timeColumn) === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.IDWTesting.scala/udf/37.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(airQualityColumnSet.head) =!= target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Demo.IDWTesting.scala/udf/41.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

airQualityData.col(airQualityColumnSet.head) === target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..AirQualityPrediction.src.main.scala.Utils.InverseDistanceWeight.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distinctTime.col(""count"") >= 10
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/40.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(airQualityColumnSet.head) =!= target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/44.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(airQualityColumnSet.head) === target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/48.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityTimeSeries.col(airQualityColumnSet.head) =!= target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/52.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

sensorGeoFeatures.col(geoFeatureColumnSet.head) =!= target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/56.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

sensorGeoFeatures.col(geoFeatureColumnSet.head) === target
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/69.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

testingAirQuality.col(airQualityColumnSet(1)) >= startTime and testingAirQuality.col(airQualityColumnSet(1)) <= endTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.CrossValidation.scala/udf/77.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

trainingAirQuality.col(airQualityColumnSet(1)) === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.FishnetPredicitonWithPurpleair.scala/udf/76.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

purpleairCleaned.col(purpleairColumnSet(1)) === maxTimestamp
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.FishnetPredicitonWithPurpleair.scala/udf/93.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

purpleairAndEpaCleaned.col(purpleairColumnSet(1)) === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.FishnetPredicitonWithPurpleair.scala/udf/98.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

purpleairCleaned.col(purpleairColumnSet(1)) === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.FishnetPrediction.scala/udf/45.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(airQualityColumnSet(1)) === maxTimestamp
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.FishnetPrediction.scala/udf/61.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(airQualityColumnSet(1)) === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.IDWTesting.scala/udf/63.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned(""date_observed"") === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.IDWTesting.scala/udf/68.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

validationIDWAll(""timestamp"") === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.Validation.scala/udf/42.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(airQualityColumnSet(1)) === maxTimestamp
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Demo.Validation.scala/udf/64.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

airQualityCleaned.col(airQualityColumnSet(1)) === eachTime
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Modeling.SQLQuery.scala/udf/52.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""channel!='$filterOption'""
"
"udf/spark_repos_3/5_spatial-computing_air-quality-prediction-scala/..PRISMS_AirQualityPrediction.src.main.scala.Utils.InverseDistanceWeight.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distinctTime.col(""count"") >= 10
"
"udf/spark_repos_3/61_azavea_osmesa/..src.analytics.src.main.scala.osmesa.analytics.EditHistogram.scala/udf/215.22.Dataset-RasterTileWithKey.map","Type: org.apache.spark.sql.Dataset[osmesa.analytics.vectorgrid.RasterTileWithKey]
Call: map

{ tile => 
          val raster = tile.raster
          val rasterExtent = RasterExtent(raster.extent, raster.tile.cols, raster.tile.rows)
          val index = new ZSpatialKeyIndex(KeyBounds(SpatialKey(0, 0), SpatialKey(raster.tile.cols - 1, raster.tile.rows - 1)))
          val features = ArrayBuffer[PointFeature[Map[String, Long]]]()
          locally {
            val _t_m_p_22 = raster.tile
            _t_m_p_22.foreach {
              (c: Int, r: Int, value: Int) => if (value > 0) {
                features.append(PointFeature(Point(rasterExtent.gridToMap(c, r)), Map(tile.key -> value, ""__id"" -> index.toIndex(SpatialKey(c, r)).toLong)))
              }
            }
          }
          VectorTileWithKey(tile.key, tile.zoom, tile.sk, features)
        }
"
"udf/spark_repos_3/61_azavea_osmesa/..src.analytics.src.main.scala.osmesa.analytics.EditHistogram.scala/udf/236.22.Dataset-RasterTileWithKeyAndSequence.map","Type: org.apache.spark.sql.Dataset[osmesa.analytics.vectorgrid.RasterTileWithKeyAndSequence]
Call: map

{ tile => 
          val raster = tile.raster
          val rasterExtent = RasterExtent(raster.extent, raster.tile.cols, raster.tile.rows)
          val index = new ZSpatialKeyIndex(KeyBounds(SpatialKey(0, 0), SpatialKey(raster.tile.cols - 1, raster.tile.rows - 1)))
          val features = ArrayBuffer[PointFeature[Map[String, Long]]]()
          locally {
            val _t_m_p_24 = raster.tile
            _t_m_p_24.foreach {
              (c: Int, r: Int, value: Int) => if (value > 0) {
                features.append(PointFeature(Point(rasterExtent.gridToMap(c, r)), Map(tile.key -> value, ""__id"" -> index.toIndex(SpatialKey(c, r)).toLong)))
              }
            }
          }
          VectorTileWithKeyAndSequence(tile.sequence, tile.key, tile.zoom, tile.sk, features)
        }
"
"udf/spark_repos_3/66_ansrivas_spark-structured-streaming/..src.main.scala.com.kafkaToSparkToCass.Main.scala/udf/47.19.Dataset-(String, String, Integer).map","Type: org.apache.spark.sql.Dataset[(String, String, Integer)]
Call: map

{ line => 
        val columns = line._1.split("";"")
        (columns(0), Commons.getTimeStamp(columns(1)), columns(2))
      }
"
"udf/spark_repos_3/6_smart-data-lake_smart-data-lake/..src.main.scala.io.smartdatalake.workflow.action.CustomFileAction.scala/udf/41.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

{
        case (srcPath, tgtPath) =>
          val result = TryWithRessource.exec(srcDO.filesystem.open(new Path(srcPath))) {
            is => TryWithRessource.exec(tgtDO.filesystem.create(new Path(tgtPath), true)) {
              os => transformerVal.transform(is, os)
            }
          }
          (srcPath, tgtPath, locally {
            val _t_m_p_4 = result
            _t_m_p_4.map(_.getMessage)
          })
      }
"
"udf/spark_repos_3/6_smart-data-lake_smart-data-lake/..src.main.scala.io.smartdatalake.workflow.dataobject.ExcelFileDataObject.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Long](""id"") > excelOptions.numLinesToSkip.get
"
"udf/spark_repos_3/6_smart-data-lake_smart-data-lake/..src.main.scala.io.smartdatalake.workflow.dataobject.SplunkDataObject.scala/udf/31.19.Dataset-QueryTimeInterval.map","Type: org.apache.spark.sql.Dataset[io.smartdatalake.workflow.dataobject.QueryTimeInterval]
Call: map

interval => readRowsFromSplunk(interval, params)
"
"udf/spark_repos_3/7_caroljmcdonald_mapr-spark-structuredstreaming-uber/..src.main.scala.sparkmaprdb.QueryUber.scala/udf/34.22.Dataset-UberwId.filter","Type: org.apache.spark.sql.Dataset[sparkmaprdb.QueryUber.UberwId]
Call: filter

$""_id"" <= ""1""
"
"udf/spark_repos_3/7_caroljmcdonald_mapr-spark-structuredstreaming-uber/..src.main.scala.streaming.StructuredStreamingConsumer.scala/udf/57.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(message: String) => parseUber(message)
"
"udf/spark_repos_3/7_caroljmcdonald_mapr-spark-structuredstreaming-uber/..src.main.scala.streaming.StructuredStreamingConsumer.scala/udf/68.19.Dataset-UberC.map","Type: org.apache.spark.sql.Dataset[streaming.StructuredStreamingConsumer.UberC]
Call: map

uber => createUberwId(uber)
"
"udf/spark_repos_3/7_PU-101_spark-etl/..src.main.scala.com.yxt.bigdata.etl.connector.hive.reader.HiveReader.scala/udf/51.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_3/7_zxd2629546_sseu/..src.main.scala.dk.zxd.control.DataController.scala/udf/133.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.toSeq.mkString(""\t"")
"
"udf/spark_repos_3/7_zxd2629546_sseu/..src.main.scala.dk.zxd.control.DataController.scala/udf/27.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Row.fromSeq(locally {
            val _t_m_p_2 = index.value
            _t_m_p_2.map(idx => cast(row(idx._1 - 1), idx._3))
          })
"
"udf/spark_repos_3/8_ChieftainX_wheels/..src.main.scala.com.wheels.spark.SQL.scala/udf/269.29.Dataset-Table.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table]
Call: filter

_.isTemporary
"
"udf/spark_repos_3/8_ChieftainX_wheels/..src.main.scala.com.wheels.spark.SQL.scala/udf/389.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(cols: Seq[Double]) => Vectors.dense(cols.toArray)
"
"udf/spark_repos_3/8_ChieftainX_wheels/..src.main.scala.com.wheels.spark.SQL.scala/udf/393.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(bigger: Seq[String], smaller: Seq[String]) => smaller.exists(bigger.contains)
"
"udf/spark_repos_3/8_ChieftainX_wheels/..src.main.scala.com.wheels.spark.SQL.scala/udf/397.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(v: DenseVector) => v.values.toSeq
"
"udf/spark_repos_3/8_ChieftainX_wheels/..src.test.scala.com.zzy.ts.spark.TS.scala/udf/200.22.Dataset-Table.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table]
Call: filter

_.isTemporary
"
"udf/spark_repos_3/8_ChieftainX_wheels/..src.test.scala.com.zzy.ts.spark.TS.scala/udf/205.22.Dataset-Table.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table]
Call: filter

_.isTemporary
"
"udf/spark_repos_3/8_ChieftainX_wheels/..src.test.scala.com.zzy.ts.spark.TS.scala/udf/210.22.Dataset-Table.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table]
Call: filter

_.isTemporary
"
"udf/spark_repos_3/8_Smallhi_HbaseETL/..src.main.scala.org.hhl.hbaseETL.example.HbaseSuit.scala/udf/115.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => rowKeyByMD5(x.getString(1), x.getString(1))
"
"udf/spark_repos_3/8_Smallhi_HbaseETL/..src.main.scala.org.hhl.hbaseETL.example.HbaseSuit.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val sid = x.getString(0)
        val id = x.getString(1)
        val idType = x.getString(3)
        (sid, id, idType)
      }
"
"udf/spark_repos_3/8_Smallhi_HbaseETL/..src.main.scala.org.hhl.hbaseETL.example.HbaseSuit.scala/udf/52.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val sid = x.getString(0)
        val id = x.getString(1)
        val idtype = x.getString(3)
        rowKeyByMD5(id, idtype)
      }
"
"udf/spark_repos_3/8_Smallhi_HbaseETL/..src.main.scala.org.hhl.hbaseETL.example.HbaseSuit.scala/udf/78.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val id = x.getString(1)
        val idType = x.getString(3)
        rowKeyByMD5(id, idType)
      }
"
"udf/spark_repos_3/8_Smallhi_HbaseETL/..src.main.scala.org.hhl.hbaseETL.example.HbaseSuit.scala/udf/92.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val sid = x.getString(0)
        val id = x.getString(1)
        val idType = x.getString(3)
        (sid, id, idType)
      }
"
"udf/spark_repos_3/90_archivesunleashed_aut/..src.main.scala.io.archivesunleashed.app.DomainGraphExtractor.scala/udf/14.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!($""dest_domain"" === """")
"
"udf/spark_repos_3/90_archivesunleashed_aut/..src.main.scala.io.archivesunleashed.app.DomainGraphExtractor.scala/udf/16.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!($""src_domain"" === """")
"
"udf/spark_repos_3/90_archivesunleashed_aut/..src.main.scala.io.archivesunleashed.app.DomainGraphExtractor.scala/udf/18.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""count"" > 5
"
"udf/spark_repos_3/90_archivesunleashed_aut/..src.main.scala.io.archivesunleashed.app.ExtractPopularImagesDF.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""width"" >= minWidth && $""height"" >= minHeight
"
"udf/spark_repos_3/90_archivesunleashed_aut/..src.main.scala.io.archivesunleashed.package.scala/udf/66.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""crawl_date"".isNotNull
"
"udf/spark_repos_3/90_archivesunleashed_aut/..src.main.scala.io.archivesunleashed.package.scala/udf/68.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!$""url"".rlike("".*robots\\.txt$"") && ($""mime_type_web_server"".rlike(""text/html"") || $""mime_type_web_server"".rlike(""application/xhtml+xml"") || $""url"".rlike(""(?i).*htm$"") || $""url"".rlike(""(?i).*html$""))
"
"udf/spark_repos_3/90_archivesunleashed_aut/..src.main.scala.io.archivesunleashed.package.scala/udf/70.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""http_status_code"" === 200
"
"udf/spark_repos_3/94_yangtong123_RoadOfStudySpark/..src.com.spark.sql.ActionOperation.scala/udf/24.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

employee => 1
"
"udf/spark_repos_3/94_yangtong123_RoadOfStudySpark/..src.com.spark.sql.DailyUV.scala/udf/24.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => (row.getAs[String](""date""), row.getAs[Long](""uv""))
        }
"
"udf/spark_repos_3/94_yangtong123_RoadOfStudySpark/..src.com.spark.sql.SparkSQLDemo.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_3/94_yangtong123_RoadOfStudySpark/..src.com.spark.sql.SQLDataSourceExample.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_3/94_yangtong123_RoadOfStudySpark/..src.com.spark.sql.UDAF.scala/udf/17.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new UDAFStringCount
"
"udf/spark_repos_3/94_yangtong123_RoadOfStudySpark/..src.com.spark.sql.UDF.scala/udf/17.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length
"
"udf/spark_repos_3/9_ToyHaoran_scala01/..src.sparkdemo.JoinDemo.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""FLAG"") === ""1""
"
"udf/spark_repos_3/9_ToyHaoran_scala01/..src.sparkdemo.JoinDemo.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""FLAG"").isNull
"
"udf/spark_repos_3/9_ToyHaoran_scala01/..src.sparkdemo.practice.Demo02.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split("","")
"
"udf/spark_repos_3/9_ToyHaoran_scala01/..src.sparkdemo.practice.Demo03.scala/udf/21.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_3/9_ToyHaoran_scala01/..src.sparkdemo.practice.Demo04.scala/udf/11.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
      val fields = line.split("","")
      (fields(0), fields(1), fields(2), fields(3))
    }
"
"udf/spark_repos_3/9_ToyHaoran_scala01/..src.utils.DataIntegration.DataUpdate.scala/udf/217.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""FLAG"") === ""1""
"
"udf/spark_repos_3/9_ToyHaoran_scala01/..src.utils.DataIntegration.DataUpdate.scala/udf/224.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""FLAG"").isNull
"
"udf/spark_repos_4/10_fabianmurariu_website-categories-nn/..sparky.process-dmoz.src.main.scala.com.bytes32.prenn.PreNNProcessor.scala/udf/103.22.Dataset-WebSiteCategoriesText.filter","Type: org.apache.spark.sql.Dataset[com.bytes32.prenn.PreNNProcessor.WebSiteCategoriesText]
Call: filter

isUriEnglishSpeakingDomain(_)
"
"udf/spark_repos_4/10_fabianmurariu_website-categories-nn/..sparky.process-dmoz.src.main.scala.com.bytes32.prenn.PreNNProcessor.scala/udf/22.22.Dataset-WebSiteCategoriesText.filter","Type: org.apache.spark.sql.Dataset[com.bytes32.prenn.PreNNProcessor.WebSiteCategoriesText]
Call: filter

{
        (ws: WebSiteCategoriesText) => Language.detectEnglish(ws.text).isDefined
      }
"
"udf/spark_repos_4/10_fabianmurariu_website-categories-nn/..sparky.process-dmoz.src.main.scala.com.bytes32.prenn.PreNNTokenizer.scala/udf/149.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(uri: String, origUri: String, category: String, _, tokens: Seq[_]) =>
          val oheLabel = locally {
            val _t_m_p_11 = indexer.labels
            _t_m_p_11.map(l => if (l.equals(category)) 1 else 0)
          }
          val numericTokens = locally {
            val _t_m_p_12 = locally {
              val _t_m_p_13 = tokens.view
              _t_m_p_13.map(word => vocabulary.getOrElse(word.toString, 0))
            }
            _t_m_p_12.filter(_ != 0)
          }.padTo(sequenceLength, 0).take(sequenceLength)
          WebSiteFeature(uri, origUri, numericTokens, oheLabel)
      }
"
"udf/spark_repos_4/10_fabianmurariu_website-categories-nn/..sparky.process-dmoz.src.main.scala.com.bytes32.prenn.PreNNTokenizer.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(category: String, count: Long) =>
          category -> count
      }
"
"udf/spark_repos_4/10_fabianmurariu_website-categories-nn/..sparky.process-dmoz.src.main.scala.com.bytes32.prenn.PreNNTokenizer.scala/udf/77.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          val tokens = line.split("" "")
          GloVector(tokens.head, locally {
            val _t_m_p_5 = tokens.tail
            _t_m_p_5.map(_.toFloat)
          })
      }
"
"udf/spark_repos_4/10_fabianmurariu_website-categories-nn/..sparky.process-dmoz.src.main.scala.com.bytes32.prenn.PreNNTokenizer.scala/udf/93.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0)
"
"udf/spark_repos_4/10_X-DataInitiative_SCALPEL-Flattening/..src.main.scala.fr.polytechnique.cmap.cnam.flattening.references.IRPHARActions.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""PHA_NOM_PA"").isNotNull
"
"udf/spark_repos_4/10_X-DataInitiative_SCALPEL-Flattening/..src.main.scala.fr.polytechnique.cmap.cnam.flattening.Table.scala/udf/26.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""year"") === year
"
"udf/spark_repos_4/10_X-DataInitiative_SCALPEL-Flattening/..src.main.scala.fr.polytechnique.cmap.cnam.flattening.Table.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""year"") === year
"
"udf/spark_repos_4/10_X-DataInitiative_SCALPEL-Flattening/..src.main.scala.fr.polytechnique.cmap.cnam.flattening.Table.scala/udf/33.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

functions.month(col(colDate)) === month
"
"udf/spark_repos_4/10_X-DataInitiative_SCALPEL-Flattening/..src.main.scala.fr.polytechnique.cmap.cnam.flattening.Table.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""month"") === month
"
"udf/spark_repos_4/10_X-DataInitiative_SCALPEL-Flattening/..src.main.scala.fr.polytechnique.cmap.cnam.statistics.descriptive.StatisticsMain.scala/udf/45.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getString(0)
"
"udf/spark_repos_4/11_UxioAndrade_Anime-Recommendation-Engine/..app.util.RecommendationModel.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => !row.anyNull
"
"udf/spark_repos_4/13_bebee4java_sqlalarm/..sa-core.src.main.java.dt.sql.alarm.core.SparkRuntime.scala/udf/174.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_ != null
"
"udf/spark_repos_4/13_bebee4java_sqlalarm/..sa-core.src.main.java.dt.sql.alarm.core.SparkRuntime.scala/udf/176.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getAs[String](0)
"
"udf/spark_repos_4/13_bebee4java_sqlalarm/..sa-core.src.main.java.dt.sql.alarm.filter.SQLFilter.scala/udf/31.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(source) === source_.`type` and col(topic) === source_.topic
"
"udf/spark_repos_4/13_bebee4java_sqlalarm/..sa-core.src.main.java.dt.sql.alarm.reduce.engine.ReduceByWindow.scala/udf/15.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(alarm) === 1
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.com.automl.classifier.ensemble.bagging.SparkGenericBagging.scala/udf/244.24.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.get(0).asInstanceOf[Double]
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.com.automl.classifier.LinearPerceptronClassifier.scala/udf/126.19.Dataset-UnlabeledVector.map","Type: org.apache.spark.sql.Dataset[utils.UnlabeledVector]
Call: map

{ row => 
        val (rawPrediction, prediction) = calculateAction(row, parameters)
        FeaturesVectorWithPredictions(row.id, row.features, rawPrediction, prediction)
      }
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.com.automl.classifier.LinearPerceptronClassifier.scala/udf/78.21.Dataset-LabeledVector.map","Type: org.apache.spark.sql.Dataset[utils.LabeledVector]
Call: map

{
          row => calculateAction(row)._2
        }
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.com.automl.classifier.LinearPerceptronClassifier.scala/udf/95.21.Dataset-LabeledVector.map","Type: org.apache.spark.sql.Dataset[utils.LabeledVector]
Call: map

{ row => 
          val featuresWithBias: Array[Double] = Array(1.0d, row.features.toArray: _*)
          val vectorOfFeaturesWithBias: VectorMLLib = Vectors.dense(featuresWithBias)
          val (learningAction, _) = calculateAction(row)
          (learningAction, vectorOfFeaturesWithBias)
        }
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.com.automl.evolution.diversity.MisclassificationDistance.scala/udf/21.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[DenseVector](0).values
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.com.visualization.Plotter3D.scala/udf/20.21.Dataset-LabeledVector.map","Type: org.apache.spark.sql.Dataset[utils.LabeledVector]
Call: map

lv => {
          val toArray: Array[Double] = lv.features.toArray
          (toArray(0), toArray(1), lv.label)
        }
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.ColumnDescriber.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$column"".isNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.ColumnDescriber.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$column"" rlike ""NA""
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.ColumnDescriber.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$column"" rlike ""[^NA]""
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.CorrelationHelper.scala/udf/15.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(xField) rlike ""[^NA]""
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/139.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(df(field) rlike ""[^NA]"") && df(field).isNotNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/147.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(df(field) rlike ""[^NA]"") && df(field).isNotNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/155.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(field) =!= 0
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/176.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$field"" =!= 0
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/195.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$field"" === 0
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/203.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$field"".isNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/211.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$field"" rlike ""NA""
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/22.19.Dataset-LabeledPoint.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.ml.feature.LabeledPoint]
Call: map

mllp => LabeledPoint(mllp.label, Vectors.fromML(mllp.features))
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/283.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(featureName) rlike ""[^NA]""
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/290.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(featureName) =!= 0 && df(featureName).isNotNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/297.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(featureName).isNotNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/303.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(df(featureName) rlike ""NA"") || df(featureName).isNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/310.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(featureName).isNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.main.scala.utils.SparkMLUtils.scala/udf/317.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(featureName) === 0 || df(featureName).isNull
"
"udf/spark_repos_4/13_deil87_automl-genetic/..src.test.scala.com.automl.spark.StandardScalerSuite.scala/udf/23.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[DenseVector](0).toArray(1)
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.data.index.structures.ecp.ECPIndexGenerator.scala/udf/43.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => (r.getAs[Int](AttributeNames.internalIdColumnName), r.getAs[DenseSparkVector](attribute))
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.data.index.structures.ecp.ECPIndex.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(AttributeNames.featureIndexColumnName) isin (idsBc.value: _*)
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.data.index.structures.lsh.LSHIndex.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

containsUDF(col(AttributeNames.featureIndexColumnName))
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.distribution.partitioning.partitioner.ECPPartitioner.scala/udf/57.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getAs[Any](AttributeNames.partitionKey), r)
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.distribution.partitioning.partitioner.RandomPartitioner.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getAs[Any](attribute.get), r)
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.distribution.partitioning.partitioner.RandomPartitioner.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getAs[Any](Index.load(indexName.get).get.pk.name), r)
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.query.ast.internal.BooleanFilterExpression.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sqlString
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.query.ast.internal.SequentialScanExpression.scala/udf/76.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterUdf(col(entity.pk.name))
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.query.ast.internal.SequentialScanExpression.scala/udf/86.34.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(entity.pk.name).isin(subids: _*)
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.utils.importer.LireImporter.scala/udf/34.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""\t"")
"
"udf/spark_repos_4/14_vitrivr_ADAMpro/..src.main.scala.org.vitrivr.adampro.utils.importer.LireImporter.scala/udf/36.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""\t"")
"
"udf/spark_repos_4/15_ndolgov_experiments/..sparkdatasourcetest.src.test.scala.net.ndolgov.sparkdatasourcetest.sql.LuceneDataSourceTestSuit.scala/udf/46.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

table(DocumentField.METRIC.name).equalTo(3660154)
"
"udf/spark_repos_4/15_ndolgov_experiments/..sparkdatasourcev2test.src.test.scala.net.ndolgov.sparkdatasourcetest.connector.LuceneDataSourceTestSuit.scala/udf/63.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

table(DocumentField.METRIC.name).equalTo(metric154)
"
"udf/spark_repos_4/16_agile-lab-dev_darwin/..spark-application.src.main.scala.it.agilelab.darwin.app.spark.SchemaManagerSparkApp.scala/udf/30.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

{ x => 
        AvroSchemaManagerFactory.initialize(conf).getSchema(registeredIDs(x % registeredIDs.size))
        x
      }
"
"udf/spark_repos_4/17_anish749_spark2-etl-examples/..src.main.scala.org.anish.spark.etl.ProcessData.scala/udf/112.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""year_joined_count = "" + maxJoined
"
"udf/spark_repos_4/17_anish749_spark2-etl-examples/..src.main.scala.org.anish.spark.etl.ProcessData.scala/udf/149.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""count_bday_month = "" + minCountBmonth
"
"udf/spark_repos_4/17_anish749_spark2-etl-examples/..src.main.scala.org.anish.spark.etl.ProcessData.scala/udf/166.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""count_bday_day = "" + maxCountBday
"
"udf/spark_repos_4/17_anish749_spark2-etl-examples/..src.main.scala.org.anish.spark.etl.ProcessData.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""octet1"" >= 192 && $""octet1"" <= 223
"
"udf/spark_repos_4/17_cleanzr_dblink/..src.main.scala.com.github.cleanzr.dblink.analysis.package.scala/udf/10.19.Dataset-RecordPair.map","Type: org.apache.spark.sql.Dataset[com.github.cleanzr.dblink.RecordPair]
Call: map

{
        recIds => recIds._1.compareTo(recIds._2) match {
          case x if x < 0 =>
            (recIds._1, recIds._2)
          case x if x > 0 =>
            (recIds._2, recIds._1)
          case 0 =>
            throw new Exception(s""Invalid link: ${recIds._1} <-> ${recIds._2}."")
        }
      }
"
"udf/spark_repos_4/17_cleanzr_dblink/..src.main.scala.com.github.cleanzr.dblink.LinkageChain.scala/udf/15.19.Dataset-LinkageState.map","Type: org.apache.spark.sql.Dataset[com.github.cleanzr.dblink.LinkageState]
Call: map

_.iteration
"
"udf/spark_repos_4/17_cleanzr_dblink/..src.main.scala.com.github.cleanzr.dblink.Project.scala/udf/110.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getAs[RecordId](recIdName), r.getAs[EntityId](eId))
"
"udf/spark_repos_4/17_cleanzr_dblink/..src.main.scala.com.github.cleanzr.dblink.Project.scala/udf/76.24.Dataset-LinkageState.filter","Type: org.apache.spark.sql.Dataset[com.github.cleanzr.dblink.LinkageState]
Call: filter

_.iteration >= lowerIterationCutoff
"
"udf/spark_repos_4/17_cleanzr_dblink/..src.main.scala.com.github.cleanzr.dblink.State.scala/udf/207.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Record(r.getString(0), r.getString(1), r.getSeq[String](2).toArray)
"
"udf/spark_repos_4/17_cleanzr_dblink/..src.main.scala.com.github.cleanzr.dblink.State.scala/udf/215.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Record(r.getString(0), ""0"", r.getSeq[String](1).toArray)
"
"udf/spark_repos_4/1_anandakrishna13_spark-age-bucketing/..src.main.scala.com.github.anandakrishna13.spark.AgeBucketing.AgeBucketingRef.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""bucketName"").isNotNull
"
"udf/spark_repos_4/1_anandakrishna13_spark-age-bucketing/..src.main.scala.com.github.anandakrishna13.spark.AgeBucketing.AgeBucketingRef.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""from"").isNotNull
"
"udf/spark_repos_4/1_anandakrishna13_spark-age-bucketing/..src.main.scala.com.github.anandakrishna13.spark.AgeBucketing.AgeBucketingRef.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""to"").isNotNull
"
"udf/spark_repos_4/1_anandakrishna13_spark-age-bucketing/..src.main.scala.com.github.anandakrishna13.spark.AgeBucketing.AgeBucketingRef.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""isRangeOk"") === false
"
"udf/spark_repos_4/1_anandakrishna13_spark-age-bucketing/..src.main.scala.com.github.anandakrishna13.spark.AgeBucketing.AgeBucketingRef.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(""isRangeInvalid"")
"
"udf/spark_repos_4/1_aniruddha-sinha_learning-spark/..src.main.scala.com.aniruddha.spark.dataframe.Etl.SparkApp.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val transformFieldIndex: Int = row.fieldIndex(fieldForTransformation)
        val rowArray: Array[Any] = row.toSeq.toArray
        val valueAtIndex = rowArray(transformFieldIndex)
        val transformedValue = dataType match {
          case ""Int"" =>
            valueAtIndex.asInstanceOf[Int] * 44
          case ""Float"" =>
            valueAtIndex.asInstanceOf[Float]
          case ""Double"" =>
            valueAtIndex.asInstanceOf[Double]
          case ""Long"" =>
            valueAtIndex.asInstanceOf[Long]
          case ""Short"" =>
            valueAtIndex.asInstanceOf[Short]
          case ""String"" =>
            valueAtIndex.asInstanceOf[String]
          case _ =>
            valueAtIndex
        }
        Row.fromSeq(rowArray ++ Array(transformedValue))
      }
"
"udf/spark_repos_4/1_aniruddha-sinha_learning-spark/..src.main.scala.com.aniruddha.spark.dataframe.SparkApplication.scala/udf/20.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
      val value = row.getAs[Int](""revenue"") * 44
      Row.fromSeq(Array(value))
    }
"
"udf/spark_repos_4/1_aniruddha-sinha_learning-spark/..src.main.scala.com.aniruddha.spark.UDAF.JsonUdaf.scala/udf/23.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new SumProductAggregateFunction
"
"udf/spark_repos_4/1_arnaudj_mooc-spark-coursera-bigdata-analysis-spark-epfl/..week4-timeuse.src.main.scala.timeusage.TimeUsage.scala/udf/122.20.Dataset-(KeyTuple3, Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[(KeyTuple3, Double, Double, Double)]
Call: map

e => TimeUsageRow(e._1._1, e._1._2, e._1._3, e._2, e._3, e._4)
"
"udf/spark_repos_4/1_avik191_Scala-Spark/..src.main.scala.org.spark.DataFrame.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""ename"").like(""A%S"")
"
"udf/spark_repos_4/1_avik191_Scala-Spark/..src.main.scala.org.spark.DataFrame.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""sal = "" + x
"
"udf/spark_repos_4/1_bryanesmith_ScalaSpark/..src.main.scala.com.bryanesmith.spark.section6.DataFrames.scala/udf/26.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.bryanesmith.spark.section6.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_4/1_ChemaGit_Specialization-in-Scala/..Specialization-in-Scala.src.assigments.big-data-analysis-spark.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/104.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs(""working""), row.getAs(""sex""), row.getAs(""age""), row.getAs(""primaryNeeds""), row.getAs(""work""), row.getAs(""other""))
"
"udf/spark_repos_4/1_ChemaGit_Specialization-in-Scala/..Specialization-in-Scala.src.assigments.big-data-analysis-spark.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/110.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), pNeeds, work, other) =>
          TimeUsageRow(working, sex, age, pNeeds, work, other)
      }
"
"udf/spark_repos_4/1_ChemaGit_Specialization-in-Scala/..Specialization-in-Scala.src.big_data_analysis_scala_spark.week_4.DataFrames.scala/udf/40.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 25 && $""city"" === ""Sydney""
"
"udf/spark_repos_4/1_ChemaGit_Specialization-in-Scala/..Specialization-in-Scala.src.big_data_analysis_scala_spark.week_4.DataFrames.scala/udf/95.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hasDebt"" && ($""hasFinancialDependents"")
"
"udf/spark_repos_4/1_ChemaGit_Specialization-in-Scala/..Specialization-in-Scala.src.big_data_analysis_scala_spark.week_4.DataFrames.scala/udf/99.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""country"" === ""Switzerland""
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkBasic.sql.TestDataSets.scala/udf/13.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkBasic.sql.TestDataSets.scala/udf/36.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""age"" + t(1)
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkBasic.sql.TestDataSets.scala/udf/61.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""name： "" + t(0)
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkBasic.sql.TestDataSets.scala/udf/81.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""name: "" + t(0)
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkSql.SqlBasic.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkSql.SqlBasic.scala/udf/26.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkSql.SqlBasic.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkSql.SqlBasic.scala/udf/49.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkSql.SqlBasic.scala/udf/55.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_4/1_codlife_scala-spark-deepStudy/..src.main.scala.sparkSql.SqlBasic.scala/udf/79.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_cong666_spark-football-stat/..src.main.scala.com.test.spark.wiki.extracts.Q2_ShowLeagueStatsTask.scala/udf/17.22.Dataset-LeagueStanding.filter","Type: org.apache.spark.sql.Dataset[com.test.spark.wiki.extracts.LeagueStanding]
Call: filter

$""league"" === ""Ligue 1""
"
"udf/spark_repos_4/1_CoshChen_Observatory/..src.main.scala.observatory.Extraction.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

rawData.col(""Lat"").isNotNull && rawData.col(""Long"").isNotNull
"
"udf/spark_repos_4/1_CoshChen_Observatory/..src.main.scala.observatory.Extraction.scala/udf/44.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

rawData.col(""Month"").isNotNull && rawData.col(""Day"").isNotNull && rawData.col(""Temp_F"").isNotNull
"
"udf/spark_repos_4/1_CoshChen_Observatory/..src.main.scala.observatory.Extraction.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Temp_C"" < 200.00d && $""Temp_C"" > -200.00d
"
"udf/spark_repos_4/1_delphi-xk_spark_learning/..src.main.scala.com.hyzs.spark.ml.ConvertLibsvmLocal.scala/udf/25.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(colName).isNotNull
"
"udf/spark_repos_4/1_delphi-xk_spark_learning/..src.main.scala.com.hyzs.spark.ml.MatrixOpsInSpark.scala/udf/55.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ row => 
        val datum: Array[Double] = locally {
          val _t_m_p_5 = row.toSeq
          _t_m_p_5.map(toDoubleDynamic)
        }.toArray
        val labeledPoint = LabeledPoint(datum(0), Vectors.dense(datum.drop(1)))
        labeledPoint
      }
"
"udf/spark_repos_4/1_delphi-xk_spark_learning/..src.main.scala.com.hyzs.spark.ml.ModelPrediction.scala/udf/107.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_4/1_delphi-xk_spark_learning/..src.main.scala.com.hyzs.spark.ml.ModelPrediction.scala/udf/155.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_4/1_delphi-xk_spark_learning/..src.main.scala.com.hyzs.spark.sql.NewDataProcess.scala/udf/56.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getString(0)
"
"udf/spark_repos_4/1_departmentfortransport_ADD-ds-ais-spark-decoder/..aisdecode.src.test.scala-2.11.uk.gov.dft.ais.decode.test.RawDecode.scala/udf/22.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""fragment_count"" === 1
"
"udf/spark_repos_4/1_departmentfortransport_ADD-ds-ais-spark-decoder/..aisdecode.src.test.scala-2.11.uk.gov.dft.ais.decode.test.RawDecode.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""fragment_count"" === 2
"
"udf/spark_repos_4/1_departmentfortransport_ADD-ds-ais-spark-decoder/..aisdecode.src.test.scala-2.11.uk.gov.dft.ais.decode.test.RawDecode.scala/udf/42.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""fragment_count"" === 3
"
"udf/spark_repos_4/1_departmentfortransport_ADD-ds-ais-spark-decoder/..aisdecode.src.test.scala-2.11.uk.gov.dft.ais.decode.test.TestUtils.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getString(0)
"
"udf/spark_repos_4/1_departmentfortransport_ADD-ds-ais-spark-decoder/..aisdecode.src.test.scala-2.11.uk.gov.dft.ais.decode.test.TestUtils.scala/udf/16.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

v => process_checksum(v)
"
"udf/spark_repos_4/1_drehi440_SparkApp/..FirstSparkProject.src.main.scala.com.rehi.spark.tutorial.FirstSparkProject.SchemaWithJoin.scala/udf/45.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          x => x(0)
        }
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.example.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.example.SparkSQLExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.example.SparkSQLExample.scala/udf/42.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.example.SparkSQLExample.scala/udf/61.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.example.SparkSQLExample.scala/udf/65.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.example.SparkSQLExample.scala/udf/70.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.example.SparkSQLExample.scala/udf/94.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.hbase.oldapi.HBaseReadRowWriteStats.scala/udf/106.19.Dataset-SensorStatsRow.map","Type: org.apache.spark.sql.Dataset[hbase.oldapi.HBaseReadRowWriteStats.SensorStatsRow]
Call: map

{
        case sensorStatsRow =>
          SensorStatsRow.convertToPutStats(sensorStatsRow)
      }
"
"udf/spark_repos_4/1_gao634209276_mySpark2/..src.main.scala.hbase.oldapi.HBaseReadRowWriteStats.scala/udf/91.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(rowkey: String, maxhz: Double, minhz: Double, avghz: Double, maxdisp: Double, mindisp: Double, avgdisp: Double, maxflo: Double, minflo: Double, avgflo: Double, maxsedPPM: Double, minsedPPM: Double, avgsedPPM: Double, maxpsi: Double, minpsi: Double, avgpsi: Double, maxchlPPM: Double, minchlPPM: Double, avgchlPPM: Double) =>
          SensorStatsRow(rowkey: String, maxhz: Double, minhz: Double, avghz: Double, maxdisp: Double, mindisp: Double, avgdisp: Double, maxflo: Double, minflo: Double, avgflo: Double, maxsedPPM: Double, minsedPPM: Double, avgsedPPM: Double, maxpsi: Double, minpsi: Double, avgpsi: Double, maxchlPPM: Double, minchlPPM: Double, avgchlPPM: Double)
      }
"
"udf/spark_repos_4/1_GRpro_recommender_lab/..export_job.src.main.scala.lab.reco.batch.ExportModelJob.scala/udf/42.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(text: String) =>
          val splitted = text.split(""\t"")
          val objectId = splitted(0)
          val splittedRecommendations: Array[(String, Double)] = if (splitted.length > 1) {
            locally {
              val _t_m_p_2 = splitted(1).split("" "")
              _t_m_p_2.map { rec => 
                val sp = rec.split("":"")
                (sp(0), sp(1).toDouble)
              }
            }.filterNot {
              _._1 == objectId
            }
          } else {
            Array.empty
          }
          val recommendations = locally {
            val _t_m_p_3 = splittedRecommendations
            _t_m_p_3.map {
              _._1
            }
          }
          (objectId, recommendations)
      }
"
"udf/spark_repos_4/1_guangge08_AnalyzeServer/..src.main.scala.com.bluedon.neModel.AssetsPortrait.scala/udf/195.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

netflow => netflowStatic(netflow.getAs[String](""DSTIP""), netflow.getAs[Double](""RECORDTIME""), netflow.getAs[Vector](""FEATURES""))
"
"udf/spark_repos_4/1_guangge08_AnalyzeServer/..src.main.scala.com.bluedon.neModel.AssetsPortrait.scala/udf/230.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val centersListBDs = centersListBD.value
          val DSTIP = row.getAs[String](""DSTIP"")
          val FEATURES = row.getAs[Vector](""FEATURES"")
          val SCLFEATURES = row.getAs[Vector](""SCLFEATURES"")
          val PREDICTION = row.getAs[Int](""prediction"")
          var DISTANCE: Double = 0
          var index: Int = 0
          val centerPoint: Vector = centersListBDs(PREDICTION)
          DISTANCE = math.sqrt(locally {
            val _t_m_p_6 = locally {
              val _t_m_p_7 = centerPoint.toArray.zip(SCLFEATURES.toArray)
              _t_m_p_7.map(p => p._1 - p._2)
            }
            _t_m_p_6.map(d => d * d)
          }.sum)
          StaticResult(DSTIP, FEATURES, SCLFEATURES, PREDICTION, DISTANCE)
        }
"
"udf/spark_repos_4/1_haihuiyang_spark/..src.main.scala.com.yhh.examples.WindowFunc.scala/udf/22.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(valueList: mutable.WrappedArray[Int]) => {
        var sum = 0.0d
        locally {
          val _t_m_p_2 = valueList
          _t_m_p_2.foreach(value => sum += value)
        }
        sum
      }
"
"udf/spark_repos_4/1_hardikfuria12_Geospatia-Analysis/..Phase2.src.main.scala.cse512.SpatialQuery.scala/udf/39.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => {
        val point = pointString.split("","")
        val pointX = point(0).toDouble
        val pointY = point(1).toDouble
        val c = Array(pointX, pointY)
        val Rect = queryRectangle.split("","")
        val RectX1 = Rect(0).toDouble
        val RectY1 = Rect(1).toDouble
        val low = Array(RectX1, RectY1)
        val RectX2 = Rect(2).toDouble
        val RectY2 = Rect(3).toDouble
        val high = Array(RectX2, RectY2)
        var dist = 0.0d
        for (i <- 0 to low.length - 1) {
          if (c(i) < low(i)) dist += math.pow(low(i) - c(i), 2) else if (c(i) > high(i)) dist += math.pow(c(i) - high(i), 2)
        }
        if (dist == 0) true else false
      }
"
"udf/spark_repos_4/1_hardikfuria12_Geospatia-Analysis/..Phase2.src.main.scala.cse512.SpatialQuery.scala/udf/67.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => {
        val p1 = pointString1.split("","")
        val p2 = pointString2.split("","")
        val d = distance
        val x1 = p1(0).toDouble
        val x2 = p2(0).toDouble
        val y1 = p1(1).toDouble
        val y2 = p2(1).toDouble
        var r = 0.0d
        val r1 = (x1 - x2) * (x1 - x2) + (y1 - y2) * (y1 - y2) - d * d
        if (r1 <= 0) true else false
      }
"
"udf/spark_repos_4/1_hardikfuria12_Geospatia-Analysis/..Phase2.src.main.scala.cse512.SpatialQuery.scala/udf/91.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => {
        val p1 = pointString1.split("","")
        val p2 = pointString2.split("","")
        val d = distance
        val x1 = p1(0).toDouble
        val x2 = p2(0).toDouble
        val y1 = p1(1).toDouble
        val y2 = p2(1).toDouble
        var r = 0.0d
        val r1 = (x1 - x2) * (x1 - x2) + (y1 - y2) * (y1 - y2) - d * d
        if (r1 <= 0) true else false
      }
"
"udf/spark_repos_4/1_hardikfuria12_Geospatia-Analysis/..Phase2.src.main.scala.cse512.SpatialQuery.scala/udf/9.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => {
        val point = pointString.split("","")
        val pointX = point(0).toDouble
        val pointY = point(1).toDouble
        val c = Array(pointX, pointY)
        val Rect = queryRectangle.split("","")
        val RectX1 = Rect(0).toDouble
        val RectY1 = Rect(1).toDouble
        val low = Array(RectX1, RectY1)
        val RectX2 = Rect(2).toDouble
        val RectY2 = Rect(3).toDouble
        val high = Array(RectX2, RectY2)
        var dist = 0.0d
        for (i <- 0 to low.length - 1) {
          if (c(i) < low(i)) dist += math.pow(low(i) - c(i), 2) else if (c(i) > high(i)) dist += math.pow(c(i) - high(i), 2)
        }
        if (dist == 0) true else false
      }
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/102.22.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

i => (i, ""nonce"", 3.1415d, true)
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/142.22.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[de.hpi.spark_tutorial.Person]
Call: map

_.name + "" says hello""
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/152.22.Dataset-Pet.map","Type: org.apache.spark.sql.Dataset[de.hpi.spark_tutorial.Pet]
Call: map

_ + "" is cute""
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/160.23.Dataset-(String, Int, Double, String).filter","Type: org.apache.spark.sql.Dataset[(String, Int, Double, String)]
Call: filter

e => names.contains(e._1)
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/164.23.Dataset-(String, Int, Double, String).filter","Type: org.apache.spark.sql.Dataset[(String, Int, Double, String)]
Call: filter

e => !names.contains(e._1)
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/168.23.Dataset-(String, Int, Double, String).filter","Type: org.apache.spark.sql.Dataset[(String, Int, Double, String)]
Call: filter

e => names(1).equals(e._1)
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/178.23.Dataset-(String, Int, Double, String).filter","Type: org.apache.spark.sql.Dataset[(String, Int, Double, String)]
Call: filter

e => bcNames.value.contains(e._1)
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/182.23.Dataset-(String, Int, Double, String).filter","Type: org.apache.spark.sql.Dataset[(String, Int, Double, String)]
Call: filter

e => !bcNames.value.contains(e._1)
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/186.23.Dataset-(String, Int, Double, String).filter","Type: org.apache.spark.sql.Dataset[(String, Int, Double, String)]
Call: filter

e => bcNames.value(1).equals(e._1)
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/267.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => !r.getString(0).equals(r.getString(4))
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/269.22.Dataset-(String, String, String, String, String, String, String, String).map","Type: org.apache.spark.sql.Dataset[(String, String, String, String, String, String, String, String)]
Call: map

t => (t._1, longestCommonSubstring(t._4, t._8))
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/57.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

i => ""This is a number: "" + i
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/61.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

s => s.contains(""1"")
"
"udf/spark_repos_4/1_HPI-Information-Systems_spark-tutorial/..src.main.scala.de.hpi.spark_tutorial.SimpleSpark.scala/udf/87.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.get(0).toString
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.DataPipeline.scala/udf/229.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""uid"") >= 0
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.DataPipeline.scala/udf/242.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""mid"") >= 0
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.DeepLearningPipeline.scala/udf/72.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date>=$trainingStart""
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.DeepLearningPipeline.scala/udf/74.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

s""date<=$trainingEnd""
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.DeepLearningPipeline.scala/udf/88.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date>$validationStart""
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.DeepLearningPipeline.scala/udf/90.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

s""date<=$validationEnd""
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.DeepLearningPipeline.scala/udf/99.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!$""m"".contains("","")
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.MllibPipeline.scala/udf/127.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""cluster"") === clusterNumber
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.MllibPipeline.scala/udf/60.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date>=$trainingStart""
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.MllibPipeline.scala/udf/62.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

s""date<=$trainingEnd""
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.MllibPipeline.scala/udf/96.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date>$validationStart""
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.MllibPipeline.scala/udf/98.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

s""date<=$validationEnd""
"
"udf/spark_repos_4/1_jack1981_aaas-demo-aicamp/..aaas-demo.src.main.scala.com.ssqcyy.aaas.demo.pipeline.StreamingPipeline.scala/udf/82.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prob"" >= ($""threshold"")
"
"udf/spark_repos_4/1_jiafengzhang_Spark-ADMM-for-sparse-logistic-regression/..src.main.scala.cn.zjf.SparkADMM.MyLoadLibSVMFile.scala/udf/58.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not((length($""line"") === 0).or($""line"".startsWith(""#"")))
"
"udf/spark_repos_4/1_Jimmy-Newtron_osm-decorator/..src.main.scala.io.github.osmDecorator.OsmDecorator.scala/udf/65.25.Dataset-OsmWay.map","Type: org.apache.spark.sql.Dataset[io.github.osmDecorator.OsmWay]
Call: map

way => Way(way)
"
"udf/spark_repos_4/1_Jimmy-Newtron_osm-decorator/..src.main.scala.io.github.osmDecorator.OsmDecorator.scala/udf/67.26.Dataset-Way.filter","Type: org.apache.spark.sql.Dataset[io.github.osmDecorator.Way]
Call: filter

way => way.tags.contains(""highway"") && way.nodes.length > 1
"
"udf/spark_repos_4/1_k-ayada_SparkETL/..pub.ayada.scala.sparkUtils.cmn.SparkUtilWIP.scala/udf/588.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""databaseName = '$db'""
"
"udf/spark_repos_4/1_k-ayada_SparkETL/..pub.ayada.scala.sparkUtils.etl.write.hive.DF2Hive.scala/udf/104.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => t(0)
"
"udf/spark_repos_4/1_k-ayada_SparkETL/..pub.ayada.scala.sparkUtils.etl.write.hive.DF2Hive.scala/udf/82.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

t => if (t(1).asInstanceOf[Boolean] == false) t(0).asInstanceOf[String]
"
"udf/spark_repos_4/1_kostaskougios_spi/..sql.src.main.scala.com.aktit.landregistry.LandRegistryJob.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""postCode"".startsWith(""BR2"")
"
"udf/spark_repos_4/1_linan32-max_secondsgroup/..day04.src.main.scala.com.Tags.TagsContext.scala/udf/17.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val userId = TagUtils.getOneUserId(row)
          val adList = TagsAd.makeTags(row)
          val businessList = BusinessTag.makeTags(row)
        }
"
"udf/spark_repos_4/1_linan32-max_secondsgroup/..day04.src.main.scala.com.util.Test.scala/udf/12.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => AmapUtil.getBusinessFromAmap(String2Type.toDouble(row.getAs[String](""long"")), String2Type.toDouble(row.getAs[String](""lat"")))
"
"udf/spark_repos_4/1_linan32-max_secondsgroup/..day05.src.main.scala.com.Tags.TagsContext.scala/udf/58.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val userId: String = TagUtils.getOneUserId(row)
          val adList = TagsAd.makeTags(row)
          val businessList = BusinessTag.makeTags(row)
          val appList = TagsAPP.makeTags(row, broadValue)
          val devList = TagsDevice.makeTags(row)
          val locList = TagsLocation.makeTags(row)
          val kwList = TagsKword.makeTags(row, broadValues)
          (userId, adList ++ appList ++ businessList ++ devList ++ locList ++ kwList)
        }
"
"udf/spark_repos_4/1_linan32-max_secondsgroup/..day06.src.main.scala.com.Tags.TagsContext.scala/udf/58.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val userId: String = TagUtils.getOneUserId(row)
          val adList = TagsAd.makeTags(row)
          val businessList = BusinessTag.makeTags(row)
          val appList = TagsAPP.makeTags(row, broadValue)
          val devList = TagsDevice.makeTags(row)
          val locList = TagsLocation.makeTags(row)
          val kwList = TagsKword.makeTags(row, broadValues)
          (userId, adList ++ appList ++ businessList ++ devList ++ locList ++ kwList)
        }
"
"udf/spark_repos_4/1_lixizheng123_TrafficSpaceTimeAnalyzeSystem/..recommender.StatisticsRecommender.src.main.scala.com.fengli.statistics.StatisticsRecommender.scala/udf/27.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Long) => simpleDateFormat.format(new Date(x))
"
"udf/spark_repos_4/1_lpadillg_Spark-The-Definitive-Guide/..project-templates.scala.src.main.scala.DataFrameExample.scala/udf/15.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.pointlessUDF(_: String): String
"
"udf/spark_repos_4/1_LYDPolaris_Polaris/..MLDataFlow.src.main.scala.lyd.ai.ml.dataflow.dbscan.TTT.scala/udf/10.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_4/1_LYDPolaris_Polaris/..MLDataFlow.src.main.scala.lyd.ai.ml.dataflow.dbscan.TTT.scala/udf/14.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_4/1_LYDPolaris_Polaris/..SSDataFlow.src.main.scala.lyd.ai.dataflow.case2ss.ContinuousKafkaStreaming.scala/udf/19.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => CarEvent(r.getString(0))
"
"udf/spark_repos_4/1_LYDPolaris_Polaris/..SSDataFlow.src.main.scala.lyd.ai.dataflow.case2ss.KafkaSourceStreaming.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => CarEvent(r.getString(0))
"
"udf/spark_repos_4/1_LYDPolaris_Polaris/..SSDataFlow.src.main.scala.lyd.ai.dataflow.case2ss.StreamingAggregations.scala/udf/16.22.Dataset-CarEvent.filter","Type: org.apache.spark.sql.Dataset[lyd.ai.dataflow.case2ss.StreamingAggregations.CarEvent]
Call: filter

_.speed.exists(_ > 70)
"
"udf/spark_repos_4/1_LYDPolaris_Polaris/..SSDataFlow.src.main.scala.lyd.ai.dataflow.sml.ss2hive.utils.TTT.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_4/1_LYDPolaris_Polaris/..SSDataFlow.src.main.scala.lyd.ai.dataflow.sml.ss2hive.utils.TTT.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_4/1_LYDPolaris_Polaris/..SSDataFlow.src.main.scala.lyd.ai.dataflow.ss2hive.SS2HSinkT.HiveStreamingExample.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ s => 
        val records = s.split("","")
        assert(records.length >= 4)
        (records(0).toInt, records(1), records(2), records(3))
      }
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.exec.sql.SparkCSvSQL2Demo.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

item => item.replace(""::"", "","")
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.ad.AdClickRealTimeStateSparkV1.scala/udf/85.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 100
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.ad.AdClickRealTimeStateSparkV2.scala/udf/100.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 100
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.ad.AdClickRealTimeStateSparkV3.scala/udf/101.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 100
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSpark.scala/udf/209.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSpark.scala/udf/213.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSpark.scala/udf/217.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSpark.scala/udf/221.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV1.scala/udf/172.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV1.scala/udf/176.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV1.scala/udf/180.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV1.scala/udf/184.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV2.scala/udf/182.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV2.scala/udf/186.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV2.scala/udf/190.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV2.scala/udf/194.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV3.scala/udf/192.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV3.scala/udf/196.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV3.scala/udf/200.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituan.product.AreaTop10ProductSparkV3.scala/udf/204.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AdClickRealTimeStateSpark.scala/udf/75.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 100
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSpark.scala/udf/209.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSpark.scala/udf/213.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSpark.scala/udf/217.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSpark.scala/udf/221.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV1.scala/udf/171.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV1.scala/udf/175.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV1.scala/udf/179.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV1.scala/udf/183.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV2.scala/udf/182.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV2.scala/udf/186.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV2.scala/udf/190.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV2.scala/udf/194.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV3.scala/udf/192.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => !(str == null || """".equals(str.trim) || ""null"".equalsIgnoreCase(str.trim))
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV3.scala/udf/196.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(id: Int, name: String) => s""$id:$name""
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV3.scala/udf/200.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

GroupConcatDistinctUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.meituantest.product.AreaTop10ProductSparkV3.scala/udf/204.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => JSON.parseObject(json).getString(field)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.project.TOP10.ItemTop10.scala/udf/86.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(extendInfo: String) => {
        val productTypeId: Int = analysisObjectMapper(extendInfo).productTypeId
        productTypeId
      }
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.spark.sql.MongoSparkSQL.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 15 && $""name"".equalTo(""小雨"")
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.spark.sql.SchemaSparkSQL.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""cnt"" > 100
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.spark.sql.SparkCSvSQL2.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.replace(""::"", "","")
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.spark.sql.SparkSQLDemo.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""order_amt"" > 50
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.spark.sql.TaoBaoBraJsonSpark.scala/udf/10.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sizeInfo: String) => sizeInfo.split(""\\;"")(0)
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.spark.sql.TaoBaoBraJsonSpark.scala/udf/14.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AvgSalUDAF
"
"udf/spark_repos_4/1_lz63_spark-learning/..src.main.scala.com.erongda.bigdata.spark.sql.TaoBaoBraJsonSpark.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(trim($""size_info"")) > 0
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.evaluator.OPLogLoss.scala/udf/13.21.Dataset-Vector, Double).map","Type: org.apache.spark.sql.Dataset[(Double, org.apache.spark.ml.linalg.Vector, org.apache.spark.ml.linalg.Vector, Double)]
Call: map

{
          case (lbl, _, prob, _) =>
            new AveragedValue(count = 1L, value = -math.log(prob.toArray(lbl.toInt)))
        }
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.feature.DecisionTreeNumericBucketizer.scala/udf/70.23.Dataset-(Double, Double).map","Type: org.apache.spark.sql.Dataset[(Double, Double)]
Call: map

{
            case (label, v) =>
              label -> Vectors.dense(Array(v))
          }
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.DataCutter.scala/udf/20.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => r.getDouble(0) -> 1L
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.DataCutter.scala/udf/30.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

r => keep.contains(r.getDouble(0))
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.DataCutter.scala/udf/43.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.getLong(1) / totalValues >= minLabelFract
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.DataCutter.scala/udf/51.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => !labelSet.contains(r.getDouble(0))
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.OpValidator.scala/udf/68.24.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

functions.col(label) === theClass
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.test.scala.com.salesforce.op.stages.impl.classification.FunctionalityForClassificationTests.scala/udf/15.23.Dataset-Vector, Double).map","Type: org.apache.spark.sql.Dataset[(Double, org.apache.spark.ml.linalg.Vector, org.apache.spark.ml.linalg.Vector, Double)]
Call: map

{
            case (lbl, _, prob, _) =>
              math.log(prob.toArray(lbl.toInt))
          }
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..core.src.test.scala.com.salesforce.op.stages.impl.tuning.DataCutterTest.scala/udf/86.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.binary.BinaryEstimator.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (convertI1.fromSpark(r.get(0)).value, convertI2.fromSpark(r.get(1)).value)
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.quaternary.QuaternaryEstimator.scala/udf/26.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (convertI1.fromSpark(r.get(0)).value, convertI2.fromSpark(r.get(1)).value, convertI3.fromSpark(r.get(2)).value, convertI4.fromSpark(r.get(3)).value)
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.sequence.BinarySequenceEstimator.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
        val rowSeq = r.toSeq
        (convertI1.fromSpark(rowSeq.head).value, locally {
          val _t_m_p_3 = rowSeq.tail
          _t_m_p_3.map(seqIConvert.fromSpark(_).value)
        })
      }
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.sequence.SequenceEstimator.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

locally {
        val _t_m_p_3 = _.toSeq
        _t_m_p_3.map(seqIConvert.fromSpark(_).value)
      }
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.ternary.TernaryEstimator.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (convertI1.fromSpark(r.get(0)).value, convertI2.fromSpark(r.get(1)).value, convertI3.fromSpark(r.get(2)).value)
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.unary.UnaryEstimator.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => iConvert.fromSpark(r.get(0)).value
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..features.src.main.scala.com.salesforce.op.utils.spark.RichDataset.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => predicate(row.getAs[T](columnName))
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..features.src.main.scala.com.salesforce.op.utils.spark.RichDataset.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => predicate(row.getFeatureType(feature).value)
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..readers.src.main.scala.com.salesforce.op.readers.DataReader.scala/udf/107.25.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

record => (key(record), Seq(record))
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..utils.src.main.scala.com.salesforce.op.test.SparkMatchers.scala/udf/11.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
          row => inv(predicate(row.getAs[T](columnName)))
        }
"
"udf/spark_repos_4/1_monk1337_TransmogrifAI/..utils.src.main.scala.com.salesforce.op.utils.io.csv.CSVInOut.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.toSeq.collect({
        case null =>
          null
        case v =>
          v.toString
      })
"
"udf/spark_repos_4/1_mrnogues_amadeus-challenge-scala/..src.main.scala.amadeusChallenge.exerciseTwo.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dfSel(""act_date"").contains(""2013"")
"
"udf/spark_repos_4/1_msinton_boris-bikes/..src.main.scala.chess.MoveAnalysis.scala/udf/10.22.Dataset-FlatGameData.filter","Type: org.apache.spark.sql.Dataset[chess.FlatGameData]
Call: filter

$""n"" < withinMoveNum
"
"udf/spark_repos_4/1_msinton_boris-bikes/..src.main.scala.chess.Sequences.scala/udf/10.22.Dataset-FlatGameData.filter","Type: org.apache.spark.sql.Dataset[chess.FlatGameData]
Call: filter

x => x.n <= sequenceLength
"
"udf/spark_repos_4/1_msinton_boris-bikes/..src.main.scala.chess.Sequences.scala/udf/16.22.Dataset-Sequence.filter","Type: org.apache.spark.sql.Dataset[chess.Sequences.Sequence]
Call: filter

_.evalSymbol < normalVal
"
"udf/spark_repos_4/1_msinton_boris-bikes/..src.main.scala.chess.Sequences.scala/udf/21.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""count"" > 10
"
"udf/spark_repos_4/1_msinton_boris-bikes/..src.main.scala.streaming.Tutorial-dataframe.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replaceAll(""[.,]"", """")
"
"udf/spark_repos_4/1_multivacplatform_multivac-nlp/..src.main.scala.sparkml.Spark_ML_NLP.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 13
"
"udf/spark_repos_4/1_murdonson_CTRmodel/..src.main.com.ggstar.evaluation.Evaluator.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => (row.apply(1).asInstanceOf[DenseVector](1), row.getAs[Int](""label"").toDouble)
      }
"
"udf/spark_repos_4/1_murdonson_CTRmodel/..src.main.com.ggstar.features.FeatureEngineering.scala/udf/10.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), Vectors.dense(row.getAs[Seq[Double]](""user_embedding"").toArray), Vectors.dense(row.getAs[Seq[Double]](""item_embedding"").toArray), row.getAs[Int](""label""))
"
"udf/spark_repos_4/1_murdonson_CTRmodel/..src.main.com.ggstar.features.FeatureEngineering.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val user_embedding = row.getAs[DenseVector](""user_embedding"")
        val item_embedding = row.getAs[DenseVector](""item_embedding"")
        var asquare = 0.0d
        var bsquare = 0.0d
        var abmul = 0.0d
        for (i <- 0 until user_embedding.size) {
          asquare += user_embedding(i) * user_embedding(i)
          bsquare += item_embedding(i) * item_embedding(i)
          abmul += user_embedding(i) * item_embedding(i)
        }
        var inner_product = 0.0d
        if (asquare == 0 || bsquare == 0) {
          inner_product = 0.0d
        } else {
          inner_product = abmul / (Math.sqrt(asquare) * Math.sqrt(bsquare))
        }
        (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), inner_product, row.getAs[Int](""label""))
      }
"
"udf/spark_repos_4/1_murdonson_CTRmodel/..src.main.com.ggstar.features.FeatureEngineering.scala/udf/53.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val user_embedding = row.getAs[DenseVector](""user_embedding"")
        val item_embedding = row.getAs[DenseVector](""item_embedding"")
        val outerProductEmbedding: Array[Double] = Array.fill[Double](user_embedding.size)(0)
        for (i <- 0 until user_embedding.size) {
          outerProductEmbedding(i) = user_embedding(i) * item_embedding(i)
        }
        (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), Vectors.dense(outerProductEmbedding), row.getAs[Int](""label""))
      }
"
"udf/spark_repos_4/1_NikolayVaklinov10_Spark_with_Scala/..src.main.scala.com.sundogsoftware.spark.DataFrames.scala/udf/26.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_4/1_ONSdigital_sbr-assembler-calculations/..src.main.scala.methods.PayeCalculator.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.isNull(payeRefs)
"
"udf/spark_repos_4/1_ONSdigital_sbr-assembler-calculations/..src.main.scala.processing.AdminDataCalculations.scala/udf/107.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => rowToEnt(row)
"
"udf/spark_repos_4/1_oskardudycz_SparkWithScalaAndDocker/..src.src.main.scala.Instagram.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => t.getString(2)
"
"udf/spark_repos_4/1_paulpaul1076_Marketing-analytics/..src.main.scala.com.company.MainNonSQL.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""is_confirmed"") =!= false
"
"udf/spark_repos_4/1_ponnur_bikedata-strm-processor/..src.main.scala.com.citybike.data.processor.BikeDataProcessor.scala/udf/59.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""num_docks_available"") > Config.maxNoOfDocks
"
"udf/spark_repos_4/1_ppdzm_scala-applications/..universal-utils.src.main.scala.org.sa.utils.bigdata.hive.HiveUtils.scala/udf/113.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""TABLE_SCHEMA='$database'""
"
"udf/spark_repos_4/1_ppdzm_scala-applications/..universal-utils.src.main.scala.org.sa.utils.bigdata.spark.SparkSQL.scala/udf/107.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""TABLE_SCHEMA='$database' and TABLE_NAME='$table'""
"
"udf/spark_repos_4/1_ppdzm_scala-applications/..universal-utils.src.main.scala.org.sa.utils.bigdata.spark.SparkSQL.scala/udf/123.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""TABLE_SCHEMA='$database'""
"
"udf/spark_repos_4/1_ppdzm_scala-applications/..universal-utils.src.main.scala.org.sa.utils.bigdata.spark.SparkSQL.scala/udf/125.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

s""TABLE_NAME='$table'""
"
"udf/spark_repos_4/1_ppdzm_scala-applications/..universal-utils.src.main.scala.org.sa.utils.bigdata.spark.SparkSQL.scala/udf/43.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""TABLE_SCHEMA='$database' and TABLE_NAME='$table'""
"
"udf/spark_repos_4/1_ppdzm_scala-applications/..universal-utils.src.main.scala.org.sa.utils.bigdata.spark.SparkSQL.scala/udf/97.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""TABLE_SCHEMA='$database'""
"
"udf/spark_repos_4/1_pxh520_waterdrop/..waterdrop-core.src.main.scala.io.github.interestinglab.waterdrop.filter.Drop.scala/udf/36.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not(new Column(spark.sessionState.sqlParser.parseExpression(conditionExpr)))
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.my.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.my.test.sql.SparkSQLUDF2.scala/udf/21.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: Integer) => input * 2
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.my.test.sql.SparkSQLUDF.scala/udf/28.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => input.length
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.my.test.sql.SparkSQLUDF.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyUDAF
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_qianbw_spark-poc/..src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_4/1_RajeshAj_BasicSpark/..src.main.scala.FirstProject.DFDataAnalysis.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

movie(""genres"") like ""%Drama%""
"
"udf/spark_repos_4/1_RajeshAj_BasicSpark/..src.main.scala.FirstProject.DFDataAnalysis.scala/udf/24.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

movierating(""userID"").isNotNull
"
"udf/spark_repos_4/1_RajeshAj_BasicSpark/..src.main.scala.FirstProject.DFDataAnalysis.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

mvt(""rating"").isNotNull && mvt(""tag"").isNull
"
"udf/spark_repos_4/1_RajeshAj_BasicSpark/..src.main.scala.FirstProject.DFDataAnalysis.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

mvt(""rating"").isNotNull && mvt(""tag"").isNotNull
"
"udf/spark_repos_4/1_RajeshAj_BasicSpark/..src.main.scala.FirstProject.DFDataAnalysis.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

mvty(""Tag_Year"").isNotNull
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/42.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""NAME in ('%s') and PARENT_CATEGORY_ID is null"".format(categorys.split("","").mkString(""','""))
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/44.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getAs[String](""CATEGORY_ID"")
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/52.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""PARENT_CATEGORY_ID in ('%s')"".format(tmpCategoryIds.mkString(""','""))
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/54.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getAs[String](""CATEGORY_ID"")
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""NAME in ('%s')"".format(filterClass.replaceAll("","", ""','""))
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/65.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getAs[String](""ID"")
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/71.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""CATEGORY_ID in ('%s')"".format(categoryIDs.mkString(""','""))
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/73.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getAs[String](""ID"")
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/81.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""WORDCLASS_ID in ('%s')"".format(classIDs.mkString(""','""))
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.artificial_v1.GetWordsFromRepository.scala/udf/83.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getAs[String](""NAME"")
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.etl.ImportByDay.scala/udf/124.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SLDAT"" >= dayStart && $""SLDAT"" <= dayEnd
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.etl.ImportByDay.scala/udf/136.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split(""\\|"")
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.etl.ImportByDay.scala/udf/36.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time"" >= startTime && $""time"" <= endTime
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.etl.ImportByDay.scala/udf/62.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time"" >= dayStart && $""time"" <= dayEnd
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.SimTest.scala/udf/12.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getString(0)
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.SimTest.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{
        case q =>
          (q, Segment.jieba_analysis(q, "" ""))
      }
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.SimTest.scala/udf/22.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getString(0)
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.SimTest.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{
        case q =>
          (q, Segment.jieba_analysis(q, "" ""))
      }
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/116.20.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/120.23.Dataset-UnansSim.filter","Type: org.apache.spark.sql.Dataset[com.xiaoi.spark.question.UnansSim.UnansSim]
Call: filter

x => !filters.contains(x.yest)
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/23.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.trim.length > 0
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/31.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

UnansQuesUtil.ignoreQuesSimplify(_)
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/33.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.length > 0
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/53.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => RobotLog(x.getString(0), x.getString(1), x.getString(2), x.getString(3), x.getString(4), x.getString(5), x.getString(6), x.getString(7), x.getString(8), x.getString(9), x.getString(10), x.getString(11), x.getString(12), x.getString(13), x.getString(14), x.getString(15), x.getString(16), x.getString(17), x.getString(18), x.getString(19))
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/58.22.Dataset-RobotLog.filter","Type: org.apache.spark.sql.Dataset[com.xiaoi.spark.question.UnansSim.RobotLog]
Call: filter

x => unansFilter(x.question, x.answer_type, x.ex)
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/63.22.Dataset-RobotLog.filter","Type: org.apache.spark.sql.Dataset[com.xiaoi.spark.question.UnansSim.RobotLog]
Call: filter

x => unansFilter(x.question, x.answer_type, x.ex)
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/71.21.Dataset-SimCross.map","Type: org.apache.spark.sql.Dataset[com.xiaoi.spark.question.UnansSim.SimCross]
Call: map

x => UnansSim(x.yestQues, x.recenQues, getSimilarNew(x.yestQues, x.recenQues, x.yestSeg, x.recenSeg))
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/73.22.Dataset-UnansSim.filter","Type: org.apache.spark.sql.Dataset[com.xiaoi.spark.question.UnansSim.UnansSim]
Call: filter

x => {
        val lengthCompare = x.yest.length.toDouble / x.recen.length.toDouble
        if (x.yest.length < params.shortQuesLen) {
          x.sim > params.highSimilarity
        } else if (lengthCompare < 0.5d || lengthCompare > 2) {
          x.sim > 0.8d
        } else {
          x.sim > 0.5d
        }
      }
"
"udf/spark_repos_4/1_refuil_iLearning/..src.main.scala.com.xiaoi.spark.question.UnansSim.scala/udf/86.19.Dataset-UnansSim.map","Type: org.apache.spark.sql.Dataset[com.xiaoi.spark.question.UnansSim.UnansSim]
Call: map

x => (x.yest, x.recen)
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.basic.WordCount.scala/udf/12.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

word => !word.isEmpty()
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.ml.MovieRecommendationsALS.scala/udf/41.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.ml.MovieRecommendationsALS.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""userId"" === 0
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.perf.CatalystOptimisedPlan.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'value2 > 1
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.perf.CatalystOptimisedPlan.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'value2 >= 2 * 4
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.sql.PeopleSQL.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.sql.PeopleSQL.scala/udf/38.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.sql.PeopleSQL.scala/udf/57.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.sql.PeopleSQL.scala/udf/61.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.sql.PeopleSQL.scala/udf/66.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_4/1_rickqiu_spark2/..src.main.scala.org.test.spark2.sql.PeopleSQL.scala/udf/90.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.dev.audit-release.sbt_app_sql.src.main.scala.SqlApp.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/40.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/92.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.mllib.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isGoodBucket($""count"")
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 0.0d
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.mllib.src.test.scala.org.apache.spark.ml.feature.QuantileDiscretizerSuite.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

result(""result"") === 4.0d
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/113.27.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: filter

funcs(i)
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/51.23.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.DatasetBenchmark.Data]
Call: map

func
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.test.scala.org.apache.spark.sql.DatasetBenchmark.scala/udf/98.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""l"" % (100L + i) === 0L
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/114.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/137.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/159.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/176.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.core.src.test.scala.org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark.scala/udf/95.21.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => datum
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/553.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/557.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""hive-hcatalog-core-0.13.1.jar"")
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/585.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/589.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getString(0).contains(""data/files/v1.txt"")
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/896.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 2
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.hive.execution.HiveQuerySuite.scala/udf/905.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""p"" === 4
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.hive.HiveContextCompatibilitySuite.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""a"" > 10 && $""b"" > 6 && ($""c"")
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/78.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_4/1_romantic123_spark_add_window_function/..spark-2.0.2.sql.hive.src.test.scala.org.apache.spark.sql.sources.HadoopFsRelationTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_4/1_ropert911_study05_bigdata/..demo_mlib.src.main.scala.com.xq.study.demo_milib.algorithm.TF_IDF.scala/udf/71.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: String, features: Vector) =>
          LabeledPoint(label.toDouble, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_4/1_ropert911_study05_bigdata/..demo_mlib.src.main.scala.com.xq.study.demo_milib.algorithm.TF_IDF.scala/udf/84.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: String, features: Vector) =>
          LabeledPoint(label.toDouble, Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_4/1_ropert911_study05_bigdata/..demo_mlib.src.main.scala.com.xq.study.demo_milib.algorithm.TF_IDF.scala/udf/91.19.Dataset-LabeledPoint.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.mllib.regression.LabeledPoint]
Call: map

p => (model.predict(p.features), p.label)
"
"udf/spark_repos_4/1_ropert911_study05_bigdata/..demo_mlib.src.main.scala.com.xq.study.demo_milib.algorithm.TF_IDF.scala/udf/95.22.Dataset-(Double, Double).filter","Type: org.apache.spark.sql.Dataset[(Double, Double)]
Call: filter

x => x._1 == x._2
"
"udf/spark_repos_4/1_ropert911_study05_bigdata/..demo_spark.src.main.scala.com.study.scala.examples.sparkShellExamples.DataFrameExamples_Shell.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 20
"
"udf/spark_repos_4/1_ropert911_study05_bigdata/..demo_spark.src.main.scala.com.study.scala.examples.sparkShellExamples.DataFrameExamples_Shell.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name:"" + t(0) + "","" + ""Age:"" + t(1)
"
"udf/spark_repos_4/1_ropert911_study05_bigdata/..demo_spark.src.main.scala.com.study.scala.examples.sparkShellExamples.DataFrameExamples_Shell.scala/udf/61.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name:"" + t(0) + "","" + ""Age:"" + t(1)
"
"udf/spark_repos_4/1_Saevel_spark-streaming-ml/..src.test.scala.prv.saevel.spark.streaming.ml.utils.StreamGenerators.scala/udf/7.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_ => elements(Random.nextInt(elements.size))
"
"udf/spark_repos_4/1_SatyabratKumarSingh_SparkUnitTestExamples/..src.main.scala.com.sparkunittest.example.main.WordCountJob.scala/udf/13.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replace("","", """")
"
"udf/spark_repos_4/1_SatyabratKumarSingh_SparkUnitTestExamples/..src.main.scala.com.sparkunittest.example.main.WordCountJob.scala/udf/15.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replace(""."", """")
"
"udf/spark_repos_4/1_SatyabratKumarSingh_SparkUnitTestExamples/..src.main.scala.com.sparkunittest.example.main.WordCountJob.scala/udf/17.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!_.isEmpty
"
"udf/spark_repos_4/1_schulzecb_CS455-TermProject/..src.main.scala.yelp.GroupBusinessReview.scala/udf/12.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""categories"").contains(""Restaurant"")
"
"udf/spark_repos_4/1_schulzecb_CS455-TermProject/..src.main.scala.yelp.GroupBusinessReview.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0)
"
"udf/spark_repos_4/1_schulzecb_CS455-TermProject/..src.main.scala.yelp.GroupBusinessReview.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""business_id"").isin(restaurantIDs.toSeq: _*)
"
"udf/spark_repos_4/1_schulzecb_CS455-TermProject/..src.main.scala.yelp.GroupBusinessReview.scala/udf/25.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.getString(0)
"
"udf/spark_repos_4/1_schulzecb_CS455-TermProject/..src.main.scala.yelp.GroupUserReviews.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0)
"
"udf/spark_repos_4/1_seahrh_time-usage-spark/..src.main.scala.timeusage.TimeUsage.scala/udf/150.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
      }
"
"udf/spark_repos_4/1_seahrh_time-usage-spark/..src.main.scala.timeusage.TimeUsage.scala/udf/159.19.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case (key, primaryNeeds, work, other) =>
          TimeUsageRow(key._1, key._2, key._3, primaryNeeds, work, other)
      }
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.com.imooc.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.com.imooc.spark.DataFrameRDDApp.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.com.imooc.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.imooc.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.com.imooc.spark.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.imooc.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.imook.TopNStartJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.imook.TopNStartJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.imook.TopNStartJob.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.imook.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.imook.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.imook.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/1_shulin123_SparkSQLProject/..src.main.scala.imook.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/1_siknight_SparkFlinkHadoopHiveTest/..spark.src.main.scala.sparkContext.SPARK_SQL.用户自定义UDF函数.UDFtest.scala/udf/10.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(name: String) => ""name:"" + name
"
"udf/spark_repos_4/1_siknight_SparkFlinkHadoopHiveTest/..spark.src.main.scala.sparkContext.SPARK_SQL.用户自定义聚合UDAF函数.MyAverage.scala/udf/38.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udaf
"
"udf/spark_repos_4/1_thekingofcool_kafka2mysql/..src.main.scala.streaming.Kafka2Mysql.scala/udf/112.26.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

sql => {
              val value = sql.substring(sql.indexOf(""("", sql.indexOf(""VALUES"")) + 1, sql.length - 2)
              val valueArr: Array[String] = locally {
                val _t_m_p_17 = value.split("","")
                _t_m_p_17.map(_.trim)
              }
              val userDealHistory: UserDealHistory = UserDealHistory(null, valueArr(0).toDouble, valueArr(1).toInt, valueArr(2).toInt, valueArr(3).substring(1, valueArr(3).length - 1), BigInt(valueArr(4)), BigInt(valueArr(5)), BigInt(valueArr(6)), valueArr(7).toInt, valueArr(8).toInt, BigDecimal(valueArr(9)), BigDecimal(valueArr(10)), BigDecimal(valueArr(11)), BigDecimal(valueArr(12)), BigDecimal(valueArr(13)), valueArr(14).substring(1, valueArr(14).length - 1), valueArr(15).substring(1, valueArr(15).length - 1), valueArr(16).substring(1, valueArr(16).length - 1))
              userDealHistory
            }
"
"udf/spark_repos_4/1_thekingofcool_kafka2mysql/..src.main.scala.streaming.Kafka2Mysql.scala/udf/20.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

x => sqlPattern.pattern.matcher(x).matches()
"
"udf/spark_repos_4/1_thekingofcool_kafka2mysql/..src.main.scala.streaming.Kafka2Mysql.scala/udf/22.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

sql => sql.substring(sql.indexOf(""INSERT""))
"
"udf/spark_repos_4/1_thekingofcool_kafka2mysql/..src.main.scala.streaming.Kafka2Mysql.scala/udf/62.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

sql => {
              val value = sql.substring(sql.indexOf(""("", sql.indexOf(""VALUES"")) + 1, sql.length - 2)
              val valueArr: Array[String] = locally {
                val _t_m_p_7 = value.split("","")
                _t_m_p_7.map(_.trim)
              }
              val balanceHistory = BalanceHistory(null, valueArr(0).toDouble, valueArr(1).toInt, valueArr(2).substring(1, valueArr(2).length - 1), valueArr(3).substring(1, valueArr(3).length - 1), BigDecimal(valueArr(4)), BigDecimal(valueArr(5)), sql.substring(sql.indexOf(""{""), sql.indexOf(""}"") + 1), valueArr(valueArr.length - 1).substring(1, valueArr(valueArr.length - 1).length))
              balanceHistory
            }
"
"udf/spark_repos_4/1_thekingofcool_kafka2mysql/..src.main.scala.streaming.Kafka2Mysql.scala/udf/87.26.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

sql => {
              val value = sql.substring(sql.indexOf(""("", sql.indexOf(""VALUES"")) + 1, sql.length - 2)
              val valueArr: Array[String] = locally {
                val _t_m_p_12 = value.split("","")
                _t_m_p_12.map(_.trim)
              }
              val orderHistory: OrderHistory = OrderHistory(null, valueArr(1).toDouble, valueArr(2).toDouble, valueArr(3).toInt, valueArr(4).substring(1, valueArr(4).length - 1), valueArr(5).substring(1, valueArr(5).length - 1), valueArr(6).toInt, valueArr(7).toInt, BigDecimal(valueArr(8)), BigDecimal(valueArr(9)), BigDecimal(valueArr(10)), BigDecimal(valueArr(11)), BigDecimal(valueArr(12)), BigDecimal(valueArr(13)), BigDecimal(valueArr(14)), valueArr(15).substring(1, valueArr(15).length - 1))
              orderHistory
            }
"
"udf/spark_repos_4/1_threecuptea_spark2_emr/..src.main.scala.org.freemind.spark.flight.MyFlightSample.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""year"" >= 2000
"
"udf/spark_repos_4/1_threecuptea_spark2_emr/..src.main.scala.org.freemind.spark.flight.MyFlightSample.scala/udf/23.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""depdelay"" >= 15
"
"udf/spark_repos_4/1_threecuptea_spark2_emr/..src.main.scala.org.freemind.spark.flight.MyFlightSample.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""depdelay"" >= 60
"
"udf/spark_repos_4/1_threecuptea_spark2_emr/..src.main.scala.org.freemind.spark.flight.MyFlightSample.scala/udf/33.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cancelled"" === 1
"
"udf/spark_repos_4/1_threecuptea_spark2_emr/..src.main.scala.org.freemind.spark.flight.MyFlightSample.scala/udf/38.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cancelled"" === 1
"
"udf/spark_repos_4/1_threecuptea_spark2_emr/..src.main.scala.org.freemind.spark.recommend.MovieLensALS.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'userId === sUserId
"
"udf/spark_repos_4/1_threecuptea_spark2_emr/..src.main.scala.org.freemind.spark.recommend.MovieLensCommon.scala/udf/95.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'userId === sUserId
"
"udf/spark_repos_4/1_thusithaC_nyTaxiPred/..src.tnc.spark.ml.nytaxi.data.Preprocess.scala/udf/21.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pickup_longitude"" >= -75 and $""pickup_longitude"" <= -73
"
"udf/spark_repos_4/1_thusithaC_nyTaxiPred/..src.tnc.spark.ml.nytaxi.data.Preprocess.scala/udf/23.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""dropoff_longitude"" >= -75 and $""dropoff_longitude"" <= -73
"
"udf/spark_repos_4/1_thusithaC_nyTaxiPred/..src.tnc.spark.ml.nytaxi.data.Preprocess.scala/udf/25.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""pickup_latitude"" >= 39 and $""pickup_latitude"" <= 42
"
"udf/spark_repos_4/1_thusithaC_nyTaxiPred/..src.tnc.spark.ml.nytaxi.data.Preprocess.scala/udf/27.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""dropoff_latitude"" >= 39 and $""dropoff_latitude"" <= 42
"
"udf/spark_repos_4/1_thusithaC_nyTaxiPred/..src.tnc.spark.ml.nytaxi.data.Preprocess.scala/udf/29.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""fare_amount"" >= 0 and $""fare_amount"" <= 275
"
"udf/spark_repos_4/1_thusithaC_nyTaxiPred/..src.tnc.spark.ml.nytaxi.data.Preprocess.scala/udf/31.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""dropoff_longitude"" =!= ($""pickup_longitude"") and $""dropoff_latitude"" =!= ($""pickup_latitude"")
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Analyzer.scala/udf/131.22.Dataset-CloneSnapshot.filter","Type: org.apache.spark.sql.Dataset[analyzer.CloneSnapshot]
Call: filter

x => majorClonesBC.value.contains(x.mutationId)
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Analyzer.scala/udf/143.23.Dataset-CloneSnapshot.filter","Type: org.apache.spark.sql.Dataset[analyzer.CloneSnapshot]
Call: filter

x => !majorClonesBC.value.contains(x.mutationId)
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Analyzer.scala/udf/163.23.Dataset-CloneSnapshot.filter","Type: org.apache.spark.sql.Dataset[analyzer.CloneSnapshot]
Call: filter

abs(col(""timePoint"") - timePoints.last) < 0.001d
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Analyzer.scala/udf/171.23.Dataset-MutationSummary.filter","Type: org.apache.spark.sql.Dataset[analyzer.MutationSummary]
Call: filter

col(""mutationCount"") >= cloneSizeThreshold
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Chronicler.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""eventKind"") =!= LIT_REMOVED
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Chronicler.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isnull(col(""parentId""))
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Chronicler.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isnull(col(""parentId""))
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Muller.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""mutationSize"") > threshold
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Phylogeny.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""mutationId"") =!= col(""parentMutationId"")
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Phylogeny.scala/udf/44.21.Dataset-Ancestry).map","Type: org.apache.spark.sql.Dataset[(analyzer.MutationTreeLink, analyzer.Ancestry)]
Call: map

x => Ancestry(x._1.mutationId, x._2.ancestors :+ x._1.mutationId)
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Snapshots.scala/udf/19.22.Dataset-ChronicleEntry.filter","Type: org.apache.spark.sql.Dataset[analyzer.ChronicleEntry]
Call: filter

col(""birthTime"") <= lit(timePoint) && lit(timePoint) < col(""deathTime"")
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Snapshots.scala/udf/48.24.Dataset-ChronicleEntry.filter","Type: org.apache.spark.sql.Dataset[analyzer.ChronicleEntry]
Call: filter

col(""birthTime"") < lit(t) && lit(t) < col(""deathTime"")
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Snapshots.scala/udf/57.20.Dataset-ChronicleEntry.filter","Type: org.apache.spark.sql.Dataset[analyzer.ChronicleEntry]
Call: filter

col(""deathTime"") === Double.PositiveInfinity
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Snapshots.scala/udf/61.20.Dataset-ChronicleEntry.filter","Type: org.apache.spark.sql.Dataset[analyzer.ChronicleEntry]
Call: filter

col(""deathTime"") === Double.PositiveInfinity
"
"udf/spark_repos_4/1_tozanski_SimBaD-analyzer/..spark.Analyzer.src.main.scala.analyzer.Snapshots.scala/udf/67.24.Dataset-ChronicleEntry.filter","Type: org.apache.spark.sql.Dataset[analyzer.ChronicleEntry]
Call: filter

col(""birth_time"") < lit(t) && col(""death_time"") > lit(t)
"
"udf/spark_repos_4/1_windyzj_sparkmall1015/..sparkmall-offline.src.main.scala.com.atguigu.sparkmall1015.offline.app.AreaTop3ProductApp.scala/udf/14.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityRatioUDAF
"
"udf/spark_repos_4/1_WordBearerYI_coursera_scala/..Big_Data_Analysis_With_Scala_and_Spark.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/105.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

l => TimeUsageRow(l._1._1, l._1._2, l._1._3, l._2, l._3, l._4)
"
"udf/spark_repos_4/1_WordBearerYI_coursera_scala/..Big_Data_Analysis_With_Scala_and_Spark.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/98.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
"
"udf/spark_repos_4/1_wudingli_text_clustering/..src.main.scala.liutao.SearchHelper.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Vector](2).toSparse.indices.contains(row.getInt(3))
"
"udf/spark_repos_4/1_Xingbei99_p2p_lending_data_analysis/..src.main.scala.p2p_data_analysis.spark.io.LoanDataReader.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""loan_status"" =!= ""Fully Paid""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_dataframe.scala/udf/22.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""carat"" > 0.2d
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_dataframe.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""y"".isin(0, 4)
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_dataframe.scala/udf/56.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

a => a.contains(""VV"")
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_dataframe.scala/udf/79.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val carat = row.getAs[String](""carat"").toDouble
        val color = row.getAs[String](""color"")
        val clarity = row.getAs[String](""clarity"")
        val x = row.getAs[String](""x"").toDouble
        val y = row.getAs[String](""y"").toDouble
        val z = row.getAs[String](""z"").toDouble
        val depth = row.getAs[String](""depth"")
        val arr1 = Seq(x, y, z)
        val arr2 = Seq(color, clarity, depth)
        val map1 = Map((color, depth))
        (carat, color, clarity, x, y, z, depth, arr1, arr2)
      }
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Tname"" === ""李逵""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/134.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" < all_cids.get(0)(0)
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/139.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[String], b: Seq[String]) => a.intersect(b)
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/143.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[String]) => a.toArray.length
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/172.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SID"" === ""01""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/174.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

a => a.getAs[Seq[String]](""c1"").toArray
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/179.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

a => {
          val sid = a.getAs[String](""SID"")
          val c_all = a.getAs[Seq[String]](""c_all"").toArray.intersect(t1_c1).length
          (sid, c_all)
        }
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/185.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SID"" =!= ""01"" && $""nums"" >= 1
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/216.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SID"" === ""01""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/218.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

a => a.getAs[Seq[String]](""c1"").toArray
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/224.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

a => {
          val sid = a.getAs[String](""SID"")
          val c_all = a.getAs[Seq[String]](""c_all"").toArray.intersect(t1_c1_s).length
          (sid, c_all)
        }
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/230.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SID"" =!= ""01"" && $""nums"" === c1_l
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/262.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Tname"" === ""张三""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/264.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

a => a.getAs[Seq[String]](""c1"").toArray
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/268.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

a => {
        val sid = a.getAs[String](""SID"")
        val c_all = a.getAs[Seq[String]](""c_all"").toArray.intersect(t1_c1_t).length
        (sid, c_all)
      }
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/41.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""CID"" === ""01"" && $""CID2"" === ""02""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/43.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""score"" > ($""score2"")
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/60.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""CID"" === ""01"" || $""CID"" === ""02""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" >= 2
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""avg_score"" >= 60
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_1_10.scala/udf/99.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Tname"" like ""李%""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_11_20.scala/udf/146.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank1"" <= 3
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_11_20.scala/udf/174.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" >= 2
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_11_20.scala/udf/40.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""score"" <= 60
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_11_20.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" >= 2
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_11_20.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""CID"" === ""01"" && $""score"" <= 60
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/100.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""score_avg"" >= 60
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Cname"" === ""数学""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/121.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""CID"" === CID_math && $""score"" < 60
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/151.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""score"" > 70
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/164.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""score"" < 60
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/180.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""CID"" === ""01"" && $""score"" > 40
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/215.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Tname"" === ""张三""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/220.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank1"" === 1
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/237.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" > 1
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/253.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank1"" <= 2
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/267.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" >= 5
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/281.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" >= 2
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/296.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" === cid_all
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Sname"" like ""%风%""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""nums"" > 1
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_sql_exercies.spark_sql_exercies_21_45.scala/udf/66.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

substring($""Sage"", 0, 4) === ""1990""
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_udf.scala/udf/19.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Double) => a * 9.0d / 5.0d + 32.0d
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_udf.scala/udf/23.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: String) => a.length
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_udf.scala/udf/27.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[String], b: Seq[String]) => a.intersect(b)
"
"udf/spark_repos_4/1_ybm1_spark_learning/..src.scala.bigdata.spark_udf.scala/udf/41.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val carat = row.getAs[String](""carat"").toDouble
        val color = row.getAs[String](""color"")
        val clarity = row.getAs[String](""clarity"")
        val x = row.getAs[String](""x"").toDouble
        val y = row.getAs[String](""y"").toDouble
        val z = row.getAs[String](""z"").toDouble
        val depth = row.getAs[String](""depth"")
        val arr1 = Seq(x, y, z)
        val arr2 = Seq(color, clarity, depth)
        (carat, color, clarity, x, y, z, depth, arr1, arr2)
      }
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/37.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/1_yeqqmatlab_spark-demo/..src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_4/1_zhao258147_spark-kafka-ssl/..src.main.scala.com.yzhao.kafkassl.Main.scala/udf/34.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{
        (time: String) => time -> time
      }
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..spark-realtime.src.main.scala.com.atguigu.realtime.app.RealtimeApp.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

v => {
        val split: Array[String] = v.split("","")
        val date: Date = new Date(split(0).toLong)
        AdsInfo(split(0).toLong, new Timestamp(split(0).toLong), dayStringFormatter.format(date), hmStringFormatter.format(date), split(1), split(2), split(3), split(4))
      }
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.BasicOperation2.scala/udf/13.24.Dataset-People.filter","Type: org.apache.spark.sql.Dataset[com.atguigu.ss.People]
Call: filter

_.age > 20
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.BasicOperation2.scala/udf/15.19.Dataset-People.map","Type: org.apache.spark.sql.Dataset[com.atguigu.ss.People]
Call: map

_.name
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.KafkaSink2.scala/udf/8.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0) + "","" + row.getLong(1)
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.KafkaSink.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0) + "","" + row.getLong(1)
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.StreamDropDuplicate.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), Timestamp.valueOf(arr(1)), arr(2))
      }
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.StreamingStatic.scala/udf/12.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr = line.split("","")
        (arr(0), arr(1).toInt)
      }
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.StreamStream1.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), arr(1), Timestamp.valueOf(arr(2)))
      }
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.StreamStream1.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), arr(1).toInt, Timestamp.valueOf(arr(2)))
      }
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.StreamStream2.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), arr(1), Timestamp.valueOf(arr(2)))
      }
"
"udf/spark_repos_4/1_zhenchao125_spark-structured-streaming/..src.main.scala.com.atguigu.ss.StreamStream2.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), arr(1).toInt, Timestamp.valueOf(arr(2)))
      }
"
"udf/spark_repos_4/1_zhewen166_MLlib-scalability/..src.main.scala.ml.ALSExample.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_4/1_zhewen166_MLlib-scalability/..src.main.scala.ml.ALSExample.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_4/23_datastax_spark-cassandra-stress/..src.main.scala.com.datastax.sparkstress.ReadTask.scala/udf/130.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""order_time"") < lit(instantToTimestamp(timePivot))
"
"udf/spark_repos_4/23_datastax_spark-cassandra-stress/..src.main.scala.com.datastax.sparkstress.ReadTask.scala/udf/142.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""order_time"") < lit(instantToTimestamp(timePivot))
"
"udf/spark_repos_4/23_datastax_spark-cassandra-stress/..src.main.scala.com.datastax.sparkstress.ReadTask.scala/udf/179.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""order_time"") < lit(instantToTimestamp(timePivot))
"
"udf/spark_repos_4/23_datastax_spark-cassandra-stress/..src.main.scala.com.datastax.sparkstress.ReadTask.scala/udf/191.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""store"") === ""Store 5""
"
"udf/spark_repos_4/23_datastax_spark-cassandra-stress/..src.main.scala.com.datastax.sparkstress.WriteTask.scala/udf/49.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.toString()
"
"udf/spark_repos_4/23_mspnp_azure-databricks-streaming-analytics/..azure.AzureDataBricksJob.src.main.scala.com.microsoft.pnp.TaxiCabReader.scala/udf/35.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

neighborhoodFinder
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.main.scala.com.waitingforcode.structuredstreaming.corrupted_records.IgnoreErrorsLogging.scala/udf/10.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => {
      val convertedLetter = row.getAs[Row](""letter"")
      if (convertedLetter == null) {
        println(s""Record ${row.getAs[String](""value_as_string"")} cannot be converted"")
        false
      } else {
        true
      }
    }
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.main.scala.com.waitingforcode.structuredstreaming.corrupted_records.Wrapper.scala/udf/10.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
      val letter = row.getAs[Row](""letter"")
      if (letter != null) {
        val sentinelData = locally {
          val _t_m_p_2 = Option(letter)
          _t_m_p_2.map(letterData => SentinelForDLQData(lower = letterData.getAs[String](""lower""), upper = letterData.getAs[String](""upper"")))
        }
        SentinelForDLQWrapper(data = sentinelData)
      } else {
        SentinelForDLQWrapper(data = None, error = Some(SentinelForDLQError(row.getAs[String](""value_as_string""))))
      }
    }
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.main.scala.com.waitingforcode.structuredstreaming.multipletopics.DifferentInputBigSchema.scala/udf/28.17.Dataset-SharedEntry.map","Type: org.apache.spark.sql.Dataset[com.waitingforcode.structuredstreaming.multipletopics.DifferentInputBigSchema.SharedEntry]
Call: map

sharedSchema => sharedSchema.toCommonFormat
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.main.scala.com.waitingforcode.structuredstreaming.multipletopics.DifferentInputTopicFilter.scala/udf/17.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""topic"" === topic1
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.main.scala.com.waitingforcode.structuredstreaming.multipletopics.DifferentInputTopicFilter.scala/udf/21.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""topic"" === topic2
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.AutomatedMetadataInsertionTest.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

order => {
        val is2019Order = order.getAs[Int](""year"") == 2019
        if (!is2019Order) filteredAccumulator.add(1L)
        allOrdersAccumulator.add(1L)
        is2019Order
      }
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.avro.AvroSparkCompatibilityTest.scala/udf/41.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.mkString(""; "")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.CaseSensitivityTest.scala/udf/34.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""""""
          |value = 1 OR Value = 1
        """""".stripMargin
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.CaseSensitivityTest.scala/udf/38.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs[Int](""value""), row.getAs[Int](""Value""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.CaseSensitivityTest.scala/udf/50.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""""""
          |Value = 1
        """""".stripMargin
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.CaseSensitivityTest.scala/udf/54.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs[String](""key""), row.getAs[Int](""Value""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.CaseSensitivityTest.scala/udf/68.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""""""
          |value = 1 OR Value = 1
        """""".stripMargin
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.CaseSensitivityTest.scala/udf/72.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs[String](""key""), row.getAs[Int](""value""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeConversionTest.scala/udf/37.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs[String](""key""), row.getAs[Timestamp](""log_action"").toString)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeConversionTest.scala/udf/60.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs[String](""key""), row.getAs[Timestamp](""log_action"").toString)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/105.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date_dayofweek"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/114.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date_dayofyear"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/123.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""string_datetime"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date_with_2_months"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/132.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""paris_datetime"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/141.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Integer](""hour"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/150.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""last_day_of_month"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/159.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Integer](""minute"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/168.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date_month"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/177.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Double](""diff"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/186.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""next_monday"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/195.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date_quarter"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/204.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Integer](""second"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/215.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/226.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""timestamp"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/235.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""utc_datetime"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""current_date"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/244.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""truncated_date"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/258.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Integer](""date_year"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/267.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Integer](""date_weekofyear"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/276.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""1_min_window"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""current_timestamp"").dropRight(8)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Timestamp](""current_timestamp"").getTime
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/51.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date_in_45_days"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""formatted_date"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date_45_days_ago"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/78.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""truncated_datetime"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/87.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Integer](""diff"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.DateTimeFunctionsTest.scala/udf/96.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""date_dayofmonth"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.deltalake.MainFeaturesTest.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Int](""nr"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.deltalake.MainFeaturesTest.scala/udf/52.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Int](""nr"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.deltalake.MainFeaturesTest.scala/udf/91.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Int](""nr"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.extraoptimizations.UnionAdvancedHintTest.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => s""${row.getAs[String](""letter"")}-${row.getAs[Int](""nr"")}-${row.getAs[Int](""a_flag"")}""
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.extraoptimizations.UnionSimpleHintTest.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => s""${row.getAs[String](""letter"")}-${row.getAs[Int](""nr"")}-${row.getAs[Int](""a_flag"")}""
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.files_issues.IgnoreMissingFilesTest.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => {
        FilesManager.removeFiles()
        println(s""filtering $row"")
        true
      }
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.HiveBehaviorInsertOverwriteTest.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => HiveTestConverter.mapRowToString(row)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.HiveBehaviorInsertOverwriteTest.scala/udf/26.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => HiveTestConverter.mapRowToString(row)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.HiveBehaviorInsertOverwriteTest.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => HiveTestConverter.mapRowToString(row)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.HiveBehaviorInsertOverwriteTest.scala/udf/46.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => HiveTestConverter.mapRowToString(row)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.HiveBehaviorInsertOverwriteTest.scala/udf/54.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => HiveTestConverter.mapRowToString(row)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.HiveBehaviorInsertOverwriteTest.scala/udf/66.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => HiveTestConverter.mapRowWithLetterToString(row)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.HiveBehaviorInsertOverwriteTest.scala/udf/74.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => HiveTestConverter.mapRowWithLetterToString(row)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.IdempotentConsumerTest.scala/udf/15.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => IdentifiedIdempotentConsumerEven(IdempotentConsumerRowMapper.id(row), user(row), dateTime(row))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.JdbcOptionsTest.scala/udf/114.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""id"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.JoinTypesTest.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Mapper.mapJoinedRow(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.JoinTypesTest.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Mapper.mapJoinedRow(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.JoinTypesTest.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Mapper.mapJoinedRow(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.JoinTypesTest.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Mapper.mapJoinedRow(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.JoinTypesTest.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Mapper.mapJoinedRow(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.JoinTypesTest.scala/udf/51.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Mapper.halfMapJoinedRow(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.JoinTypesTest.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Mapper.halfMapJoinedRow(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/100.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""var_pop""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/10.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Integer](""nr""), row.getAs[Seq[String]](""collected_letters""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/110.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""skewness""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Integer](""nr""), row.getAs[Seq[String]](""collected_letters""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""correlation""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/40.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""kurtosis""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""covar_pop""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""covar_samp""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/70.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""ssd""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/80.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""ssp""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LessKnownAggregationFunctionsTest.scala/udf/90.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""nr1""), row.getAs[Double](""var_samp""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LogicalPlanOptimizersTest.scala/udf/220.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toString()
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LogicalPlanOptimizersTest.scala/udf/222.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.toString()
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LogicalPlanOptimizersTest.scala/udf/235.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toString
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.LogicalPlanOptimizersTest.scala/udf/237.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.toString
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.ProcessingAbstractionTest.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Long](""offset"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.ProcessingAbstractionTest.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""topic"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Long](""generated_id"") == null
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Long](""generated_id"") != null
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/21.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => row.getAs[Double](""generated_amount"") != row.getAs[Double](""amount"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/25.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => row.getAs[Set[Long]](""generated_itemIds"") != row.getAs[Set[Long]](""itemIds"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Long](""generated_id"") == null
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Long](""id"") != null && row.getAs[Long](""generated_id"") != null
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/48.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => row.getAs[Double](""generated_amount"") != row.getAs[Double](""amount"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/52.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => row.getAs[Set[Long]](""generated_itemIds"") != row.getAs[Set[Long]](""itemIds"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.RegressionTestsExampleTest.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[Long](""generated_id"") != null && row.getAs[Long](""id"") == null
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.SaveModesTest.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Converter.rowToOrder(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.SaveModesTest.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Converter.rowToOrder(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.SaveModesTest.scala/udf/56.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Converter.rowToOrder(_)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.StatisticsTest.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

shops(""id"") % 2 === 0
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedAggregationFunctionTest.scala/udf/29.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new SessionDurationAggregator
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionChainOfResponsibilityContainerTest.scala/udf/12.55.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new ChainOfResponsibilityContainer(chainedFunctions).execute _
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionChainOfResponsibilityContainerTest.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getInt(1))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/101.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/109.55.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

evenNumbersFilter
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/113.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getString(0), row.getInt(1))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/23.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

evenFlagResolver _
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getBoolean(1))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/36.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getBoolean(1))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getBoolean(1))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/53.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getInt(1))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getString(1))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/71.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getString(1))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.UserDefinedFunctionTest.scala/udf/86.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WholeStageCodegenExecTest.scala/udf/13.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_active"" === true
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WholeStageCodegenExecTest.scala/udf/15.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => s""User ${row.getAs[String](""login"")} is active :-)""
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WholeStageCodegenExecTest.scala/udf/28.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_active"" === true
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WholeStageCodegenExecTest.scala/udf/30.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => s""User ${row.getAs[String](""login"")} is active :-)""
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/109.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Long](""all_players_in_team""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/120.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Long](""all_players_in_team""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/130.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Double](""rows_avg""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/141.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Long](""range_sum""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/151.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Double](""rows_avg""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/161.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Double](""rows_avg""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/172.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Double](""rows_avg""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[String](""next_scorer""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[String](""previous_scorer""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Int](""scorer_position""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/48.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Int](""scorer_position""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/58.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Int](""nr""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Int](""group""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/79.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Double](""avg_goals""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/89.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Double](""players_below""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.sql.WindowFunctionsTest.scala/udf/99.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""name""), row.getAs[Int](""goals""), row.getAs[Double](""relative_position_from_the_best""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.structuredstreaming.ContinuousProcessingTest.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

rateRow => {
        Thread.sleep(500L)
        """"
      }
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.structuredstreaming.FaultToleranceTest.scala/udf/33.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
            val fieldName = row.getAs[String](""name"")
            if (fieldName == ""content1=7"" && !GlobalFailureFlag.alreadyFailed.get() && GlobalFailureFlag.mustFail.get()) {
              GlobalFailureFlag.alreadyFailed.set(true)
              GlobalFailureFlag.mustFail.set(false)
              throw new RuntimeException(""Something went wrong"")
            }
            fieldName
          }
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.structuredstreaming.OutputModeRemainingProcessingTest.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""name"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.structuredstreaming.OutputModeRemainingProcessingTest.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""name"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.structuredstreaming.OutputModeRemainingProcessingTest.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""name"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.tips.NestedDataManipulationTest.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val browserContext = row.getAs[Row](""browser"")
        s""${row.getAs[String](""user_id"")}-${browserContext.getAs[String](""name"")}-${browserContext.getAs[String](""version"")}""
      }
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.tips.RemoveDuplicatesTest.scala/udf/13.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getAs[String](""letter"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.tips.RemoveDuplicatesTest.scala/udf/23.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => (row.getAs[String](""letter""), row.getAs[Int](""number""), row.getAs[Boolean](""was_read""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.tips.SqlInClauseTest.scala/udf/38.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getString(1)
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.tips.SqlQueryReadabilityTest.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""user_id"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.tips.SqlQueryReadabilityTest.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""user_id"")
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.tips.UdfFromColumnOperationTest.scala/udf/10.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(orderId: Long) => s""user$orderId""
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.com.waitingforcode.tips.UdfFromColumnOperationTest.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Long](""order_id""), row.getAs[String](""user_id""))
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.org.apache.spark.sql.types.UDTExample.scala/udf/42.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[City](""city"").isFrench
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.org.apache.spark.sql.types.UDTExample.scala/udf/47.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[City](""city"").country.toString
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.org.apache.spark.sql.types.UDTExample.scala/udf/55.19.Dataset-City.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.types.City]
Call: map

row => row.isFrench
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.org.apache.spark.sql.types.UDTExample.scala/udf/60.19.Dataset-City.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.types.City]
Call: map

row => row.country.toString
"
"udf/spark_repos_4/24_bartosz25_spark-scala-playground/..src.test.scala.org.apache.spark.sql.types.UDTExample.scala/udf/66.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[City](""city"").isFrench
"
"udf/spark_repos_4/27_Azure_azure-kusto-spark/..connector.src.test.scala.com.microsoft.kusto.spark.KustoSinkBatchE2E.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.get(1) == 1
"
"udf/spark_repos_4/2_artkostm_hotels-kafka-streams/..spark-common.src.main.scala.by.artsiom.bigdata101.hotels.HotelImplicits.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val bais = new ByteArrayInputStream(row.getAs[Array[Byte]](columnToParse))
        val input = AvroInputStream.binary[Event].from(bais).build(new Schema.Parser().parse(rowEventSchema.value))
        acc.add(1)
        input.iterator.next()
      }
"
"udf/spark_repos_4/2_BhushG_StructuredStreaming/..src.main.scala.Practice.ProcessLogsPractice.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs(""Price"") != null
"
"udf/spark_repos_4/2_BhushG_StructuredStreaming/..src.main.scala.ProcessServerLogs.NumberOfEventsPerUnitTime.scala/udf/26.19.Dataset-RawEvent.map","Type: org.apache.spark.sql.Dataset[ProcessServerLogs.NumberOfEventsPerUnitTime.RawEvent]
Call: map

createEvent
"
"udf/spark_repos_4/2_BhushG_StructuredStreaming/..src.main.scala.ProcessServerLogs.Sessionisation.scala/udf/29.19.Dataset-RawEvent.map","Type: org.apache.spark.sql.Dataset[ProcessServerLogs.Sessionisation.RawEvent]
Call: map

createEvent
"
"udf/spark_repos_4/2_BhushG_StructuredStreaming/..src.main.scala.ProcessServerLogs.SessionizeCartItems.scala/udf/34.19.Dataset-RawEvent.map","Type: org.apache.spark.sql.Dataset[ProcessServerLogs.SessionizeCartItems.RawEvent]
Call: map

createEvent
"
"udf/spark_repos_4/2_BhushG_StructuredStreaming/..src.main.scala.ProcessServerLogs.SessionizeCartItems.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs(""ProductId"") != null
"
"udf/spark_repos_4/2_emilyaherbert_SimulatingAntMovement/..src.main.scala.misc.NOAAClustering.scala/udf/22.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
      val id = line.substring(0, 11)
      val lat = line.substring(12, 20).trim.toDouble
      val lon = line.substring(21, 30).trim.toDouble
      val elev = line.substring(31, 37).trim.toDouble
      val name = line.substring(41, 71).trim
      Station(id, lat, lon, elev, name)
    }
"
"udf/spark_repos_4/2_emilyaherbert_SimulatingAntMovement/..src.main.scala.utility.SimilarSizeKMeans.scala/udf/98.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""cluster"") === cluster.toString()
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.basic_aggregate.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""value"".isNotNull
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.caching.scala/udf/23.22.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

$""id"" % 2 === 0
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.caching.scala/udf/27.22.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

$""id"" % 2 === 0
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.collection_func.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fil
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.column.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.dataset.scala/udf/11.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != headers
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.dataset.scala/udf/33.22.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

$""id"" === 0
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.dataset.scala/udf/37.22.Dataset-Long.filter","Type: org.apache.spark.sql.Dataset[Long]
Call: filter

_ == 0
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.typed_transformations.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 20
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.typed_transformations.scala/udf/32.19.Dataset-Sentence.map","Type: org.apache.spark.sql.Dataset[spark.typed_transformations.Sentence]
Call: map

s => s.text.length > 12
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.UDAF.scala/udf/92.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

myMax
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.UDF.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

equalUDF($""word"")
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.UDF.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""word"".equalTo(""hello"")
"
"udf/spark_repos_4/2_Flyraty_daily_scala/..src.main.scala.spark.window_aggregate.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""dense_rank"" <= 2
"
"udf/spark_repos_4/2_g1thubhub_philstopwatch/..src.main.scala.profile.sparkjobs.JobFatso.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

fatFunctionOuter
"
"udf/spark_repos_4/2_g1thubhub_philstopwatch/..src.main.scala.profile.sparkjobs.JobHeckler.scala/udf/30.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => {
        val props = new Properties()
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,parse"")
        val annotation = new Annotation(string)
        val pipeline = new StanfordCoreNLP(props)
        pipeline.annotate(annotation)
        val parseTree = annotation.get(classOf[SentencesAnnotation]).get(0).get(classOf[TreeAnnotation])
        parseTree.toString
      }
"
"udf/spark_repos_4/2_g1thubhub_philstopwatch/..src.main.scala.profile.sparkjobs.JobHeckler.scala/udf/59.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

string => {
        val annotation = new Annotation(string)
        pipelineBroadcast.value.pipeline.annotate(annotation)
        val parseTree = annotation.get(classOf[SentencesAnnotation]).get(0).get(classOf[TreeAnnotation])
        parseTree.toString
      }
"
"udf/spark_repos_4/2_g1thubhub_philstopwatch/..src.main.scala.profile.sparkjobs.JobSlacker.scala/udf/22.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

slacking
"
"udf/spark_repos_4/2_g1thubhub_philstopwatch/..src.main.scala.profile.sparkjobs.JobStraggler.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

letter => if (letter == ""d"") (letter, 245000L) else (letter, 30000L)
"
"udf/spark_repos_4/2_g1thubhub_philstopwatch/..src.main.scala.profile.sparkjobs.JobStraggler.scala/udf/20.19.Dataset-(String, Long).map","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: map

pair => {
        var list = List.empty[Long]
        val letter = pair._1
        val frequ = pair._2
        for (i <- 0L until frequ) {
          list = list :+ i
        }
        (letter, list.sum)
      }
"
"udf/spark_repos_4/2_gitAxin_MovieRecommendSystem/..Recommend.Statistics.src.main.scala.com.axin.statistics.Algorithm.scala/udf/17.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Long) => simpleDateFormat.format(new Date(x * 1000L)).toLong
"
"udf/spark_repos_4/2_Hodapp87_mimic3_phenotyping/..src.main.scala.mimic3_phenotyping.main.Main.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" >= lab_min_patients
"
"udf/spark_repos_4/2_Hodapp87_mimic3_phenotyping/..src.main.scala.mimic3_phenotyping.main.Main.scala/udf/129.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(isnull($""HADM_ID""))
"
"udf/spark_repos_4/2_Hodapp87_mimic3_phenotyping/..src.main.scala.mimic3_phenotyping.main.Main.scala/udf/143.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(isnull($""LOINC_CODE""))
"
"udf/spark_repos_4/2_Hodapp87_mimic3_phenotyping/..src.main.scala.mimic3_phenotyping.main.Main.scala/udf/162.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""num_code1"" > 0 =!= $""num_code2"" > 0
"
"udf/spark_repos_4/2_Hodapp87_mimic3_phenotyping/..src.main.scala.mimic3_phenotyping.main.Main.scala/udf/170.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""LOINC_CODE"" === config.loincTest
"
"udf/spark_repos_4/2_Hodapp87_mimic3_phenotyping/..src.main.scala.mimic3_phenotyping.main.Main.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" >= lab_min_series
"
"udf/spark_repos_4/2_kwaikar_blueray/..src.test.scala.edu.utd.security.blueray.GenericTests.scala/udf/129.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.mkString.contains(stringToBeBlocked)
"
"udf/spark_repos_4/2_kwaikar_blueray/..src.test.scala.edu.utd.security.blueray.GenericTests.scala/udf/162.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.mkString.contains(stringToBeBlocked)
"
"udf/spark_repos_4/2_kwaikar_blueray/..src.test.scala.edu.utd.security.blueray.GenericTests.scala/udf/187.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.mkString.contains(stringToBeBlocked)
"
"udf/spark_repos_4/2_martinprobson_spark-weave/..src.main.scala.net.martinprobson.spark.sparkweave.Main.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        (r: Row) => r match {
          case Row(k: String, _) =>
            k == ""E""
        }
      }
"
"udf/spark_repos_4/2_martinprobson_spark-weave/..src.main.scala.net.martinprobson.spark.sparkweave.Partition.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""partition"".contains(partSpec)
"
"udf/spark_repos_4/2_martinprobson_spark-weave/..src.test.scala.net.martinprobson.spark.sparkweave.ResultTransformerTest.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
        (r: Row) => r match {
          case Row(k: String, _) =>
            k == ""E""
        }
      }
"
"udf/spark_repos_4/2_masteryang4_spark/..sql.SparkSQL04_UDF.scala/udf/12.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(name: String) => ""姓名 ："" + name
"
"udf/spark_repos_4/2_masteryang4_spark/..sql.SparkSQL04_UDF.scala/udf/16.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(age: Int) => ""年龄 ："" + age
"
"udf/spark_repos_4/2_masteryang4_spark/..sql.SparkSQL05_UDF1.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => ""Name:"" + row.getString(1)
"
"udf/spark_repos_4/2_masteryang4_spark/..sql.SparkSQL08_UDAF_DIY.scala/udf/14.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udaf
"
"udf/spark_repos_4/2_masteryang4_spark/..sql.SparkSQL16_Req_Impl2.scala/udf/13.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

cityRemarkUDAF
"
"udf/spark_repos_4/2_nirvana198801_tispark_for_spark-2.0.2/..core.src.main.scala.com.pingcap.tispark.TiUtils.scala/udf/167.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

() => TiSparkVersion.version
"
"udf/spark_repos_4/2_nnkumar13_nelamalli-projects-test/..spark-examples.src.main.scala.com.nelamalli.spark.dataframe.DataFrameWithSimpleDSL.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""State"") === ""PR""
"
"udf/spark_repos_4/2_nnkumar13_nelamalli-projects-test/..spark-examples.src.main.scala.com.nelamalli.spark.dataframe.UDFDataFrame.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

to_date(replaceUDF($""date1"")) > date_add(to_date(replaceUDF(lit(minDate))), 7)
"
"udf/spark_repos_4/2_nnkumar13_nelamalli-projects-test/..spark-examples.src.main.scala.com.nelamalli.spark.dataframe.xml.ReadBooksXML.scala/udf/26.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => BooksDiscounted(f.getAs(""_id""), f.getAs(""author""), f.getAs(""description""), f.getAs(""price""), f.getAs(""publish_date""), f.getAs(""title""), f.getDouble(4) - f.getDouble(4) * 20 / 100)
"
"udf/spark_repos_4/2_nnkumar13_nelamalli-projects-test/..spark-examples.src.main.scala.com.nelamalli.spark.stackoverflow.AddingLiterral.scala/udf/17.19.Dataset-Employee.map","Type: org.apache.spark.sql.Dataset[com.sparkbyexamples.spark.stackoverflow.Employee]
Call: map

rec => (EmpData(""1"", rec.EmpId), EmpData(""2"", rec.Experience.toString), EmpData(""3"", rec.Salary.toString))
"
"udf/spark_repos_4/2_SamsonAudrey_WI/..src.main.scala.models.LRModel.scala/udf/35.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(""label"") === 1.0d
"
"udf/spark_repos_4/2_SamsonAudrey_WI/..src.main.scala.models.LRModel.scala/udf/39.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(""label"") === 0.0d
"
"udf/spark_repos_4/2_SamsonAudrey_WI/..src.main.scala.models.LRModel.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataset(""label"") === 0.0d
"
"udf/spark_repos_4/2_SamsonAudrey_WI/..src.main.scala.tools.DataCleaner.scala/udf/114.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""label"") === 1.0d
"
"udf/spark_repos_4/2_SamsonAudrey_WI/..src.main.scala.tools.DataCleaner.scala/udf/119.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""label"") === 0.0d
"
"udf/spark_repos_4/2_scality_clueso/..src.main.scala.com.scality.clueso.query.MetadataQueryExecutor.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!(col(""key"") contains "" "")
"
"udf/spark_repos_4/2_TalentOrigin_source-code/..sparkcourse.src.main.scala.advanced.spark.SparkUDFExample2.scala/udf/10.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

averageUDF
"
"udf/spark_repos_4/2_TalentOrigin_source-code/..sparkcourse.src.main.scala.transformations.SparkReference.scala/udf/14.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

toUpperUDF
"
"udf/spark_repos_4/2_TalentOrigin_source-code/..sparkcourse.src.main.scala.transformations.SparkReference.scala/udf/18.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

fetchFirstGenreUDF
"
"udf/spark_repos_4/2_TalentOrigin_source-code/..sparkcourse.src.main.scala.transformations.SparkReference.scala/udf/22.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

concatStringsUDF
"
"udf/spark_repos_4/2_TalentOrigin_source-code/..sparkcourse.src.main.scala.transformations.SparkReference.scala/udf/26.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

columnLengthsUDF
"
"udf/spark_repos_4/2_umayrh_sketchy-polytopes/..sparkScala.intervalGraph.src.test.scala.com.umayrh.intervalGraph.DateOverlapIntegrationTest.scala/udf/24.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"") > Date.valueOf(""2019-01-02"")
"
"udf/spark_repos_4/2_xxiaollong_spark-maven/..src.main.scala.com.example.spark.sql.SparkSQL.scala/udf/39.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

myAvg
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.ml.recommendation.ALSPredict.scala/udf/45.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.ml.recommendation.ALSPredict.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val predict = row.getAs[Double](""prediction"")
        val label = row.getAs[Double](""rating"")
        s""$predict $label""
      }
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.ml.recommendation.ALSTrain.scala/udf/46.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.preprocess.DataFrame2Text.scala/udf/53.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(delemiter)
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.preprocess.FeatureIndex.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val rowIdx = row.getAs[Long](""id"")
        val text = row.getAs[String](p.inputCol)
        val newText = locally {
          val _t_m_p_5 = locally {
            val _t_m_p_6 = locally {
              val _t_m_p_7 = text.split("" "")
              _t_m_p_7.map { feature => 
                val arr = feature.split("":"")
                val featureName = arr(0)
                val feafureVal = if (arr.length <= 1 || """".equals(arr(1))) 0d else arr(1).toDouble
                val featureIndex = featureIdxMap.getOrElse(featureName, -1)
                (featureIndex, feafureVal)
              }
            }
            _t_m_p_6.filter({
              case (featureIndex, feafureVal) =>
                featureIndex >= 0 && feafureVal != 0
            })
          }.sortBy(_._1)
          _t_m_p_5.map({
            case (featureIndex, feafureVal) =>
              featureIndex + "":"" + feafureVal
          })
        }.mkString("" "")
        (rowIdx, newText)
      }
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.preprocess.IDF.scala/udf/37.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[String](p.inputCol)
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.preprocess.IDF.scala/udf/39.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("" "")
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.preprocess.WordSegment.scala/udf/41.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(text: String) => tokenize(text)
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.preprocess.WordTFIDF.scala/udf/60.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val rowIdx = row.getAs[Long](""id"")
          val text = row.getAs[String](p.inputCol)
          locally {
            val _t_m_p_7 = text.split("" "").distinct.zipWithIndex
            _t_m_p_7.map({
              case (word, colIdx) =>
                (word, colIdx, rowIdx)
            })
          }
        }
"
"udf/spark_repos_4/2_yizt_easyml-examples/..src.main.scala.com.es.preprocess.WordTFIDF.scala/udf/76.19.Dataset-(String, Int, Long).map","Type: org.apache.spark.sql.Dataset[(String, Int, Long)]
Call: map

{
        case (word, colIdx, rowIdx) =>
          (word, 1)
      }
"
"udf/spark_repos_4/2_yxf0101_useractionanrealtimeanalysis/..src.main.scala.com.ods.spark.service.AreaTop3Commodity.scala/udf/35.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

myUDF1
"
"udf/spark_repos_4/2_yxf0101_useractionanrealtimeanalysis/..src.main.scala.com.ods.spark.service.AreaTop3Commodity.scala/udf/39.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

myUDF2
"
"udf/spark_repos_4/2_yxf0101_useractionanrealtimeanalysis/..src.main.scala.com.ods.spark.service.AreaTop3Commodity.scala/udf/43.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new GroupConcatDistinctUDAF
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.Actual-Project.SparkEcomLogAnalysis.spark-project.src.main.scala.spark.ActualProject.LogJob.TopNStatJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.Actual-Project.SparkEcomLogAnalysis.spark-project.src.main.scala.spark.ActualProject.LogJob.TopNStatJob.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.Actual-Project.SparkEcomLogAnalysis.spark-project.src.main.scala.spark.ActualProject.LogJob.TopNStatJob.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.Actual-Project.SparkEcomLogAnalysis.spark-project.src.main.scala.spark.ActualProject.LogJob.TopNStatJobYarn.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.Actual-Project.SparkEcomLogAnalysis.spark-project.src.main.scala.spark.ActualProject.LogJob.TopNStatJobYarn.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.Actual-Project.SparkEcomLogAnalysis.spark-project.src.main.scala.spark.ActualProject.LogJob.TopNStatJobYarn.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.learning-project.Spark-SQL.project-1.src.main.com.scala.spark.DataFrameApp.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.learning-project.Spark-SQL.project-1.src.main.com.scala.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.learning-project.Spark-SQL.project-1.src.main.com.scala.spark.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_4/2_Zhang-Yixuan_SparkProject/..SparkSQL.learning-project.Spark-SQL.project-1.src.main.com.scala.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.imooc.scala.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_4/2_zhenchao125_spark0225/..spark-sql0225.src.main.scala.com.atguigu.sparksql.AggeFunDemo.scala/udf/11.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyRemark
"
"udf/spark_repos_4/30_pedrovgs_Roma/..src.main.scala.com.github.pedrovgs.roma.machinelearning.Corpus.scala/udf/23.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
          val sentiment = row.getAs[Int](sentimentColumnName)
          sentiment == negativeSentimentCsvValue || sentiment == positiveSentimentCsvValue
        }
"
"udf/spark_repos_4/30_pedrovgs_Roma/..src.main.scala.com.github.pedrovgs.roma.machinelearning.Corpus.scala/udf/28.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ row => 
        val label = if (row.getAs[Int](sentimentColumnName).equals(positiveSentimentCsvValue)) {
          positiveLabel
        } else {
          negativeLabel
        }
        (label, row.getAs[String](contentColumnName))
      }
"
"udf/spark_repos_4/33_YuvalItzchakov_spark-stateful-example/..src.main.scala.com.github.yuvalitzchakov.structuredstateful.StatefulStructuredSessionization.scala/udf/22.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

deserializeUserEvent
"
"udf/spark_repos_4/35_dstlry_dstlr/..src.main.scala.io.dstlr.LoadTriples.scala/udf/127.25.Dataset-TripleRow.filter","Type: org.apache.spark.sql.Dataset[io.dstlr.TripleRow]
Call: filter

$""objectType"" === ""Fact""
"
"udf/spark_repos_4/35_dstlry_dstlr/..src.main.scala.io.dstlr.LoadTriples.scala/udf/18.22.Dataset-TripleRow.filter","Type: org.apache.spark.sql.Dataset[io.dstlr.TripleRow]
Call: filter

$""objectType"" =!= ""Fact""
"
"udf/spark_repos_4/35_dstlry_dstlr/..src.main.scala.io.dstlr.LoadTriples.scala/udf/23.24.Dataset-TripleRow.filter","Type: org.apache.spark.sql.Dataset[io.dstlr.TripleRow]
Call: filter

$""relation"" === ""MENTIONS""
"
"udf/spark_repos_4/35_dstlry_dstlr/..src.main.scala.io.dstlr.LoadTriples.scala/udf/63.24.Dataset-TripleRow.filter","Type: org.apache.spark.sql.Dataset[io.dstlr.TripleRow]
Call: filter

$""relation"" === ""LINKS_TO"" && $""objectValue"".isNotNull
"
"udf/spark_repos_4/35_dstlry_dstlr/..src.main.scala.io.dstlr.LoadTriples.scala/udf/93.27.Dataset-TripleRow.filter","Type: org.apache.spark.sql.Dataset[io.dstlr.TripleRow]
Call: filter

$""relation"" =!= ""MENTIONS""
"
"udf/spark_repos_4/35_dstlry_dstlr/..src.main.scala.io.dstlr.LoadTriples.scala/udf/95.25.Dataset-TripleRow.filter","Type: org.apache.spark.sql.Dataset[io.dstlr.TripleRow]
Call: filter

$""relation"" =!= ""LINKS_TO""
"
"udf/spark_repos_4/35_PacktPublishing_Mastering-Machine-Learning-with-Spark-2.x/..Chapter04.src.main.scala.com.packtpub.mmlwspark.chapter4.Chapter4.scala/udf/48.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ (v: String) => (v, 1) }
"
"udf/spark_repos_4/35_PacktPublishing_Mastering-Machine-Learning-with-Spark-2.x/..Chapter04.src.main.scala.com.packtpub.mmlwspark.chapter4.Chapter4.scala/udf/50.21.Dataset-(String, (String, Int)).map","Type: org.apache.spark.sql.Dataset[(String, (String, Int))]
Call: map

_._2
"
"udf/spark_repos_4/35_PacktPublishing_Mastering-Machine-Learning-with-Spark-2.x/..Chapter04.src.main.scala.com.packtpub.mmlwspark.chapter4.Chapter4.scala/udf/52.22.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

t => t._2 < RARE_TOKEN
"
"udf/spark_repos_4/35_PacktPublishing_Mastering-Machine-Learning-with-Spark-2.x/..Chapter04.src.main.scala.com.packtpub.mmlwspark.chapter4.Chapter4.scala/udf/54.17.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

_._1
"
"udf/spark_repos_4/35_PacktPublishing_Mastering-Machine-Learning-with-Spark-2.x/..Chapter05.src.main.scala.com.packtpub.mmlwspark.chapter5.Chapter5.scala/udf/17.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => Review(1, line)
"
"udf/spark_repos_4/35_PacktPublishing_Mastering-Machine-Learning-with-Spark-2.x/..Chapter05.src.main.scala.com.packtpub.mmlwspark.chapter5.Chapter5.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => Review(0, line)
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.centrifuge.sql.centrifuge_sql.scala/udf/150.66.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mergeAnnotations _
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.centrifuge.sql.centrifuge_sql.scala/udf/163.64.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mergeAnnotations _
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.centrifuge.sql.centrifuge_sql.scala/udf/184.62.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

cleanAnnotation _
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.centrifuge.sql.centrifuge_sql.scala/udf/29.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

f
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.centrifuge.sql.Explore.scala/udf/54.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

to_age _
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.centrifuge.sql.Explore.scala/udf/58.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

non_empty_string _
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.centrifuge.sql.Explore.scala/udf/62.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

add_annotations _
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.centrifuge.sql.Explore.scala/udf/66.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

flatten_annotations[Annotation] _
"
"udf/spark_repos_4/37_univalence_spark-tools/..centrifuge.src.main.scala.io.univalence.centrifuge.Executor.scala/udf/72.25.Dataset-M.map","Type: org.apache.spark.sql.Dataset[M]
Call: map

_._2
"
"udf/spark_repos_4/37_univalence_spark-tools/..plumbus.src.main.scala.io.univalence.plumbus.compress.CompressDump.scala/udf/179.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.get(pos).asInstanceOf[Seq[Row]].size > 1
"
"udf/spark_repos_4/37_univalence_spark-tools/..plumbus.src.main.scala.io.univalence.plumbus.functions.scala/udf/115.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

expr(""not (nbLeft = 1 and nbRight = 1)"")
"
"udf/spark_repos_4/37_univalence_spark-tools/..spark-test.src.main.scala.io.univalence.sparktest.SparkTest.scala/udf/31.22.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

{ (x: T) => !pred(x) }
"
"udf/spark_repos_4/37_univalence_spark-tools/..spark-test.src.main.scala.io.univalence.sparktest.SparkTest.scala/udf/37.22.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

pred
"
"udf/spark_repos_4/37_univalence_spark-tools/..spark-test.src.main.scala.io.univalence.sparktest.SparkTest.scala/udf/45.24.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

col
"
"udf/spark_repos_4/37_univalence_spark-tools/..spark-test.src.main.scala.io.univalence.sparktest.SparkTest.scala/udf/54.24.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

col
"
"udf/spark_repos_4/38_src-d_gemini/..src.main.scala.tech.sourced.gemini.Hash.scala/udf/115.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'content_size > fileSizeThresholdBytes
"
"udf/spark_repos_4/38_src-d_gemini/..src.main.scala.tech.sourced.gemini.Hash.scala/udf/157.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[String](""feature""), row.getAs[Long](""cnt"").toInt)
"
"udf/spark_repos_4/38_src-d_gemini/..src.main.scala.tech.sourced.gemini.Hash.scala/udf/195.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
        case Row(token: String, doc: String, weight: Long) =>
          (doc, Feature(token, weight))
      }
"
"udf/spark_repos_4/3_abdheshkumar_spark-practices/..src.main.scala.standrad.WordCount.scala/udf/17.28.Dataset-ChatLog.filter","Type: org.apache.spark.sql.Dataset[standrad.storage.ChatLog]
Call: filter

$""date"" === date
"
"udf/spark_repos_4/3_abdheshkumar_spark-practices/..src.main.scala.standrad.WordCount.scala/udf/22.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_4/3_abdheshkumar_spark-practices/..src.main.scala.standrad.WordCount.scala/udf/24.19.Dataset-(String, (String, Int)).map","Type: org.apache.spark.sql.Dataset[(String, (String, Int))]
Call: map

x => WordCountSchema(x._1, x._2._2)
"
"udf/spark_repos_4/3_Curycu_SparkStudy/..src.main.scala.com.gmail.hancury.sparkstudy.H.scala/udf/16.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

expr(""birth_date is not null"")
"
"udf/spark_repos_4/3_Curycu_SparkStudy/..src.main.scala.com.gmail.hancury.sparkstudy.I.scala/udf/11.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

expr(""rank <= 2"")
"
"udf/spark_repos_4/3_DAMA-UPC_DataSynth/..src.main.scala.org.dama.datasynth.runtime.spark.operators.PropertyTableOperator.scala/udf/12.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => Tuple2[Long, Boolean](i.toLong, generator(i))
"
"udf/spark_repos_4/3_DAMA-UPC_DataSynth/..src.main.scala.org.dama.datasynth.runtime.spark.operators.PropertyTableOperator.scala/udf/22.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => Tuple2[Long, Int](i.toLong, generator(i))
"
"udf/spark_repos_4/3_DAMA-UPC_DataSynth/..src.main.scala.org.dama.datasynth.runtime.spark.operators.PropertyTableOperator.scala/udf/32.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => Tuple2[Long, Long](i.toLong, generator(i))
"
"udf/spark_repos_4/3_DAMA-UPC_DataSynth/..src.main.scala.org.dama.datasynth.runtime.spark.operators.PropertyTableOperator.scala/udf/42.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => Tuple2[Long, Float](i.toLong, generator(i))
"
"udf/spark_repos_4/3_DAMA-UPC_DataSynth/..src.main.scala.org.dama.datasynth.runtime.spark.operators.PropertyTableOperator.scala/udf/52.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => Tuple2[Long, Double](i.toLong, generator(i))
"
"udf/spark_repos_4/3_DAMA-UPC_DataSynth/..src.main.scala.org.dama.datasynth.runtime.spark.operators.PropertyTableOperator.scala/udf/62.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => Tuple2[Long, String](i.toLong, generator(i))
"
"udf/spark_repos_4/3_fire-basketball_SparkSqlProject/..src.main.scala.com.ligh.log.TopNStatJob.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""20130919"" && $""cmsType"" === ""js""
"
"udf/spark_repos_4/3_fire-basketball_SparkSqlProject/..src.main.scala.com.ligh.log.TopNStatJob.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === ""20130919"" && $""cmsType"" === ""js""
"
"udf/spark_repos_4/3_longcao_frameless-examples/..src.main.scala.examples.DatasetExample.scala/udf/11.24.Dataset-Artist.filter","Type: org.apache.spark.sql.Dataset[examples.Artist]
Call: filter

_.age > 30
"
"udf/spark_repos_4/3_PacktPublishing_Troubleshooting-Apache-Spark/..src.main.scala.com.example.CreatingDatasets.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/3_PacktPublishing_Troubleshooting-Apache-Spark/..src.main.scala.com.example.ProgrammingGuideSQL.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/3_vbounyasit_DataFlow/..src.main.scala.com.vbounyasit.bigdata.testing.JobsTestGenerator.scala/udf/147.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""column_name"") =!= idColumn
"
"udf/spark_repos_4/3_vbounyasit_DataFlow/..src.main.scala.com.vbounyasit.bigdata.testing.JobsTestGenerator.scala/udf/154.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""expected"") =!= col(""obtained"")
"
"udf/spark_repos_4/3_vincenzosantopietro_Covid-19-Statistics/..src.main.scala.com.vinx.covid.statistics.metrics.ItalyCasesPerCityOverTime.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date >= cast(\'${dateFilter(0)(0)}\' as date)""
"
"udf/spark_repos_4/3_xubo245_Spark2Learning/..src.main.scala.org.apache.spark.sql.learning.DataFrameLearning.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/3_xubo245_Spark2Learning/..src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/13.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_4/3_xubo245_Spark2Learning/..src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/40.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_4/3_xubo245_Spark2Learning/..src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/44.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_4/3_xubo245_Spark2Learning/..src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/50.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_4/3_xubo245_Spark2Learning/..src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/78.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/3_xubo245_Spark2Learning/..src.main.scala.org.apache.spark.sql.learning.implicitsLearning.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_4/3_xubo245_Spark2Learning/..src.main.scala.org.apache.spark.sql.learning.ParquetLearning.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_4/3_Zuehlke_hackzurich-sensordataanalysis/..common-utils.src.main.scala.com.zuehlke.hackzurich.common.dataformats.ReadJSONFromFileToRDD.scala/udf/14.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

gyroFilter(_)
"
"udf/spark_repos_4/4_airbnb_sputnik/..src.main.scala.com.airbnb.sputnik.checks.NotNull.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df.col(columnName).isNull
"
"udf/spark_repos_4/4_airbnb_sputnik/..src.main.scala.com.airbnb.sputnik.example.VisitsPerCountryJob.scala/udf/17.22.Dataset-CountryStats.filter","Type: org.apache.spark.sql.Dataset[com.airbnb.sputnik.example.Schemas.CountryStats]
Call: filter

countryStats => countryStats.distinct_url_number == 0 || countryStats == 0
"
"udf/spark_repos_4/4_airbnb_sputnik/..src.main.scala.com.airbnb.sputnik.hive.MetricsCollecting.scala/udf/15.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          accumulator.add(1)
          row
        }
"
"udf/spark_repos_4/4_airbnb_sputnik/..src.main.scala.com.airbnb.sputnik.SparkJob.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

inputDF.col(""ds"").equalTo(day)
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.com.lihaogn.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.com.lihaogn.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.com.lihaogn.spark.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.log.TopNStatJob.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""classType"" === ""video""
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.log.TopNStatJob.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""classType"" === ""video""
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.log.TopNStatJob.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""classType"" === ""video""
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.log.TopNStatJobYARN.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""classType"" === ""video""
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.log.TopNStatJobYARN.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""classType"" === ""video""
"
"udf/spark_repos_4/4_lihaogm_spark-analyse-log/..src.main.scala.log.TopNStatJobYARN.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""classType"" === ""video""
"
"udf/spark_repos_4/4_Mageswaran1989_aja/..src.examples.scala.org.aja.tej.examples.streaming.twitter.analysis.ExamineAndTrain.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toString
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.extraction.preproccess.scala/udf/107.25.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.length > 0
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.extraction.preproccess.scala/udf/109.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(""\t"")
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.extraction.preproccess.scala/udf/215.23.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.length > 0
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.extraction.spark.dataSet.wordTranslationProb.scala/udf/29.19.Dataset-(String, String, Int).map","Type: org.apache.spark.sql.Dataset[(String, String, Int)]
Call: map

elem => (elem._1, elem._3)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.extraction.spark.dataSet.wordTranslationProb.scala/udf/43.19.Dataset-(String, String, Int).map","Type: org.apache.spark.sql.Dataset[(String, String, Int)]
Call: map

elem => (elem._2, elem._3)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.Evaluation.scala/udf/230.35.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

uni_table.get(""words"") === split(split.length - 1)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.GSBEvaluation.scala/udf/170.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""ID"") === keyID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.GSBEvaluation.scala/udf/206.33.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""ID"") === v
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.GSBEvaluation.scala/udf/292.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""ID"") === keyID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.GSBEvaluation.scala/udf/323.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""ID"") === v
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.GTEvaluation.scala/udf/128.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

dataFrame(""ID"") === elem._1
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.GTEvaluation.scala/udf/161.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

dataFrame(""ID"") === elem._1
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.KNEvaluation.scala/udf/160.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

probTable(""ID"") === probID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.KNEvaluation.scala/udf/185.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

backoffTable(""ID"") === backOffID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.KNEvaluation.scala/udf/212.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""ID"") === probID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.KNEvaluation.scala/udf/279.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

probTable(""ID"") === probID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.KNEvaluation.scala/udf/302.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

backoffTable(""ID"") === backOffID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.KNEvaluation.scala/udf/326.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""ID"") === probID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.MKNEvaluation.scala/udf/161.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

probTable(""ID"") === probID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.MKNEvaluation.scala/udf/186.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

backoffTable(""ID"") === backOffID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.MKNEvaluation.scala/udf/213.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""ID"") === probID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.MKNEvaluation.scala/udf/280.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

probTable(""ID"") === probID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.MKNEvaluation.scala/udf/303.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

backoffTable(""ID"") === backOffID(ind)
"
"udf/spark_repos_4/4_PasaLab_seal/..src.main.scala.edu.nju.pasalab.mt.LanguageModel.Perplexity.MKNEvaluation.scala/udf/327.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(""ID"") === probID(ind)
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.PersistSparkLauncher.scala/udf/129.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + i
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.PersistSparkLauncher.scala/udf/162.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + i
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.PersistSparkLauncher.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + i
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.PersistSparkLauncher.scala/udf/96.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + i
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.SparkLauncher.scala/udf/107.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + i
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.SparkLauncher.scala/udf/119.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + i
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.SparkLauncher.scala/udf/131.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + i
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.SparkLauncher.scala/udf/41.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + months.get(i)
"
"udf/spark_repos_4/4_Simplilearn-Edu_BDHS/..scala-spark-master.src.main.scala.com.com.simplilearn.bigdata.spark.SparkLauncher.scala/udf/60.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""month = "" + months.get(i)
"
"udf/spark_repos_4/4_spmygithub_ImoocLog_SparkSQL/..src.main.scala.cn.njupt.bigdata.spark.TopNCourseJob.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/4_spmygithub_ImoocLog_SparkSQL/..src.main.scala.cn.njupt.bigdata.spark.TopNCourseJob.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/4_spmygithub_ImoocLog_SparkSQL/..src.main.scala.cn.njupt.bigdata.spark.TopNCourseJobYarn.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/4_spmygithub_ImoocLog_SparkSQL/..src.main.scala.cn.njupt.bigdata.spark.TopNCourseJobYarn.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_4/4_syhan_coursera/..scala-spark-big-data.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/103.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
"
"udf/spark_repos_4/4_syhan_coursera/..scala-spark-big-data.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/110.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

row => TimeUsageRow(row._1._1, row._1._2, row._1._3, round(row._2), round(row._3), round(row._4))
"
"udf/spark_repos_4/4_tmcgrath_spark-2-streaming/..src.main.scala.com.supergloo.Skeleton.scala/udf/12.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => i + 1
"
"udf/spark_repos_4/4_xiaogp_recsys_structured_streaming/..structured_streaming.hot_item_statis.scala/udf/21.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val user = row.getString(0)
          val item = row.getString(1)
          val time = row.getLong(2)
          val score = row.getInt(3)
          (user, time, (item, score))
        }
"
"udf/spark_repos_4/4_xiaogp_recsys_structured_streaming/..structured_streaming.hot_item_statis.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val hour = row.getString(0)
        val tmp_list = row.getAs[mutable.WrappedArray[org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema]](2)
        val tmp_dict = new scala.collection.mutable.HashMap[String, Long]()
        for (x <- tmp_list) {
          val key = x(0).toString
          val score = x(1).toString.toLong
          if (tmp_dict.contains(key: String)) {
            tmp_dict(key: String) += score
          } else {
            tmp_dict(key) = score
          }
        }
        val res = locally {
          val _t_m_p_3 = tmp_dict.toSeq.sortWith(_._2 > _._2).slice(0, 200)
          _t_m_p_3.map(_._1)
        }.mkString(""["", "","", ""]"")
        (hour, res)
      }
"
"udf/spark_repos_4/4_xiaogp_recsys_structured_streaming/..structured_streaming.user_topic_like.scala/udf/24.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""user"".isin(""unk1"", ""unk2"", ""unk3"", ""unk4"", ""unk5"", ""unk6"", ""unk7"", ""unk8"", ""unk9"")
"
"udf/spark_repos_4/4_xiaogp_recsys_structured_streaming/..structured_streaming.user_topic_like.scala/udf/26.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val user = row.getString(0)
          val pty = row.getString(3)
          val pref = row.getDouble(5)
          val datetime = row.getLong(4)
          (user, (pty, pref), datetime)
        }
"
"udf/spark_repos_4/4_xiaogp_recsys_structured_streaming/..structured_streaming.user_topic_like.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val user = row.getString(0).reverse
        val tmp_list = row.getAs[mutable.WrappedArray[org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema]](2)
        val tmp_dict = new scala.collection.mutable.HashMap[String, Double]()
        val datetime = row.getString(1)
        for (x <- tmp_list) {
          val key = x(0).toString
          val score = x(1).toString.toDouble
          if (tmp_dict.contains(key: String)) {
            tmp_dict(key: String) += score
          } else {
            tmp_dict(key) = score
          }
        }
        val res = locally {
          val _t_m_p_4 = tmp_dict.toSeq.sortWith(_._2 > _._2).slice(0, 3)
          _t_m_p_4.map(_._1)
        }.toBuffer
        val diff = 3 - res.length
        for (i <- 1 to diff) {
          res ++= Array(""null"")
        }
        val top1 = res(0)
        val top2 = res(1)
        val top3 = res(2)
        (user, datetime, top1, top2, top3)
      }
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.ETLdev.CollectValueZonesEMR.scala/udf/27.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""tpep_pickup_datetime"").gt(""2017"")
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.ETLdev.CollectValueZonesEMR.scala/udf/29.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""tpep_pickup_datetime"").lt(""2019"")
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.ETLdev.CollectValueZonesEMR.scala/udf/35.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""lpep_pickup_datetime"").gt(""2017"")
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.ETLdev.CollectValueZonesEMR.scala/udf/37.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""lpep_pickup_datetime"").lt(""2019"")
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.ETLdev.CollectValueZones.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""tpep_pickup_datetime"").gt(""2017"")
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.ETLdev.CollectValueZones.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""tpep_pickup_datetime"").lt(""2019"")
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.ETLdev.CollectValueZones.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""tpep_pickup_datetime"").gt(""2017"")
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.ETLdev.CollectValueZones.scala/udf/35.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""tpep_pickup_datetime"").lt(""2019"")
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.EventProcess.CumulativeTripCountByTimeWindowPerDriver.scala/udf/23.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != null
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.EventProcess.CumulativeTripCountPerDriver.scala/udf/23.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != null
"
"udf/spark_repos_4/4_yennanliu_NYC_Taxi_Pipeline/..src.main.scala.SaveToHive.SparkHiveExample.scala/udf/32.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_4/5_chenharryhua_nanjin/..spark.src.main.scala.com.github.chenharryhua.nanjin.spark.streaming.SparkStream.scala/udf/14.20.Dataset-A.filter","Type: org.apache.spark.sql.Dataset[A]
Call: filter

f
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.concordancers.OrigPosLemConcordancer.scala/udf/21.24.Dataset-AQAnnotation.filter","Type: org.apache.spark.sql.Dataset[com.elsevier.aq.annotations.AQAnnotation]
Call: filter

$""docId"" === sentence.docId && $""annotType"" === wordType && $""startOffset"" >= sentence.startOffset && $""endOffset"" <= sentence.endOffset
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.After.scala/udf/12.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""R.docId"".isNull
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.And.scala/udf/12.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""R.docId"".isNull
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.Before.scala/udf/12.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""R.docId"".isNull
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.Between.scala/udf/16.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""R.docId"".isNull
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.ContainedIn.scala/udf/12.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""R.docId"".isNull
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.Contains.scala/udf/12.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""R.docId"".isNull
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.FilterProperty.scala/udf/24.22.Dataset-AQAnnotation.filter","Type: org.apache.spark.sql.Dataset[com.elsevier.aq.annotations.AQAnnotation]
Call: filter

query
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.FilterSet.scala/udf/24.22.Dataset-AQAnnotation.filter","Type: org.apache.spark.sql.Dataset[com.elsevier.aq.annotations.AQAnnotation]
Call: filter

query
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.FilterType.scala/udf/24.22.Dataset-AQAnnotation.filter","Type: org.apache.spark.sql.Dataset[com.elsevier.aq.annotations.AQAnnotation]
Call: filter

query
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.MatchProperty.scala/udf/13.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""R.docId"".isNull
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.Or.scala/udf/11.19.Dataset-AQAnnotation).map","Type: org.apache.spark.sql.Dataset[(com.elsevier.aq.annotations.AQAnnotation, com.elsevier.aq.annotations.AQAnnotation)]
Call: map

rec => if (rec._1 == null) rec._2 else if (rec._2 == null) rec._1 else rec._1
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.query.RegexProperty.scala/udf/15.22.Dataset-AQAnnotation.filter","Type: org.apache.spark.sql.Dataset[com.elsevier.aq.annotations.AQAnnotation]
Call: filter

query
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.utilities.GetAQAnnotations.scala/udf/20.19.Dataset-CATAnnotation.map","Type: org.apache.spark.sql.Dataset[com.elsevier.aq.annotations.CATAnnotation]
Call: map

annotation => {
        var propsMap = Map[String, String]()
        var attrBuf: ListBuffer[String] = new ListBuffer[String]()
        if (!props.isEmpty) {
          val otherToks = annotation.other.getOrElse("""").split(""&"")
          for (otherTok <- otherToks) {
            val toks = otherTok.split(""="")
            if (toks.size == 2) {
              val key = toks(0)
              var value = toks(1)
              if (props.contains(key) || props.contains(WILDCARD)) {
                if (decodeProps.contains(key) || decodeProps.contains(WILDCARD)) {
                  value = URLDecoder.decode(value, ""UTF-8"")
                }
                if (lcProps.contains(key) || lcProps.contains(WILDCARD)) {
                  value = value.toLowerCase
                }
                if (props.contains(WILDCARD) && annotation.annotSet.toLowerCase == OM_ANNOT_SET && !OM_NON_ATTRIBUTE_PROPERTIES.contains(key)) {
                  attrBuf += otherTok
                } else {
                  propsMap += key -> value
                }
              } else if (props.contains(ATTR) && annotation.annotSet.toLowerCase == OM_ANNOT_SET) {
                if (!OM_NON_ATTRIBUTE_PROPERTIES.contains(key)) {
                  attrBuf += otherTok
                }
              }
            }
          }
          if (attrBuf.size > 0 && !propsMap.contains(ATTR)) {
            propsMap += ATTR -> attrBuf.mkString(""&"")
          }
        }
        AQAnnotation(annotation.docId, annotation.annotSet, annotation.annotType, annotation.startOffset, annotation.endOffset, annotation.annotId, if (propsMap.size > 0) Some(propsMap) else None)
      }
"
"udf/spark_repos_4/5_elsevierlabs-os_AnnotationQuery/..src.main.scala.com.elsevier.aq.utilities.GetCATAnnotations.scala/udf/14.19.Dataset-AQAnnotation.map","Type: org.apache.spark.sql.Dataset[com.elsevier.aq.annotations.AQAnnotation]
Call: map

aqAnnotation => {
        var otherBuf: ListBuffer[String] = new ListBuffer[String]()
        for ((key: String, value: String) <- aqAnnotation.properties.getOrElse(Map[String, String]())) {
          if (props.contains(key) || props.contains(WILDCARD)) {
            if (encodeProps.contains(key) || encodeProps.contains(WILDCARD)) {
              otherBuf += key + ""="" + URLEncoder.encode(value, ""UTF-8"")
            } else {
              otherBuf += key + ""="" + value
            }
          }
        }
        CATAnnotation(aqAnnotation.docId, aqAnnotation.annotSet, aqAnnotation.annotType, aqAnnotation.startOffset, aqAnnotation.endOffset, aqAnnotation.annotId, if (otherBuf.size > 0) Some(otherBuf.mkString(""&"")) else None)
      }
"
"udf/spark_repos_4/5_stettix_spark-sessions/..src.main.scala.net.janvsmachine.sparksessions.Sessions.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Click(row.getAs[String](""uuid""), row.getAs[Int](""document_id"").toString, row.getAs[Int](""timestamp"").toLong + 1465876799998L)
"
"udf/spark_repos_4/74_SANSA-Stack_SANSA-RDF/..sansa-rdf-spark.src.main.scala.net.sansa_stack.rdf.spark.kge.convertor.ByIndex.scala/udf/15.19.Dataset-StringTriples.map","Type: org.apache.spark.sql.Dataset[net.sansa_stack.rdf.spark.kge.triples.StringTriples]
Call: map

{
        i => IntegerTriples(e.indexOf(Row(i.Subject)) + 1, r.indexOf(Row(i.Predicate)) + 1, e.indexOf(Row(i.Object)) + 1)
      }
"
"udf/spark_repos_4/7_AnemoneIndicum_ECommerceRecommendSystem/..ec-recommender.re-content.src.main.scala.com.rui.cn.content.ContentRecommend.scala/udf/35.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => (row.getAs[Int](""productId""), row.getAs[SparseVector](""features"").toArray)
        }
"
"udf/spark_repos_4/7_AnemoneIndicum_ECommerceRecommendSystem/..ec-recommender.re-ItemCF.src.main.scala.com.rui.cn.itemcf.ItemCfRecommend.scala/udf/33.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val coocSim = cooccurrenceSim(row.getAs[Long](""cocount""), row.getAs[Long](""count1""), row.getAs[Long](""count2""))
          (row.getInt(0), (row.getInt(1), coocSim))
        }
"
"udf/spark_repos_4/7_AnemoneIndicum_ECommerceRecommendSystem/..ec-recommender.re-offline.src.main.scala.com.rui.cn.offline.ProductRecommend.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

item => Rating(item.getAs[Int](""userId""), item.getAs[Int](""productId""), item.getAs[Double](""score""))
"
"udf/spark_repos_4/7_AnemoneIndicum_ECommerceRecommendSystem/..ec-recommender.re-statistics.src.main.scala.com.rui.cn.Statistics.scala/udf/24.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Int) => simpleDateFormat.format(new Date(x * 1000L)).toInt
"
"udf/spark_repos_4/7_sudheerpalyam_stock_stream_processing/..src.main.scala.au.com.thoughtworks.assessment.spark.streaming.ContinuousKafkaStreaming.scala/udf/19.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => StockEvent(r.getString(0))
"
"udf/spark_repos_4/7_sudheerpalyam_stock_stream_processing/..src.main.scala.au.com.thoughtworks.assessment.spark.streaming.FileStreamingAggregations.scala/udf/19.22.Dataset-AggretatedStockEvent.filter","Type: org.apache.spark.sql.Dataset[au.com.thoughtworks.assessment.spark.streaming.FileStreamingAggregations.AggretatedStockEvent]
Call: filter

_.maxPrice.exists(_ > 70)
"
"udf/spark_repos_4/7_sudheerpalyam_stock_stream_processing/..src.main.scala.au.com.thoughtworks.assessment.spark.streaming.KafkaStructuredStreaming.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => StockEvent(r.getString(0))
"
"udf/spark_repos_4/81_holdenk_spark-structured-streaming-ml/..src.main.scala.com.high-performance-spark-examples.structuredstreaming.QueryBasedStreamingNaiveBayes.scala/udf/17.22.Dataset-LabeledTokenCounts.filter","Type: org.apache.spark.sql.Dataset[com.highperformancespark.examples.structuredstreaming.LabeledTokenCounts]
Call: filter

r => tokens.contains(r.value)
"
"udf/spark_repos_4/81_holdenk_spark-structured-streaming-ml/..src.test.scala.com.high-performance-spark-examples.structuredstreaming.StreamingNaiveBayesSuite.scala/udf/31.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, features: org.apache.spark.ml.linalg.Vector) =>
          org.apache.spark.ml.feature.LabeledPoint(label, org.apache.spark.ml.linalg.Vectors.dense(features.toArray))
      }
"
"udf/spark_repos_4/8_bigchange_AI/..src.main.scala.com.bigchange.basic.DataFrameTest.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""city"") === ""北京""
"
"udf/spark_repos_4/8_bigchange_AI/..src.main.scala.com.bigchange.basic.DataFrameTest.scala/udf/44.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: Int) => if (s <= 20) ""lower"" else ""high""
"
"udf/spark_repos_4/8_bigchange_AI/..src.main.scala.com.bigchange.basic.HiveOperationTest.scala/udf/21.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: Int) => if (s <= 20) ""lower"" else ""high""
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/13.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.exp)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/17.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeTwoParamOp(math.pow)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/21.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.sqrt)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/25.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.sin)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/29.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.cos)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/33.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.tan)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/37.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.log)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/41.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeTwoParamOp(math.min)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/45.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeTwoParamOp(math.max)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/49.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.floor)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/53.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.ceil)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/57.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.signum)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.commons.src.main.scala.ai.deepsense.commons.spark.sql.UserDefinedFunctions.scala/udf/9.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

nullSafeSingleParamOp(math.abs)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.deeplang.src.it.scala.ai.deepsense.deeplang.DeeplangIntegTestSupport.scala/udf/30.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

myOp
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.deeplang.src.main.scala.ai.deepsense.deeplang.doperables.MissingValuesHandler.scala/udf/105.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

CommonQueries.isMissingInColumnPredicate(df, columnName, declaredAsMissingValues)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.deeplang.src.main.scala.ai.deepsense.deeplang.doperables.MissingValuesHandler.scala/udf/151.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!CommonQueries.isMissingInColumnPredicate(sparkDataFrame, column, declaredAsMissing)
"
"udf/spark_repos_4/90_deepsense-ai_seahorse/..seahorse-workflow-executor.deeplang.src.main.scala.ai.deepsense.deeplang.doperables.MissingValuesHandler.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!CommonQueries.isMissingInRowPredicate(df, columns, declaredAsMissingValues)
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.algo.core.TableStatistics.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

partitionValue => {
        val partitionStatementValues: Seq[String] = locally {
          val _t_m_p_3 = targetPartitions
          _t_m_p_3.map(partitionColumn => s""$partitionColumn=${getParameterValue(partitionValue, partitionColumn)}"")
        }
        s""ANALYZE TABLE $targetTable PARTITION(${partitionStatementValues.mkString("","")}) COMPUTE STATISTICS""
      }
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.algo.DeltaLoad.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isRequiredPartition
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.algo.DeltaLoad.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

upsertRecordsModesFilterFunction
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.algo.DeltaLoad.scala/udf/42.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

rankedDeltaRecords(rankingColumnName) === 1
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.algo.FixedSizeStringExtractor.scala/udf/18.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isRequiredPartition
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.algo.PartitionMaterialization.scala/udf/17.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isRequiredPartition
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.util.OutputWriter.scala/udf/63.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isRequiredPartition
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.util.OutputWriter.scala/udf/72.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isRequiredPartition
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.util.RecoverPartitionsCustom.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

partitionValue => {
        val partitionStatementValues: Seq[String] = locally {
          val _t_m_p_3 = targetPartitions
          _t_m_p_3.map(partitionColumn => s""$partitionColumn=${getParameterValue(partitionValue, partitionColumn)}"")
        }
        s""ALTER TABLE $tableName ADD IF NOT EXISTS PARTITION(${partitionStatementValues.mkString("","")})""
      }
"
"udf/spark_repos_4/9_adidas_m3d-engine/..src.main.scala.com.adidas.analytics.util.SparkRecoverPartitionsCustom.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val partitionStatementValues: Seq[String] = locally {
          val _t_m_p_4 = targetPartitions
          _t_m_p_4.map(partitionString => s""$partitionString=${getParameterValue(row, partitionString)}"")
        }
        s""ALTER TABLE $tableName ADD IF NOT EXISTS PARTITION(${partitionStatementValues.mkString("","")})""
      }
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.extractors.Extractor.scala/udf/20.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isInExtractorScope _
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.extractors.Extractor.scala/udf/26.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isInExtractorScope _
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.extractors.Extractor.scala/udf/28.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

isInStudy _
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.extractors.patients.HadPatients.scala/udf/11.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""SOR_MOD"") === deathCode
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.extractors.patients.IrBenPatients.scala/udf/21.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""BEN_DCD_DTE"").isNotNull
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.extractors.patients.IrBenPatients.scala/udf/28.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""BEN_NAI_MOI"").between(1, 12) && col(""BEN_NAI_ANN"").between(minYear, maxYear)
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.extractors.patients.McoPatients.scala/udf/11.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""SOR_MOD"") === deathCode
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.extractors.patients.Patients.scala/udf/40.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""patientID"").isNotNull && col(""gender"").isNotNull && col(""birthDate"").isNotNull
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.filters.PatientFiltersImplicits.scala/udf/12.17.Dataset-Patient.map","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.patients.Patient]
Call: map

_.patientID
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.filters.PatientFiltersImplicits.scala/udf/39.22.Dataset-Patient.filter","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.patients.Patient]
Call: filter

{
        patient => !patientsToRemove.contains(patient.patientID)
      }
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.filters.PatientFiltersImplicits.scala/udf/54.22.Dataset-Patient.filter","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.patients.Patient]
Call: filter

{
        patient => patientsToKeep.contains(patient.patientID)
      }
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.sources.data.HadFilters.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fictionalAndFalseHospitalStaysFilter
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.sources.data.McoFilters.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

duplicateHospitalsFilter
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.sources.data.McoFilters.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

McoSource.GRG_GHM =!= ""14Z08Z""
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.sources.data.McoFilters.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

McoSource.GHS_NUM =!= ""9999""
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.sources.data.McoFilters.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fictionalAndFalseHospitalStaysFilter
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.sources.data.McoFilters.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fictionalAndFalseHospitalStaysFilter
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.sources.data.SsrFilters.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fictionalAndFalseHospitalStaysFilter
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.sources.data.SsrFilters.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fictionalAndFalseHospitalStaysFilter
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.exposures.ExposurePeriodAdder.scala/udf/38.21.Dataset-ExposureDuration.map","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.transformers.exposures.ExposureDuration]
Call: map

ed => ed.copy(period = ed.period.copy(end = (ed.period.end - endDelay).get))
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformer.scala/udf/21.24.Dataset-PatientDates.filter","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformerUtilities.PatientDates]
Call: filter

e => e.followUpStart.nonEmpty
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformer.scala/udf/23.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

e => PatientDates(e.getAs[String](PatientID), Option(e.getAs[Timestamp](DeathDate)), Option(e.getAs[Timestamp](FollowUpStart)), Option(e.getAs[Timestamp](Columns.ObservationEnd)))
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformer.scala/udf/31.24.Dataset-TrackLossDate.filter","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformerUtilities.TrackLossDate]
Call: filter

e => e.trackloss.nonEmpty
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformer.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

e => TrackLossDate(e.getAs[String](PatientID), Option(e.getAs[Timestamp](TracklossDate)))
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformer.scala/udf/38.21.Dataset-TrackLossDate).map","Type: org.apache.spark.sql.Dataset[(fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformerUtilities.PatientDates, fr.polytechnique.cmap.cnam.etl.transformers.follow_up.FollowUpTransformerUtilities.TrackLossDate)]
Call: map

{ e => 
          val trackloss: Option[Timestamp] = Try(e._2.trackloss).getOrElse(None)
          val followUpEndReason = endReason(DeathReason(date = e._1.deathDate), TrackLossReason(date = trackloss), ObservationEndReason(date = e._1.observationEnd))
          FollowUp(e._1.patientID, followUpEndReason.reason, e._1.followUpStart.get, followUpEndReason.date.get)
        }
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.interaction.NLevelInteractionTransformer.scala/udf/15.22.Dataset-ExposureN.filter","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.transformers.interaction.ExposureN]
Call: filter

i => i.toDuration >= minimumDuration
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.interaction.NLevelInteractionTransformer.scala/udf/72.20.Dataset-ExposureN.map","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.transformers.interaction.ExposureN]
Call: map

l => l.toInteraction
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.etl.transformers.observation.ObservationPeriodTransformer.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

e => ObservationPeriod(e.getAs[String](PatientID), e.getAs[Timestamp](Start), studyEnd)
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.study.fall.follow_up.FallStudyFollowUps.scala/udf/12.19.Dataset-Patient.map","Type: org.apache.spark.sql.Dataset[fr.polytechnique.cmap.cnam.etl.patients.Patient]
Call: map

{ patient => 
        val endReason = if (patient.deathDate.isDefined && patient.deathDate.get.before(studyEnd)) {
          ""death""
        } else {
          ""study_end""
        }
        val endDate: Timestamp = endReason match {
          case ""death"" =>
            patient.deathDate.get
          case ""study_end"" =>
            studyEnd
        }
        (patient, FollowUp(patient.patientID, endReason, startDate, endDate))
      }
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.study.fall.fractures.FracturesTransformerImplicits.scala/udf/17.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(Event.Columns.Weight) === col(""maxweight"")
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.study.fall.fractures.FracturesTransformerImplicits.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""status"") === ""first""
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.main.scala.fr.polytechnique.cmap.cnam.study.fall.statistics.DiagnosisCounter.scala/udf/33.19.Dataset-DiagnosisStat).map","Type: org.apache.spark.sql.Dataset[(String, fr.polytechnique.cmap.cnam.study.fall.statistics.DiagnosisStat)]
Call: map

{
        case (patientID, DiagnosisStat(_, dp, da, dr)) =>
          DiagnosisStat(patientID, dp, da, dr)
      }
"
"udf/spark_repos_5/10_X-DataInitiative_SCALPEL-Extraction/..src.test.scala.fr.polytechnique.cmap.cnam.etl.extractors.patients.PatientsSuite.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Patients.validateDeathDate(deathDates, birthDates, 2020) === true
"
"udf/spark_repos_5/11_codemeow5_Vector-Tile-Spark-Process/..src.main.scala.org.apache.spark.sql.SQLGeometricExtensions.scala/udf/48.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakePoint
"
"udf/spark_repos_5/11_codemeow5_Vector-Tile-Spark-Process/..src.main.scala.org.apache.spark.sql.SQLGeometricExtensions.scala/udf/52.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakeBox2D
"
"udf/spark_repos_5/11_codemeow5_Vector-Tile-Spark-Process/..src.main.scala.org.apache.spark.sql.SQLGeometricExtensions.scala/udf/56.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKB
"
"udf/spark_repos_5/11_codemeow5_Vector-Tile-Spark-Process/..src.main.scala.org.apache.spark.sql.SQLGeometricExtensions.scala/udf/60.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Intersects
"
"udf/spark_repos_5/11_codemeow5_Vector-Tile-Spark-Process/..src.main.scala.org.ieee.codemeow.geometric.spark.data.SQLDataProvider.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val id = row.getAs[Long](""__id__"")
        val geom = row.getAs[Geometry](""__geometry__"")
        val fields = locally {
          val _t_m_p_3 = locally {
            val _t_m_p_4 = row.schema
            _t_m_p_4.filter(field => !Seq(""__id__"", ""__geometry__"").contains(field.name))
          }
          _t_m_p_3.map(field => field.name)
        }
        val props = row.getValuesMap[String](fields)
        Feature(id, geom, props)
      }
"
"udf/spark_repos_5/12_sev7e0_wow-spark/..src.main.scala.com.sev7e0.wow.sql.A_1_DataFrameTest.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

person => ""name:"" + person(0)
"
"udf/spark_repos_5/12_sev7e0_wow-spark/..src.main.scala.com.sev7e0.wow.sql.A_1_DataFrameTest.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

person => ""name:"" + person.getAs[String](""name"")
"
"udf/spark_repos_5/12_sev7e0_wow-spark/..src.main.scala.com.sev7e0.wow.sql.A_1_DataFrameTest.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

person => person.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_5/12_sev7e0_wow-spark/..src.main.scala.com.sev7e0.wow.sql.A_2_DataSetTest.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_5/12_sev7e0_wow-spark/..src.main.scala.com.sev7e0.wow.sql.A_6_HiveTables.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_5/12_sev7e0_wow-spark/..src.main.scala.com.sev7e0.wow.sql.A_8_MyAverage.scala/udf/30.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

A_8_MyAverage
"
"udf/spark_repos_5/12_sev7e0_wow-spark/..src.main.scala.com.sev7e0.wow.structured_streaming.A_1_BasicOperation.scala/udf/16.24.Dataset-DeviceData.filter","Type: org.apache.spark.sql.Dataset[com.sev7e0.wow.structured_streaming.A_1_BasicOperation.DeviceData]
Call: filter

_.signal > 10
"
"udf/spark_repos_5/12_sev7e0_wow-spark/..src.main.scala.com.sev7e0.wow.structured_streaming.A_1_BasicOperation.scala/udf/18.19.Dataset-DeviceData.map","Type: org.apache.spark.sql.Dataset[com.sev7e0.wow.structured_streaming.A_1_BasicOperation.DeviceData]
Call: map

_.device
"
"udf/spark_repos_5/14_zouzias_spark-lucenerdd-examples/..src.main.scala.org.zouzias.spark.lucenerdd.examples.linkage.LinkageAbtvsBuy.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).toString, row.getString(1), row.getString(2), row.getString(3))
"
"udf/spark_repos_5/14_zouzias_spark-lucenerdd-examples/..src.main.scala.org.zouzias.spark.lucenerdd.examples.linkage.LinkageGooglevsAmazon.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.get(0).toString, row.getString(1), row.getString(2), row.getString(3))
"
"udf/spark_repos_5/14_zouzias_spark-lucenerdd-examples/..src.main.scala.org.zouzias.spark.lucenerdd.examples.linkage.shape.ShapeLuceneRDDLinkageCountriesvsCapitals.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(1), row.getString(0))
"
"udf/spark_repos_5/14_zouzias_spark-lucenerdd-examples/..src.main.scala.org.zouzias.spark.lucenerdd.examples.linkage.shape.ShapeLuceneRDDLinkageCountriesvsCapitals.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(1), row.getString(0))
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformation.scala/udf/123.22.Dataset-TagRaw.filter","Type: org.apache.spark.sql.Dataset[at.ac.ait.TagRaw]
Call: filter

col(F.currency) === currency
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformation.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(col(""input.address"")) === 1
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformation.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(col(""output.address"")) === 1
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformation.scala/udf/34.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""rowNumber"") === 1
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformation.scala/udf/36.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_ getString 0
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformation.scala/udf/49.20.Dataset-A.filter","Type: org.apache.spark.sql.Dataset[A]
Call: filter

col(F.value) < 0
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformation.scala/udf/52.20.Dataset-A.filter","Type: org.apache.spark.sql.Dataset[A]
Call: filter

col(F.value) > 0
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformation.scala/udf/87.22.Dataset-TagRaw.filter","Type: org.apache.spark.sql.Dataset[at.ac.ait.TagRaw]
Call: filter

col(F.currency) === currency
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformator.scala/udf/33.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(F.coinjoin) === false
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformator.scala/udf/40.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") > 1
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformator.scala/udf/46.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") > 1
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformator.scala/udf/56.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""rank"") === 1
"
"udf/spark_repos_5/15_graphsense_graphsense-transformation/..src.main.scala.at.ac.ait.Transformator.scala/udf/60.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") === 1
"
"udf/spark_repos_5/15_sparsecode_DaFlow/..daflow-core.src.main.scala.com.abhioncbr.daflow.core.transformData.TransformRule.scala/udf/140.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

ruleCondition
"
"udf/spark_repos_5/1_ahoy-jon_autoBuild/..src.main.scala.centrifuge.sql.Explore.scala/udf/57.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

to_age _
"
"udf/spark_repos_5/1_ahoy-jon_autoBuild/..src.main.scala.centrifuge.sql.Explore.scala/udf/61.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

non_empty_string _
"
"udf/spark_repos_5/1_ahoy-jon_autoBuild/..src.main.scala.centrifuge.sql.Explore.scala/udf/65.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

add_annotations _
"
"udf/spark_repos_5/1_ahoy-jon_autoBuild/..src.main.scala.centrifuge.sql.Explore.scala/udf/69.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

flatten_annotations[Annotation] _
"
"udf/spark_repos_5/1_ahoy-jon_autoBuild/..src.test.scala.io.univalence.centrifuge.sql.univalence.scala/udf/196.66.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mergeAnnotations _
"
"udf/spark_repos_5/1_ahoy-jon_autoBuild/..src.test.scala.io.univalence.centrifuge.sql.univalence.scala/udf/209.64.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mergeAnnotations _
"
"udf/spark_repos_5/1_ahoy-jon_autoBuild/..src.test.scala.io.univalence.centrifuge.sql.univalence.scala/udf/230.62.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

cleanAnnotation _
"
"udf/spark_repos_5/1_ahoy-jon_autoBuild/..src.test.scala.io.univalence.centrifuge.sql.univalence.scala/udf/24.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

f
"
"udf/spark_repos_5/1_amesar_spark-tools/..src.main.scala.org.amm.spark.sql.DatabaseReport.scala/udf/27.22.Dataset-Database.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Database]
Call: filter

$""name"".isin(desiredDatabases: _*)
"
"udf/spark_repos_5/1_anastasiya-solodkaya_coursera-spark-capstone/..src.main.scala.observatory.Extraction.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (r: Row) => !r.isNullAt(2) && !r.isNullAt(3) }
"
"udf/spark_repos_5/1_anastasiya-solodkaya_coursera-spark-capstone/..src.main.scala.observatory.Extraction.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ (r: Row) => !r.isNullAt(4) }
"
"udf/spark_repos_5/1_andersonkmi_kaggle-nyc-parking-violations-issued/..src.main.scala.org.codecraftlabs.nyc.utils.DataTransformationUtil.scala/udf/25.22.Dataset-ViolationCountByYear.filter","Type: org.apache.spark.sql.Dataset[org.codecraftlabs.nyc.data.ViolationCountByYear]
Call: filter

item => item.issueYear.getOrElse(0) >= start && item.issueYear.getOrElse(0) <= end
"
"udf/spark_repos_5/1_andersonkmi_kaggle-nyc-parking-violations-issued/..src.main.scala.org.codecraftlabs.nyc.utils.DataTransformationUtil.scala/udf/43.22.Dataset-ParkingViolation.filter","Type: org.apache.spark.sql.Dataset[org.codecraftlabs.nyc.data.ParkingViolation]
Call: filter

s""issueYear == $year""
"
"udf/spark_repos_5/1_andersonkmi_kaggle-nyc-parking-violations-issued/..src.main.scala.org.codecraftlabs.nyc.utils.DataTransformationUtil.scala/udf/49.22.Dataset-ParkingViolation.filter","Type: org.apache.spark.sql.Dataset[org.codecraftlabs.nyc.data.ParkingViolation]
Call: filter

ds.col(""issueYear"").isin(years).desc(""issueYear"")
"
"udf/spark_repos_5/1_andreformento_big-data-analysis-with-scala-and-spark/..03-programming-assignment-wikipedia-sql-mode.src.main.scala.wikipedia.WikipediaRanking.scala/udf/43.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mentionsLanguage
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.cli.StringGraphDriver.scala/udf/138.19.Dataset-NumberedReads.map","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.model.NumberedReads]
Call: map

x => ""VT\t"" + x.sn.toString + ""\t"" + x.sequence + ""\t"" + x.qual + ""\tSS:i:0""
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.cli.StringGraphDriver.scala/udf/141.19.Dataset-NumberedReads.map","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.model.NumberedReads]
Call: map

x => Vertex(x.sn, x.sequence, x.qual)
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.cli.StringGraphDriver.scala/udf/145.19.Dataset-NumberedReads.map","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.model.NumberedReads]
Call: map

x => ""@"" + x.sn.toString + """"""
"""""" + x.sequence + """"""
+
"""""" + x.qual
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.cli.StringGraphDriver.scala/udf/156.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ x => 
            val items = x.toString().split(""\t"")
            NumberedReads(items(1).toLong, items(2), items(3))
          }
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.cli.StringGraphDriver.scala/udf/67.24.Dataset-Alignment.filter","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.cli.Alignment]
Call: filter

_.rc == 0
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.algorithm.StringGraph.scala/udf/263.23.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => x.getString(1) -> x.getString(2)
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.algorithm.StringGraph.scala/udf/282.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getString(0), x.getString(1), x.getString(2))
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.rdd.Fragments.scala/udf/16.23.Dataset-(Long, (Long, String)).map","Type: org.apache.spark.sql.Dataset[(Long, (Long, String))]
Call: map

_._2.swap
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.rdd.Fragments.scala/udf/23.27.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getAs[String](0).split("" "", 2)(0), x.getAs[String](1))
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.rdd.Fragments.scala/udf/25.25.Dataset-(String, (String, String)).map","Type: org.apache.spark.sql.Dataset[(String, (String, String))]
Call: map

_._2._2
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/126.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""outLabel"") =!= LABEL_T
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/132.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""inLabel"") === LABEL_S and col(""outLabel"") === LABEL_B
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/136.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""inLabel"") === LABEL_B
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/140.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""inLabel"") === LABEL_S and col(""outLabel"") === LABEL_T
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/144.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(outId) === -1
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/148.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(inId) === -1 and col(""in._2"") =!= -1
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/153.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""in._1"") === -1 and col(""in._2"") === -1
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/157.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(inId) =!= -1 and col(""in._2"") =!= -1 and col(outId) =!= -1
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/194.22.Dataset-MetaContigWithId.map","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.AssembleUtils.MetaContigWithId]
Call: map

{
          i => (i.cid, i.startId, i.endId)
        }
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/274.25.Dataset-RmDupContig.filter","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.AssembleUtils.RmDupContig]
Call: filter

_.rc == 0
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/43.24.Dataset-Pair.filter","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.GraphUtils.Pair]
Call: filter

_.in == (-1, -1)
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/47.24.Dataset-Pair.filter","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.GraphUtils.Pair]
Call: filter

_.in != (-1, -1)
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/72.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!(col(""in._1"") === -1 and col(""in._2"") === -1)
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/76.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!(col(""in._1"") === -1 and col(""in._2"") === -1)
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/90.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(inId) === -1 or col(outId) === -1 and col(""inLabel"") === LABEL_S and col(""outLabel"") === LABEL_T
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.AssembleUtils.scala/udf/99.22.Dataset-PairId.map","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.AssembleUtils.PairId]
Call: map

{
          p => PairLen(p.id, p.startId, p.endId, calcLen(p))
        }
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.GraphUtils.scala/udf/498.25.Dataset-ReadTriplet.filter","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.GraphUtils.ReadTriplet]
Call: filter

p => p.label1 != LABEL_PT_PS || p.label2 != LABEL_PT_PS
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.GraphUtils.scala/udf/504.27.Dataset-Pair.filter","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.GraphUtils.Pair]
Call: filter

p => p.out._1 != -1 && p.out._2 != -1
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.GraphUtils.scala/udf/506.22.Dataset-Pair.map","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.GraphUtils.Pair]
Call: map

_.out._2
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.GraphUtils.scala/udf/511.27.Dataset-Pair.filter","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.GraphUtils.Pair]
Call: filter

p => p.in._1 != -1 && p.in._2 != -1
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.GraphUtils.scala/udf/513.22.Dataset-Pair.map","Type: org.apache.spark.sql.Dataset[com.atgenomix.connectedreads.core.util.GraphUtils.Pair]
Call: map

_.in._1
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.GraphUtils.scala/udf/546.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""dst.label"") === label
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.core.util.GraphUtils.scala/udf/550.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""src.label"") === label
"
"udf/spark_repos_5/1_atgenomix_connectedreads/..src.main.scala.com.atgenomix.connectedreads.pipeline.ErrorCorrectionPipeline.scala/udf/128.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getAs[String](0), x.getAs[String](1), x.getAs[String](2))
"
"udf/spark_repos_5/1_axsaucedo_spark-with-scala/..docs.eclipse-src.SparkScalaCourse.src.io.e_x.spark.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_5/1_axsaucedo_spark-with-scala/..eclipse-src.SparkScalaCourse.src.io.e_x.spark.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_5/1_BanekMan_SparkKafkaSolr/..TestScala.src.rule.checkResponseCode404.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r(5) == 404
"
"udf/spark_repos_5/1_BanekMan_SparkKafkaSolr/..TestScala.src.rule.checkResponseCode503.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r(5) == 503
"
"udf/spark_repos_5/1_BanekMan_SparkKafkaSolr/..TestScala.src.rule.countResponseCode404.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r(5) == 404
"
"udf/spark_repos_5/1_BanekMan_SparkKafkaSolr/..TestScala.src.rule.countResponseCode503.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r(5) == 503
"
"udf/spark_repos_5/1_BanekMan_SparkKafkaSolr/..TestScala.src.rule.MaxResponseSize200.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r(5) == 200
"
"udf/spark_repos_5/1_bjaggi_ScalaSparkSample/..src.main.scala.com.eva.app.TTMAnalytics.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Session_Time"" > 0
"
"udf/spark_repos_5/1_BobLovesData_Apache-Spark-In-24-Hours/..src.net.massstreet.examples.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[net.massstreet.examples.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_5/1_BobLovesData_Apache-Spark-In-24-Hours/..src.net.massstreet.examples.SparkSQLExample.scala/udf/100.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_5/1_BobLovesData_Apache-Spark-In-24-Hours/..src.net.massstreet.examples.SparkSQLExample.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_5/1_BobLovesData_Apache-Spark-In-24-Hours/..src.net.massstreet.examples.SparkSQLExample.scala/udf/48.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_5/1_BobLovesData_Apache-Spark-In-24-Hours/..src.net.massstreet.examples.SparkSQLExample.scala/udf/67.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_5/1_BobLovesData_Apache-Spark-In-24-Hours/..src.net.massstreet.examples.SparkSQLExample.scala/udf/71.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_5/1_BobLovesData_Apache-Spark-In-24-Hours/..src.net.massstreet.examples.SparkSQLExample.scala/udf/76.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_5/1_BobLovesData_Apache-Spark-In-24-Hours/..src.net.massstreet.hour10.BayAreaBikeAnalysis.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time"".between(""2014/02/22"", ""2014/02/28"")
"
"udf/spark_repos_5/1_chailei9005_Bioqas/..src.main.scala.com.bioqas.test.CheckStatApp.scala/udf/7.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MiddleValueUDAF()
"
"udf/spark_repos_5/1_chailei9005_Bioqas/..src.main.scala.com.bioqas.test.StatTestApp.scala/udf/27.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MiddleValueUDAF()
"
"udf/spark_repos_5/1_chailei9005_Bioqas/..src.main.scala.com.bioqas.test.StatTestApp.scala/udf/37.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getAs[String](1), 1)
"
"udf/spark_repos_5/1_chris1132_spark_lecture/..src.main.scala.com.chovy.spark.DataFrame.CaseOne.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 20
"
"udf/spark_repos_5/1_chris1132_spark_lecture/..src.main.scala.com.chovy.spark.DataFrame.CaseSecond.scala/udf/21.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

e => ""Name:"" + e(0) + "",Age:"" + e(1)
"
"udf/spark_repos_5/1_chris1132_spark_lecture/..src.main.scala.com.chovy.spark.DataFrame.CaseThird.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

e => ""name:"" + e(0) + "",age:"" + e(1)
"
"udf/spark_repos_5/1_cong666_SparkSQL/..src.main.scala.com.chen.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_5/1_dengtianxiang_spark-learn/..src.main.scala.com.xyg.spark.examples.TestDemo.scala/udf/11.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_5/1_dengtianxiang_spark-learn/..src.main.scala.com.xyg.spark.examples.TestDemo.scala/udf/15.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_5/1_dengtianxiang_spark-learn/..src.main.scala.com.xyg.sparkSql.sparkSession.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_5/1_dengtianxiang_spark-learn/..src.main.scala.com.xyg.sparkSql.sparkSession.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_5/1_dengtianxiang_spark-learn/..src.main.scala.com.xyg.sparkSql.sparkSession.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_5/1_dengtianxiang_spark-learn/..src.main.scala.com.xyg.sparkSql.sparkSession.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_5/1_dengtianxiang_spark-learn/..src.main.scala.com.xyg.sparkSql.sparkSession.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_5/1_dengtianxiang_spark-learn/..src.main.scala.com.xyg.sparkSql.sparkSession.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_5/1_dengziming_hongya-taxi-data/..taxispark.src.main.scala.com.hongya.bigdata.util.RunGeoTime.scala/udf/33.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

hours
"
"udf/spark_repos_5/1_eozel44_datagraph/..src.main.scala.modelgraph.scala/udf/19.19.Dataset-EdgeData.map","Type: org.apache.spark.sql.Dataset[modelgraph.EdgeData]
Call: map

{
        r => Edge(r.sourceid, r.targetid, r.weight)
      }
"
"udf/spark_repos_5/1_fbjoker_Spark/..SparkSql.src.main.scala.com.alex.spark.demo1.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 20
"
"udf/spark_repos_5/1_fbjoker_Spark/..SparkSql.src.main.scala.com.alex.spark.TestUDAF.scala/udf/11.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

CustomerUDAF
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.boy.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.boy.spark.DataFrameRDDApp.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.boy.spark.DataFrameRDDApp.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.boy.spark.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.boy.spark.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.imooc.log.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.imooc.log.TopNStatJob.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.imooc.log.TopNStatJob.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.imooc.log.TopNStatJobYarn.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/1_fpkgithub_Spark_sql_learning/..ImoocSparkSQLProject.src.main.scala.com.imooc.log.TopNStatJobYarn.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/1_FranciscoLLopez_pfm-spark-kmeans/..src.org.pfm.spark.utils.Functions.scala/udf/25.19.Dataset-Vector, Int).map","Type: org.apache.spark.sql.Dataset[(Int, org.apache.spark.ml.linalg.Vector, Int)]
Call: map

{
        case (id, pcaScaledFeatureVector, cluster) =>
          (id, Vectors.sqdist(centroids(cluster), pcaScaledFeatureVector))
      }
"
"udf/spark_repos_5/1_goomhow_spark-job-assemble/..src.com.ctc.juan.SeedsGenerator.scala/udf/12.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new Vote
"
"udf/spark_repos_5/1_goomhow_spark-job-assemble/..src.com.ctc.juan.SeedsGenerator.scala/udf/186.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

user_cell(""BILLING_NBR"").isin(set.toArray: _*)
"
"udf/spark_repos_5/1_goomhow_spark-job-assemble/..src.com.ctc.juan.SeedsGenerator.scala/udf/197.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

set_cell(""CELL_ID"").isin(cells: _*)
"
"udf/spark_repos_5/1_goomhow_spark-job-assemble/..src.com.ctc.juan.SeedsGenerator.scala/udf/91.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

corp_cells(""COMPANY"").isin(locally {
        val _t_m_p_10 = corps.toArray
        _t_m_p_10.map(_._1)
      }: _*)
"
"udf/spark_repos_5/1_gujita_learningFromSparkExamples/..src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_5/1_hagarciag_Big-Data-Analysis-with-Scala-and-Spark/..src.week4.Lecture_4_3_Dataframes_1.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""price"" >= 31
"
"udf/spark_repos_5/1_hagarciag_Big-Data-Analysis-with-Scala-and-Spark/..src.week4.Lecture_4_3_Dataframes_1.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'price >= 31
"
"udf/spark_repos_5/1_hagarciag_Big-Data-Analysis-with-Scala-and-Spark/..src.week4.Lecture_4_3_Dataframes_1.scala/udf/66.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

purchaseDF(""price"") >= 31
"
"udf/spark_repos_5/1_hagarciag_Big-Data-Analysis-with-Scala-and-Spark/..src.week4.Lecture_4_3_Dataframes_1.scala/udf/91.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""price"" >= 31 && $""city"" === ""Zurich""
"
"udf/spark_repos_5/1_hagarciag_Big-Data-Analysis-with-Scala-and-Spark/..src.week4.Lecture_4_5_Datasets.scala/udf/49.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row(0).asInstanceOf[Int] + 1
"
"udf/spark_repos_5/1_Hammad-Ikhlaq_Spark/..MovieRecommendationsALS.scala/udf/35.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split('\t')
"
"udf/spark_repos_5/1_hbghhy_coursera_spark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/121.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => new TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
"
"udf/spark_repos_5/1_hbghhy_coursera_spark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/130.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

x => new TimeUsageRow(x._1._1, x._1._2, x._1._3, round(x._2), round(x._3), round(x._4))
"
"udf/spark_repos_5/1_hklgit_SparkTests/..src.main.scala.es.UserTagsDid2ES.scala/udf/31.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ row => 
        val did = row.getAs[String](""did"")
        val province = row.getAs[String](""province"")
        val city = row.getAs[String](""city"")
        val tc_version = row.getAs[String](""tc_version"")
        val app_version = row.getAs[String](""app_version"")
        val model = row.getAs[String](""model"")
        val chip = row.getAs[String](""chip"")
        val size = row.getAs[String](""size"")
        val source = row.getAs[String](""source"")
        val event_tags = row.getAs[String](""event_tags"")
        val user_tags = row.getAs[String](""user_tags"")
        val pay_tags = row.getAs[String](""pay_tags"")
        val content_tags = row.getAs[String](""content_tags"")
        Tags(did, tc_version, chip, city, event_tags, content_tags, source, size, model, app_version, pay_tags, user_tags, province)
      }
"
"udf/spark_repos_5/1_hpaDoIt_sparkdemo/..src.main.scala.com.hpa.spark.cep.main.CepBootstrap.scala/udf/56.21.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => StringUtils.splitByWholeSeparatorPreserveAllTokens(x._2, ""|"")
"
"udf/spark_repos_5/1_hyokwan_scala-lecture/..c3_dataframe.s1_dataFrameBasic.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""PRODUCTGROUP"" === ""ST0002"" && $""VOLUME"" > 150000
"
"udf/spark_repos_5/1_hyokwan_scala-lecture/..c3_dataframe.s2_dataFrameSelect.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""PRODUCTGROUP"" === ""ST0002"" && $""VOLUME"" > 150000
"
"udf/spark_repos_5/1_hyokwan_scala-lecture/..c3_dataframe.s3_dataFrameMissingValue.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""VOLUME"".isNull || $""TARGET"".isNull
"
"udf/spark_repos_5/1_hyokwan_scala-lecture/..c3_dataframe.s3_dataFrameMissingValue.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.anyNull
"
"udf/spark_repos_5/1_hyokwan_scala-lecture/..c6_machineLearning.s1_mlSupervised.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""YEARWEEK"" <= targetYearweek
"
"udf/spark_repos_5/1_hyokwan_scala-lecture/..c6_machineLearning.s1_mlSupervised.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""YEARWEEK"" > targetYearweek
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.TestSources.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""log_id"" === ""b37d63d3-4829-4609-b9eb-866e3b75b91a-b94097bde9""
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/108.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec1: Vector, vec2: Vector) => {
        val dot = new org.apache.spark.mllib.feature.ElementwiseProduct(OldVectors.fromML(vec1)).transform(OldVectors.fromML(vec2))
        var value = 0d
        val value_add = (a: Int, b: Double) => value += b
        dot.foreachActive(value_add)
        value / (Vectors.norm(vec1, 2) * Vectors.norm(vec2, 2))
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/11.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(co: String) => {
        val parseMethod = Class.forName(""org.ansj.splitWord.analysis.NlpAnalysis"").getMethod(""parse"", classOf[String])
        val tmp = parseMethod.invoke(null, co)
        val terms = tmp.getClass.getMethod(""getTerms"").invoke(tmp).asInstanceOf[java.util.List[Any]]
        locally {
          val _t_m_p_2 = terms
          _t_m_p_2.map(f => f.asInstanceOf[{ def getName: String }].getName)
        }.toArray
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/120.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Int, size: Int) => {
        val oneValue = Array(1.0d)
        val emptyValues = Array.empty[Double]
        val emptyIndices = Array.empty[Int]
        if (a < size) {
          Vectors.sparse(size, Array(a), oneValue)
        } else {
          Vectors.sparse(size, emptyIndices, emptyValues)
        }
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/135.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec1: Seq[String], vec2: Seq[String]) => vec1.intersect(vec2)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/141.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec1: Seq[String], word: Any) => vec1.indexOf(word)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/147.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec: Seq[String]) => locally {
        val _t_m_p_21 = vec
        _t_m_p_21.map(f => f.toDouble)
      }.max
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/156.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec1: Seq[String], from: Int, to: Int) => if (to == -1) {
        vec1.slice(from, vec1.length)
      } else {
        vec1.slice(from, to)
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/166.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[Seq[Number]]) => locally {
        val _t_m_p_24 = locally {
          val _t_m_p_25 = a
          _t_m_p_25.flatMap(f => f)
        }
        _t_m_p_24.map(f => f.doubleValue())
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/178.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[Seq[String]]) => locally {
        val _t_m_p_27 = a
        _t_m_p_27.flatMap(f => f)
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/187.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[Number]) => locally {
        val _t_m_p_29 = a
        _t_m_p_29.map(f => f.toString)
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/196.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[String]) => locally {
        val _t_m_p_31 = a
        _t_m_p_31.map(f => f.toDouble)
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/205.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[String]) => locally {
        val _t_m_p_33 = a
        _t_m_p_33.map(f => f.toFloat)
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/214.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(a: Seq[String]) => locally {
        val _t_m_p_35 = a
        _t_m_p_35.map(f => f.toInt)
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/223.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: Seq[String], offset: Int) => str(offset)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/229.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(words: Seq[String], n: Int) => locally {
        val _t_m_p_38 = words.iterator.sliding(n).withPartial(false)
        _t_m_p_38.map(_.mkString("" ""))
      }.toSeq
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/238.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(item: Array[Byte]) => new String(item, ""utf-8"")
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/244.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

() => java.util.UUID.randomUUID().toString
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/250.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(path: String) => HDFSOperator.readBinaryFile(path, 1024)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/25.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sep: String, co: mutable.WrappedArray[String]) => co.mkString(sep)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/31.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sleep: Long) => {
        Thread.sleep(sleep)
        """"
      }
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/40.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vector: Vector) => vector.argmax
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/46.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec1: Vector, vec2: Vector) => Vectors.sqdist(vec1, vec2)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/52.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec1: Vector, p: Double) => Vectors.norm(vec1, p)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/58.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec1: Seq[Double]) => Vectors.dense(vec1.toArray)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/64.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec1: Vector) => vec1.toArray
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/70.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(splitter: String, vec1: Vector) => vec1.toArray.mkString(splitter)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/76.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec: Vector) => vec.toSparse
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/82.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(size: Int, vec1: Map[Int, Double]) => Vectors.sparse(size, vec1.toSeq)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/88.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vecs: Seq[Vector]) => FVectors.assemble(vecs: _*)
"
"udf/spark_repos_5/1_iodone_rotor/..engine.src.main.scala.rotor.engine.udf.Functions.scala/udf/94.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(vec: Vector, inds: Seq[Int]) => vec match {
        case features: DenseVector =>
          Vectors.dense(locally {
            val _t_m_p_15 = inds.toArray
            _t_m_p_15.map(features.apply)
          })
        case features: SparseVector =>
          FVectors.slice(features, inds.toArray)
      }
"
"udf/spark_repos_5/1_jirojo2_sibd-spark/..recommender-spark.src.main.scala.es.upm.dit.sibd.josi.SongRecommender.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$""name"".contains(""[unknown]"")
"
"udf/spark_repos_5/1_jirojo2_sibd-spark/..recommender-spark.src.main.scala.es.upm.dit.sibd.josi.SongRecommender.scala/udf/87.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val Array(userID, artistID, count) = locally {
          val _t_m_p_5 = line.split(' ')
          _t_m_p_5.map(_.toInt)
        }
        val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)
        (userID, finalArtistID, count)
      }
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1117.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AnnotationPropertySQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1124.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AnnotationPropertyValueSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1131.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AnonymousConceptUnionAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1138.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AspectSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1145.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AspectSpecializationAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1152.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.BinaryScalarRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1159.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.BundleSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1166.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.BundledTerminologyAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1173.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.CardinalityRestrictedAspectSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1180.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.CardinalityRestrictedConceptSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1187.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.CardinalityRestrictedReifiedRelationshipSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1194.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ChainRuleSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1201.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ConceptSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1208.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ConceptDesignationTerminologyAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1215.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ConceptInstanceSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1222.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ConceptSpecializationAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1229.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.DescriptionBoxSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1236.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.DescriptionBoxExtendsClosedWorldDefinitionsSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1243.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.DescriptionBoxRefinementSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1250.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityExistentialRestrictionAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1257.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityScalarDataPropertySQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1264.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityScalarDataPropertyExistentialRestrictionAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1271.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityScalarDataPropertyParticularRestrictionAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1278.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityScalarDataPropertyUniversalRestrictionAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1285.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityStructuredDataPropertySQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1292.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityStructuredDataPropertyParticularRestrictionAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1299.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityUniversalRestrictionAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1306.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ForwardPropertySQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1313.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.IRIScalarRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1320.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipEnumerationRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1327.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipExistentialRangeRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1334.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipOneOfRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1341.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipUniversalRangeRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1348.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipValueRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1355.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InversePropertySQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1362.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.NumericScalarRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1369.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.PlainLiteralScalarRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1376.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1383.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipInstanceSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1390.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipInstanceDomainSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1397.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipInstanceRangeSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1404.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1411.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipSpecializationAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1418.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.RestrictionScalarDataPropertyValueSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1425.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.RestrictionStructuredDataPropertyTupleSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1432.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.RootConceptTaxonomyAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1439.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.RuleBodySegmentSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1446.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1453.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarDataPropertySQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1460.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarDataPropertyValueSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1467.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarOneOfLiteralAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1474.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarOneOfRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1481.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SegmentPredicateSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1488.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SingletonInstanceScalarDataPropertyValueSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1495.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SingletonInstanceStructuredDataPropertyValueSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1502.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SpecificDisjointConceptAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1509.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.StringScalarRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1516.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.StructureSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1523.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.StructuredDataPropertySQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1530.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.StructuredDataPropertyTupleSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1537.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SubDataPropertyOfAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1544.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SubObjectPropertyOfAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1551.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SynonymScalarRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1558.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.TerminologyExtensionAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1565.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.TerminologyGraphSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1572.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.TerminologyNestingAxiomSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1579.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.TimeScalarRestrictionSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1586.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.UnreifiedRelationshipSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/1593.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.UnreifiedRelationshipInstanceTupleSQL2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/456.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AnnotationPropertyRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/463.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AnnotationPropertyValueRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/470.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AnonymousConceptUnionAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/477.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AspectRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/484.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.AspectSpecializationAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/491.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.BinaryScalarRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/498.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.BundleRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/505.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.BundledTerminologyAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/512.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.CardinalityRestrictedAspectRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/519.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.CardinalityRestrictedConceptRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/526.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.CardinalityRestrictedReifiedRelationshipRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/533.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ChainRuleRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/540.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ConceptRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/547.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ConceptDesignationTerminologyAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/554.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ConceptInstanceRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/561.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ConceptSpecializationAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/568.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.DescriptionBoxRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/575.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.DescriptionBoxExtendsClosedWorldDefinitionsRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/582.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.DescriptionBoxRefinementRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/589.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityExistentialRestrictionAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/596.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityScalarDataPropertyRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/603.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityScalarDataPropertyExistentialRestrictionAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/610.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityScalarDataPropertyParticularRestrictionAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/617.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityScalarDataPropertyUniversalRestrictionAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/624.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityStructuredDataPropertyRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/631.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityStructuredDataPropertyParticularRestrictionAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/638.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.EntityUniversalRestrictionAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/645.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ForwardPropertyRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/652.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.IRIScalarRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/659.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipEnumerationRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/666.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipExistentialRangeRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/673.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipOneOfRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/680.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipUniversalRangeRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/687.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InstanceRelationshipValueRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/694.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.InversePropertyRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/701.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.NumericScalarRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/708.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.PlainLiteralScalarRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/715.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/722.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipInstanceRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/729.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipInstanceDomainRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/736.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipInstanceRangeRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/743.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/750.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ReifiedRelationshipSpecializationAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/757.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.RestrictionScalarDataPropertyValueRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/764.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.RestrictionStructuredDataPropertyTupleRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/771.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.RootConceptTaxonomyAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/778.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.RuleBodySegmentRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/785.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/792.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarDataPropertyRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/799.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarDataPropertyValueRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/806.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarOneOfLiteralAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/813.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.ScalarOneOfRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/820.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SegmentPredicateRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/827.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SingletonInstanceScalarDataPropertyValueRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/834.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SingletonInstanceStructuredDataPropertyValueRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/841.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SpecificDisjointConceptAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/848.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.StringScalarRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/855.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.StructureRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/862.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.StructuredDataPropertyRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/869.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.StructuredDataPropertyTupleRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/876.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SubDataPropertyOfAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/883.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SubObjectPropertyOfAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/890.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.SynonymScalarRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/897.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.TerminologyExtensionAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/904.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.TerminologyGraphRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/911.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.TerminologyNestingAxiomRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/918.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.TimeScalarRestrictionRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/925.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.UnreifiedRelationshipRow2Tuple
"
"udf/spark_repos_5/1_JPL-IMCE_gov.nasa.jpl.imce.oml.frameless/..src.main.scala.gov.nasa.jpl.imce.oml.frameless.OMLSpecificationTypedDatasets.scala/udf/932.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

OMLReaders.UnreifiedRelationshipInstanceTupleRow2Tuple
"
"udf/spark_repos_5/1_konradcala_spark-h2-tests/..src.main.scala.com.konrad.spark_h2_tests.App.scala/udf/11.19.Dataset-Student.map","Type: org.apache.spark.sql.Dataset[com.konrad.spark_h2_tests.model.Student]
Call: map

student => Person(student.firstName + "" "" + student.lastName)
"
"udf/spark_repos_5/1_kunnum_sandbox/..clustering-basics.src.main.scala.com.ss.ml.clustering.ClusteringBasics.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""_c1 = '$name'""
"
"udf/spark_repos_5/1_kunnum_sandbox/..clustering-basics.src.main.scala.com.ss.ml.clustering.ClusteringBasics.scala/udf/48.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getString(1), dotProduct(sourceTfIdf, r.getAs[Vector](""tf-idf"")))
"
"udf/spark_repos_5/1_kunnum_sandbox/..regression-basics.src.main.scala.com.ss.ml.regression.GradientDescent.scala/udf/19.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

pdTheta0(t1, t2)
"
"udf/spark_repos_5/1_kunnum_sandbox/..regression-basics.src.main.scala.com.ss.ml.regression.GradientDescent.scala/udf/26.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

pdTheta1(t1, t2)
"
"udf/spark_repos_5/1_kunnum_sandbox/..regression-basics.src.main.scala.com.ss.ml.regression.GradientDescent.scala/udf/48.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
          val x1 = r.getAs[Double](""x1"")
          val y = r.getAs[Double](""y"")
          math.pow(t1 + x1 * t2 - y, 2)
        }
"
"udf/spark_repos_5/1_kunnum_sandbox/..regression-basics.src.main.scala.com.ss.ml.regression.MultipleRegression.scala/udf/15.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => buildLp(r, ""sqft_living"")
"
"udf/spark_repos_5/1_kunnum_sandbox/..regression-basics.src.main.scala.com.ss.ml.regression.MultipleRegression.scala/udf/20.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => buildLp(r, ""sqft_living"")
"
"udf/spark_repos_5/1_kunnum_sandbox/..regression-basics.src.main.scala.com.ss.ml.regression.MultipleRegression.scala/udf/24.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => buildLp(r, ""sqft_living"", ""sqft_living15"")
"
"udf/spark_repos_5/1_kunnum_sandbox/..regression-basics.src.main.scala.com.ss.ml.regression.MultipleRegression.scala/udf/29.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => buildLp(r, ""sqft_living"", ""sqft_living15"")
"
"udf/spark_repos_5/1_lansaloltd_spark-add-columns/..src.main.scala.com.lansalo.columns.package.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

mappingRows(df.schema)(words)
"
"udf/spark_repos_5/1_Liaol666_aliyun-gmall/..src.main.scala.member.service.AdsMemberService.scala/udf/18.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys: Array[String] = item._1.split(""_"")
        val appregurl: String = keys(0)
        val dn: String = keys(1)
        val dt: String = keys(2)
        (appregurl, item._2, dt, dn)
      }
"
"udf/spark_repos_5/1_Liaol666_aliyun-gmall/..src.main.scala.member.service.AdsMemberService.scala/udf/34.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys: Array[String] = item._1.split(""_"")
        val sitename: String = keys(0)
        val dn: String = keys(1)
        val dt: String = keys(2)
        (sitename, item._2, dt, dn)
      }
"
"udf/spark_repos_5/1_Liaol666_aliyun-gmall/..src.main.scala.member.service.AdsMemberService.scala/udf/50.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys: Array[String] = item._1.split(""_"")
        val adname: String = keys(0)
        val dn: String = keys(1)
        val dt: String = keys(2)
        (adname, item._2, dt, dn)
      }
"
"udf/spark_repos_5/1_Liaol666_aliyun-gmall/..src.main.scala.member.service.AdsMemberService.scala/udf/66.20.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys: Array[String] = item._1.split(""_"")
        val memberlevel = keys(0)
        val dn = keys(1)
        val dt = keys(2)
        (memberlevel, item._2, dt, dn)
      }
"
"udf/spark_repos_5/1_Liaol666_aliyun-gmall/..src.main.scala.member.service.AdsMemberService.scala/udf/82.20.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys = item._1.split(""_"")
        val vip_level = keys(0)
        val dn = keys(1)
        val dt = keys(2)
        (vip_level, item._2, dt, dn)
      }
"
"udf/spark_repos_5/1_maglighter_glonassdatamining/..src.main.scala.org.grint.glonassdatamining.app.App.scala/udf/50.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => OutputCsv(row(0).asInstanceOf[Int], row(1).toString, row.getDouble(2), row.getDouble(3), row.getLong(4), row.getString(5), row.getString(6))
"
"udf/spark_repos_5/1_maglighter_glonassdatamining/..src.main.scala.org.grint.glonassdatamining.clusterization.SpatioTemporalPointDbscan.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Input(row(0).asInstanceOf[Timestamp], row(1).toString, row.getDouble(2), row.getDouble(3), ""POINT("" + row(2) + "" "" + row(3) + "")"", (if (row.isNullAt(4)) """" else row.getString(4)).replaceAll(""""""
"""""", """"), if (row.isNullAt(5)) """" else row.getString(5))
"
"udf/spark_repos_5/1_maglighter_glonassdatamining/..src.main.scala.org.grint.glonassdatamining.nlp.NLPTools.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""description"".isNotNull
"
"udf/spark_repos_5/1_mksonline_Lambda-Architecture/..salez.src.main.scala.streaming.LambdaJob.scala/udf/15.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

r => r.split("","")
"
"udf/spark_repos_5/1_mksonline_Lambda-Architecture/..salez.src.main.scala.streaming.LambdaJob.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => ProvinceSale(r.getString(0), r.getString(1), r.getLong(2))
"
"udf/spark_repos_5/1_MMendke_SansaExampleAssociationRule/..Preprocessing.scala/udf/172.27.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetA.id"".isNotNull
"
"udf/spark_repos_5/1_MMendke_SansaExampleAssociationRule/..Preprocessing.scala/udf/174.25.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetB.id"".isNotNull
"
"udf/spark_repos_5/1_MMendke_SansaExampleAssociationRule/..Preprocessing.scala/udf/176.23.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""datasetA.id"" =!= ($""datasetB.id"")
"
"udf/spark_repos_5/1_mryingjie_spark-demo/..spark-sql.src.main.scala.com.demo.spark.sql.HelloWord.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""salary"" > 3300
"
"udf/spark_repos_5/1_mryingjie_spark-demo/..spark-sql.src.main.scala.com.demo.spark.sql.udf.UDFClient.scala/udf/16.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: String) => ""name:"" + x
"
"udf/spark_repos_5/1_mryingjie_spark-demo/..spark-sql.src.main.scala.com.demo.spark.sql.udf.UDFClient.scala/udf/35.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AVG()
"
"udf/spark_repos_5/1_NookLook2014_MedItemRecSys/..spark-job.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/45.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_5/1_NookLook2014_MedItemRecSys/..spark-job.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_5/1_NookLook2014_MedItemRecSys/..spark-job.src.main.scala.org.apache.spark.ml.feature.LSH.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_5/1_NookLook2014_MedItemRecSys/..spark-job.src.main.scala.org.apache.spark.mllib.tree.model.treeEnsembleModels.scala/udf/255.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

NodeData.apply
"
"udf/spark_repos_5/1_opentargets_platform-etl-backend/..src.main.scala.io.opentargets.etl.backend.DataDrivenRelation.scala/udf/19.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""type"") === ""shared-disease""
"
"udf/spark_repos_5/1_opentargets_platform-etl-backend/..src.main.scala.io.opentargets.etl.backend.DataDrivenRelation.scala/udf/22.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""type"") === ""shared-target""
"
"udf/spark_repos_5/1_opentargets_platform-etl-backend/..src.main.scala.io.opentargets.etl.backend.Disease.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ontology.isTherapeuticArea"") === true
"
"udf/spark_repos_5/1_opentargets_platform-etl-backend/..src.main.scala.io.opentargets.etl.backend.Drug.scala/udf/101.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""sourceID"") === ""chembl""
"
"udf/spark_repos_5/1_origin-fly_Scala/..ScalaDemo.src.main.scala.ayibo.day1209.SparkSQLUDFUDAF.scala/udf/21.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyUDAF
"
"udf/spark_repos_5/1_PacktPublishing_Hands-On-Big-Data-Analysis-with-Hadoop-3/..section_2.apache-spark-2-scala-starter-template.src.main.scala.com.example.CreatingDatasets.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_5/1_PacktPublishing_Hands-On-Big-Data-Analysis-with-Hadoop-3/..section_2.apache-spark-2-scala-starter-template.src.main.scala.com.example.ProgrammingGuideSQL.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_5/1_PacktPublishing_Hands-On-Big-Data-Analysis-with-Hadoop-3/..section_2.apache-spark-2-scala-starter-template.src.main.scala.com.tomekl007.anomalydetection.RunKMeans.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
        val cluster = row.getAs[Int](""cluster"")
        val vec = row.getAs[Vector](""scaledFeatureVector"")
        Vectors.sqdist(centroids(cluster), vec) >= farthestDistanceBetweenTwoNormalClusters
      }
"
"udf/spark_repos_5/1_PacktPublishing_Hands-On-Big-Data-Analysis-with-Hadoop-3/..section_2.apache-spark-2-scala-starter-template.src.main.scala.com.tomekl007.anomalydetection.RunKMeans.scala/udf/62.19.Dataset-Vector).map","Type: org.apache.spark.sql.Dataset[(Int, org.apache.spark.ml.linalg.Vector)]
Call: map

{
        case (cluster, vec) =>
          Vectors.sqdist(centroids(cluster), vec)
      }
"
"udf/spark_repos_5/1_PacktPublishing_Hands-On-Big-Data-Analysis-with-Hadoop-3/..section_2.apache-spark-2-scala-starter-template.src.test.scala.com.tomekl007.SparkApisTests.scala/udf/28.22.Dataset-UserData.filter","Type: org.apache.spark.sql.Dataset[com.tomekl007.UserData]
Call: filter

_.userId == ""a""
"
"udf/spark_repos_5/1_patilmadhu_SCD2_Scala/..IncrementalProcessing.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""md5Flag"" === ""U""
"
"udf/spark_repos_5/1_patilmadhu_SCD2_Scala/..IncrementalProcessing.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""md5Flag"" === ""I"" || $""md5Flag"" === ""U""
"
"udf/spark_repos_5/1_patilmadhu_SCD2_Scala/..IncrementalProcessing.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""md5Flag"" === ""NCR""
"
"udf/spark_repos_5/1_prad-a-RuntimeException_semantic-store/..src.main.scala.recipestore.db.tinkerpop.DataFrameToDSEVertex.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => dseVertex(row)
"
"udf/spark_repos_5/1_prad-a-RuntimeException_semantic-store/..src.main.scala.recipestore.db.tinkerpop.RecipeToUserGraph.scala/udf/30.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.schema.fields.find(f => f.name.equals(""type"")).isDefined
"
"udf/spark_repos_5/1_prad-a-RuntimeException_semantic-store/..src.main.scala.recipestore.db.tinkerpop.RecipeToUserGraph.scala/udf/32.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => allowedVertices.contains(Option(row.getString(row.fieldIndex(""type""))).getOrElse(null))
"
"udf/spark_repos_5/1_prad-a-RuntimeException_semantic-store/..src.main.scala.recipestore.db.tinkerpop.RecipeToUserGraph.scala/udf/70.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => allowedEdges.contains(Option(row.getString(row.fieldIndex(""relationship""))).getOrElse(""""))
"
"udf/spark_repos_5/1_prad-a-RuntimeException_semantic-store/..src.main.scala.recipestore.db.tinkerpop.TinkerpopGraphModule.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => Option(row.getString(row.fieldIndex(""type""))).getOrElse("""").equals(""Recipe"")
"
"udf/spark_repos_5/1_prad-a-RuntimeException_semantic-store/..src.main.scala.recipestore.graph.Dataextractors.scala/udf/6.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => getRowVal(row, ""type"").equals(t)
"
"udf/spark_repos_5/1_prad-a-RuntimeException_semantic-store/..src.test.scala.recipestore.graph.GraphCreator$Test.scala/udf/23.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => Option(row.getString(row.fieldIndex(""type""))).getOrElse("""").equals(""Recipe"")
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.GEFI.join.GEFIJoin.scala/udf/65.22.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[edu.purdue.knowledgecubes.rdf.RDFTriple]
Call: filter

value => {
        def foo(value: RDFTriple): Boolean = {
          val filters = ListBuffer[GEFI]()
          for (entry <- all) {
            if (column == ""s"") {
              if (!broadcastVariable.value.get(entry._1).get(entry._2).contains(value.s)) {
                return false
              }
            } else {
              if (!value.o.startsWith(""\"""") && !broadcastVariable.value.get(entry._1).get(entry._2).contains(value.o.toInt)) {
                return false
              }
            }
          }
          true
        }
        foo(value)
      }
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/319.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"").equalTo(col(""p"") && col(""s"").equalTo(col(""o"")))
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/324.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"").equalTo(col(""p""))
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/329.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"").equalTo(col(""o""))
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/334.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""p"").equalTo(col(""o""))
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/343.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""o"").equalTo(t1.getObject.toString()) && col(""s"").equalTo(col(""p""))
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/348.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""o"").equalTo(t1.getObject.toString())
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/355.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""p"").equalTo(t1.getPredicate.toString()) && col(""s"").equalTo(col(""o""))
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/360.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""p"").equalTo(t1.getPredicate.toString())
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/366.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""p"").equalTo(t1.getPredicate.toString()) && col(""o"").equalTo(t1.getObject.toString())
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/372.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"").equalTo(t1.getSubject.toString()) && col(""p"").equalTo(col(""o""))
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/377.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"").equalTo(t1.getSubject.toString())
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/383.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"").equalTo(t1.getSubject.toString()) && col(""o"").equalTo(t1.getObject.toString())
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/388.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"").equalTo(t1.getSubject.toString()) && col(""p"").equalTo(t1.getPredicate.toString())
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.queryprocessor.executor.Executor.scala/udf/393.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""s"").equalTo(t1.getSubject.toString()) && col(""p"").equalTo(t1.getPredicate.toString()) && col(""o"").equalTo(t1.getObject.toString())
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.rdf.RDFPropertyIdentifier.scala/udf/78.28.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[edu.purdue.knowledgecubes.rdf.RDFTriple]
Call: filter

col(""s"").equalTo(sub).and(col(""o"").equalTo(obj))
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.rdf.RDFPropertyIdentifier.scala/udf/83.28.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[edu.purdue.knowledgecubes.rdf.RDFTriple]
Call: filter

col(""s"").equalTo(sub)
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.rdf.RDFPropertyIdentifier.scala/udf/88.28.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[edu.purdue.knowledgecubes.rdf.RDFTriple]
Call: filter

col(""o"").equalTo(obj)
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.storage.persistent.Store.scala/udf/38.24.Dataset-RDFTriple.filter","Type: org.apache.spark.sql.Dataset[edu.purdue.knowledgecubes.rdf.RDFTriple]
Call: filter

col(""p"").equalTo(uri)
"
"udf/spark_repos_5/1_purduedb_knowledgecubes/..src.main.scala.edu.purdue.knowledgecubes.utils.NTParser.scala/udf/8.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        var parts = line.split("" "")
        var sub = parts(0)
        var prop = parts(1)
        var obj = parts.slice(2, parts.size).mkString("" "")
        RDFTriple(sub.toInt, prop.toInt, obj)
      }
"
"udf/spark_repos_5/1_sankumarbigdata_sparkStreamingETL-master/..src.main.scala.ingestion.transformations.ColumnsHandler.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCond
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.DataSetWindowPlain.scala/udf/17.24.Dataset-Event2.filter","Type: org.apache.spark.sql.Dataset[com.griddynamics.stopbot.model.Event2]
Call: filter

m => m.ip != null && m.eventTime != null
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.DataSetWindowPlain.scala/udf/19.19.Dataset-Event2.map","Type: org.apache.spark.sql.Dataset[com.griddynamics.stopbot.model.Event2]
Call: map

{
        m => ToAggregate(m.ip, if (m.action == ""click"") 1 else 0, if (m.action == ""watch"") 1 else 0, m.eventTime)
      }
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.DataSetWindowUdf.scala/udf/14.22.Dataset-Event2.filter","Type: org.apache.spark.sql.Dataset[com.griddynamics.stopbot.model.Event2]
Call: filter

m => m.ip != null && m.action != null
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.StructureWindowPlain.scala/udf/10.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""total_events"") > minEvents
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.StructureWindowPlain.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""incident"").isNotNull
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.StructureWindowSql.scala/udf/10.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""total_events"") > minEvents
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.StructureWindowSql.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

expr(""incident is not null"")
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.StructureWindowUdf.scala/udf/11.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""total_events"") > minEvents
"
"udf/spark_repos_5/1_sergei-grigorev_spark-streaming-project/..sparkstreaming.src.main.scala.com.griddynamics.stopbot.spark.logic.StructureWindowUdf.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""incident"").isNotNull
"
"udf/spark_repos_5/1_shubhm-agrwl_spark-streaming/..KafkaToKafka.scala/udf/14.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""msg_id"" === ""one"" || $""type"" === ""two""
"
"udf/spark_repos_5/1_shubhm-agrwl_spark-streaming/..KafkaToKafka.scala/udf/16.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

(unix_timestamp(current_timestamp()) - ($""tstamp"")) / 60 < 10
"
"udf/spark_repos_5/1_sibatHighway_GuangDong/..highway.src.main.scala.cn.sibat.highway.DefineDataField.scala/udf/8.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.NLP.Ansj.AnsjDemo4.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""getKW"" =!= ""NULL""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.NLP.Ansj.AnsjDemo5.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""getKW"" =!= ""NULL""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.NLP.Ansj.AnsjLoadDicDemo1.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""getKW"" =!= ""NULL""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.NLP.Ansj.AnsjLoadDicDemo2.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""getKW"" =!= ""NULL""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DataMiningPlatform.machine.learning.recommend.ALStest1.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!$"""".contains("""")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DataMiningPlatform.machine.learning.recommend.ItemBasedModelApplication.scala/udf/114.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DataMiningPlatform.machine.learning.recommend.ItemBasedModelApplication.scala/udf/34.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNotNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DataMiningPlatform.machine.learning.recommend.ItemBasedModelApplication.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DataMiningPlatform.machine.learning.recommend.UserBasedModelApplication.scala/udf/115.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DataMiningPlatform.machine.learning.recommend.UserBasedModelApplication.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNotNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DataMiningPlatform.machine.learning.recommend.UserBasedModelApplication.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DocsSimilarity.CheckResult.CheckJaccard.scala/udf/146.23.Dataset-GetJaccardSchema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.Project.DocsSimilarity.CheckResult.CheckJaccard.GetJaccardSchema]
Call: filter

$""doc1"" === ""03e73f5f-7896-44f8-860e-8102a220b1cf""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DocsSimilarity.CheckResult.CheckJaccard.scala/udf/150.23.Dataset-GetJaccardSchema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.Project.DocsSimilarity.CheckResult.CheckJaccard.GetJaccardSchema]
Call: filter

$""doc2"" === ""03e73f5f-7896-44f8-860e-8102a220b1cf""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DocsSimilarity.DocsimiEuclidean.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" =!= ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DocsSimilarity.DocsimiEuclidean.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""distCol"" > 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DocsSimilarity.DocsimiJaccard.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" =!= ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DocsSimilarity.DocsimiTitle.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" =!= ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DocsSimilarity.DocsimiTitle.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" =!= ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.DocsSimilarity.DocsimiTitleV2.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" =!= ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.Recommend.AppRecom.UsersimiSub.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.Recommend.userModelV2.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.Recommend.Wu.reCompuRating.scala/udf/18.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""userID"" === allUser(user)(0)
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.Recommend.Wu.reCompuRating.scala/udf/37.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""userID"" === allUser(user)(0)
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.Recommend.Wu.reCompuRating.scala/udf/39.28.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""urlID"" === haveRead(all)(0)
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.Recommend.Wu.reCompuRating.scala/udf/85.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x(4) != null && x(6) != null
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.Recommend.Wu.WuRecommender.scala/udf/145.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""whether"").isNull
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.BaiDu.GetTittle.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length($""getTitle"") >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.CombineData.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""weight"" > 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.DemoTest.CheckRelationResult.scala/udf/27.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""source_name"".contains(""岳寿伟"") || $""source_name"".contains(""张迪"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.DemoTest.CheckRelationResult.scala/udf/29.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""target_name"".contains(""岳寿伟"") || $""target_name"".contains(""张迪"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.DocSimi.DocSimi_Demo1.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length($""address"") >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.DocSimi.DocSimi_Demo1.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length($""full_name"") >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.DocSimi.DocSimi_Demo1.scala/udf/36.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length($""content"") >= 3
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.DocSimi.DocSimi_Demo1.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length($""seg_words"") >= 3
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.DocSimi.DocSimi_Demo1.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""tag"" === ""address""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.DocSimi.DocSimi_Demo1.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""tag"" === ""full_name""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/137.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""meeting_name"" =!= ""无""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/141.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time_diff"" >= 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/149.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/180.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time_diff"" >= 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/188.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/222.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""weight"" > 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/226.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""relation"" === ""校友""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/230.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""relation"" === ""同事""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""company"" =!= ""无""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""organization_name"" =!= ""无""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time_diff"" >= 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.FamiliarityRelation.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.CheckData.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""target_id"" === ""502d58d0-ea9e-4521-8a31-06e5e7aa13aa""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time_diff"" >= 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/109.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/148.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""meeting_name"" =!= ""无""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/152.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time_diff"" >= 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/160.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/191.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""time_diff"" >= 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/199.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""company"" =!= ""无""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.FamiliarityRelationHBase.scala/udf/97.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""organization_name"" =!= ""无""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.TongShiRelationHBase.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.XiaoYouRelationHBase.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.XiaoYouRelationTest.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" === id
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.PerformanceTest.XiaoYouRelationTest.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.SimiCompanyName.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""company"".contains(""IBM"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.SimiCompanyName.scala/udf/83.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sims"" >= 0.0d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.TongShiRelation.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.RenCai.XiaoYouRelation.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id_1"" =!= ($""id_2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_blog.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_blog.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_luntan.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_luntan.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_menhu.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_search.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""CONTENT"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_search.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""CONTENT"")) >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_weibo.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""content"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_weibo.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""content"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_weibo.scala/udf/75.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""content"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_weixin.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.dataClean.dc_weixin.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendCheck.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendCheck.scala/udf/80.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""title"")) >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendCheck.scala/udf/82.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length(col(""content"")) >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrend.scala/udf/42.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length(col(""time"")) === 19
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV1.scala/udf/47.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length(col(""time"")) === 19
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV1.scala/udf/84.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""title"")) >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV1.scala/udf/86.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length(col(""content"")) >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV2.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""contentPre"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV2.scala/udf/80.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""title"")) >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV2.scala/udf/82.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length(col(""content"")) >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV3.scala/udf/113.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""title"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV3.scala/udf/51.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length(col(""time"")) === 19
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.sentimentTrendV3.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length(col(""content"")) >= 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.T1.scala/udf/43.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length(col(""time"")) === 19
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.sentiment.sentimentAnalysis.T1.scala/udf/45.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

length(col(""title"")) >= 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.shijijinbang.DocSimi.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" =!= ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.shijijinbang.DocSimi.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" < ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.shijijinbang.SelfSimiEuclidean.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" < ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.shijijinbang.SelfSimi.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" =!= ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.shijijinbang.SelfSimi.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""doc1Id"" < ($""doc2Id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.TextSummarization.DataClean.weibo.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TEXT"").contains(""【"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.appDAU.scala/udf/110.24.Dataset-LogView.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.Project.UserProfile.appDAU.LogView]
Call: filter

$""REQUEST_URI"".contains(""updatePushIdByToken.do"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.appDAU.scala/udf/112.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""phoneId"" =!= ""null""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.appDAU.scala/udf/118.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length($""CREATE_BY_ID"") >= 5
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.appDAU.scala/udf/133.24.Dataset-LogView.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.Project.UserProfile.appDAU.LogView]
Call: filter

$""REQUEST_URI"".contains(""init"") && $""PARAMS"".contains(""IMEI"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.appDAU.scala/udf/135.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""phoneId"" =!= ""null""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.appDAU.scala/udf/141.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length($""CREATE_BY_ID"") >= 5
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.appDAU.scala/udf/147.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length($""CREATE_BY_ID"") >= 5
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.appDAU.scala/udf/95.22.Dataset-LogView.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.Project.UserProfile.appDAU.LogView]
Call: filter

$""CREATE_TIME_L"" === yesterdayL
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.checkIMEI.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""REQUEST_URI"".contains(""updatePushIdByToken.do"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.Retention.scala/udf/83.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(value: Double) =>
          value * 100
      }
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.UserProfile.Retention.scala/udf/90.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(time: String) => time
      }
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.Check_Data.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""title"" === s
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.CheckSimiData.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rn"" === 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.CheckSimiData.scala/udf/29.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""shandong_title"" === ($""chine_title"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.DocSimi_Title.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""unified_time"" =!= ""2019-05-10""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.DocSimi_Title.scala/udf/75.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""website"" === ""山东政府采购网""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.DocSimi_Title.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""website"" === ""中国政府采购网""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.DocSimi_Title.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""shandong_Id"" =!= ($""chine_id"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.Title_Simi_Join.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""unified_time"" =!= ""2019-05-10""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.Title_Simi_Join.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""title"" === ($""title_tag"") && $""website"" =!= ($""website_tag"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.Title_Simi_Join.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""website"" === ""山东政府采购网""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.Title_Simi_Join.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""website"" === ""山东政府采购网""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.Project.YLZX_ZTB.Title_Simi_Join.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""title"" === s
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkAnalysisData.scala/udf/113.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""columnId"" =!= ""NULL""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkAnalysisData.scala/udf/117.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""itemString"".contains(""edf89aae-e381-4203-b77b-a137a1a57968"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkAnalysisData.scala/udf/121.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""columnId"" === ""NULL""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkAnalysisData.scala/udf/127.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""itemString"" === ""edf89aae-e381-4203-b77b-a137a1a57968""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkAnalysisData.scala/udf/132.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""columnId"" === ""440b42e6-6029-49ae-bb0a-2b242e85cb54""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkAnalysisData.scala/udf/137.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""columnId"".contains(""440b42e6-6029-49ae-bb0a-2b242e85cb54"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkAnalysisData.scala/udf/142.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""title"".contains(""从农业大数据看农业未来发展新方向"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.CheckDupData.scala/udf/161.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sum(value)"" > 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.CheckDupData.scala/udf/168.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sum(value)"" > 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.scala/udf/29.24.Dataset-LogView.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.LogView]
Call: filter

col(""time"").contains(""2017-07-13"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.scala/udf/31.22.Dataset-LogView.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.LogView]
Call: filter

col(""REQUEST_URI"").contains(""getContentById.do"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.scala/udf/38.24.Dataset-LogView2.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.LogView2]
Call: filter

col(""userID"") === myID
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.scala/udf/40.22.Dataset-LogView2.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.LogView2]
Call: filter

col(""time"").contains(""2017-07-13"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"".contains(""双创"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""manuallabel"".contains(""双创"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.scala/udf/64.22.Dataset-LyzxView.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkLogData.LyzxView]
Call: filter

$""manuallabel"".contains(""双创"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkSWTdata.scala/udf/155.25.Dataset-ylzxSchema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkSWTdata.ylzxSchema]
Call: filter

col(""itemString"") === ""3f8ed9a1-541b-4fb1-97f8-d2987935ab3a""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkSWTdata.scala/udf/162.25.Dataset-ylzxSchema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkSWTdata.ylzxSchema]
Call: filter

col(""itemString"") === ""1cbff174-2194-4556-90af-a11e4656c2d4""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkSWTdata.scala/udf/172.25.Dataset-ylzxSchema2.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkSWTdata.ylzxSchema2]
Call: filter

col(""itemString"") === ""3f8ed9a1-541b-4fb1-97f8-d2987935ab3a""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkSWTdata.scala/udf/179.25.Dataset-ylzxSchema2.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkSWTdata.ylzxSchema2]
Call: filter

col(""itemString"") === ""1cbff174-2194-4556-90af-a11e4656c2d4""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkUserLogs.scala/udf/193.23.Dataset-LogView.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkUserLogs.LogView]
Call: filter

$""CREATE_BY_ID"" === t1_id
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkUserLogs.scala/udf/233.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""userString"" === t1_id
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkUserLogs.scala/udf/240.23.Dataset-LogView3.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkUserLogs.LogView3]
Call: filter

$""CREATE_BY_ID"" === t1_id
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/147.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""simiScore"" >= 0.95d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/151.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""simiScore"" >= 0.8d && $""simiScore"" <= 0.9d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/155.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""simiScore"" >= 0.7d && $""simiScore"" <= 0.8d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/159.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""simiScore"" === 0
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/163.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""simiScore"" > 0.0d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/168.23.Dataset-XGWZschema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.XGWZschema]
Call: filter

$""simiScore"" <= 0.5d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/172.23.Dataset-XGWZschema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.XGWZschema]
Call: filter

$""simiScore"" <= 0.6d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/176.23.Dataset-XGWZschema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.XGWZschema]
Call: filter

$""simiScore"" <= 0.7d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/180.23.Dataset-XGWZschema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.XGWZschema]
Call: filter

$""simiScore"" <= 0.8d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/184.23.Dataset-XGWZschema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.XGWZschema]
Call: filter

$""simiScore"" <= 0.9d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.scala/udf/188.23.Dataset-XGWZschema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.checkXgwzData.XGWZschema]
Call: filter

$""simiScore"" <= 0.95d
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.deleteHbaseData.scala/udf/61.24.Dataset-Schema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.deleteHbaseData.Schema]
Call: filter

col(""manuallabel"").contains(""政府"") || col(""manuallabel"").contains(""通讯电子"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.deleteHbaseData.scala/udf/84.22.Dataset-Schema.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.deleteHbaseData.Schema]
Call: filter

col(""manuallabel"").contains(""科学"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.getLogsDqwz.scala/udf/152.23.Dataset-LogView2.filter","Type: org.apache.spark.sql.Dataset[com.evayInfo.Inglory.SparkDiary.database.hbase.getLogsDqwz.LogView2]
Call: filter

$""timeL"" >= monthL
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.database.hbase.getLogsDqwz.scala/udf/157.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sum(tag)"" <= 5
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.files.readFiles.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""word"".contains(""十九大"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.files.readFiles.scala/udf/41.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""words2"".contains(""者更"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.files.readText.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""word"".contains(""者更"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.ml.features.MinHashLSHExample2.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" >= 3
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.colContansDemo1.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1"".contains($""col2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.getDuplicateDemo.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""AcctCount"" === 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.getDuplicateDemo.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sum(value)"" > 1
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.mapDemo1.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(col1: String, col2: Int) =>
          (col1, col2 + 1)
      }
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.NormalizedDemo1.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(v: DenseVector) =>
          v(0).toString.toDouble
      }
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.RemoveSameColValue.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Category"" =!= ($""Category2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.RemoveSameColValue.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Category"" === ($""Category2"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.SparkDataFrameTimeWindow.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

functions.year($""eventTime"").between(2017, 2018)
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.SparkDataFrameTimeWindow.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

functions.year($""eventTime"").between(2017, 2018)
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.SparkDiary.sparkSQL.dataClean.TakeFirstValue.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""x2"" > 2
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.TestCode.filterTest.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""source"" === ""长江学者""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.TestCode.filterTest.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""source"".contains(""长江学者"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.TestCode.Test003.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""source"" === ""长江学者""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.TestCode.Test003.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""intro"".contains($""work_unit"")
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.TestCode.Test003sun.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""source"" === ""长江学者""
"
"udf/spark_repos_5/1_sunshinelu_SparkDiary/..src.main.scala.com.evayInfo.Inglory.TestCode.Test003sun.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""intro"".contains($""work_unit"")
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_5/1_wenasgz_study/..examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_5/1_windyzj_sparkmall_0705/..spark-offline.src.main.scala.com.atguigu.sparkmall0705.offline.AreaTop3ProductApp.scala/udf/10.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityRemarkUDAF()
"
"udf/spark_repos_5/1_witlox_spark_sentiment/..src.main.scala-2.11.ch.uzh.sentiment.utils.Detection.scala/udf/71.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getInt(r.fieldIndex(column))
"
"udf/spark_repos_5/1_witlox_spark_sentiment/..src.main.scala-2.11.ch.uzh.sentiment.utils.Detection.scala/udf/82.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.getString(r.fieldIndex(column))
"
"udf/spark_repos_5/1_wzds2015_spark-data-engineering-ads/..src.main.scala.com.nyu.summary.Attribution.scala/udf/47.21.Dataset-GeneralRecordAgg.map","Type: org.apache.spark.sql.Dataset[com.nyu.summary.GeneralRecordAgg]
Call: map

gra => GeneralRecordMap(gra.aid, gra.uid, buildAttr(gra.tsList, gra.etypeList))
"
"udf/spark_repos_5/1_wzds2015_spark-data-engineering-ads/..src.main.scala.com.nyu.summary.Attribution.scala/udf/59.19.Dataset-AttributeCountBig.map","Type: org.apache.spark.sql.Dataset[com.nyu.summary.AttributeCountBig]
Call: map

acb => AttributeCount(acb.aid, acb.etype, acb.cnt.toInt)
"
"udf/spark_repos_5/1_wzds2015_spark-data-engineering-ads/..src.main.scala.com.nyu.summary.Attribution.scala/udf/67.24.Dataset-GeneralAtrribute.filter","Type: org.apache.spark.sql.Dataset[com.nyu.summary.GeneralAtrribute]
Call: filter

ga => ga.cnt != 0
"
"udf/spark_repos_5/1_wzds2015_spark-data-engineering-ads/..src.main.scala.com.nyu.summary.Attribution.scala/udf/69.19.Dataset-UniqueUserList.map","Type: org.apache.spark.sql.Dataset[com.nyu.summary.UniqueUserList]
Call: map

uus => UniqueUserCount(uus.aid, uus.etype, uus.uidList.distinct.size)
"
"udf/spark_repos_5/1_wzds2015_spark-data-engineering-ads/..src.main.scala.com.nyu.summary.Attribution.scala/udf/9.19.Dataset-Impression.map","Type: org.apache.spark.sql.Dataset[com.nyu.summary.Impression]
Call: map

imp => GeneralRecord1(imp.ts, imp.aid, imp.uid, ""impression"")
"
"udf/spark_repos_5/1_wzds2015_spark-data-engineering-ads/..src.main.scala.com.nyu.summary.DeDuplication.scala/udf/23.21.Dataset-CollectionByAdUser.map","Type: org.apache.spark.sql.Dataset[com.nyu.summary.CollectionByAdUser]
Call: map

cba => deDupHelper(cba)
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day7.IpLoactionSQL2.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day7.IpLoactionSQL2.scala/udf/26.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day7.IpLoactionSQL2.scala/udf/36.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(ipNum: Long) => {
        val ipRulesInExecutor: Array[(Long, Long, String)] = broadcastRef.value
        val index = MyUtils.binarySearch(ipRulesInExecutor, ipNum)
        var province = ""未知""
        if (index != -1) {
          province = ipRulesInExecutor(index)._3
        }
        province
      }
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day7.IpLoactionSQL.scala/udf/12.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day7.IpLoactionSQL.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day7.JdbcDataSource.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" <= 13
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day7.JoinTest.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split("","")
        val id = fields(0).toLong
        val name = fields(1)
        val nationCode = fields(2)
        (id, name, nationCode)
      }
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day7.JoinTest.scala/udf/22.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

l => {
        val fields = l.split("","")
        val ename = fields(0)
        val cname = fields(1)
        (ename, cname)
      }
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.day8.SQLFavTeache.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val tIndex = line.lastIndexOf(""/"") + 1
        val teacher = line.substring(tIndex)
        val host = new URL(line).getHost
        val sIndex = host.indexOf(""."")
        val subject = host.substring(0, sIndex)
        (subject, teacher)
      }
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.game.day3.DataSetGameKPI_2.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""[|]"")
"
"udf/spark_repos_5/1_xinyessf_big-data-soa/..big-data-spark.src.main.scala.com.huanyu.spark.scala.game.day3.DataSetGameKPI.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(""[|]"")
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.log.TopNStatJobReusable.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hour"".startsWith(day) && $""cmsType"" === ""topicId""
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.log.TopNStatJob.scala/udf/35.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hour"".startsWith(day) && $""cmsType"" === ""topicId""
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.log.TopNStatJob.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hour"".startsWith(day) && $""cmsType"" === ""topicId""
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.log.TopNStatJob.scala/udf/99.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hour"".startsWith(day) && $""cmsType"" === ""topicId""
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.log.TopNStatJobYarn.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hour"".startsWith(day) && $""cmsType"" === ""topicId""
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.log.TopNStatJobYarn.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hour"".startsWith(day) && $""cmsType"" === ""topicId""
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.log.TopNStatJobYarn.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hour"".startsWith(day) && $""cmsType"" === ""topicId""
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.spark.sql.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

people.col(""age"") > 19
"
"udf/spark_repos_5/1_yanglin89_SparkSqlProject/..src.main.scala.com.run.spark.sql.DataFrameRDDApp.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_5/1_zhenchao125_sparkmall/..sparkmall-offline.src.main.scala.com.atguigu.sparkmall.offline.app.AreaClickTop3App.scala/udf/9.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityClickCountUDAF
"
"udf/spark_repos_5/20_caroljmcdonald_mapr-sparkml-streaming-uber/..src.main.scala.com.sparkkafka.uber.SparkHBaseReadDF.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""11""
"
"udf/spark_repos_5/24_1ambda_scala/..scala-spark-big-data.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/105.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other""))
"
"udf/spark_repos_5/24_1ambda_scala/..scala-spark-big-data.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/112.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

r => TimeUsageRow(r._1._1, r._1._2, r._1._3, r._2, r._3, r._4)
"
"udf/spark_repos_5/2_421250_SparkTutorials/..SparkTutorials.src.main.scala.SparkTutorials.Tutorial2.Tutorial2Main.scala/udf/19.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

powerUDF
"
"udf/spark_repos_5/2_421250_SparkTutorials/..SparkTutorials.src.main.scala.SparkTutorials.Tutorial2.Tutorial2Main.scala/udf/32.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

powerListUDF
"
"udf/spark_repos_5/2_421250_SparkTutorials/..SparkTutorials.src.main.scala.SparkTutorials.Tutorial3.Tutorial3Main.scala/udf/17.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mean
"
"udf/spark_repos_5/25_PacktPublishing_Scala-and-Spark-for-Big-Data-Analytics/..AppendixB.SpamFilteringDemo.scala/udf/35.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_5/25_PacktPublishing_Scala-and-Spark-for-Big-Data-Analytics/..AppendixB.SpamFilteringDemo.scala/udf/39.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

word => word.length() > 1
"
"udf/spark_repos_5/25_PacktPublishing_Scala-and-Spark-for-Big-Data-Analytics/..AppendixB.SpamFilteringDemo.scala/udf/47.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_5/25_PacktPublishing_Scala-and-Spark-for-Big-Data-Analytics/..AppendixB.SpamFilteringDemo.scala/udf/51.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

word => word.length() > 1
"
"udf/spark_repos_5/25_PacktPublishing_Scala-and-Spark-for-Big-Data-Analytics/..AppendixB.SpamFilteringDemo.scala/udf/76.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_5/27_endymecy_AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.knn.KNNClassifier.scala/udf/65.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case (id, labels) =>
          val vector = new Array[Double](numClasses)
          var i = 0
          while (i < labels.length) {
            vector(labels(i).getDouble(0).toInt) += getWeight(labels(i))
            i += 1
          }
          val rawPrediction = Vectors.dense(vector)
          lazy val probability = raw2probability(rawPrediction)
          lazy val prediction = probability2prediction(probability)
          val values = new ArrayBuffer[Any]
          if ($(rawPredictionCol).nonEmpty) {
            values.append(rawPrediction)
          }
          if ($(probabilityCol).nonEmpty) {
            values.append(probability)
          }
          if ($(predictionCol).nonEmpty) {
            values.append(prediction)
          }
          (id, values)
      }
"
"udf/spark_repos_5/27_endymecy_AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.sampling.OverSampling.scala/udf/43.32.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(dependentColName)) === label
"
"udf/spark_repos_5/27_endymecy_AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.sampling.OverSampling.scala/udf/51.32.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(dependentColName)) === label
"
"udf/spark_repos_5/27_endymecy_AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.sampling.UnderSampling.scala/udf/44.30.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(dependentColName)) === label
"
"udf/spark_repos_5/27_endymecy_AlgorithmsOnSpark/..src.main.scala.org.apache.spark.ml.sampling.UnderSampling.scala/udf/47.30.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(dependentColName)) === label
"
"udf/spark_repos_5/29_blackrock_TopNotch/..src.main.scala.com.bfm.topnotch.tnassertion.TnAssertionRunner.scala/udf/109.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$INVALID_COL_NAME != ''""
"
"udf/spark_repos_5/29_blackrock_TopNotch/..src.main.scala.com.bfm.topnotch.tnassertion.TnAssertionRunner.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""not("" + assertion.query + "")""
"
"udf/spark_repos_5/29_blackrock_TopNotch/..src.test.scala.com.bfm.topnotch.tnassertion.TnAssertionRunnerTest.scala/udf/271.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

mergedRowsFilter
"
"udf/spark_repos_5/2_AnnabelleGillet_TDM/..src.main.scala.tdm.core.Tensor.scala/udf/105.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => condition(r.get(dimensionIndex).asInstanceOf[T])
"
"udf/spark_repos_5/2_AnnabelleGillet_TDM/..src.main.scala.tdm.core.Tensor.scala/udf/128.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => typedCondition(r.get(dimensionIndex).asInstanceOf[DT])
"
"udf/spark_repos_5/2_AnnabelleGillet_TDM/..src.main.scala.tdm.core.Tensor.scala/udf/139.34.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => typedCondition(r.get(dimensionIndex).asInstanceOf[DT])
"
"udf/spark_repos_5/2_AnnabelleGillet_TDM/..src.main.scala.tdm.core.Tensor.scala/udf/160.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

values.col(_dimension.name) === value
"
"udf/spark_repos_5/2_AnnabelleGillet_TDM/..src.main.scala.tdm.core.Tensor.scala/udf/39.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

currentDataFrame.col(dimension.name) === value
"
"udf/spark_repos_5/2_Azula22_Observatory_BigData_example/..src.main.scala.observatory.Extraction.scala/udf/54.21.Dataset-GatheredData.map","Type: org.apache.spark.sql.Dataset[observatory.Extraction.GatheredData]
Call: map

data => (data.month, data.day, data.lat, data.lon, data.temperature)
"
"udf/spark_repos_5/2_Azula22_Observatory_BigData_example/..src.main.scala.observatory.Visualization.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""$DistanceColumn"" < 1
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.evaluation.Evaluator.scala/udf/9.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => (row.apply(1).asInstanceOf[DenseVector](1), row.getAs[Int](""label"").toDouble)
      }
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.features.FeatureEngineering.scala/udf/10.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), Vectors.dense(row.getAs[Seq[Double]](""user_embedding"").toArray), Vectors.dense(row.getAs[Seq[Double]](""item_embedding"").toArray), row.getAs[Int](""label""))
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.features.FeatureEngineering.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val user_embedding = row.getAs[DenseVector](""user_embedding"")
        val item_embedding = row.getAs[DenseVector](""item_embedding"")
        var asquare = 0.0d
        var bsquare = 0.0d
        var abmul = 0.0d
        for (i <- 0 until user_embedding.size) {
          asquare += user_embedding(i) * user_embedding(i)
          bsquare += item_embedding(i) * item_embedding(i)
          abmul += user_embedding(i) * item_embedding(i)
        }
        var inner_product = 0.0d
        if (asquare == 0 || bsquare == 0) {
          inner_product = 0.0d
        } else {
          inner_product = abmul / (Math.sqrt(asquare) * Math.sqrt(bsquare))
        }
        (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), inner_product, row.getAs[Int](""label""))
      }
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.features.FeatureEngineering.scala/udf/53.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val user_embedding = row.getAs[DenseVector](""user_embedding"")
        val item_embedding = row.getAs[DenseVector](""item_embedding"")
        val outerProductEmbedding: Array[Double] = Array.fill[Double](user_embedding.size)(0)
        for (i <- 0 until user_embedding.size) {
          outerProductEmbedding(i) = user_embedding(i) * item_embedding(i)
        }
        (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), Vectors.dense(outerProductEmbedding), row.getAs[Int](""label""))
      }
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.CollaborativeFiltering.scala/udf/121.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank"" <= 10
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.CollaborativeFiltering.scala/udf/126.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank"" <= 10
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.CollaborativeFiltering.scala/udf/138.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank"" <= RANKS
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.CollaborativeFiltering.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating(_)
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.CollaborativeFiltering.scala/udf/54.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating(_)
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.CollaborativeFiltering.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank"" <= 10
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.CollaborativeFiltering.scala/udf/73.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val movieId = row.getAs[Int](""movieId"")
        val movieIdPair = row.getAs[Int](""movieIdPair"")
        val size = row.getAs[Long](""size"")
        val dotProduct = row.getAs[Double](""dotProduct"")
        val ratingSum = row.getAs[Double](""ratingSum"")
        val ratingPairSum = row.getAs[Double](""ratingPairSum"")
        val ratingPowSum = row.getAs[Double](""ratingPowSum"")
        val ratingPairPowSum = row.getAs[Double](""ratingPairPowSum"")
        val numRaters = row.getAs[Long](""numRaters"")
        val numRatersPair = row.getAs[Long](""numRatersPair"")
        val cooc = cooccurrence(size, numRaters, numRatersPair)
        val corr = correlation(size, dotProduct, ratingSum, ratingPairSum, ratingPowSum, ratingPairPowSum)
        val regCorr = regularCorrelation(size, dotProduct, ratingSum, ratingPairSum, ratingPowSum, ratingPairPowSum, PRIOR_COUNT, PRIOR_CORRELATION)
        val cos = cosineSimilarity(dotProduct, math.sqrt(ratingPowSum), math.sqrt(ratingPairPowSum))
        val impCos = improvedCosineSimilarity(dotProduct, math.sqrt(ratingPowSum), math.sqrt(ratingPairPowSum), size, numRaters, numRatersPair)
        val jac = jaccardSimilarity(size, numRaters, numRatersPair)
        val score = coef(0) * cooc + coef(1) * corr + coef(2) * regCorr - coef(3) * cos - coef(4) * impCos + coef(5) * jac
        (movieId, movieIdPair, cooc, corr, regCorr, cos, impCos, jac, score)
      }
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.PeopleNewsPipelinesCV.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

news(""title"").isNotNull && news(""created_time"").isNotNull && news(""tab"").isNotNull && news(""content"").isNotNull && news(""source"").isNotNull
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.PeopleNewsPipelinesCV.scala/udf/20.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

peopleWebNews(""tab"").isin(""国际"", ""军事"", ""财经"", ""金融"", ""时政"", ""法制"", ""社会"")
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.PeopleNewsPipelines.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

news(""title"").isNotNull && news(""created_time"").isNotNull && news(""tab"").isNotNull && news(""content"").isNotNull && news(""source"").isNotNull
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.PeopleNewsPipelines.scala/udf/25.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

peopleWebNews(""tab"").isin(""国际"", ""军事"", ""财经"", ""金融"", ""时政"", ""法制"", ""社会"")
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.PeopleNews.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

news(""title"").isNotNull && news(""created_time"").isNotNull && news(""tab"").isNotNull && news(""content"").isNotNull && news(""source"").isNotNull
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkMLlibStudy.model.PeopleNews.scala/udf/25.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

peopleWebNews(""tab"").isin(""国际"", ""军事"", ""财经"", ""金融"", ""时政"", ""法制"", ""社会"")
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkStudy.chapter.SparkSql.scala/udf/71.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(weight: Double, height: Double) => {
        val ht = if (height > 100) height / 100 else height
        weight / Math.pow(ht, 2)
      }
"
"udf/spark_repos_5/2_baymaxKevin_SparkMLlibStudy/..src.main.java.com.sparkStudy.chapter.SparkSql.scala/udf/79.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AverageAge1
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/100.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/102.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/109.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/114.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/116.21.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/82.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/86.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/2_bom4v_ti-spark-examples/..src.main.scala.org.bom4v.ti.Demonstrator.scala/udf/95.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.Algorithm.DailyWarningAlgorithm.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""examination_approval_type"") === ""已撤控"" && col(""avaliable"") === ""1""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.daily.scala/udf/38.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avaliable"") === 1
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.daily.scala/udf/40.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""examination_approval_type"") === ""已撤控""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.daily.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""warning_date"") === day
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.daily.scala/udf/94.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""warning_date"") === day
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/100.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""starttime"") !== null
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/102.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""starttime"") !== ""null""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""mac"") === ""000BABDA77DA""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/45.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""idno"") !== ""null""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/47.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""idno"") !== null
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/67.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""timeStamp"") !== null
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""timeStamp"") !== ""null""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/86.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""stime"") !== null
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.GongAn.scala/udf/88.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""stime"") !== ""null""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.Predict.scala/udf/258.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""deal_time"").substr(0, 10) !== time
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.Predict.scala/udf/262.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""deal_time"").substr(0, 10) === time
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""station_name"") === ""布吉"" || ""大剧院"" || ""市民中心"" || ""坪洲""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/35.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 8) >= ""20181115""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/37.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_type"") === ""地铁入站""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/39.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 8) <= ""20181121""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/45.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 8) >= ""20181115""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/47.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_type"") === ""地铁出站""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/49.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 8) <= ""20181121""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/61.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""deal_type"") === deal_type
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/63.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""station_name"") === station_name
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/65.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 10) >= beginday
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/67.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 10) <= endday
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/76.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""station_name"") === station_name
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/78.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 10) >= beginday
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/80.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 10) <= endday
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/88.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""deal_time"").substr(0, 10) >= beginday
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.CalSizeFlow.scala/udf/90.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 10) <= endday
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.dealACC.scala/udf/28.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""station_name"") === ""布吉"" || col(""station_name"") === ""大剧院"" || col(""station_name"") === ""市民中心"" || col(""station_name"") === ""坪洲""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.dealACC.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""deal_time"").substr(0, 10) >= ""2018-09-15""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.dealACC.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""deal_time"").substr(0, 10) <= ""2018-09-21""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.dealACC.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""station_name"") === ""布吉""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.dealACC.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""station_name"") === ""大剧院""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.dealACC.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""station_name"") === ""坪洲""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.SZT.dealACC.scala/udf/53.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""station_name"") === ""市民中心""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.testGongAn.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""deal_time"").substr(0, 8) >= ""20181113""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/102.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""data_device_type"") === ""感知门""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/104.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""data_sources"") === ""face""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/108.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""data_sources"") === ""idno""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/112.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""data_sources"") === ""face""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/120.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avaliable"") === 1
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/122.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""early_warning_tag_value"") === ""误报""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/127.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""data_device_type"") === ""感知门""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/129.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""data_sources"") === ""face""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/134.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""data_device_type"") !== ""感知门""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/136.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""data_sources"") === ""face""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/142.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avaliable"") === 1
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/144.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""early_warning_tag_value"") !== ""误报""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/149.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avaliable"") === 1
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/151.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""early_warning_tag_value"") !== ""误报""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/165.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""value"") === ""na""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/176.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avaliable"") === 1
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/178.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""early_warning_tag_value"") !== ""误报""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/183.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avaliable"") === 1
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/185.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""early_warning_tag_value"") === ""平台撤控""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/193.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avaliable"") === 1
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/195.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""early_warning_tag_value"") !== ""误报""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/197.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""value"") === ""na""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/206.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avaliable"") === 1
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/208.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""early_warning_tag_value"") === ""平台撤控""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/248.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""warning_date"") === day
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""examination_approval_type"") === ""已撤控"" && col(""avaliable"") === ""1""
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/87.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""warning_date"") >= beginDay
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/89.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""warning_date"") <= day
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.kim.scala/udf/94.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""warning_date"") === day
"
"udf/spark_repos_5/2_Charleskie_module/..src.main.scala.cn.sibat.gongan.warning.thief.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""keyperson_type"") === ""扒窃""
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.com.spark.sparkSql.ReadTest.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""type"" === ""F""
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.common.KafkaAndAbnormalDBWriter.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""exceptionWriteMode=${ExceptionWriteMode.NO_WRITE} "" + (s""AND timePeriodWriteMode=${TimePeriodWriteMode.NO_WRITE}"")
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.common.KafkaAndAbnormalDBWriter.scala/udf/46.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""exceptionWriteMode!=${ExceptionWriteMode.NO_WRITE} "" + (s""OR timePeriodWriteMode !=${TimePeriodWriteMode.NO_WRITE}"")
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.common.KafkaDataWithRuleReader.scala/udf/103.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.getAs[String](""v"").contains(""S#"")
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.common.KafkaDataWithRuleReader.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.getAs[String](""value"").contains(""S#"")
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.AmplitudeDiscriminate.scala/udf/27.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.AmplitudeDiscriminate.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""isOK"".equalTo(false)
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.AmplitudeDiscriminate.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""isOK"".equalTo(true)
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.AmplitudeDiscriminate.scala/udf/40.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.AmplitudeWithExceptionTimeNoDelay.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.AmplitudeWithExceptionTimeWithDelay.scala/udf/27.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.AmplitudeWithExceptionTimeWithDelay.scala/udf/31.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.CumulativeAmount.scala/udf/24.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.CumulativeAmountWithExceptionTimeNoDelay.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.CumulativeAmountWithExceptionTime.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.CumulativeAmountWithExceptionTime.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.LimitDiscriminateWithExceptionTime.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.LimitDiscriminateWithExceptionTime.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.LimitDiscriminateWithExceptionTimeSimple.scala/udf/24.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.MinuteAvg2HBase.scala/udf/22.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.MinuteAvgValue_foreach.scala/udf/25.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.MinuteAvgValue.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.MinuteValue2HBase.scala/udf/22.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.MinuteValue_foreach.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.MinuteValue.scala/udf/39.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.MovingAvg.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.MovingAvg.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.SwitchValue.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""status"".equalTo(false)
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.SwitchValue.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""status"".isNull or $""status"".equalTo(true)
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.SwitchValueWithExceptionTime.scala/udf/28.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.processing.SwitchValueWithExceptionTime.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.sink.jdbcSink.JdbcSinkWriterTest.scala/udf/31.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => json2Object(x)
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.sink.jdbcSink.JdbcSinkWriterTest.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""type"" === ""F""
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.dc.streaming.sink.kafkaSink.KafkaSinkWriterTest.scala/udf/20.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => json2Object(x)
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.structuredStreaming.addDataCompare.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pointId"" === ""JYJ.JYJ.PAC2_LL"" || $""pointId"" === ""ns=1001;s=P13_AI_001.In_Channel0"" || $""pointId"" === ""ChnOPC.AFY.AFY.AFY-FLOAT.TAG33""
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.structuredStreaming.minAvgflatMapGropsWithState.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pointId"" === ""JYJ.JYJ.PAC2_LL"" || $""pointId"" === ""ns=1001;s=P13_AI_001.In_Channel0"" || $""pointId"" === ""ChnOPC.AFY.AFY.AFY-FLOAT.TAG33""
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.structuredStreaming.MinuteValueFlatMapGroupWith.scala/udf/41.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dingzhenying_spark2_4/..src.main.scala.structuredStreaming.MinuteValueMoreTable.scala/udf/31.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"".isNotNull
"
"udf/spark_repos_5/2_dmarcous_dDBGSCAN/..src.main.scala.com.github.dmarcous.ddbgscan.core.algo.merging.ClusterMerger.scala/udf/27.19.Dataset-ClusteringInstance).map","Type: org.apache.spark.sql.Dataset[(Long, com.github.dmarcous.ddbgscan.model.ClusteringInstance)]
Call: map

{
        case (cellId, instance) =>
          (instance.recordId, instance)
      }
"
"udf/spark_repos_5/2_dmarcous_dDBGSCAN/..src.main.scala.com.github.dmarcous.ddbgscan.core.algo.merging.ClusterMerger.scala/udf/89.19.Dataset-((Long, Long, Int), (Long, Long)).map","Type: org.apache.spark.sql.Dataset[((Long, Long, Int), (Long, Long))]
Call: map

{
        case (instance, clusterMapping) =>
          ClusteredInstance(recordId = instance._1, cluster = if (clusterMapping != null) clusterMapping._2 else instance._2, instanceStatus = instance._3)
      }
"
"udf/spark_repos_5/2_dmarcous_dDBGSCAN/..src.main.scala.com.github.dmarcous.ddbgscan.core.algo.partitioning.DataPartitionerCostBased.scala/udf/13.19.Dataset-ClusteringInstance).map","Type: org.apache.spark.sql.Dataset[(com.github.dmarcous.ddbgscan.model.KeyGeoEntity, com.github.dmarcous.ddbgscan.model.ClusteringInstance)]
Call: map

{
        case (key, inst) =>
          (toMinimumBoundingRectangle(inst.lonLatLocation, minimumRectangleSize), 1)
      }
"
"udf/spark_repos_5/2_dmarcous_dDBGSCAN/..src.main.scala.com.github.dmarcous.ddbgscan.core.algo.partitioning.DataPartitionerS2.scala/udf/12.19.Dataset-ClusteringInstance).map","Type: org.apache.spark.sql.Dataset[(com.github.dmarcous.ddbgscan.model.KeyGeoEntity, com.github.dmarcous.ddbgscan.model.ClusteringInstance)]
Call: map

{
        case (key, instance) =>
          (key.s2CellId, DataPartitionerS2.getDensityReachableCells(instance.lonLatLocation._1, instance.lonLatLocation._2, neighborhoodPartitioningLvl, epsilon), instance)
      }
"
"udf/spark_repos_5/2_dmarcous_dDBGSCAN/..src.main.scala.com.github.dmarcous.ddbgscan.core.preprocessing.GeoPropertiesExtractor.scala/udf/14.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{
          case line =>
            line.split(ioConfig.inputDelimiter)
        }
"
"udf/spark_repos_5/2_dmarcous_dDBGSCAN/..src.main.scala.com.github.dmarcous.ddbgscan.core.preprocessing.GeoPropertiesExtractor.scala/udf/49.19.Dataset-Vector).map","Type: org.apache.spark.sql.Dataset[(Long, Double, Double, org.apache.spark.ml.linalg.Vector)]
Call: map

{
        case (id, lon, lat, features) =>
          (new KeyGeoEntity(LonLatGeoEntity(lon, lat), neighborhoodPartitioningLvl), ClusteringInstance(recordId = id, lonLatLocation = (lon, lat), features = features))
      }
"
"udf/spark_repos_5/2_dmarcous_dDBGSCAN/..src.main.scala.com.github.dmarcous.ddbgscan.core.preprocessing.GeoPropertiesExtractor.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case fields =>
          val id = if (ioConfig.positionId == NO_UNIQUE_ID_FIELD) DEFAULT_RECORD_ID else fields.getLong(ioConfig.positionId)
          val lon = fields.getDouble(ioConfig.positionLon)
          val lat = fields.getDouble(ioConfig.positionLat)
          val fieldsToRemovePositions = List(ioConfig.positionId, ioConfig.positionLon, ioConfig.positionLat).sorted
          val featureFields = locally {
            val _t_m_p_8 = locally {
              val _t_m_p_9 = fields.mkString(DEFAULT_GEO_FILE_DELIMITER).split(DEFAULT_GEO_FILE_DELIMITER).zipWithIndex
              _t_m_p_9.filter({
                case (field, pos) =>
                  !fieldsToRemovePositions.contains(pos)
              })
            }
            _t_m_p_8.map(_._1)
          }
          val features = Vectors.dense(locally {
            val _t_m_p_10 = featureFields
            _t_m_p_10.map(_.toDouble)
          })
          (id, lon, lat, features)
      }
"
"udf/spark_repos_5/2_dmarcous_dDBGSCAN/..src.main.scala.com.github.dmarcous.ddbgscan.core.preprocessing.GeoPropertiesExtractor.scala/udf/89.20.Dataset-Vector).map","Type: org.apache.spark.sql.Dataset[(Long, Double, Double, org.apache.spark.ml.linalg.Vector)]
Call: map

{
        case (id, lon, lat, features) =>
          (new KeyGeoEntity(LonLatGeoEntity(lon, lat), neighborhoodPartitioningLvl), new ClusteringInstance(recordId = id, lonLatLocation = (lon, lat), features = features))
      }
"
"udf/spark_repos_5/2_doudianer_ImoocLoganalysis/..src.main.scala.com.wl.Log.TopNStatJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day
"
"udf/spark_repos_5/2_doudianer_ImoocLoganalysis/..src.main.scala.com.wl.Log.TopNStatJob.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/2_doudianer_ImoocLoganalysis/..src.main.scala.com.wl.Log.TopNStatJobYarn.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/2_FINRAOS_CodeSamples/..machine-learning-samples.src.main.scala.org.finra.ezmachinelearning.RFWithSurrogateCensusIncomeExample.scala/udf/96.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Double, y: Double) => scala.math.pow(x - y, 2)
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.main.scala.org.freedomandy.mole.transform.Filter.scala/udf/13.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(fieldName) > target
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.main.scala.org.freedomandy.mole.transform.Filter.scala/udf/18.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(fieldName) < target
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.main.scala.org.freedomandy.mole.transform.Filter.scala/udf/23.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(fieldName) >= target
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.main.scala.org.freedomandy.mole.transform.Filter.scala/udf/28.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(fieldName) <= target
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.main.scala.org.freedomandy.mole.transform.Filter.scala/udf/33.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(fieldName) === target
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.main.scala.org.freedomandy.mole.transform.Filter.scala/udf/38.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(fieldName) =!= target
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.main.scala.org.freedomandy.mole.transform.Filter.scala/udf/46.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(fieldName).rlike(regex)
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.test.scala.org.freedomandy.mole.sinks.MongoSinkTest.scala/udf/43.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getInt(1) > 99
"
"udf/spark_repos_5/2_freedomandy_mole/..Core.src.test.scala.org.freedomandy.mole.sinks.MongoSinkTest.scala/udf/69.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getInt(1) > 99
"
"udf/spark_repos_5/2_Gabzer_Spark-with-Scala/..SparkScalaCourse.src.com.sundogsoftware.spark.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_5/2_inf0rmatiker_flySTAT/..core.src.main.scala.analytics.AllCities.scala/udf/21.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""OriginAirportID"" === 11292
"
"udf/spark_repos_5/2_inf0rmatiker_flySTAT/..core.src.main.scala.analytics.AllCities.scala/udf/23.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""FlightDate"" === ""2018-11-20""
"
"udf/spark_repos_5/2_inf0rmatiker_flySTAT/..core.src.main.scala.analytics.AllCities.scala/udf/55.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row(0).toString.toInt, row(1).toString)
"
"udf/spark_repos_5/2_inf0rmatiker_flySTAT/..core.src.main.scala.analytics.AllCities.scala/udf/57.22.Dataset-(Int, String).filter","Type: org.apache.spark.sql.Dataset[(Int, String)]
Call: filter

x => top25Dest.contains(x._1)
"
"udf/spark_repos_5/2_inf0rmatiker_flySTAT/..core.src.main.scala.analytics.FiveNumberSummary.scala/udf/21.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""OriginAirportID"" === 11292
"
"udf/spark_repos_5/2_inf0rmatiker_flySTAT/..core.src.main.scala.analytics.FiveNumberSummary.scala/udf/23.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""DestAirportID"" === 13930
"
"udf/spark_repos_5/2_inf0rmatiker_flySTAT/..core.src.main.scala.analytics.LinearRegression.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""OriginAirportID"" === args(2)
"
"udf/spark_repos_5/2_jkhlr_distributed-ind/..src.main.scala.inclusion_dependencies.Pipeline.scala/udf/62.22.Dataset-Cell.filter","Type: org.apache.spark.sql.Dataset[inclusion_dependencies.Cell]
Call: filter

_.attributes.nonEmpty
"
"udf/spark_repos_5/2_leletan_maiev/..src.main.scala.org.leletan.maiev.job.RedshiftToRedis.scala/udf/57.20.Dataset-Boolean.filter","Type: org.apache.spark.sql.Dataset[Boolean]
Call: filter

_ == true
"
"udf/spark_repos_5/2_leletan_maiev/..src.main.scala.org.leletan.maiev.job.RedshiftToRedis.scala/udf/60.20.Dataset-Boolean.filter","Type: org.apache.spark.sql.Dataset[Boolean]
Call: filter

_ == false
"
"udf/spark_repos_5/2_leletan_maiev/..src.main.scala.org.leletan.maiev.sinks.TwitterUserSinkProvider.scala/udf/34.25.Dataset-KafkaTopicData.map","Type: org.apache.spark.sql.Dataset[org.leletan.maiev.lib.KafkaTopicData]
Call: map

_.value
"
"udf/spark_repos_5/2_LevelUpEducation_spark-streaming-scala/..src.main.scala.StructuredStreaming.solutions.StructuredStreamingOperationsSolution.scala/udf/13.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2.split("":"")
"
"udf/spark_repos_5/2_LevelUpEducation_spark-streaming-scala/..src.main.scala.StructuredStreaming.solutions.WindowOperationBonusSolution.scala/udf/19.23.Dataset-(BigInt, String, String, String, String, BigInt, String).map","Type: org.apache.spark.sql.Dataset[(BigInt, String, String, String, String, BigInt, String)]
Call: map

r => TweetData(r._1, r._2, r._3, r._4, r._5, r._6, r._7)
"
"udf/spark_repos_5/2_LevelUpEducation_spark-streaming-scala/..src.main.scala.StructuredStreaming.solutions.WindowOperationBonusSolution.scala/udf/21.24.Dataset-TweetData.filter","Type: org.apache.spark.sql.Dataset[TweetData]
Call: filter

r => r.createdAt != null && r.createdAt != ""null""
"
"udf/spark_repos_5/2_LevelUpEducation_spark-streaming-scala/..src.main.scala.StructuredStreaming.solutions.WindowOperationBonusSolution.scala/udf/23.19.Dataset-TweetData.map","Type: org.apache.spark.sql.Dataset[TweetData]
Call: map

t => (t.replyToScreenName, new Timestamp(t.createdAt.toLong), t.id, if (t.firstHashtag == null) 0 else t.firstHashtag.length, if (t.replyToScreenName == null) 0 else 1)
"
"udf/spark_repos_5/2_LevelUpEducation_spark-streaming-scala/..src.main.scala.StructuredStreaming.solutions.WindowOperationSolution.scala/udf/21.23.Dataset-(BigInt, String, String, String, String, BigInt, String).map","Type: org.apache.spark.sql.Dataset[(BigInt, String, String, String, String, BigInt, String)]
Call: map

r => TweetData(r._1, r._2, r._3, r._4, r._5, r._6, r._7)
"
"udf/spark_repos_5/2_LevelUpEducation_spark-streaming-scala/..src.main.scala.StructuredStreaming.solutions.WindowOperationSolution.scala/udf/23.24.Dataset-TweetData.filter","Type: org.apache.spark.sql.Dataset[TweetData]
Call: filter

_.replyToScreenName != null
"
"udf/spark_repos_5/2_LevelUpEducation_spark-streaming-scala/..src.main.scala.StructuredStreaming.solutions.WindowOperationSolution.scala/udf/25.19.Dataset-TweetData.map","Type: org.apache.spark.sql.Dataset[TweetData]
Call: map

t => (t.replyToScreenName, new Timestamp(t.createdAt.toLong), t.id, if (t.firstHashtag == null) 0 else t.firstHashtag.length)
"
"udf/spark_repos_5/2_LevelUpEducation_spark-streaming-scala/..src.main.scala.StructuredStreaming.StructuredStreamingOperations.scala/udf/13.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2.split("":"")
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SimpleAppSQL.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""val"")
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/23.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != firstLine
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/25.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

CensusData.parseLine
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/30.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/35.21.Dataset-CensusData.map","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: map

_.age
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/41.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.age >= 50
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/45.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/49.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.maritalStatus == ""Married-civ-spouse""
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/53.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/57.20.Dataset-CensusData.map","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: map

_.age
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataA.scala/udf/68.23.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.hoursPerWeek > 40
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataB.scala/udf/13.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'income === "">50K""
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataB.scala/udf/19.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'age >= 50
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataB.scala/udf/23.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'income === "">50K""
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataB.scala/udf/27.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'maritalStatus === ""Married-civ-spouse""
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataB.scala/udf/31.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'income === "">50K""
"
"udf/spark_repos_5/2_MarkCLewis_WC2017Spark/..src.main.scala.withsql.SQLCensusDataB.scala/udf/42.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'hoursPerWeek > 40
"
"udf/spark_repos_5/2_piotr-kalanski_spark-local/..src.main.scala.com.datawizards.sparklocal.impl.spark.dataset.DataSetAPISparkImpl.scala/udf/15.17.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

map
"
"udf/spark_repos_5/2_piotr-kalanski_spark-local/..src.main.scala.com.datawizards.sparklocal.impl.spark.dataset.DataSetAPISparkImpl.scala/udf/21.20.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

p
"
"udf/spark_repos_5/2_pvillacorta-stratio_funkSVD-spark/..src.test.scala.org.apache.spark.mllib.optimization.FunkSVDGradientIT.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getAs[Double](""rating""), Vectors.fromML(r.getAs[MLDense](""features"")))
"
"udf/spark_repos_5/2_realguoshuai_flink-train/..flink-train.src.main.scala.com.bigdata.train.spark.json.ggg.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

m => {
        val spalitStr = m.get(0).toString.split(""\\[|\\]"")(1).split("","")
        (spalitStr(0), spalitStr(1), spalitStr(2), spalitStr(3))
      }
"
"udf/spark_repos_5/2_realguoshuai_flink-train/..flink-train.src.main.scala.com.bigdata.train.spark.json.ParseJsonToDf.scala/udf/31.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ x => 
        val splits: Array[String] = x.get(1).toString.split(""\\[|\\]"")(1).split("","")
        (splits(0), splits(1), splits(2), splits(3))
      }
"
"udf/spark_repos_5/2_roksolana-d_spark-streaming-examples/..src.main.scala.utils.SparkSqlFunctionsTemplates.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

inputDataFrame.col(colName).contains(condition)
"
"udf/spark_repos_5/2_roksolana-d_spark-streaming-examples/..src.main.scala.utils.SparkSqlFunctionsTemplates.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

inputDataFrame.col(colName) > condition
"
"udf/spark_repos_5/2_skp33_pointinpolygon/..src.test.scala.com.github.skp33.pointinpolygon.AggregatePolygonsSpec.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

point => _polygons.value.finalGeoId(isNearest)(point.getAs[Double](""lat""), point.getAs[Double](""lon""))
"
"udf/spark_repos_5/2_spark-dataprocessing_spark-dataprocessing/..src.main.scala.io.github.sparkdataprocessing.State.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""tableName"".startsWith(prefix)
"
"udf/spark_repos_5/2_spark-dataprocessing_spark-dataprocessing/..src.main.scala.io.github.sparkdataprocessing.State.scala/udf/98.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""isTemporary""
"
"udf/spark_repos_5/2_tresata_spark-datasetops/..src.main.scala.com.tresata.spark.datasetops.package.scala/udf/101.20.Dataset-(K, V).map","Type: org.apache.spark.sql.Dataset[(K, V)]
Call: map

_._2
"
"udf/spark_repos_5/2_tresata_spark-datasetops/..src.main.scala.com.tresata.spark.datasetops.package.scala/udf/19.19.Dataset-(K, V).map","Type: org.apache.spark.sql.Dataset[(K, V)]
Call: map

{
        kv => (kv._1, f(kv._2))
      }
"
"udf/spark_repos_5/2_tresata_spark-datasetops/..src.main.scala.com.tresata.spark.datasetops.package.scala/udf/48.21.Dataset-((K, V), (K, V1)).map","Type: org.apache.spark.sql.Dataset[((K, V), (K, V1))]
Call: map

{
          x => (x._1._1, (x._1._2, x._2._2))
        }
"
"udf/spark_repos_5/2_tresata_spark-datasetops/..src.main.scala.com.tresata.spark.datasetops.package.scala/udf/58.21.Dataset-((K, V), (K, V1)).map","Type: org.apache.spark.sql.Dataset[((K, V), (K, V1))]
Call: map

{
          x => (x._1._1, (x._1._2, locally {
            val _t_m_p_6 = Option(x._2)
            _t_m_p_6.map(_._2)
          }))
        }
"
"udf/spark_repos_5/2_tresata_spark-datasetops/..src.main.scala.com.tresata.spark.datasetops.package.scala/udf/71.21.Dataset-((K, V), (K, V1)).map","Type: org.apache.spark.sql.Dataset[((K, V), (K, V1))]
Call: map

{
          x => (x._2._1, (locally {
            val _t_m_p_8 = Option(x._1)
            _t_m_p_8.map(_._2)
          }, x._2._2))
        }
"
"udf/spark_repos_5/2_tresata_spark-datasetops/..src.main.scala.com.tresata.spark.datasetops.package.scala/udf/84.21.Dataset-((K, V), (K, V1)).map","Type: org.apache.spark.sql.Dataset[((K, V), (K, V1))]
Call: map

{
          x => (if (x._1 == null) x._2._1 else x._1._1, (locally {
            val _t_m_p_10 = Option(x._1)
            _t_m_p_10.map(_._2)
          }, locally {
            val _t_m_p_11 = Option(x._2)
            _t_m_p_11.map(_._2)
          }))
        }
"
"udf/spark_repos_5/2_tresata_spark-datasetops/..src.main.scala.com.tresata.spark.datasetops.package.scala/udf/97.20.Dataset-(K, V).map","Type: org.apache.spark.sql.Dataset[(K, V)]
Call: map

_._1
"
"udf/spark_repos_5/35_target_data-validator/..src.main.scala.com.target.data_validator.ValidatorTable.scala/udf/179.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_5/3_azavea_augdiff-pipeline/..common.src.main.scala.ComputeIndex.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""type"") === ""way""
"
"udf/spark_repos_5/3_azavea_augdiff-pipeline/..common.src.main.scala.ComputeIndex.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""type"") === ""relation""
"
"udf/spark_repos_5/3_CHN-guanxinyu_keene/..src.main.scala.com.keene.spark.examples.hive.UDAFTest.scala/udf/12.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyUdaf
"
"udf/spark_repos_5/3_CHN-guanxinyu_keene/..src.main.scala.com.keene.spark.xsql.base.MainContext.scala/udf/127.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MapAgg
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.join.joins.scala/udf/11.19.Dataset-Gender).map","Type: org.apache.spark.sql.Dataset[(chrism.sdsc.join.Name, chrism.sdsc.join.Gender)]
Call: map

r => Profile(r._1.id, first = r._1.first, last = r._1.last, gender = r._2.gender)
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.join.joins.scala/udf/17.22.Dataset-Employment.filter","Type: org.apache.spark.sql.Dataset[chrism.sdsc.join.Employment]
Call: filter

_.jobTitle != null
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.join.joins.scala/udf/27.23.Dataset-(L, R).map","Type: org.apache.spark.sql.Dataset[(L, R)]
Call: map

joinFunc.tupled
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.ml.spamDetector.scala/udf/18.22.Dataset-EncodedDataRow.filter","Type: org.apache.spark.sql.Dataset[chrism.sdsc.ml.EncodedDataRow]
Call: filter

_.label == ""ham""
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.ml.spamDetector.scala/udf/22.22.Dataset-EncodedDataRow.filter","Type: org.apache.spark.sql.Dataset[chrism.sdsc.ml.EncodedDataRow]
Call: filter

_.label == ""spam""
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.streaming.wordCount.scala/udf/25.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.toLowerCase
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.streaming.wordCount.scala/udf/27.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

WordFrequency(_)
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.streaming.wordCount.scala/udf/29.21.Dataset-WordFrequency).map","Type: org.apache.spark.sql.Dataset[(String, chrism.sdsc.streaming.WordFrequency)]
Call: map

_._2
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.tasknotserializable.taskNotSerializableExamples.scala/udf/13.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

num => prefix + num
"
"udf/spark_repos_5/3_chrismin1202_SDSC2018-Spark-Bootcamp/..src.main.scala.chrism.sdsc.tasknotserializable.taskNotSerializableExamples.scala/udf/29.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

num => prefix + num
"
"udf/spark_repos_5/3_Ethan199111_sparkwiki/..src.main.scala.spark.basic.TextDemo.scala/udf/12.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => r.get(0) + ""_happy""
"
"udf/spark_repos_5/3_jiedog_bigdata/..spark-train.src.main.scala.com.bigdata.spark.sparksql03.TestApp.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

data(""url"") === ""v3.com""
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.clean.FlightClean2.scala/udf/25.19.Dataset-Flight.map","Type: org.apache.spark.sql.Dataset[clean.FlightClean2.Flight]
Call: map

createFlightwId
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/109.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/113.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/120.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/122.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/127.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/129.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/134.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/136.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/141.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/143.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.ml.Flight.scala/udf/97.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_dtree"" === 0.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_dtree"" === 1.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === ($""pred_dtree"")
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not($""label"" === ($""pred_dtree""))
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/76.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_dtree"" === 0.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/78.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""pred_dtree"")
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/83.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_dtree"" === 1.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/85.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""pred_dtree"")
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/90.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_dtree"" === 0.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/92.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""pred_dtree""))
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/97.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""pred_dtree"" === 1.0d
"
"udf/spark_repos_5/3_mapr-demos_mapr-es-db-60-spark-flight/..src.main.scala.sparkmaprdb.QueryFlight.scala/udf/99.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""pred_dtree""))
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.representations.RepresentativeGraph.scala/udf/792.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""NOT (estart >= "" + p.getEndSeconds + "" OR eend <= "" + p.getStartSeconds + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.representations.RepresentativeGraph.scala/udf/798.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""NOT (estart >= "" + p.getEndSeconds + "" OR eend <= "" + p.getStartSeconds + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.GlobalQueries.scala/udf/17.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""estart <= "" + secs + "" and eend > "" + secs
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.GlobalQueries.scala/udf/23.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""estart <= "" + secs + "" and eend > "" + secs
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.GlobalQueries.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""estart <= "" + secs + "" and eend > "" + secs
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.GlobalQueries.scala/udf/38.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""estart <= "" + secs + "" and eend > "" + secs
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/103.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""eid == "" + eId
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/105.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""NOT (estart >= "" + secs2 + "" OR eend <= "" + secs1 + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/17.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""vid == "" + id
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/19.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""estart <= "" + secs + "" and eend > "" + secs
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/26.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""vid == "" + id
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/28.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""estart <= "" + secs + "" and eend > "" + secs
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/42.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""eid == "" + eId
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/44.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""estart <= "" + secs + "" and eend > "" + secs
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/51.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""eid == "" + eId
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/53.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""estart <= "" + secs + "" and eend > "" + secs
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/68.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""vid == "" + id
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/70.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""NOT (estart >= "" + secs2 + "" OR eend <= "" + secs1 + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/77.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""vid == "" + id
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/79.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""NOT (estart >= "" + secs2 + "" OR eend <= "" + secs1 + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/94.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""eid == "" + eId
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.tools.LocalQueries.scala/udf/96.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

""NOT (estart >= "" + secs2 + "" OR eend <= "" + secs1 + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.util.GraphLoader.scala/udf/153.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""NOT (estart >= "" + secs2 + "" OR eend <= "" + secs1 + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.util.GraphLoader.scala/udf/157.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""NOT (estart >= "" + secs2 + "" OR eend <= "" + secs1 + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.util.GraphLoader.scala/udf/76.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""NOT (estart >= "" + secs2 + "" OR eend <= "" + secs1 + "")""
"
"udf/spark_repos_5/3_PortalDB_portal/..src.main.scala.edu.drexel.cs.dbgroup.portal.util.GraphLoader.scala/udf/86.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""NOT (estart >= "" + secs2 + "" OR eend <= "" + secs1 + "")""
"
"udf/spark_repos_5/3_walkingopen_foxifrog/..src.main.scala.com.moon.foxi.core.impl.FrogImpl.scala/udf/293.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(delimiter)
"
"udf/spark_repos_5/3_walkingopen_foxifrog/..src.main.scala.com.moon.foxi.core.impl.FrogImpl.scala/udf/315.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(delimiter)
"
"udf/spark_repos_5/3_walkingopen_foxifrog/..src.main.scala.com.moon.foxi.core.impl.FrogImpl.scala/udf/324.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(delimiter)
"
"udf/spark_repos_5/3_walkingopen_foxifrog/..src.main.scala.com.moon.foxi.core.impl.FrogImpl.scala/udf/329.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(delimiter)
"
"udf/spark_repos_5/3_walkingopen_foxifrog/..src.main.scala.com.moon.foxi.core.impl.FrogImpl.scala/udf/353.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(delimiter)
"
"udf/spark_repos_5/3_walkingopen_foxifrog/..src.main.scala.com.moon.foxi.core.impl.FrogImpl.scala/udf/358.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString(delimiter)
"
"udf/spark_repos_5/3_wolf-song-ml_RecommendationSystem/..Recommendation.ContentRecommendation.src.main.scala.com.z.content.ContentRecommender.scala/udf/22.19.Dataset-Movie.map","Type: org.apache.spark.sql.Dataset[com.z.content.Movie]
Call: map

x => (x.mid, x.name, locally {
        val _t_m_p_2 = x.genres
        _t_m_p_2.map(c => if (c == '|') ' ' else c)
      })
"
"udf/spark_repos_5/3_wolf-song-ml_RecommendationSystem/..Recommendation.ContentRecommendation.src.main.scala.com.z.content.ContentRecommender.scala/udf/37.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => (row.getAs[Int](""mid""), row.getAs[SparseVector](""features"").toArray)
        }
"
"udf/spark_repos_5/3_wolf-song-ml_RecommendationSystem/..Recommendation.ItemCFRecommendation.src.main.scala.com.z.itemcf.ItemCFRecommender.scala/udf/20.19.Dataset-MovieRating.map","Type: org.apache.spark.sql.Dataset[com.z.itemcf.MovieRating]
Call: map

x => (x.uid, x.mid, x.score)
"
"udf/spark_repos_5/3_wolf-song-ml_RecommendationSystem/..Recommendation.ItemCFRecommendation.src.main.scala.com.z.itemcf.ItemCFRecommender.scala/udf/38.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ row => 
          val coocSim = cooccurrenceSim(row.getAs[Long](""cocount""), row.getAs[Long](""count1""), row.getAs[Long](""count2""))
          (row.getInt(0), (row.getInt(1), coocSim))
        }
"
"udf/spark_repos_5/3_wolf-song-ml_RecommendationSystem/..Recommendation.StatisticsRecommendation.src.main.scala.com.z.statistics.StatisticsRecommender.scala/udf/32.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: Int) => simpleDateFormat.format(new Date(x * 1000L)).toInt
"
"udf/spark_repos_5/3_xuanbo_spark-programming/..src.main.scala.com.example.spark.sql.BasicsSQLApp.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_5/3_xuanbo_spark-programming/..src.main.scala.com.example.spark.sql.hive.HiveApp.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_5/41_yhao2014_CkoocNLP/..ckooc-ml.src.main.scala.functions.clean.Cleaner.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ record => 
        val outputIndex = record.fieldIndex($(outputCol))
        record.getString(outputIndex).length >= getMinLineLen
      }
"
"udf/spark_repos_5/41_yhao2014_CkoocNLP/..ckooc-ml.src.main.scala.functions.segment.Segmenter.scala/udf/92.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ record => 
        val outputIndex = record.fieldIndex($(outputCol))
        val tokens = record.getList(outputIndex)
        tokens.nonEmpty && tokens.size() > getMinTermNum
      }
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-dataframe.src.main.scala.io.frama.parisni.spark.dataframe.DFTool.scala/udf/171.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

key.isNotNull
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-dataframe.src.main.scala.io.frama.parisni.spark.dataframe.DFTool.scala/udf/177.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

key.isInCollection(levels)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.MetaExtractor.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""lib_schema"").rlike(schemaRegexFilter.getOrElse("".*""))
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.MetaExtractor.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

expr(""type_value = 'nullCount'"")
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.MetaExtractor.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

expr(""type_value = 'distinctCount'"")
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.CovidomFeatureExtractImpl.scala/udf/18.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.is_fk"") === lit(true)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.CovidomFeatureExtractImpl.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""t.lib_column"").isNotNull
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.DefaultFeatureExtractImpl.scala/udf/20.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.is_fk"") === lit(true)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.DefaultFeatureExtractImpl.scala/udf/22.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!(col(""s.lib_column"") rlike ""^ids_eds"")
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.DefaultFeatureExtractImpl.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""t.lib_column"").isNotNull
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.DefaultFeatureExtractImpl.scala/udf/29.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.is_fk"") === lit(true)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.DefaultFeatureExtractImpl.scala/udf/31.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.lib_schema"").rlike(""omop"")
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.DefaultFeatureExtractImpl.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""t.lib_column"").isNotNull
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.EdsFeatureExtractImpl.scala/udf/19.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.is_fk"") === lit(true)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.EdsFeatureExtractImpl.scala/udf/21.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!(col(""s.lib_column"") rlike ""^ids_eds"")
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.EdsFeatureExtractImpl.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""t.lib_column"").isNotNull
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.I2b2FeatureExtractImpl.scala/udf/17.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.is_fk"") === lit(true)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.I2b2FeatureExtractImpl.scala/udf/22.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.is_fk"") === lit(true)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.I2b2FeatureExtractImpl.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""t.lib_column"").isNotNull
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.OmopFeatureExtractImpl.scala/udf/17.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.is_fk"") === lit(true)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.OmopFeatureExtractImpl.scala/udf/22.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""s.is_fk"") === lit(true)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-meta.src.main.scala.io.frama.parisni.spark.meta.strategy.extractor.OmopFeatureExtractImpl.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""t.lib_column"").isNotNull
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-postgres.src.main.scala.io.frama.parisni.spark.postgres.rowconverters.CSVUtils.scala/udf/14.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!$""value"".startsWith(options.comment.toString)
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-postgres.src.main.scala.io.frama.parisni.spark.postgres.rowconverters.CSVUtils.scala/udf/9.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

length(trim($""value"")) > 0
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-sync.src.main.scala.io.frama.parisni.spark.sync.analyze.AnalyzeTable.scala/udf/46.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.get(0).toString
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-sync.src.main.scala.io.frama.parisni.spark.sync.conf.ParquetConf.scala/udf/19.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f""$s_date_field >= '$date_Max'""
"
"udf/spark_repos_5/4_EDS-APHP_spark-etl/..spark-sync.src.main.scala.io.frama.parisni.spark.sync.conf.SolrConf.scala/udf/28.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f""$s_date_field >= '$date_Max'""
"
"udf/spark_repos_5/4_Higmin_Spark-Learning/..src.main.scala.org.sparkSQL.ApacheAccessLog.LogAnalyzerSQL.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_5/4_Higmin_Spark-Learning/..src.main.scala.org.sparkSQL.ApacheAccessLog.LogAnalyzerSQL.scala/udf/8.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

ApacheAccessLog.parseLogLine
"
"udf/spark_repos_5/4_Higmin_Spark-Learning/..src.main.scala.org.sparkSQL.SensorLog.SensorStatistics.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getDouble(0)
"
"udf/spark_repos_5/4_Raini-zhe_Byz--SparkNotes/..GraphX知识图谱.GalacticSpark-谱聚类git项目代码(很多种算法).src.ml.LogisticRLearn.scala/udf/116.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => if (r.getDouble(0) == r.getDouble(1)) 1.0d else 0.0d
"
"udf/spark_repos_5/4_spoddutur_spark-ml-dashboard/..src.main.scala.com.spoddutur.web.SparkMLModelService.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        var res = row.mkString("","").split("","")
        new TestModelResponse(res(0), res(1).equals(""1.0""))
      }
"
"udf/spark_repos_5/4_xiadx_rec-sys/..rec-recall.src.main.scala.ge.DynamicWalk.scala/udf/55.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length($""items"").>(0)
"
"udf/spark_repos_5/4_xiadx_rec-sys/..rec-recall.src.main.scala.ge.DynamicWalk.scala/udf/59.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""count > %d"".format(minSearch)
"
"udf/spark_repos_5/4_xiadx_rec-sys/..rec-recall.src.main.scala.ge.DynamicWalk.scala/udf/92.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""tokens"".contains(""weng"")
"
"udf/spark_repos_5/4_zhang3550545_structuredstreamngdemo/..src.main.scala.com.test.CommonStructuedKafka.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ value => 
        val list = Tools.parseJson(value, className)
        Tools.list2Tuple7(list)
      }
"
"udf/spark_repos_5/4_zhang3550545_structuredstreamngdemo/..src.main.scala.com.test.Hdfs2RocketMQStream.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val name = row.getString(0)
        val age = row.getString(1)
        val sex = row.getString(2)
        val obj = new JsonObject()
        obj.addProperty(""name"", name)
        obj.addProperty(""age"", age)
        obj.addProperty(""sex"", sex)
        val body = obj.toString
        body
      }
"
"udf/spark_repos_5/4_zhang3550545_structuredstreamngdemo/..src.main.scala.com.test.KafkaFormatJson.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ value => 
        val gson = new Gson
        val people = gson.fromJson(value, classOf[People])
        (people.name, people.age, people.sex)
      }
"
"udf/spark_repos_5/4_zhang3550545_structuredstreamngdemo/..src.main.scala.com.test.KafkaFormat.scala/udf/13.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

{ line => 
        val columns = line._2.split("","")
        (columns(0), columns(1), columns(2))
      }
"
"udf/spark_repos_5/4_zhenchao125_sparkmall0225/..sparkmall0225-offline.src.main.scala.com.atguigu.sparkmall0225.offline.app.AreaProductTop3.scala/udf/10.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AreaClickUDAF
"
"udf/spark_repos_5/5_maziyarpanahi_spark2-template/..src.main.scala.sql_practice.examples.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""tourPrice"" > 500
"
"udf/spark_repos_5/5_ryandavidhartman_ScalaSchool/..courseraClasses.old_stuff.coursera4.week04.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/119.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        r => TimeUsageRow(working = r.getAs[String](""working""), sex = r.getAs[String](""sex""), age = r.getAs[String](""age""), primaryNeeds = r.getAs[Double](""primaryNeeds""), work = r.getAs[Double](""work""), other = r.getAs[Double](""other""))
      }
"
"udf/spark_repos_5/5_ryandavidhartman_ScalaSchool/..courseraClasses.old_stuff.coursera4.week04.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/128.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((w, s, a), pn, working, o) =>
          TimeUsageRow(w, s, a, pn, working, o)
      }
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/11.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.resize _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/15.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.calcOverlap _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/19.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.flank _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/23.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.promoters _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/27.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.reflect _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/31.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.calcOverlap _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/35.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.shift _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/39.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.resize _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/43.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.calcOverlap _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/47.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.flank _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/51.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.promoters _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/55.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.reflect _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/59.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.calcOverlap _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.main.scala.org.biodatageeks.sequila.utils.UDFRegister.scala/udf/7.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

RangeMethods.shift _
"
"udf/spark_repos_5/5_ZSI-Bio_bdg-sequila/..src.test.scala.org.biodatageeks.sequila.tests.coverage.LongReadsTestSuite.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""${Columns.COVERAGE}== 0""
"
"udf/spark_repos_5/63_mskimm_ann4s/..src.test.scala.ann4s.spark.example.ALSBasedUserItemIndexing.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ str => 
        val fields = str.split(""::"")
        assert(fields.size == 4)
        Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)
      }
"
"udf/spark_repos_5/63_mskimm_ann4s/..src.test.scala.ann4s.spark.example.DistributedBuilds.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ str => 
        val Array(id, features) = str.split(""\t"")
        (id.toInt, locally {
          val _t_m_p_2 = features.split("","")
          _t_m_p_2.map(_.toFloat)
        })
      }
"
"udf/spark_repos_5/6_lemonahit_LogAnalysis/..SparkLogAnalysis.com.imooc.log.TopNStatJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/6_lemonahit_LogAnalysis/..SparkLogAnalysis.com.imooc.log.TopNStatJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/6_lemonahit_LogAnalysis/..SparkLogAnalysis.com.imooc.log.TopNStatJob.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_5/75_adornes_spark_scala_ml_examples/..src.main.scala.com.adornes.spark.kaggle.AllstateClaimsSeverityGBTRegressor.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/75_adornes_spark_scala_ml_examples/..src.main.scala.com.adornes.spark.kaggle.AllstateClaimsSeverityGBTRegressor.scala/udf/75.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/75_adornes_spark_scala_ml_examples/..src.main.scala.com.adornes.spark.kaggle.AllstateClaimsSeverityRandomForestRegressor.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/75_adornes_spark_scala_ml_examples/..src.main.scala.com.adornes.spark.kaggle.AllstateClaimsSeverityRandomForestRegressor.scala/udf/75.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/76_snowflakedb_spark-snowflake/..src.main.scala.net.snowflake.spark.snowflake.SnowflakeWriter.scala/udf/42.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.toString
"
"udf/spark_repos_5/7_tolomaus_languagedetector/..src.main.scala.biz.meetmatch.modules.CountSentencesByLanguage.scala/udf/17.19.Dataset-(String, Long).map","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: map

{
        case (language, count) =>
          SentenceCountByLanguage(language, count)
      }
"
"udf/spark_repos_5/7_tolomaus_languagedetector/..src.main.scala.biz.meetmatch.modules.CountWrongDetectionsByLanguageAsDataFrame.scala/udf/17.22.Dataset-Sentence.filter","Type: org.apache.spark.sql.Dataset[biz.meetmatch.model.Sentence]
Call: filter

$""detectedLanguage"" =!= ($""actualLanguage"")
"
"udf/spark_repos_5/7_tolomaus_languagedetector/..src.main.scala.biz.meetmatch.modules.CountWrongDetectionsByLanguage.scala/udf/18.24.Dataset-Sentence.filter","Type: org.apache.spark.sql.Dataset[biz.meetmatch.model.Sentence]
Call: filter

sentence => sentence.detectedLanguage != sentence.actualLanguage
"
"udf/spark_repos_5/7_tolomaus_languagedetector/..src.main.scala.biz.meetmatch.modules.CountWrongDetectionsByLanguage.scala/udf/20.19.Dataset-(String, Long).map","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: map

{
        case (detectedLanguage, count) =>
          WrongDetectionByLanguage(detectedLanguage, count)
      }
"
"udf/spark_repos_5/7_tolomaus_languagedetector/..src.main.scala.biz.meetmatch.modules.DetectLanguage.scala/udf/22.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(""\t"")
"
"udf/spark_repos_5/7_TomLous_meetup-spark-airflow-demo/..src.main.scala.etl.V_ExportToElasticSearch.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'latitude.isNotNull && 'longitude.isNotNull
"
"udf/spark_repos_5/9_josonle_Learning-Spark/..Spark_With_Scala_Testing.src.sparkSql.RDDtoDataFrame2.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_5/9_PacktPublishing_Hands-On-Data-Analysis-with-Scala/..Chapter6.src.main.scala.handson.example.SparkDatasetExample.scala/udf/17.21.Dataset-PrimeNumber.map","Type: org.apache.spark.sql.Dataset[handson.example.PrimeNumber]
Call: map

_.num
"
"udf/spark_repos_5/9_PacktPublishing_Hands-On-Data-Analysis-with-Scala/..Chapter9.src.main.scala.handson.example.HandsOnSparkMedian.scala/udf/38.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => scala.util.Random.nextInt(20)
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter02.src.main.scala.RegressionAnalysis.UrbanTrafficGeneralizedLinearRegression.scala/udf/42.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter02.src.main.scala.RegressionAnalysis.UrbanTrafficGeneralizedLinearRegression.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter02.src.main.scala.RegressionAnalysis.UrbanTrafficLinearRegression.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter02.src.main.scala.RegressionAnalysis.UrbanTrafficLinearRegression.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionLR.scala/udf/44.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionLR.scala/udf/46.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionLR.scala/udf/51.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionLR.scala/udf/53.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionLR.scala/udf/58.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionLR.scala/udf/60.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionLR.scala/udf/65.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionLR.scala/udf/67.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionNB.scala/udf/41.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionNB.scala/udf/43.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionNB.scala/udf/48.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionNB.scala/udf/50.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionNB.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionNB.scala/udf/57.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionNB.scala/udf/62.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionNB.scala/udf/64.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionSVM.scala/udf/46.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionSVM.scala/udf/48.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionSVM.scala/udf/53.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionSVM.scala/udf/55.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionSVM.scala/udf/60.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionSVM.scala/udf/62.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionSVM.scala/udf/67.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter03.src.main.scala.ScalaClassification.ChurnPredictionSVM.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not($""label"" === ($""prediction""))
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter04.src.main.scala.ScalaTreeEnsimbles.UrbanTrafficDTRegressor.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter04.src.main.scala.ScalaTreeEnsimbles.UrbanTrafficGBTRegressor.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_5/9_PacktPublishing_Machine-Learning-with-Scala-Quick-Start-Guide/..Chapter04.src.main.scala.ScalaTreeEnsimbles.UrbanTrafficRFRegressor.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(label: Double, prediction: Double) =>
          (label, prediction)
      }
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.imooc_log_analysis.TopNStatJob.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.imooc_log_analysis.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.imooc_log_analysis.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.imooc_log_analysis.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.spark_basic.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.spark_basic.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.spark_basic.DataFrameRDDApp.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.spark_basic.DatasetApp.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[spark_basic.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_6/12_xuanzhao_imooc_spark_log_analysis/..src.main.scala.spark_basic.DatasetApp.scala/udf/18.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[spark_basic.DatasetApp.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_6/15_jasonsatran_spark-meta/..src.main.scala.com.jasonsatran.spark.meta.profile.ColumnProfile.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""isEmpty"") === true
"
"udf/spark_repos_6/15_jasonsatran_spark-meta/..src.main.scala.com.jasonsatran.spark.meta.profile.ColumnProfile.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""isNull"")
"
"udf/spark_repos_6/15_jasonsatran_spark-meta/..src.main.scala.com.jasonsatran.spark.meta.profile.ColumnProfile.scala/udf/40.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""isNumeric"") === true
"
"udf/spark_repos_6/15_miraisolutions_spark-bigquery/..src.main.scala.com.miraisolutions.spark.bigquery.examples.Shakespeare.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""corpus"".like(""hamlet"")
"
"udf/spark_repos_6/17_rwhaling_embeddings/..src.main.scala.SparkJob.scala/udf/66.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          case Row(word: String, similarity: Double) => word
        }
"
"udf/spark_repos_6/1_aastha0304_KafkaSparkTwitter/..spark-kafka-sql.src.main.scala.kafkasparksql.KafkaSparkSql.scala/udf/49.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

sentiment _
"
"udf/spark_repos_6/1_abdullahainun_bro-aal/..src.main.scala.com.aal.spark.jobs.StreamClassification.scala/udf/113.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""idOrigP"".isNotNull && $""idRespP"".isNotNull && $""orig_bytes"".isNotNull && $""resp_bytes"".isNotNull && $""missedBytes"".isNotNull && $""origPkts"".isNotNull && $""origIpBytes"".isNotNull && $""respPkts"".isNotNull && $""respIpBytes"".isNotNull && $""PX"".isNotNull && $""NNP"".isNotNull && $""NSP"".isNotNull && $""PSP"".isNotNull && $""IOPR"".isNotNull && $""Reconnect"".isNotNull && $""FPS"".isNotNull && $""TBT"".isNotNull && $""APL"".isNotNull && $""PPS"".isNotNull
"
"udf/spark_repos_6/1_abdullahainun_bro-aal/..src.main.scala.com.aal.spark.jobs.StreamClassification.scala/udf/121.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (r: Row) => ClassificationObj(r.getAs[Timestamp](0), r.getAs[String](1), r.getAs[String](2), r.getAs[Integer](3), r.getAs[String](4), r.getAs[Integer](5), r.getAs[String](6)) }
"
"udf/spark_repos_6/1_abdullahainun_bro-aal/..src.main.scala.com.aal.spark.jobs.StreamClassification.scala/udf/163.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (r: Row) => DnsCountObj(r.getAs[Timestamp](0), r.getAs[String](1), r.getAs[String](2), r.getAs[Integer](3), r.getAs[String](4), r.getAs[Integer](5), r.getAs[String](6), r.getAs[Integer](7), r.getAs[String](8), r.getAs[Integer](9), r.getAs[String](10), r.getAs[Boolean](11), r.getAs[Boolean](12), r.getAs[Boolean](13), r.getAs[Boolean](14), r.getAs[Integer](15), r.getAs[String](16), r.getAs[Integer](17), r.getAs[Boolean](18)) }
"
"udf/spark_repos_6/1_abdullahainun_bro-aal/..src.main.scala.com.aal.spark.jobs.StreamClassification.scala/udf/167.23.Dataset-DnsCountObj.filter","Type: org.apache.spark.sql.Dataset[com.aal.spark.jobs.StreamClassification.DnsCountObj]
Call: filter

$""timestamp"".isNotNull
"
"udf/spark_repos_6/1_abdullahainun_bro-aal/..src.main.scala.com.aal.spark.jobs.StreamClassification.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (r: Row) => ConnCountObj(r.getAs[Timestamp](0), r.getAs[String](1), r.getAs[String](2), r.getAs[Integer](3), r.getAs[String](4), r.getAs[Integer](5), r.getAs[String](6), r.getAs[String](7), r.getAs[Double](8), r.getAs[Integer](9), r.getAs[Integer](10), r.getAs[String](11), r.getAs[Boolean](12), r.getAs[Boolean](13), r.getAs[Integer](14), r.getAs[String](15), r.getAs[Integer](16), r.getAs[Integer](17), r.getAs[Integer](18), r.getAs[Integer](19)) }
"
"udf/spark_repos_6/1_abdullahainun_bro-aal/..src.main.scala.com.aal.spark.jobs.StreamClassification.scala/udf/47.22.Dataset-ConnCountObj.filter","Type: org.apache.spark.sql.Dataset[com.aal.spark.jobs.StreamClassification.ConnCountObj]
Call: filter

$""timestamp"".isNotNull && $""uid"".isNotNull && $""idOrigH"".isNotNull && $""idOrigP"".isNotNull && $""idRespH"".isNotNull && $""idRespP"".isNotNull && $""proto"".isNotNull && $""service"".isNotNull && $""duration"".isNotNull && $""orig_bytes"".isNotNull && $""resp_bytes"".isNotNull && $""connState"".isNotNull && $""localOrig"".isNotNull && $""localResp"".isNotNull && $""missedBytes"".isNotNull && $""history"".isNotNull && $""origPkts"".isNotNull && $""origIpBytes"".isNotNull && $""respPkts"".isNotNull && $""respIpBytes"".isNotNull
"
"udf/spark_repos_6/1_akhld_parquet-csv-converter/..src.main.scala.hacked.work.ParquetCSVConvertMain.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.mkString("","")
"
"udf/spark_repos_6/1_aloise_Misc-Spark-Tasks/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/107.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, round2(primaryNeeds, 1), round2(work, 1), round2(other, 1))
      }
"
"udf/spark_repos_6/1_aloise_Misc-Spark-Tasks/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/99.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

rows => TimeUsageRow(rows.getAs(""working""), rows.getAs(""sex""), rows.getAs(""age""), rows.getAs(""primaryNeeds""), rows.getAs(""work""), rows.getAs(""other""))
"
"udf/spark_repos_6/1_AntikaSmith_biendata/..src.main.scala.cf.ToutiaoCF.scala/udf/33.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

convertProb(_)(defaultValue)
"
"udf/spark_repos_6/1_AntikaSmith_biendata/..src.main.scala.cf.ToutiaoCF.scala/udf/49.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

pasreRating(_)
"
"udf/spark_repos_6/1_AntikaSmith_biendata/..src.main.scala.cf.ToutiaoCF.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Prediction(row.getInt(0), row.getInt(1), 0.0f, row.getFloat(2))
"
"udf/spark_repos_6/1_AntikaSmith_biendata/..src.main.scala.cf.ToutiaoCF.scala/udf/67.19.Dataset-Prediction.map","Type: org.apache.spark.sql.Dataset[cf.ToutiaoCF.Prediction]
Call: map

prediction => Result(ParseUtil.numberId2qidMap(prediction.questionId), ParseUtil.numberId2uidMap(prediction.userId), prediction.prediction)
"
"udf/spark_repos_6/1_arpangrwl_FirstScalaSparkPOC/..src.main.scala.ScalaIO.IOScalaPOC.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => x.split(""\\|"")
"
"udf/spark_repos_6/1_arpangrwl_FirstScalaSparkPOC/..src.main.scala.ScalaIO.IOScalaPOC.scala/udf/34.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(val1: String) => val1.substring(0, 1)
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.ActionOperation1.scala/udf/17.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

employee => 1
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.pricatice.Demo1.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

zips.col(""pop"") > 4000
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.pricatice.Demo1.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'state === ""CA""
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.SparkSQLDemo1.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 28
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.SparkSQLDemo1.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df.col(""age"") > 23
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.SparkSQLDemo3.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, values: String) =>
          s""key:$key,value:$values""
      }
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.SparkSQLDemo4.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

zipsDf.col(""pop"") > 40000
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.SparkSQLDemo4.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'state === ""CA""
"
"udf/spark_repos_6/1_Batadog_mycodingwarehourse/..src.main.spark.spark2.SparkSQLDemo4.scala/udf/36.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(c: String) => c.length
"
"udf/spark_repos_6/1_bhaskisharma_SparkUsingScala/..PlayingWithSpark.src.main.scala.sparkSql.StackOverFlowSurveyUsingSQL.scala/udf/18.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(""country"").===(""Afghanistan"")
"
"udf/spark_repos_6/1_bhaskisharma_SparkUsingScala/..PlayingWithSpark.src.main.scala.sparkSql.StackOverFlowSurveyUsingSQL.scala/udf/26.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

responseWithSelectedColumns.col(AGE_MIDPOINT) < 20
"
"udf/spark_repos_6/1_bhaskisharma_SparkUsingScala/..PlayingWithSpark.src.main.scala.sparkSql.TypedDataset.scala/udf/19.20.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[sparkSql.Response]
Call: filter

response => response.country == ""Afghanistan""
"
"udf/spark_repos_6/1_bhaskisharma_SparkUsingScala/..PlayingWithSpark.src.main.scala.sparkSql.TypedDataset.scala/udf/26.20.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[sparkSql.Response]
Call: filter

response => response.age_midpoint.isDefined && response.age_midpoint.get < 20.0d
"
"udf/spark_repos_6/1_bhaskisharma_SparkUsingScala/..PlayingWithSpark.src.main.scala.sparkSql.TypedDataset.scala/udf/33.20.Dataset-Response.filter","Type: org.apache.spark.sql.Dataset[sparkSql.Response]
Call: filter

response => response.salary_midpoint.isDefined
"
"udf/spark_repos_6/1_bhaskisharma_SparkUsingScala/..PlayingWithSpark.src.main.scala.sparkSql.TypedDataset.scala/udf/38.17.Dataset-Response.map","Type: org.apache.spark.sql.Dataset[sparkSql.Response]
Call: map

response => locally {
      val _t_m_p_5 = response.salary_midpoint
      _t_m_p_5.map(point => Math.round(point / 20000) * 20000)
    }.orElse(None)
"
"udf/spark_repos_6/1_bigdatapassionpl_spark-training/..spark-sql.src.main.scala.com.bigdatapassion.spark.sql.SparkDatasetBasic.scala/udf/11.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

x => x * x
"
"udf/spark_repos_6/1_bigdatapassionpl_spark-training/..spark-sql.src.main.scala.com.bigdatapassion.spark.sql.SparkDatasetBasic.scala/udf/24.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""kot"")
"
"udf/spark_repos_6/1_cauchy8389_SparkTest/..src.main.scala.mlib.ALS.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(',') match {
        case Array(user, item, rate) =>
          Rating(user.toInt, item.toInt, rate.toDouble)
      }
"
"udf/spark_repos_6/1_cauchy8389_SparkTest/..src.main.scala.mlib.ALS_sl.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split(',') match {
        case Array(user, item, rate) =>
          Rating(user.toInt, item.toInt, rate.toDouble)
      }
"
"udf/spark_repos_6/1_cauchy8389_SparkTest/..src.main.scala.mlib.FPGrowth.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_6/1_cauchy8389_SparkTest/..src.main.scala.mlib.LinearRegression.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val parts = line.split(',')
        val _2parts = parts(1).split(' ')
        (parts(0).toDouble, _2parts(0).toDouble, _2parts(1).toDouble, _2parts(2).toDouble, _2parts(3).toDouble, _2parts(4).toDouble, _2parts(5).toDouble, _2parts(6).toDouble, _2parts(7).toDouble)
      }
"
"udf/spark_repos_6/1_cauchy8389_SparkTest/..src.main.scala.mlib.LinearRegression.scala/udf/39.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val parts = line.split(',')
        val _2parts = parts(1).split(' ')
        (parts(0).toDouble, _2parts(0).toDouble, _2parts(1).toDouble, _2parts(2).toDouble, _2parts(3).toDouble, _2parts(4).toDouble, _2parts(5).toDouble, _2parts(6).toDouble, _2parts(7).toDouble)
      }
"
"udf/spark_repos_6/1_cauchy8389_SparkTest/..src.main.scala.SparkSQLScala.MySparkSql.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 18
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_mlib.preparing_data.StatisticsRandomDataAndSamplingOnDataframes.scala/udf/45.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

s => ""%s ==>  %s"".format(s(0).toString, s(1).toString)
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.DataFrames.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 25 && $""city"" === ""Sydney""
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.DataFrames.scala/udf/93.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hasDebt"" && ($""hasFinancialDependents"")
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.DataFrames.scala/udf/97.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""country"" === ""Switzerland""
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.DevelopAndTesting.scala/udf/21.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

distanceOf
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.LoadingDataProgramatically.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

arr => ""Full Name: %s %s "".format(arr(0), arr(1))
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.OperationsOnHiveTables.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.OtherDataFramesTransformations.scala/udf/56.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r(1).toString == ""COMPLETE""
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.QueryingUsingSparkSQL.scala/udf/28.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.QueryingUsingSparkSQL.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => ""Name: %s, Id: %s"".format(r(1), r(0))
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.SparkSQLforETL.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""web_page_num""), row.getAs[String](""associated_files""))
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.SparkSQLUDF.scala/udf/15.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(input: String) => input.toUpperCase
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.SparkSQLUDF.scala/udf/19.22.Dataset-Function.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Function]
Call: filter

'name like ""%upper%""
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.SpecifyingSchema.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

arr => ""Name: "" + arr(0)
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.timeusage.TimeUsage.scala/udf/104.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs(""working""), row.getAs(""sex""), row.getAs(""age""), row.getAs(""primaryNeeds""), row.getAs(""work""), row.getAs(""other""))
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.timeusage.TimeUsage.scala/udf/110.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), pNeeds, work, other) =>
          TimeUsageRow(working, sex, age, pNeeds, work, other)
      }
"
"udf/spark_repos_6/1_ChemaGit_Apache-Spark-2.0-with-Scala/..src.main.scala.spark_sql.WorkingWithDataframes.scala/udf/22.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Duration"" < 18
"
"udf/spark_repos_6/1_comma337_connector-spark/..src.main.scala.kafka.connector.spark.batch.readers.HiveReader.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

partition
"
"udf/spark_repos_6/1_compae_structured-streaming-tests/..src.main.scala.com.stratio.spark.structured.streaming.help.WindowWatermarksMain.scala/udf/30.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
      val oldvalues = row.toSeq
      val newValues = {
        if (row.toString().contains(""minute"")) oldvalues :+ (new Timestamp(System.currentTimeMillis() - 216000)) else if (row.toString().contains(""hour"")) oldvalues :+ (new Timestamp(System.currentTimeMillis() - 12960000)) else if (row.toString().contains(""day"")) oldvalues :+ (new Timestamp(System.currentTimeMillis() - 311040000)) else oldvalues :+ (new Timestamp(System.currentTimeMillis()))
      }
      new GenericRowWithSchema(newValues.toArray, schema).asInstanceOf[Row]
    }
"
"udf/spark_repos_6/1_daniel-wl_learning-spark-with-scala/..spark.src.main.scala.com.daniel_wl.ReadSingleFile.scala/udf/11.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains('b')
"
"udf/spark_repos_6/1_daniel-wl_learning-spark-with-scala/..spark.src.main.scala.com.daniel_wl.ReadSingleFile.scala/udf/7.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains('a')
"
"udf/spark_repos_6/1_David082_SparkML/..src.main.scala.hotel.features.SparkSim.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val merge = SimSql.mergeCol(row, 9, userPriceFreq.length)
        (row(0).toString, row(1).toString, row(2).toString, row(3).toString.toInt, row(4).toString.toInt, row(5).toString, row(6).toString, row(7).toString.toInt, row(8).toString.toDouble, merge.toArray)
      }
"
"udf/spark_repos_6/1_David082_SparkML/..src.main.scala.hotel.features.SparkSim.scala/udf/44.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => (row(0).toString, row(1).toString, row(2).toString, row(3).toString.toInt, row(4).toString.toInt, row(5).toString, row(6).toString, row(7).toString.toInt, row(8).toString.toDouble, row(9).toString.toDouble, Similarity.simpleSimilarity(row(10).toString, row(11).toString))
      }
"
"udf/spark_repos_6/1_David082_SparkML/..src.main.scala.hotel.orders.ArrayFeatures.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val merge = SparkSQL.mergeIntCol(row, 1, colLen)
        (row(0).toString.toInt, merge.toArray)
      }
"
"udf/spark_repos_6/1_David082_SparkML/..src.main.scala.hotel.orders.ArrayFeatures.scala/udf/85.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val merge = SparkSQL.mergeDoubleCol(row, 1, colLen)
        (row(0).toString.toInt, merge.toArray)
      }
"
"udf/spark_repos_6/1_drnushooz_statanswers/..src.main.scala.com.github.drnushooz.StatAnswers.scala/udf/49.24.Dataset-CrawlRow.filter","Type: org.apache.spark.sql.Dataset[com.github.drnushooz.CrawlRow]
Call: filter

row => row.crawlDate >= startDate && row.crawlDate <= endDate
"
"udf/spark_repos_6/1_drnushooz_statanswers/..src.main.scala.com.github.drnushooz.StatAnswers.scala/udf/51.22.Dataset-CrawlRow.filter","Type: org.apache.spark.sql.Dataset[com.github.drnushooz.CrawlRow]
Call: filter

_.rank < 10
"
"udf/spark_repos_6/1_drnushooz_statanswers/..src.main.scala.com.github.drnushooz.StatAnswers.scala/udf/75.24.Dataset-CrawlRow.filter","Type: org.apache.spark.sql.Dataset[com.github.drnushooz.CrawlRow]
Call: filter

row => row.crawlDate >= startDate && row.crawlDate <= endDate
"
"udf/spark_repos_6/1_drnushooz_statanswers/..src.main.scala.com.github.drnushooz.StatAnswers.scala/udf/77.22.Dataset-CrawlRow.filter","Type: org.apache.spark.sql.Dataset[com.github.drnushooz.CrawlRow]
Call: filter

_.rank == 1
"
"udf/spark_repos_6/1_drnushooz_statanswers/..src.main.scala.com.github.drnushooz.StatAnswers.scala/udf/88.24.Dataset-CrawlRow.filter","Type: org.apache.spark.sql.Dataset[com.github.drnushooz.CrawlRow]
Call: filter

row => row.crawlDate >= startDate && row.crawlDate <= endDate && row.location == inputLocation
"
"udf/spark_repos_6/1_drnushooz_statanswers/..src.main.scala.com.github.drnushooz.StatAnswers.scala/udf/90.22.Dataset-CrawlRow.filter","Type: org.apache.spark.sql.Dataset[com.github.drnushooz.CrawlRow]
Call: filter

row => row.keyword == inputKeyword && row.market == inputMarket
"
"udf/spark_repos_6/1_drnushooz_statanswers/..src.main.scala.com.github.drnushooz.StatAnswers.scala/udf/94.23.Dataset-CrawlRow.filter","Type: org.apache.spark.sql.Dataset[com.github.drnushooz.CrawlRow]
Call: filter

_.device == ""desktop""
"
"udf/spark_repos_6/1_drnushooz_statanswers/..src.main.scala.com.github.drnushooz.StatAnswers.scala/udf/98.23.Dataset-CrawlRow.filter","Type: org.apache.spark.sql.Dataset[com.github.drnushooz.CrawlRow]
Call: filter

_.device == ""smartphone""
"
"udf/spark_repos_6/1_dspathak_spark-structured-streaming-example/..src.main.scala.com.deb.example.sparkstreaming.StructuredStreamingAggregation_Base.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(id: String, command: String, recordType: String, miscInfo: String, category: String, cloudService: String, cloudProvider: String, eventCount: Int, dataBytes: Int, time: Timestamp) =>
          EventBean(id, command, recordType, miscInfo, category, cloudService, cloudProvider, eventCount, dataBytes, convertToUTC(time))
      }
"
"udf/spark_repos_6/1_dspathak_spark-structured-streaming-example/..src.main.scala.com.deb.example.sparkstreaming.StructuredStreamingAggregation_FromKafka2Kafka.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(id: String, sefCommand: String, sefRecordType: String, sefMiscInfo: String, sefDestinationEPClassCategory: String, sefDestinationEPClassService: String, sefDestinationEPServiceProvider: String, sefContributorCount: Int, sefUnitsTransferred: Int, time: Timestamp) =>
          OsefEvent(id, sefCommand, sefRecordType, sefMiscInfo, sefDestinationEPClassCategory, sefDestinationEPClassService, sefDestinationEPServiceProvider, sefContributorCount, sefUnitsTransferred, time)
      }
"
"udf/spark_repos_6/1_dungvo_bigdata-radius/..src.main.scala.com.ftel.bigdata.radius.anomaly.ADStreaming.scala/udf/112.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""signIn"" =!= 0 && $""logOff"" =!= 0
"
"udf/spark_repos_6/1_dungvo_bigdata-radius/..src.main.scala.com.ftel.bigdata.radius.anomaly.ADStreaming.scala/udf/120.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""date_time"".between(prev15m, t)
"
"udf/spark_repos_6/1_dungvo_bigdata-radius/..src.main.scala.com.ftel.bigdata.radius.anomaly.ADStreaming.scala/udf/131.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""date_time"" === t
"
"udf/spark_repos_6/1_dungvo_bigdata-radius/..src.main.scala.com.ftel.bigdata.radius.anomaly.ADStreaming.scala/udf/164.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ""outlier""
"
"udf/spark_repos_6/1_dungvo_bigdata-radius/..src.main.scala.com.ftel.bigdata.radius.anomaly.ADStreaming.scala/udf/168.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ""outlier""
"
"udf/spark_repos_6/1_dungvo_bigdata-radius/..src.main.scala.com.ftel.bigdata.radius.anomaly.ADStreaming.scala/udf/172.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ""outlier""
"
"udf/spark_repos_6/1_dungvo_bigdata-radius/..src.main.scala.com.ftel.bigdata.radius.postgres.PostgresUtil.scala/udf/54.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case row =>
          (timeStamp, row.getString(0), row.getInt(3).toLong, row.getLong(1), row.getLong(2), row.getLong(9), row.getLong(10), row.getLong(5), row.getLong(6), row.getLong(7), row.getLong(8), row.getString(4))
      }
"
"udf/spark_repos_6/1_dungvo_bigdata-radius/..src.main.scala.test.DataFrameTest.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.getAs[String](""count"") != ""1""
"
"udf/spark_repos_6/1_ethan0606_SparkApp/..src.main.scala.com.github.spark.util.EncodeUtil.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > limit
"
"udf/spark_repos_6/1_ethan0606_SparkApp/..src.test.scala.com.github.spark.feature.TFRecord.scala/udf/11.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""time_stamp"" <= 1494636910
"
"udf/spark_repos_6/1_ethan0606_SparkApp/..src.test.scala.com.github.spark.feature.TFRecord.scala/udf/15.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""time_stamp"" > 1494636910
"
"udf/spark_repos_6/1_ethan0606_SparkApp/..src.test.scala.com.github.spark.feature.TFRecord.scala/udf/19.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_6/1_ethan0606_SparkApp/..src.test.scala.com.github.spark.feature.UserProfile.scala/udf/10.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val id = x.getString(0)
        val cate = x.getString(1).toInt
        val price = x.getString(2).toDouble
        val priceLevel = if (price < 10) {
          1
        } else if (price < 100) {
          2
        } else if (price < 500) {
          3
        } else if (price < 1000) {
          4
        } else {
          0
        }
        (id, cate, priceLevel)
      }
"
"udf/spark_repos_6/1_ethan0606_SparkApp/..src.test.scala.com.github.spark.feature.UserProfile.scala/udf/31.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val id = x.getString(0)
        val gender = if (x.getString(1) == null || x.getString(1).contains(""null"")) 0 else x.getString(1).toInt
        val age = if (x.getString(2) == null || x.getString(2).contains(""null"")) 0 else x.getString(2).toInt
        val pvalue = if (x.getString(3) == null || x.getString(3).contains(""null"")) 0 else x.getString(3).toInt
        val shop = if (x.getString(4) == null || x.getString(4).contains(""null"")) 0 else x.getString(4).toInt
        val occupation = if (x.getString(5) == null || x.getString(5).contains(""null"")) 0 else x.getString(5).toInt
        val new_user_class_level = if (x.getString(6) == null || x.getString(6).contains(""null"")) 0 else x.getString(6).toInt
        (id, gender, age, pvalue, shop, occupation, new_user_class_level)
      }
"
"udf/spark_repos_6/1_fnet123_piflow/..piflow-core.src.main.scala.cn.piflow.processor.ds.dataset.scala/udf/10.19.Dataset-X.map","Type: org.apache.spark.sql.Dataset[X]
Call: map

fn
"
"udf/spark_repos_6/1_fnet123_piflow/..piflow-core.src.main.scala.cn.piflow.processor.ds.dataset.scala/udf/45.22.Dataset-X.filter","Type: org.apache.spark.sql.Dataset[X]
Call: filter

fn
"
"udf/spark_repos_6/1_fnet123_piflow/..piflow-core.src.main.scala.cn.piflow.processor.ds.dataset.scala/udf/69.26.Dataset-X.filter","Type: org.apache.spark.sql.Dataset[X]
Call: filter

x._2
"
"udf/spark_repos_6/1_fnet123_piflow/..piflow-core.src.test.scala.cn.piflow.io.SparkTest.scala/udf/16.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

{ (x: Int) => 
          val t = x + 1
          println(t)
          t
        }
"
"udf/spark_repos_6/1_fnet123_piflow/..piflow-core.src.test.scala.cn.piflow.io.SparkTest.scala/udf/22.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

{ (x: Int) => {
        val t = x * 2
        println(t)
        t
      } }
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.ActorOperationDS.scala/udf/104.23.Dataset-(String, String, BigInt, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, String, BigInt, BigInt)]
Call: filter

""count = "" + minActorPerHourTypeAndRepo._4
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.ActorOperationDS.scala/udf/34.22.Dataset-(String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, BigInt)]
Call: filter

""count = "" + maxActorPerHour._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.ActorOperationDS.scala/udf/48.22.Dataset-(String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, BigInt)]
Call: filter

""count = "" + minActorPerHour._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.ActorOperationDS.scala/udf/62.22.Dataset-(String, String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, String, BigInt)]
Call: filter

""count = "" + maxActorPerHourAndType._3
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.ActorOperationDS.scala/udf/76.22.Dataset-(String, String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, String, BigInt)]
Call: filter

""count = "" + minActorPerHourAndType._3
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.ActorOperationDS.scala/udf/90.23.Dataset-(String, String, BigInt, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, String, BigInt, BigInt)]
Call: filter

""count = "" + maxActorPerHourTypeAndRepo._4
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.CommitOperationDS.scala/udf/51.22.Dataset-(String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, BigInt)]
Call: filter

""count = "" + maxCommitPerHour._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.CommitOperationDS.scala/udf/65.22.Dataset-(String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, BigInt)]
Call: filter

""count = "" + minCommitPerHour._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/103.22.Dataset-(String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, BigInt)]
Call: filter

""count = "" + minEventPerHour._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/117.23.Dataset-(BigInt, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, BigInt)]
Call: filter

""count = "" + maxEventPerRepo._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/131.23.Dataset-(BigInt, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, BigInt)]
Call: filter

""count = "" + minEventPerRepo._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/145.23.Dataset-(BigInt, String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, String, BigInt)]
Call: filter

""count = "" + maxEventPerActorAndHour._3
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/159.23.Dataset-(BigInt, String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, String, BigInt)]
Call: filter

""count = "" + minEventPerActorAndHour._3
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/173.23.Dataset-(BigInt, String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, String, BigInt)]
Call: filter

""count = "" + maxEventPerRepoAndHour._3
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/187.23.Dataset-(BigInt, String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, String, BigInt)]
Call: filter

""count = "" + minEventPerRepoAndHour._3
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/201.23.Dataset-(BigInt, BigInt, String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, BigInt, String, BigInt)]
Call: filter

""count = "" + maxEventPerActorRepoAndHour._4
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/215.23.Dataset-(BigInt, BigInt, String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, BigInt, String, BigInt)]
Call: filter

""count = "" + minEventPerActorRepoAndHour._4
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/61.22.Dataset-(BigInt, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, BigInt)]
Call: filter

""count = "" + maxEventPerActor._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/75.22.Dataset-(BigInt, BigInt).filter","Type: org.apache.spark.sql.Dataset[(BigInt, BigInt)]
Call: filter

""count = "" + minEventPerActor._2
"
"udf/spark_repos_6/1_GabrieleMoia_gitArchiveProject/..src.main.scala.dataset.dataset.EventOperationDS.scala/udf/89.22.Dataset-(String, BigInt).filter","Type: org.apache.spark.sql.Dataset[(String, BigInt)]
Call: filter

""count = "" + maxEventPerHour._2
"
"udf/spark_repos_6/1_Gelerion_spark-bin-packing-partitioner/..src.main.scala.com.gelerion.spark.bin.packing.partitioner.ScoreQueryRunner.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""similarity_level"" > 0.001d
"
"udf/spark_repos_6/1_Gelerion_spark-bin-packing-partitioner/..src.main.scala.com.gelerion.spark.bin.packing.partitioner.ScoreQueryRunner.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""topN"" <= 2
"
"udf/spark_repos_6/1_Gelerion_spark-bin-packing-partitioner/..src.main.scala.com.gelerion.spark.bin.packing.partitioner.service.repartition.Repartitioner.scala/udf/12.19.Dataset-EBooksUrls).map","Type: org.apache.spark.sql.Dataset[(com.gelerion.spark.bin.packing.partitioner.domain.model.BookshelfUrl, com.gelerion.spark.bin.packing.partitioner.domain.model.EBooksUrls)]
Call: map

{
        case (bookshelfUrl, ebookUrls) =>
          (bookshelfUrl, ebookUrls.totalTextSize.toLong)
      }
"
"udf/spark_repos_6/1_ghl7231699_bigdata/..src.main.java.sparksql.homework.ExampleTest.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_6/1_ghl7231699_bigdata/..src.main.java.sparksql.homework.PhoneSql.scala/udf/13.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => s.split(""/"")(0)
"
"udf/spark_repos_6/1_ghl7231699_bigdata/..src.main.java.sparksql.homework.PhoneSql.scala/udf/17.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => s.replaceAll(""¥"", """").toFloat
"
"udf/spark_repos_6/1_ghl7231699_bigdata/..src.main.java.sparksql.homework.PhoneSql.scala/udf/21.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => if (s == null) {
        0f
      } else {
        if (s.contains(""该款月成交 "")) {
          val str = s.replaceAll(""该款月成交 "", """")
          if (str.contains(""万笔"")) {
            str.replaceAll(""万笔"", ""0000"").toFloat
          } else if (s.contains(""笔"")) {
            str.replaceAll(""笔"", """").toFloat
          } else {
            0f
          }
        } else {
          0f
        }
      }
"
"udf/spark_repos_6/1_ghl7231699_bigdata/..src.main.java.sparksqljava.teacher.SparkSqlHiveTest.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          ""key:"" + key + "",value:"" + value
      }
"
"udf/spark_repos_6/1_ghl7231699_bigdata/..src.main.java.sparksqljava.teacher.ZipProblemSolution.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'state === ""CA""
"
"udf/spark_repos_6/1_ghl7231699_bigdata/..src.main.java.sparksqljava.teacher.ZipProblemSolution.scala/udf/34.22.Dataset-Zips.filter","Type: org.apache.spark.sql.Dataset[com.brave.sparksql.ZipProblemSolution.Zips]
Call: filter

'state === ""CA""
"
"udf/spark_repos_6/1_ghl7231699_bigdata/..src.main.java.sparksql.test.DataFramePractice.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") < 20
"
"udf/spark_repos_6/1_happycode4ever_spark/..spark-sql.src.main.scala.com.jj.spark.sql.SqlTransformTest.scala/udf/32.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getString(0), x.getInt(1))
"
"udf/spark_repos_6/1_happycode4ever_spark/..spark-sql.src.main.scala.com.jj.spark.sql.SqlTransformTest.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[String](""name"")
"
"udf/spark_repos_6/1_happycode4ever_spark/..spark-sql.src.main.scala.com.jj.spark.sql.SqlTransformTest.scala/udf/80.19.Dataset-People.map","Type: org.apache.spark.sql.Dataset[com.jj.spark.sql.SqlTransformTest.People]
Call: map

x => (x.name, x.age)
"
"udf/spark_repos_6/1_happycode4ever_spark/..spark-sql.src.main.scala.com.jj.spark.sql.UDAFWithSqlTest.scala/udf/43.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAverageWithSql
"
"udf/spark_repos_6/1_happycode4ever_spark/..spark-sql.src.main.scala.com.jj.spark.sql.UDFTest.scala/udf/10.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: String) => ""name:"" + x
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_6/1_hongs01_spark-example/..examples.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_6/1_hongwang_spark-etl/..src.main.scala.com.uuabc.etl.common.definitions.steps.input.CassandraInputStep.scala/udf/14.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

getOrElse(""whereCql"", whereCql)
"
"udf/spark_repos_6/1_hongwang_spark-etl/..src.main.scala.com.uuabc.etl.common.udfs.package.scala/udf/11.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

_UDFs(name)
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.BatchProcess.scala/udf/13.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'Country === ""United Kingdom""
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.BatchProcess.scala/udf/15.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""Quantity"" > 0
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.BatchProcess.scala/udf/17.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'InvoiceDate gt lit(""2010-12-09"")
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.BatchProcess.scala/udf/76.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

R_QuartileUDF
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.BatchProcess.scala/udf/84.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

F_QuartileUDF
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.BatchProcess.scala/udf/92.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

M_QuartileUDF
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.Example.UDFExample.scala/udf/12.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDFUtils.pointlessUDF(_: String): String
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.StockMACDStream.scala/udf/30.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ content => 
      val contents = content.split("","")
      val key = contents(0)
      val value = content
      val evenTime = Timestamp.valueOf(LocalDateTime.parse(s""${content(31)} ${content(32)}"", formatter)).toString
      spark.sql(s""INSERT INTO $hiveDb.stock_parameters VALUES ('$key', '$value')"")
      StockItem(key, Array(value), evenTime)
    }
"
"udf/spark_repos_6/1_izhangzhihao_data-process-tutorial/..src.main.scala.com.github.izhangzhihao.StockStream.scala/udf/21.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ content => 
      val key = content.split("","")(0)
      val value = content
      spark.sql(s""INSERT INTO $hiveDb.stock_parameters VALUES ('$key', '$value')"")
      val indexs: Seq[Stock] = List(Stock(key, ""AMO"", AmoStockIndex.getValue(value.split("","")), value.split("","")(32)), Stock(key, ""OBV"", ObvStockIndex.getValue(value.split("","")), value.split("","")(32)))
      indexs.toDF().createOrReplaceTempView(""stock_index_views"")
      spark.sql(s""INSERT INTO $hiveDb.stock_indexes SELECT * FROM stock_index_views"")
      indexs
    }
"
"udf/spark_repos_6/1_jaylynstoesz_query-module/..app.querymodule.dbconnection.SparkMaterializer.scala/udf/20.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filters.head.clause
"
"udf/spark_repos_6/1_jaylynstoesz_query-module/..app.querymodule.dbconnection.SparkMaterializer.scala/udf/31.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$col IS NOT NULL AND $col > 0""
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.dataframes.DataFrameTransformationExample.scala/udf/12.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.dataframes.DataFrameTransformationExample.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Company"").startsWith(""T"")
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.dataframes.DataFrameUDFExample.scala/udf/12.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.dataframes.DataFrameWriteExample.scala/udf/11.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetJoinExample.scala/udf/13.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetTransformationExample.scala/udf/13.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetTransformationExample.scala/udf/18.19.Dataset-User.map","Type: org.apache.spark.sql.Dataset[com.thoughtworks.schemas.User]
Call: map

_.Name
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetTransformationExample.scala/udf/22.19.Dataset-User.map","Type: org.apache.spark.sql.Dataset[com.thoughtworks.schemas.User]
Call: map

user => (user.Name, user.Company)
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetTransformationExample.scala/udf/32.19.Dataset-User.map","Type: org.apache.spark.sql.Dataset[com.thoughtworks.schemas.User]
Call: map

_.Name.toUpperCase
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetTransformationExample.scala/udf/36.19.Dataset-User.map","Type: org.apache.spark.sql.Dataset[com.thoughtworks.schemas.User]
Call: map

user => if (user.Name.startsWith(""J"")) ""Nice Name"" else ""Whatever""
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetTransformationExample.scala/udf/43.22.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[com.thoughtworks.schemas.User]
Call: filter

_.Company.startsWith(""T"")
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetUDFExample.scala/udf/14.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.datasets.DatasetUDFExample.scala/udf/20.22.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[com.thoughtworks.schemas.User]
Call: filter

user => user.Company.startsWith(""T"")
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.exercises.streaming.NeatTotal.scala/udf/17.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.parseTimestampWithTimezone(_: String): Long
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.kafka.SparkKafkaExample.scala/udf/13.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.parseTimestampWithTimezone(_: String): Long
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.sql.SqlAggregationExample.scala/udf/8.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.sql.SqlTransformationExample.scala/udf/11.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.sql.SqlUDFExample.scala/udf/8.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.startsWith(_: String, _: String): Boolean
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.streaming.StructuredStreamingExample.scala/udf/17.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.reverse(_: String): String
"
"udf/spark_repos_6/1_jpaulorio_spark-training/..src.main.scala.com.thoughtworks.streaming.StructuredStreamingWindowExample.scala/udf/11.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.parseTimestampWithTimezone(_: String): Long
"
"udf/spark_repos_6/1_lyubent_LeagueToxicity/..src.main.scala.io.github.lyubent.DynamicMatchQuery.scala/udf/22.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""matchMode"" === ""CLASSIC""
"
"udf/spark_repos_6/1_lyubent_LeagueToxicity/..src.main.scala.io.github.lyubent.DynamicMatchQuery.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""queueType"" === ""RANKED_FLEX_SR""
"
"udf/spark_repos_6/1_lyubent_LeagueToxicity/..src.main.scala.io.github.lyubent.DynamicMatchQuery.scala/udf/32.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""matchMode"" === ""CLASSIC""
"
"udf/spark_repos_6/1_lyubent_LeagueToxicity/..src.main.scala.io.github.lyubent.DynamicMatchQuery.scala/udf/34.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""queueType"" === ""TEAM_BUILDER_RANKED_SOLO""
"
"udf/spark_repos_6/1_lyubent_LeagueToxicity/..src.main.scala.io.github.lyubent.StaticGame.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Winner""
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.utility.GHCNData.scala/udf/26.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

apply
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.utility.GHCNStation.scala/udf/27.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

apply
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.GHCNStationsColor2018.scala/udf/20.22.Dataset-GHCNData.filter","Type: org.apache.spark.sql.Dataset[utility.GHCNData]
Call: filter

_.element == ""TMAX""
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SimpleAppSQL.scala/udf/10.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""val"")
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/24.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != firstLine
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/26.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

CensusData.parseLine
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/31.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/36.21.Dataset-CensusData.map","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: map

_.age
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/42.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.age >= 50
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/46.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/50.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.maritalStatus == ""Married-civ-spouse""
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/54.22.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.incomeOver50
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/58.20.Dataset-CensusData.map","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: map

_.age
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataA.scala/udf/69.23.Dataset-CensusData.filter","Type: org.apache.spark.sql.Dataset[utility.CensusData]
Call: filter

_.hoursPerWeek > 40
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/14.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""income"" === "">50K""
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/20.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'age >= 50
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'income === "">50K""
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'maritalStatus === ""Married-civ-spouse""
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'income === "">50K""
"
"udf/spark_repos_6/1_MarkCLewis_WorldCongress2019/..scala.src.main.scala.withsql.SQLCensusDataB.scala/udf/43.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

'hoursPerWeek > 40
"
"udf/spark_repos_6/1_mazaytsev_big-data-processing/..src.main.java.practice.practice6.MovieRecomendationSystem.scala/udf/30.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseMovie
"
"udf/spark_repos_6/1_mazaytsev_big-data-processing/..src.main.java.practice.practice6.MovieRecomendationSystem.scala/udf/34.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/1_mazaytsev_big-data-processing/..src.main.java.practice.practice6.MovieRecomendationSystem.scala/udf/38.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/1_mazaytsev_big-data-processing/..src.main.java.practice.practice6.MovieRecomendationSystem.scala/udf/57.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Movie(r.getInt(1), """", r.getFloat(2))
"
"udf/spark_repos_6/1_medale_little-scala-course/..src.main.scala.scalatour.Spark15.scala/udf/16.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[scalatour.Person]
Call: filter

p => p.age < 40
"
"udf/spark_repos_6/1_medvekoma_spark-workshop/..src.main.scala.com.uniobuda.sparkworkshop.part3.DatasetJob.scala/udf/9.22.Dataset-Laureate.filter","Type: org.apache.spark.sql.Dataset[com.uniobuda.sparkworkshop.part3.Laureate]
Call: filter

_.bornCountryCode == ""HU""
"
"udf/spark_repos_6/1_mlesniak_github-downloader/..src.main.scala.com.mlesniak.github.Processing.scala/udf/19.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""actor.login = '$login'""
"
"udf/spark_repos_6/1_MuziMin0222_AnalysisOfUserBehaviors/..src.main.scala.analysis.product.Function.AreaTop3ProductFunc.scala/udf/13.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(v1: Long, v2: String, split: String) => v1.toString + split + v2
"
"udf/spark_repos_6/1_MuziMin0222_AnalysisOfUserBehaviors/..src.main.scala.analysis.product.Function.AreaTop3ProductFunc.scala/udf/17.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => {
        val jsonObject = JSONObject.fromObject(json)
        jsonObject.getString(field)
      }
"
"udf/spark_repos_6/1_MuziMin0222_AnalysisOfUserBehaviors/..src.main.scala.analysis.product.Function.AreaTop3ProductFunc.scala/udf/24.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new GroupConcatDistinctUDAF
"
"udf/spark_repos_6/1_myflash163_spark-181205/..src.main.scala.com.atguigu.bigdata.spark.sql.SparkSQL05_UDAF.scala/udf/14.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udaf
"
"udf/spark_repos_6/1_pengzhaopeng_SparkClusterTest/..src.main.scala.cn.pengzhaopeng.spark.SparkSQL.ipLocation.IpLocationSQL1.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields: Array[String] = line.split(""[|]"")
        val startNum: Long = fields(2).toLong
        val endNum: Long = fields(3).toLong
        val province: String = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_6/1_pengzhaopeng_SparkClusterTest/..src.main.scala.cn.pengzhaopeng.spark.SparkSQL.ipLocation.IpLocationSQL1.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields: Array[String] = log.split(""[|]"")
        val ip: String = fields(1)
        val ipNum: Long = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_6/1_pengzhaopeng_SparkClusterTest/..src.main.scala.cn.pengzhaopeng.spark.SparkSQL.ipLocation.IpLocationSQL2.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_6/1_pengzhaopeng_SparkClusterTest/..src.main.scala.cn.pengzhaopeng.spark.SparkSQL.ipLocation.IpLocationSQL2.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_6/1_pengzhaopeng_SparkClusterTest/..src.main.scala.cn.pengzhaopeng.spark.SparkSQL.ipLocation.IpLocationSQL2.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(ipNum: Long) => {
        val ipRulesInExcutor: Array[(Long, Long, String)] = broadcastRef.value
        val index: Int = MyUtils.binarySearch(ipRulesInExcutor, ipNum)
        var province = ""未知""
        if (index != -1) {
          province = ipRulesInExcutor(index)._3
        }
        province
      }
"
"udf/spark_repos_6/1_pengzhaopeng_SparkClusterTest/..src.main.scala.cn.pengzhaopeng.spark.SparkSQL.join.JoinTest.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

nation => {
        val fields: Array[String] = nation.split("","")
        val ename: String = fields(0)
        val cname: String = fields(1)
        (ename, cname)
      }
"
"udf/spark_repos_6/1_pengzhaopeng_SparkClusterTest/..src.main.scala.cn.pengzhaopeng.spark.SparkSQL.join.JoinTest.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields: Array[String] = line.split("","")
        val id: String = fields(0)
        val name: String = fields(1)
        val nationCode: String = fields(2)
        (id, name, nationCode)
      }
"
"udf/spark_repos_6/1_pengzhaopeng_SparkClusterTest/..src.main.scala.cn.pengzhaopeng.spark.SparkSQL.topTearcher.SQLFavTeacher.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val tIndex: Int = line.lastIndexOf(""/"") + 1
        val teacher: String = line.substring(tIndex)
        val host: String = new URL(line).getHost
        val sIndex: Int = host.indexOf(""."")
        val subject: String = host.substring(0, sIndex)
        (subject, teacher)
      }
"
"udf/spark_repos_6/1_proffesboris_TheBiggestSparkRep/..Code.HiveContext.Basis_tran.src.main.scala.ru.sberbank.sdcb.k7m.core.pack.BasisTrOutClass.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(s""$krasField"").isNull
"
"udf/spark_repos_6/1_proffesboris_TheBiggestSparkRep/..Code.HiveContext.exsgen.src.main.scala.ru.sberbank.sdcb.k7m.core.pack.exsGenClass.scala/udf/40.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

readHiveTable22(""client_id"").notEqual(readHiveTable28(""from_eks_id""))
"
"udf/spark_repos_6/1_proffesboris_TheBiggestSparkRep/..Code.HiveContext.exsgen.src.main.scala.ru.sberbank.sdcb.k7m.core.pack.exsGenClass.scala/udf/42.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

to_date(readHiveTable28(""from_class"")).notEqual(lit(s""$clPriv"")) || readHiveTable28(""from_inn"").notEqual(readHiveTable21(""to_inn""))
"
"udf/spark_repos_6/1_proffesboris_TheBiggestSparkRep/..Code.HiveContext.exsgen.src.main.scala.ru.sberbank.sdcb.k7m.core.pack.exsGenClass.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

readExsTableCLU(""from_class"").notEqual(lit(clPriv))
"
"udf/spark_repos_6/1_proffesboris_TheBiggestSparkRep/..Code.HiveContext.exsgen.src.main.scala.ru.sberbank.sdcb.k7m.core.pack.exsGenClass.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

readExsTableCLF(""from_class"").like(s""$clPriv"")
"
"udf/spark_repos_6/1_proffesboris_TheBiggestSparkRep/..Code.HiveContext.set.src.main.scala.ru.sberbank.sdcb.k7m.core.pack.CalcClass.scala/udf/45.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""ro_min"" === ($""ro"")
"
"udf/spark_repos_6/1_proffesboris_TheBiggestSparkRep/..Code.HiveContext.VDOKROut.src.main.scala.ru.sberbank.sdcb.k7m.core.pack.CL.scala/udf/151.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""clf_from"").isNotNull or col(""psi_from"").isNotNull
"
"udf/spark_repos_6/1_RamGhadiyaram_spark-drools-playground/..src.main.scala.com.examples.DistributedDataIndex.scala/udf/78.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

indexedUniqueCategories(""uniquecategory"") === categoryName
"
"udf/spark_repos_6/1_RaulEstrada_Spark-MammographicMass/..src.main.scala.ml.Ensembler.scala/udf/37.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => ((row.getDouble(0), row.getAs[DenseVector](1)), Array[Double](row.getDouble(2)))
        }
"
"udf/spark_repos_6/1_RocJeMaintiendrai_Spark-Demo/..src.main.scala.com.pgone.SparkSQL04.PvUvApp.scala/udf/29.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: String) => x.split("","").size
"
"udf/spark_repos_6/1_srp19_spark-scala/..SparkScala.src.com.srp.spark.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.srp.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_6/1_sumitya_apache-spark-examples/..src.main.scala.definitiveguide.aggregations_7.DataFrameAggregates.scala/udf/53.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ba
"
"udf/spark_repos_6/1_sumitya_apache-spark-examples/..src.main.scala.definitiveguide.structuredapioverview_4.DataFrameReader.scala/udf/34.20.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[definitiveguide.structuredapioverview_4.DataFrameReader.User]
Call: filter

_.NAME == ""abc""
"
"udf/spark_repos_6/1_sumitya_apache-spark-examples/..src.main.scala.definitiveguide.workingwithdifferentdata_6.TypesOfData.scala/udf/58.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row != header
"
"udf/spark_repos_6/1_sumitya_apache-spark-examples/..src.main.scala.definitiveguide.workingwithdifferentdata_6.TypesOfData.scala/udf/69.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

power3(_: Double): Double
"
"udf/spark_repos_6/1_tenkeiu8_spark-streaming-jdbc/..src.main.scala.org.apache.spark.sql.execution.streaming.sources.JDBCStreamSourceProvider.scala/udf/25.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(offsetColumn).between(startOffset, endOffset)
"
"udf/spark_repos_6/1_unmeshjoshi_hbaseservices/..spark.src.main.scala.com.financialservices.spark.streaming.SparkStructuredStreaming.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val xmlMessage = row.getAs[String](1)
        import com.thoughtworks.xstream.XStream
        import com.thoughtworks.xstream.io.xml.StaxDriver
        val xstream = new XStream(new StaxDriver)
        xstream.alias(""account"", classOf[Account])
        val accountMessage = xstream.fromXML(xmlMessage).asInstanceOf[Account]
        AccountPosition(accountMessage.num, accountMessage.accountKey, accountMessage.amount, accountMessage.date, """")
      }
"
"udf/spark_repos_6/1_vcharvet_density-based_clustering/..src.main.scala.clustering.MutualReachability.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val i = row.getAs[Long](iCol)
        val j = row.getAs[Long](jCol)
        val dist = row.getAs[Double](distCol)
        val coreDistI = row.getAs[Double](coreDistICol)
        val coreDistJ = row.getAs[Double](coreDistJCol)
        val mReach = List(dist, coreDistI, coreDistJ).max
        Edge(i, j, mReach)
      }
"
"udf/spark_repos_6/1_vcharvet_density-based_clustering/..src.main.scala.clustering.Neighbors.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(idCol) < col(idCol + ""_2"")
"
"udf/spark_repos_6/1_vcharvet_density-based_clustering/..src.main.scala.clustering.Neighbors.scala/udf/34.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

if (filter) $""df1.$idCol"" < ($""df2.$idCol"") else $""df1.$idCol"" =!= ($""df2.$idCol"")
"
"udf/spark_repos_6/1_vcharvet_density-based_clustering/..src.main.scala.clustering.Neighbors.scala/udf/51.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

$""distCol"" =!= 0d
"
"udf/spark_repos_6/1_vidma_snb-tests/..simple-spark.src.main.scala.App.scala/udf/62.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

transform
"
"udf/spark_repos_6/1_vidma_snb-tests/..simple-spark.src.main.scala.App.scala/udf/68.21.Dataset-(Int, Int).map","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: map

_._2
"
"udf/spark_repos_6/1_vineethMM_spark-examples/..src.main.scala.com.study.spark.df.DateFrameSQL.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => Person(r.getAs[String](""name""), r.getAs[String](""gender""), r.getAs[Int](""age""))
"
"udf/spark_repos_6/1_vineethMM_spark-examples/..src.main.scala.com.study.spark.ds.DataSetSQL.scala/udf/22.21.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[com.study.spark.df.Person]
Call: map

_.age
"
"udf/spark_repos_6/1_vineethMM_spark-examples/..src.main.scala.com.study.spark.ds.DataSetSQL.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.study.spark.df.Person]
Call: filter

_.age == minAge
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/108.22.Dataset-TripleFloat.map","Type: org.apache.spark.sql.Dataset[TripleFloat]
Call: map

{
          (x: (Float, Float, Float)) => sqrt(x._1)
        }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/128.25.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

_.muons.length > 2
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/133.25.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

_.muons.length == 1
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/138.25.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

_.muons.length == 2
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/146.24.Dataset-(Int, Long).map","Type: org.apache.spark.sql.Dataset[(Int, Long)]
Call: map

_._2
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/155.25.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

{
          (e: Event) => e.muons.length == 2
        }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/165.27.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

{
            (e: Event) => e.muons.length == 2
          }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/179.27.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

{
            (e: Event) => e.muons.length == 2
          }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/192.29.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

{
              (e: Event) => e.muons.length == 2
            }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/198.27.Dataset-RecoLeafCandidate.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.defs.aodpublic.RecoLeafCandidate]
Call: filter

{
            (m: RecoLeafCandidate) => m.pt > 10.0d && m.eta < 2.4d
          }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/209.29.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

{
              (e: Event) => e.muons.length >= 2
            }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/213.24.Dataset-Event.map","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: map

{ (e: Event) => 
            val muons = e.muons.sortWith(_.pt > _.pt)
            buildDiCandidate(muons(0), muons(1))
          }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/227.29.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

{
              (e: Event) => e.muons.length >= 2
            }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/231.24.Dataset-Event.map","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: map

{ (e: Event) => 
            val muons = e.muons.sortWith(_.pt > _.pt)
            buildDiCandidate(muons(0), muons(1))
          }
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/278.27.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

_.muons.length > 0
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/288.27.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

_.muons.length == 2
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/298.27.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

_.muons.length >= 2
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/311.27.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

_.muons.length > 0
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/321.27.Dataset-Event.filter","Type: org.apache.spark.sql.Dataset[org.dianahep.sparkrootapplications.benchmarks.Event]
Call: filter

_.muons.length >= 2
"
"udf/spark_repos_6/1_vkhristenko_spark-root-applications/..src.main.scala.org.dianahep.sparkrootapplications.benchmarks.AODPublicBenchmarkApp.scala/udf/94.24.Dataset-TripleFloat.map","Type: org.apache.spark.sql.Dataset[TripleFloat]
Call: map

{
            (x: (Float, Float, Float)) => sqrt(x._1)
          }
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/101.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(colName).isNull
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/119.45.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!col(colName).rlike(""Поездки, доставка, хранение"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/121.43.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Супермаркеты и продуктовые магазины"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/123.41.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Телефония и интернет"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/125.39.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Всё остальное"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/127.37.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Интернет-магазины"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/129.35.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Всё для дома"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/131.33.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Общепит"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/133.31.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Одежда, обувь и аксессуары"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/135.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Медицина и косметика"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/137.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Личный транспорт"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/139.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Оплата счетов"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/141.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!col(colName).rlike(""Дети и животные"")
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/74.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r != row
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/82.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(colName).eqNullSafe(f)
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/87.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(colName).isNull
"
"udf/spark_repos_6/1_VyunSergey_census-brands-clusters/..src.main.scala.com.vyunsergey.SparkApp.scala/udf/96.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(colName).rlike(f)
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.log.TopNStatJob2.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.log.TopNStatJob.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.log.TopNStatJob.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.log.TopNStatJob.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.log.TopNStatJobYARN.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.log.TopNStatJobYARN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.log.TopNStatJobYARN.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""cmsType"" === ""video""
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopleDF.col(""age"") > 19
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.spark.DataFrameRDDApp.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.spark.DataFrameRDDApp.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

infoDF.col(""age"") > 30
"
"udf/spark_repos_6/1_wc1993524_ETL_SparkSQL/..src.main.scala.com.imooc.spark.DatasetApp.scala/udf/12.19.Dataset-Student.map","Type: org.apache.spark.sql.Dataset[com.imooc.spark.DatasetApp.Student]
Call: map

line => line.xm
"
"udf/spark_repos_6/1_windyzj_sparkmall0901/..sparkmall-offline.src.main.scala.com.atguigu.sparkmall0901.offline.app.AreaCountApp.scala/udf/13.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityRatioUDAF
"
"udf/spark_repos_6/1_xyang4_hadoop-learn/..spark.src.main.scala.com.example.spark.SimpleApp.scala/udf/11.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""a"")
"
"udf/spark_repos_6/1_xyang4_hadoop-learn/..spark.src.main.scala.com.example.spark.SimpleApp.scala/udf/15.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => line.contains(""b"")
"
"udf/spark_repos_6/1_xyang4_hadoop-learn/..spark.src.main.scala.com.example.spark.sql.UDF.scala/udf/15.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: String) => ""Name:"" + x
"
"udf/spark_repos_6/1_xyang4_hadoop-learn/..spark.src.main.scala.com.example.spark.sql.UDF.scala/udf/20.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAgeAvgFunction
"
"udf/spark_repos_6/1_yennanliu_spark_emr_dev/..src.main.scala.EmrHelloworld.emr_hive_IO_demo.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_6/1_yennanliu_spark_emr_dev/..src.main.scala.NYCtlcTaxiETL.GetPickupZoneStats.scala/udf/27.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

addOneUdf
"
"udf/spark_repos_6/1_yennanliu_spark_emr_dev/..src.main.scala.NYCtlcTaxiETL.GetPickupZoneStats.scala/udf/31.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

squared
"
"udf/spark_repos_6/1_yennanliu_spark_emr_dev/..src.main.scala.NYCtlcTaxiETL.GetPickupZoneStats.scala/udf/35.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

divided
"
"udf/spark_repos_6/1_yennanliu_spark_emr_dev/..src.main.scala.NYCtlcTaxiETL.GetTaxiValueZones.scala/udf/31.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Trip_Pickup_DateTime"").gt(""2017"")
"
"udf/spark_repos_6/1_yennanliu_spark_emr_dev/..src.main.scala.NYCtlcTaxiETL.GetTaxiValueZones.scala/udf/33.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""Trip_Dropoff_DateTime"").lt(""2019"")
"
"udf/spark_repos_6/1_ytake_training-scala-es-spark-als/..src.main.scala.net.jp.ytake.SparkRecommenderApp.scala/udf/28.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

extractYear _
"
"udf/spark_repos_6/1_ytake_training-scala-es-spark-als/..src.main.scala.net.jp.ytake.SparkRecommenderApp.scala/udf/32.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

extractGenres
"
"udf/spark_repos_6/1_yukochperdok_Course-Coursera-Spark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/102.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TimeUsageRow(row.getAs(""workingStatus""), row.getAs(""sex""), row.getAs(""age""), row.getAs(""primaryNeeds""), row.getAs(""work""), row.getAs(""other""))
"
"udf/spark_repos_6/1_yukochperdok_Course-Coursera-Spark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/108.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, primaryNeeds, work, other)
      }
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.people.census.ireland.Irish1901App.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""name"").rlike(""^([A-Z])+$"")
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.people.census.ireland.Irish1901App.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""gender"").rlike(""^[MF]$"")
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.people.census.ireland.Irish1911App.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""name"").rlike(""^([A-Z])+$"")
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.people.census.ireland.Irish1911App.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""gender"").rlike(""^[MF]$"")
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.places.grammar.GrBuilder.scala/udf/71.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fc(col(""name""), col(""total""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.places.grammar.GrBuilder.scala/udf/76.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

terminusUDF(col(""name""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.places.grammar.GrBuilder.scala/udf/80.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

remainderUDF(col(""name""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.places.PlaceGrammar.scala/udf/100.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

startFilterUDF(col(""name""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.places.PlaceGrammar.scala/udf/109.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

startFilterUDF(col(""name""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.ships.wiki.rn.RoyalNavySchema.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

servedInWWII(col(""name""), col(""classAndTypeDesc""), col(""startDate""), col(""endDate""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-people.src.main.scala.com.szadowsz.cadisainmduit.ships.wiki.us.UsNavySchema.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

servedInWWII(col(""name""), col(""classAndTypeDesc""), col(""startDate""), col(""endDate""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-ships.src.main.scala.com.szadowsz.cadisainmduit.ships.wiki.rn.RoyalNavySchema.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

servedInWWII(col(""name""), col(""classAndTypeDesc""), col(""startDate""), col(""endDate""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.dataprep-ships.src.main.scala.com.szadowsz.cadisainmduit.ships.wiki.us.UsNavySchema.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

servedInWWII(col(""name""), col(""classAndTypeDesc""), col(""startDate""), col(""endDate""))
"
"udf/spark_repos_6/1_zakski_project-cadisainmduit/..module.spark.src.main.scala.com.szadowsz.spark.ml.feature.RegexValidator.scala/udf/27.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

sqlfunc(col($(inputCol)))
"
"udf/spark_repos_6/1_zhenchao125_spark1015/..spark-sql.src.main.scala.com.atguigu.spark.sql.day01.udf.UDAFDemo.scala/udf/12.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAvg
"
"udf/spark_repos_6/1_zhenchao125_spark1015/..spark-sql.src.main.scala.com.atguigu.spark.sql.project.SqlApp.scala/udf/8.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityRemarkUDAF
"
"udf/spark_repos_6/1_zhiyang-liu_spark-demo/..src.main.scala.com.example.sparksql.SqlTest.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: String).length
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/11.21.Dataset-MemberWideTable.map","Type: org.apache.spark.sql.Dataset[com.atguigu.bean.MemberWideTable]
Call: map

item => (item.appregurl + ""_"" + item.dt + ""_"" + item.dn, 1)
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/13.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val dataArr = item._1.split(""_"")
        (dataArr(0), item._2, dataArr(1), dataArr(2))
      }
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/21.21.Dataset-MemberWideTable.map","Type: org.apache.spark.sql.Dataset[com.atguigu.bean.MemberWideTable]
Call: map

item => (item.sitename + ""_"" + item.dt + ""_"" + item.dn, 1)
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/23.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

{
        case (key: String, count: Int) =>
          val dataArr = key.split(""_"")
          (dataArr(0), count, dataArr(1), dataArr(2))
      }
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/32.21.Dataset-MemberWideTable.map","Type: org.apache.spark.sql.Dataset[com.atguigu.bean.MemberWideTable]
Call: map

item => (item.regsourcename + ""_"" + item.dt + ""_"" + item.dn, 1)
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/34.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

{
        case (key: String, count: Int) =>
          val dataArr = key.split(""_"")
          (dataArr(0), count, dataArr(1), dataArr(2))
      }
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/43.21.Dataset-MemberWideTable.map","Type: org.apache.spark.sql.Dataset[com.atguigu.bean.MemberWideTable]
Call: map

item => (item.adname + ""_"" + item.dt + ""_"" + item.dn, 1)
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/45.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

{
        case (key: String, count: Int) =>
          val dataArr = key.split(""_"")
          (dataArr(0), count, dataArr(1), dataArr(2))
      }
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/54.22.Dataset-MemberWideTable.map","Type: org.apache.spark.sql.Dataset[com.atguigu.bean.MemberWideTable]
Call: map

item => (item.memberlevel + ""_"" + item.dt + ""_"" + item.dn, 1)
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/56.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

{
        case (key: String, count: Int) =>
          val dataArr = key.split(""_"")
          (dataArr(0), count, dataArr(1), dataArr(2))
      }
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/65.22.Dataset-MemberWideTable.map","Type: org.apache.spark.sql.Dataset[com.atguigu.bean.MemberWideTable]
Call: map

item => (item.vip_level + ""_"" + item.dt + ""_"" + item.dn, 1)
"
"udf/spark_repos_6/1_ZYZ0001_Spark_Datahouse_OffLine_Realtime/..register.src.main.scala.com.atguigu.server.DWSToADS.scala/udf/67.20.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

{
        case (key: String, count: Int) =>
          val dataArr = key.split(""_"")
          (dataArr(0), count, dataArr(1), dataArr(2))
      }
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..hive-practise.2-hive-course.src.main.scala.com.westar.ALSExample.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..hive-practise.5-spark-nba-player.src.main.scala.com.westar.AgeAndExpTrendAnalysis.scala/udf/24.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => AgeOrExpStats(row.getAs[String](""name""), row.getAs[Int](""year""), row.getAs[Int](""age""), row.getAs[Int](""exp""), row.getAs[Double](""zTOT""), row.getAs[Double](""nTOT""))
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..hive-practise.spark-sql-hive.src.main.scala.com.westar.sql.hive.DomainScoresCalculator.scala/udf/14.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(cookieLabel: String) => getCookieLableScores(cookieLabel)
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..hive-practise.spark-sql-hive.src.main.scala.com.westar.sql.hive.example.UserMovieRatingEtl.scala/udf/24.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(url: String) => if (StringUtils.isNotEmpty(url)) {
        URLDecoder.decode(url, ""UTF-8"")
      } else {
        url
      }
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..hive-practise.spark-sql-hive.src.main.scala.com.westar.sql.hive.SparkSQLSupportHiveClusterTest.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(cookie: String, count: Long) =>
          s""cookie: $cookie, count: $count""
      }
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..hive-practise.spark-sql-hive.src.main.scala.com.westar.sql.hive.SparkSQLSupportHiveLocalTest.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(cookie: String, count: Long) =>
          s""cookie: $cookie, count: $count""
      }
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.1-spark-wordcount.src.main.scala.com.westar.ALSExample.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.2-spark-introduce.src.main.scala.com.westar.spark.rdd.DatasetTest.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.2-spark-introduce.src.main.scala.com.westar.spark.rdd.DatasetTest.scala/udf/37.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.westar.spark.rdd.DatasetTest.Person]
Call: filter

person => if (person.age.isDefined && person.age.get > 21) {
        true
      } else {
        logger.info(s""======= my test filter ${person.age}"")
        false
      }
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.SparkSessionTest.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

personDF.col(""age"") > 20
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.SparkSqlDabblerFirst.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 23
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.SparkSqlDabblerFirst.scala/udf/23.19.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[com.westar.dataset.Person]
Call: map

p => p.name
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.SparkSqlDabblerFirst.scala/udf/27.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.westar.dataset.Person]
Call: filter

p => p.age > 20
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.SparkSqlDabbler.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.SparkSqlDabbler.scala/udf/39.19.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[com.westar.dataset.Person]
Call: map

p => if (p.age > 10) {
        Person(p.name, 10)
      } else p
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.BasicSQLTest.scala/udf/29.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arg1: Int, arg2: String) => if (arg1 > 1 && arg2.equals(""固执"")) {
        arg2 + arg1
      } else {
        arg2 + ""less""
      }
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.example.als.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.example.complextype.JsonSQLFunctionTest.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""devices.temp"" > 10 and $""devices.signal"" > 15
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.udaf.TypedApiTest.scala/udf/28.21.Dataset-TrackerSession.map","Type: org.apache.spark.sql.Dataset[com.westar.dataset.TrackerSession]
Call: map

_.cookie
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.udaf.TypedApiTest.scala/udf/30.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != ""cookie1""
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.udaf.UntypedUDAFTest.scala/udf/8.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UntypedMyAverage
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/13.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"" === ""固执""
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/28.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arg1: Int, arg2: String) => arg2 + arg1
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/50.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""pageview_count"".between(0, 2)
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/54.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"".isNaN
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/58.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"".isNull
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/62.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"".isNotNull
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/66.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"" === ""固执"" || $""pageview_count"" === 1
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/70.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"" === ""固执"" && $""pageview_count"" === 1
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/74.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!($""cookie_label"" === ""固执"")
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/83.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"" isin (""固执"", ""执着"", ""不可理喻"")
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/87.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"" like ""%固执%""
"
"udf/spark_repos_6/1_zzqliwei_hadoopProject/..spark-parctise.9-spark-dataset.src.main.scala.com.westar.dataset.sql.usage.UntypedApiTest.scala/udf/91.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""cookie_label"" rlike ""%固执%""
"
"udf/spark_repos_6/20_godatadriven_scala-spark-application/..src.main.scala.thw.vancann.WordCount.scala/udf/17.28.Dataset-ChatLog.filter","Type: org.apache.spark.sql.Dataset[thw.vancann.storage.ChatLog]
Call: filter

$""date"" === date
"
"udf/spark_repos_6/20_godatadriven_scala-spark-application/..src.main.scala.thw.vancann.WordCount.scala/udf/22.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => (x, 1)
"
"udf/spark_repos_6/20_godatadriven_scala-spark-application/..src.main.scala.thw.vancann.WordCount.scala/udf/24.19.Dataset-(String, (String, Int)).map","Type: org.apache.spark.sql.Dataset[(String, (String, Int))]
Call: map

x => WordCountSchema(x._1, x._2._2)
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.BroadcastExample.scala/udf/14.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
      val country = row.getString(2)
      val state = row.getString(3)
      val fullCountry = broadcastCountries.value.get(country).get
      val fullState = broadcastStates.value.get(state).get
      (row.getString(0), row.getString(1), fullCountry, fullState)
    }
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.DataFrameWithSimpleDSL.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""State"") === ""PR""
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.FilterExample.scala/udf/14.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""state"") === ""OH""
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.FilterExample.scala/udf/22.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""state"") === ""OH"" && df(""gender"") === ""M""
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.FilterExample.scala/udf/26.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

array_contains(df(""languages""), ""Java"")
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.FilterExample.scala/udf/30.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""name.lastname"") === ""Williams""
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.FromTextFile.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => {
        val elements = f.getString(0).split("","")
        (elements(0), elements(1))
      }
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.FromTextFile.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

f => {
        val elements = f.split("","")
        (elements(0), elements(1))
      }
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.join.InnerJoinExample.scala/udf/22.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

empDF(""emp_dept_id"") === deptDF(""dept_id"")
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.join.JoinMultipleColumns.scala/udf/18.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

empDF(""dept_id"") === deptDF(""dept_id"") && empDF(""branch_id"") === deptDF(""branch_id"")
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.join.JoinMultipleDataFrames.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

empDF(""emp_dept_id"") === deptDF(""dept_id"")
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.join.JoinMultipleDataFrames.scala/udf/25.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

empDF(""emp_id"") === addDF(""emp_id"")
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.join.SelfJoinExample.scala/udf/18.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""emp1.superior_emp_id"") === col(""emp2.emp_id"")
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.UDFDataFrame.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

to_date(replaceUDF($""date1"")) > date_add(to_date(replaceUDF(lit(minDate))), 7)
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataframe.WithColumn.scala/udf/31.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

f => {
        val nameSplit = f.getAs[String](0).split("","")
        val addSplit = f.getAs[String](1).split("","")
        (nameSplit(0), nameSplit(1), addSplit(0), addSplit(1), addSplit(2), addSplit(3))
      }
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.dataset.xml.ReadBooksXML.scala/udf/10.19.Dataset-Books.map","Type: org.apache.spark.sql.Dataset[com.sparkbyexamples.spark.beans.Books]
Call: map

f => BooksDiscounted(f._id, f.author, f.description, f.price, f.publish_date, f.title, f.price - f.price * 20 / 100)
"
"udf/spark_repos_6/20_spark-examples_spark-scala-examples/..src.main.scala.com.sparkbyexamples.spark.stackoverflow.AddingLiterral.scala/udf/17.19.Dataset-Employee.map","Type: org.apache.spark.sql.Dataset[com.sparkbyexamples.spark.stackoverflow.Employee]
Call: map

rec => (EmpData(""1"", rec.EmpId), EmpData(""2"", rec.Experience.toString), EmpData(""3"", rec.Salary.toString))
"
"udf/spark_repos_6/21_spoddutur_graph-knowledge-browser/..src.main.scala.com.spoddutur.CountryApiService.scala/udf/26.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.mkString("","")
"
"udf/spark_repos_6/22_dnvriend_apache-spark-test/..jobs.src.test.scala.com.github.dnvriend.spark.dataframe.DataFrameWordCountTest.scala/udf/24.22.Dataset-(String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: filter

lower('word) === ""alice""
"
"udf/spark_repos_6/22_dnvriend_apache-spark-test/..jobs.src.test.scala.com.github.dnvriend.spark.dataframe.DataFrameWordCountTest.scala/udf/28.22.Dataset-(String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: filter

lower('word) === ""queen""
"
"udf/spark_repos_6/22_dnvriend_apache-spark-test/..jobs.src.test.scala.com.github.dnvriend.spark.dataframe.DataFrameWordCountTest.scala/udf/32.22.Dataset-(String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: filter

lower('word) === ""rabbit""
"
"udf/spark_repos_6/22_dnvriend_apache-spark-test/..jobs.src.test.scala.com.github.dnvriend.spark.dataframe.DataFrameWordCountTest.scala/udf/36.22.Dataset-(String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: filter

lower('word) === ""cheshire""
"
"udf/spark_repos_6/22_dnvriend_apache-spark-test/..jobs.src.test.scala.com.github.dnvriend.spark.dataframe.TextSearchTest.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

lower('line).like(""%error%"")
"
"udf/spark_repos_6/22_dnvriend_apache-spark-test/..jobs.src.test.scala.com.github.dnvriend.spark.dataset.QueryPeopleTest.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 30
"
"udf/spark_repos_6/2_3cgg_rec-assemble/..spark.src.main.scala.scala.me.libme.recsystem.ml.AlsScript.scala/udf/19.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/2_3cgg_rec-assemble/..spark.src.main.scala.scala.me.libme.recsystem.ml.FileRatingDataset.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/2_3cgg_rec-assemble/..spark.src.test.scala.test.scala.me.libme.recsystem.ml.Als.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_6/24_MarkCLewis_SwiftVis2/..manualtesting.src.main.scala.playground.SparkPlotTesting.scala/udf/39.17.Dataset-Point.map","Type: org.apache.spark.sql.Dataset[playground.Point]
Call: map

p => p.copy(x = p.x + 3)
"
"udf/spark_repos_6/24_MarkCLewis_SwiftVis2/..spark.src.main.scala.swiftvis2.spark.package.scala/udf/56.19.Dataset-A.map","Type: org.apache.spark.sql.Dataset[A]
Call: map

f
"
"udf/spark_repos_6/24_MarkCLewis_SwiftVis2/..spark.src.main.scala.swiftvis2.spark.package.scala/udf/73.19.Dataset-A.map","Type: org.apache.spark.sql.Dataset[A]
Call: map

f
"
"udf/spark_repos_6/24_MarkCLewis_SwiftVis2/..spark.src.main.scala.swiftvis2.spark.package.scala/udf/90.19.Dataset-A.map","Type: org.apache.spark.sql.Dataset[A]
Call: map

f
"
"udf/spark_repos_6/2_AmadeusITGroup_Anomaly-Detection-with-Gaussian-Mixtures/..src.test.scala.com.amadeus.gmm_scoring.GmmWithScoringSuite.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""x"") === 1.0d
"
"udf/spark_repos_6/2_AmadeusITGroup_Anomaly-Detection-with-Gaussian-Mixtures/..src.test.scala.com.amadeus.gmm_scoring.GmmWithScoringSuite.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""x"") === 500.0d
"
"udf/spark_repos_6/2_AmadeusITGroup_Anomaly-Detection-with-Gaussian-Mixtures/..src.test.scala.com.amadeus.gmm_scoring.GmmWithScoringSuite.scala/udf/64.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""x"") === 1.0d
"
"udf/spark_repos_6/2_AmadeusITGroup_Anomaly-Detection-with-Gaussian-Mixtures/..src.test.scala.com.amadeus.gmm_scoring.GmmWithScoringSuite.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""x"") === 250.0d
"
"udf/spark_repos_6/2_AmadeusITGroup_Anomaly-Detection-with-Gaussian-Mixtures/..src.test.scala.com.amadeus.gmm_scoring.GmmWithScoringSuite.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""x"") === 500.0d
"
"udf/spark_repos_6/2_behas_ransomware-dataset/..src.main.scala.at.ac.ait.RansomwareDataset.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""cluster"".isNotNull
"
"udf/spark_repos_6/2_behas_ransomware-dataset/..src.main.scala.at.ac.ait.RansomwareDataset.scala/udf/89.22.Dataset-FocusAddress.filter","Type: org.apache.spark.sql.Dataset[at.ac.ait.FocusAddress]
Call: filter

!$""family"".isin(""Unknown"", ""Ransomware"")
"
"udf/spark_repos_6/2_Cecca_diversity-maximization/..experiments.src.main.scala.it.unipd.dei.diversity.matroid.Song.scala/udf/113.22.Dataset-Song.filter","Type: org.apache.spark.sql.Dataset[it.unipd.dei.diversity.matroid.Song]
Call: filter

song => brGenres.value.getOrElse(song.genre, 0) > 0
"
"udf/spark_repos_6/2_Cecca_diversity-maximization/..experiments.src.main.scala.it.unipd.dei.diversity.matroid.Song.scala/udf/59.21.Dataset-Song.map","Type: org.apache.spark.sql.Dataset[it.unipd.dei.diversity.matroid.Song]
Call: map

song => song.genre
"
"udf/spark_repos_6/2_Cecca_diversity-maximization/..experiments.src.main.scala.it.unipd.dei.diversity.matroid.TrainLDA.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""topic == ${opts.describe()}""
"
"udf/spark_repos_6/2_Cecca_diversity-maximization/..experiments.src.main.scala.it.unipd.dei.diversity.matroid.Vectorize.scala/udf/46.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[org.apache.spark.ml.linalg.Vector](""vector"").numNonzeros > 0
"
"udf/spark_repos_6/2_Cecca_diversity-maximization/..experiments.src.main.scala.it.unipd.dei.diversity.matroid.WikiPageLDA.scala/udf/152.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val vector = locally {
          val _t_m_p_9 = row.getString(row.fieldIndex(""vector"")).split("" "")
          _t_m_p_9.map(_.toDouble)
        }.toArray
        WikiPageLDA(row.getLong(row.fieldIndex(""id"")), row.getString(row.fieldIndex(""title"")), locally {
          val _t_m_p_10 = row.getSeq[Long](row.fieldIndex(""topic""))
          _t_m_p_10.map(_.toInt)
        }.toArray, Vectors.dense(vector))
      }
"
"udf/spark_repos_6/2_Cecca_diversity-maximization/..experiments.src.main.scala.it.unipd.dei.diversity.matroid.WikiPageLDA.scala/udf/163.20.Dataset-WikiPageLDA.filter","Type: org.apache.spark.sql.Dataset[it.unipd.dei.diversity.matroid.WikiPageLDA]
Call: filter

page => page.vector.numNonzeros > 0 && page.topic.length > 0
"
"udf/spark_repos_6/2_Cecca_diversity-maximization/..experiments.src.main.scala.it.unipd.dei.diversity.matroid.WikiPageLDA.scala/udf/87.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val title = row.getString(row.fieldIndex(""title""))
        val identifier = row.getString(row.fieldIndex(""id"")).toInt
        val text = row.getString(row.fieldIndex(""text""))
        val topics = row.getSeq[Int](row.fieldIndex(""topic"")).toArray
        val vector = Array.ofDim[Double](gloveMap.value.dimension)
        var cnt = 0
        for (word <- text.split(' ')) {
          val lowerCase = word.toLowerCase()
          gloveMap.value.apply(lowerCase) match {
            case None =>
              println(s""Missing word $lowerCase"")
            case Some(v) =>
              for (i <- 0 until vector.length) {
                vector(i) += v(i)
              }
              cnt += 1
          }
        }
        for (i <- 0 until vector.length) {
          vector(i) /= cnt
        }
        WikiPageLDA(identifier, title, topics, Vectors.dense(vector))
      }
"
"udf/spark_repos_6/2_Coder-Chandler_SparkSQL_project/..SparkProjects.src.main.scala.com.ApacheCommon.log.StatisticsJob.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day
"
"udf/spark_repos_6/2_Coder-Chandler_SparkSQL_project/..SparkProjects.src.main.scala.com.ApacheCommon.log.StatisticsJobYarn.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day
"
"udf/spark_repos_6/2_Coder-Chandler_SparkSQL_project/..SparkProjects.src.main.scala.com.ApacheCommon.log.StatisticsJobYarn.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day
"
"udf/spark_repos_6/2_Coder-Chandler_SparkSQL_project/..SparkProjects.src.main.scala.com.ApacheCommon.log.StatisticsJobYarn.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day
"
"udf/spark_repos_6/2_Coder-Chandler_SparkSQL_project/..SparkProjects.src.main.scala.com.hadoop.spark.DataFrameApp.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopledataframe.col(""age"") > 20
"
"udf/spark_repos_6/2_Coder-Chandler_SparkSQL_project/..SparkProjects.src.main.scala.com.hadoop.spark.DataFrameRDDApp.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

peopledataframe.col(""age"") > 20
"
"udf/spark_repos_6/2_Coder-Chandler_SparkSQL_project/..SparkProjects.src.main.scala.com.hadoop.spark.DatasetAPP.scala/udf/12.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.hadoop.spark.DatasetAPP.Sales]
Call: map

line => line.itemId
"
"udf/spark_repos_6/2_cwida_edge-frames/..src.main.scala.experiments.Datasets.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""src"" =!= ($""dst"")
"
"udf/spark_repos_6/2_cwida_edge-frames/..src.main.scala.experiments.Datasets.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""src"".isNotNull && $""dst"".isNotNull
"
"udf/spark_repos_6/2_cwida_edge-frames/..src.main.scala.experiments.Datasets.scala/udf/85.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""src"" =!= ($""dst"")
"
"udf/spark_repos_6/2_cwida_edge-frames/..src.main.scala.experiments.ExperimentRunner.scala/udf/396.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""count"" > 1
"
"udf/spark_repos_6/2_cwida_edge-frames/..src.main.scala.experiments.Queries.scala/udf/281.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""c"" < ($""b"") && $""b"" < ($""d"") && $""d"" < ($""a"")
"
"udf/spark_repos_6/2_cwida_edge-frames/..src.main.scala.sparkIntegration.WCOJFunctions.scala/udf/102.21.Dataset-(Int, Int).map","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: map

{
          case (src, dst) =>
            (src, dst, true)
        }
"
"udf/spark_repos_6/2_hortonworks_profiler_service/..profilers.hive-metastore-profiler.src.main.scala.com.hortonworks.dataplane.profilers.HiveTableMetadata.scala/udf/52.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => attributeFilter(x(0).toString)
"
"udf/spark_repos_6/2_hortonworks_profiler_service/..profilers.sensitive-info-profiler.src.main.scala.com.hortonworks.dataplane.profilers.SensitiveLabelProfiler.scala/udf/11.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          row => locally {
            val _t_m_p_3 = row.toSeq
            _t_m_p_3.map { col => 
              val nullCount = if (col == null) 1 else 0
              val matches = locally {
                val _t_m_p_4 = locally {
                  val _t_m_p_5 = locally {
                    val _t_m_p_6 = labelMatchers
                    _t_m_p_6.filter(labelMatcher => labelMatcher.isEnabled)
                  }
                  _t_m_p_5.map(_.transformAndMatch(col))
                }
                _t_m_p_4.map(b => if (b) 1 else 0)
              }
              val total = 1
              ColumnCount(matches, nullCount, total)
            }
          }
        }
"
"udf/spark_repos_6/2_koosha-t_WeblogSessionizationSpark/..src.main.scala.com.paytm.challenge.Sessionization.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""InactiveSince"").equalTo(-1) || col(""InactiveSince"") > Settings.SessionInactivityThreshold || col(""InactiveUntil"").equalTo(-1) || col(""InactiveUntil"") > Settings.SessionInactivityThreshold
"
"udf/spark_repos_6/2_koosha-t_WeblogSessionizationSpark/..src.main.scala.com.paytm.challenge.Sessionization.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""InactiveSince"") > Settings.SessionInactivityThreshold || col(""InactiveSince"").equalTo(-1)
"
"udf/spark_repos_6/2_koosha-t_WeblogSessionizationSpark/..src.main.scala.com.paytm.challenge.Sessionization.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""SessionID"").equalTo(2)
"
"udf/spark_repos_6/2_koosha-t_WeblogSessionizationSpark/..src.main.scala.com.paytm.challenge.Sessionization.scala/udf/52.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""count"").equalTo(1)
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""user_id == '"" + userid + ""'""
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/199.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""user_id == '"" + row.getString(0) + ""'""
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""class == "" + group
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/211.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Feature1(row.getString(0), row.getString(1), row.getAs[DenseVector](2), row.getInt(3), row.getAs[DenseVector](4), row.getAs[DenseVector](5))
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/213.20.Dataset-Feature1.map","Type: org.apache.spark.sql.Dataset[scala.algorithm.UserLinearRegression.Feature1]
Call: map

row => {
        val userid = row.user_id
        val itemid = row.business_id
        val star = row.star
        val topic = row.topic
        val usfeature = row.user_feature
        val bufeature = row.business_feature
        val arr = new Array[Double](usfeature.size)
        var total = 0.0d
        for (i <- 0 to usfeature.size - 1) {
          val k = usfeature.apply(i) * bufeature.apply(i)
          total = total + k
          arr.update(i, k)
        }
        for (i <- 0 to usfeature.size - 1) {
          arr.update(i, arr.apply(i) / total)
        }
        Feature2(itemid, userid, star, usfeature, bufeature, Vectors.dense(arr).toDense)
      }
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""class == "" + group
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/268.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Feature5(row.getInt(0).toString, row.getInt(1).toString, row.getInt(2), row.getAs[DenseVector](3), row.getAs[DenseVector](4), row.getInt(5))
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/270.20.Dataset-Feature5.map","Type: org.apache.spark.sql.Dataset[scala.algorithm.UserLinearRegression.Feature5]
Call: map

row => {
        val userid = row.user_id
        val itemid = row.business_id
        val star = row.star
        val usfeature = row.user_feature
        val bufeature = row.business_feature
        val group = row.group
        val arr = new Array[Double](usfeature.size)
        var total = 0.0d
        for (i <- 0 to usfeature.size - 1) {
          val k = usfeature.apply(i) * bufeature.apply(i)
          total = total + k
          arr.update(i, k)
        }
        for (i <- 0 to usfeature.size - 1) {
          arr.update(i, arr.apply(i) / total)
        }
        Feature6(itemid, userid, star, usfeature, bufeature, Vectors.dense(arr).toDense, group)
      }
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""business_id == '"" + itemid + ""'""
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/340.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Feature5(row.getInt(0).toString, row.getInt(1).toString, row.getInt(2), row.getAs[DenseVector](3), row.getAs[DenseVector](4), row.getInt(5))
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/342.20.Dataset-Feature5.map","Type: org.apache.spark.sql.Dataset[scala.algorithm.UserLinearRegression.Feature5]
Call: map

row => {
        val userid = row.user_id
        val itemid = row.business_id
        val star = row.star
        val usfeature = row.user_feature
        val bufeature = row.business_feature
        val group = row.group
        val arr = new Array[Double](usfeature.size)
        var total = 0.0d
        for (i <- 0 to usfeature.size - 1) {
          val x = usfeature.apply(i)
          val y = bufeature.apply(i)
          var k = 0.0d
          k = x + y
          arr.update(i, k)
        }
        for (i <- 0 to usfeature.size - 1) {
          arr.update(i, arr.apply(i) / 2.0d)
        }
        Feature6(itemid, userid, star, usfeature, bufeature, Vectors.dense(arr).toDense, group)
      }
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/413.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => Feature5(row.getInt(0).toString, row.getInt(1).toString, row.getInt(2), row.getAs[DenseVector](3), row.getAs[DenseVector](4), row.getInt(5))
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/415.20.Dataset-Feature5.map","Type: org.apache.spark.sql.Dataset[scala.algorithm.UserLinearRegression.Feature5]
Call: map

row => {
        val userid = row.user_id
        val itemid = row.business_id
        val star = row.star
        val usfeature = row.user_feature
        val bufeature = row.business_feature
        val group = row.group
        val arr = new Array[Double](usfeature.size)
        var total = 0.0d
        for (i <- 0 to usfeature.size - 1) {
          val k = usfeature.apply(i) * bufeature.apply(i)
          total = total + k
          arr.update(i, k)
        }
        for (i <- 0 to usfeature.size - 1) {
          arr.update(i, arr.apply(i) / total)
        }
        Feature6(itemid, userid, star, usfeature, bufeature, Vectors.dense(arr).toDense, group)
      }
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/451.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val user_id = row.getString(1)
          val business_id = row.getString(0)
          val star = row.getInt(2)
          val prediction1 = row.getDouble(3)
          val prediction2 = row.getDouble(4)
          val prediction = prediction1 * per + prediction2 * (1 - per)
          Row(user_id, business_id, star, prediction)
        }
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/475.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val star = row.getInt(0)
        var prediction = row.getDouble(1)
        if (prediction < 0.0d) prediction = 1.0d else if (prediction > 5.0d) prediction = 5.0d else if (prediction.equals(Double.NaN)) {
          prediction = 3.7707160637796866d
          println(""Evaluate NaN"")
        }
        Evaluate(star, prediction)
      }
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.algorithm.UserLinearRegression.scala/udf/492.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val id = row.getString(0)
        println(id)
        if (id != null) {
          val frame = filterByUserId(id, dataFrame)
          val model = new LinearRegressionAl().fit(frame, ""topicDistribution"", ""s"")
          model.coefficients.toString + "","" + model.intercept
        } else """"
      }
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/140.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""business_id"" === business_id
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/23.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.toString().replace(""["", """").replace(""]"", """").split(""\t"")
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/285.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r(0).toString.toDouble.toInt, r(1).toString.toDouble.toInt, r(2).toString.toDouble)
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/296.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r(0).toString.toDouble.toInt, r(1).toString.toDouble.toInt)
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/315.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r(0).toString.toDouble.toInt, r(1).toString.toDouble.toInt, r(2).toString.toDouble, r(3).toString)
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/324.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r(0).toString.toDouble.toInt, r(1).toString.toDouble.toInt, r(2).toString.toDouble, r(3).toString, r(4).toString)
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/333.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r(0).toString.toDouble.toInt, r(1).toString.toDouble.toInt, r(2).toString.toDouble)
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/347.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r(0).toString.toDouble.toInt, r(1).toString.toDouble.toInt, r(2).toString.toDouble, r(3).toString.toDouble)
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.datafilter.GetRandomData.scala/udf/95.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""user_id"" === userid
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.DataSetTest.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_6/2_pp1230_HybridRecommendation/..scala.vector.VectorProcessing.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val user_id = row.getInt(0)
        val business_id = row.getInt(1)
        val stars = row.getDouble(2)
        val feature = row.getAs[SparseVector](3).toDense
        (user_id, business_id, stars, feature)
      }
"
"udf/spark_repos_6/2_tupol_spark-catalyst-study/..src.main.scala.study.catalyst.kmeans.KMeansPipelineBuilder.scala/udf/15.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{ (input: String) => 
        val in = input.split("" "")
        if (in.size < 1) ""Unknown"" else in(0)
      }
"
"udf/spark_repos_6/2_tupol_spark-catalyst-study/..src.main.scala.study.catalyst.kmeans.KMeansPipelineBuilder.scala/udf/22.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{ (input: String) => 
        val in = input.split("" "")
        if (in.size < 2) ""Unknown"" else in(1)
      }
"
"udf/spark_repos_6/2_Vincibean_SparkJdbcHiveIntegration/..src.main.scala.org.vincibean.spark.jdbc.hive.integration.Main.scala/udf/39.28.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[org.vincibean.spark.jdbc.hive.integration.domain.Flight]
Call: filter

$""f.cancelled"" === 0
"
"udf/spark_repos_6/2_Vincibean_SparkJdbcHiveIntegration/..src.main.scala.org.vincibean.spark.jdbc.hive.integration.Main.scala/udf/41.26.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[org.vincibean.spark.jdbc.hive.integration.domain.Flight]
Call: filter

$""f.time.actualElapsedTime"" > 0
"
"udf/spark_repos_6/2_Vincibean_SparkJdbcHiveIntegration/..src.main.scala.org.vincibean.spark.jdbc.hive.integration.Main.scala/udf/43.24.Dataset-Flight.filter","Type: org.apache.spark.sql.Dataset[org.vincibean.spark.jdbc.hive.integration.domain.Flight]
Call: filter

$""f.time.arrivalDelay"" > 15
"
"udf/spark_repos_6/2_Vincibean_SparkJdbcHiveIntegration/..src.main.scala.org.vincibean.spark.jdbc.hive.integration.Main.scala/udf/92.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Flight.parse
"
"udf/spark_repos_6/2_vinodkc_SparkExperiments/..Spark2.Spark-Securekafka-Structured-streaming.src.main.scala.com.vkc.SparkSecureKafkaStructuredStreamingDemo.scala/udf/17.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

value => {
          val columns = value.split("","")
          Sensor(new java.sql.Timestamp(columns(0).toLong), columns(1), columns(2).toDouble)
        }
"
"udf/spark_repos_6/2_zapjone_tazk/..src.main.scala.com.tazk.sink.SparkHiveSink.scala/udf/20.22.Dataset-Boolean.filter","Type: org.apache.spark.sql.Dataset[Boolean]
Call: filter

x => x
"
"udf/spark_repos_6/32_mozilla_telemetry-batch-view/..src.main.scala.com.mozilla.telemetry.ml.AddonRecommender.scala/udf/142.19.Dataset-(String, String, Int, Int).map","Type: org.apache.spark.sql.Dataset[(String, String, Int, Int)]
Call: map

{
        case (_, _, hashedClientId, hashedAddonId) =>
          Rating(hashedClientId, hashedAddonId, 1.0f)
      }
"
"udf/spark_repos_6/32_mozilla_telemetry-batch-view/..src.main.scala.com.mozilla.telemetry.ml.AddonRecommender.scala/udf/154.19.Dataset-(String, String, Int, Int).map","Type: org.apache.spark.sql.Dataset[(String, String, Int, Int)]
Call: map

{
        case (_, addonId, _, hashedAddonId) =>
          (hashedAddonId, addonId)
      }
"
"udf/spark_repos_6/32_mozilla_telemetry-batch-view/..src.main.scala.com.mozilla.telemetry.utils.udfs.scala/udf/16.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

hllCreate _
"
"udf/spark_repos_6/32_mozilla_telemetry-batch-view/..src.main.scala.com.mozilla.telemetry.utils.udfs.scala/udf/20.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

hllCardinality _
"
"udf/spark_repos_6/32_mozilla_telemetry-batch-view/..src.main.scala.com.mozilla.telemetry.utils.udfs.scala/udf/24.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

bucketed _
"
"udf/spark_repos_6/32_mozilla_telemetry-batch-view/..src.main.scala.com.mozilla.telemetry.views.pioneer.PioneerOnlineNewsDedupe.scala/udf/55.21.Dataset-ExplodedEntry).map","Type: org.apache.spark.sql.Dataset[(com.mozilla.telemetry.views.pioneer.PioneerOnlineNewsDedupeView.EntryKey, com.mozilla.telemetry.views.pioneer.PioneerOnlineNewsDedupeView.ExplodedEntry)]
Call: map

_._2
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.CSVProfiler.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[String](0)
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.CSVProfiler.scala/udf/34.21.Dataset-AsciiReport.map","Type: org.apache.spark.sql.Dataset[io.gzet.profilers.raw.AsciiReport]
Call: map

{
          report => (""row.ascii"", report.metricValue, Map(Tags.ASCII_NAME -> report.ascii, Tags.ASCII_BINARY -> report.binary))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.CSVProfiler.scala/udf/65.22.Dataset-MaskBasedReport.map","Type: org.apache.spark.sql.Dataset[io.gzet.profilers.field.MaskBasedReport]
Call: map

{
          report => (""field.ascii.high"", report.metricValue, Map(Tags.FIELD_IDX -> report.field.toString, Tags.MASK -> report.mask, Tags.EXTRA -> locally {
            val _t_m_p_13 = report.description
            _t_m_p_13.map(l => s""[$l]"")
          }.mkString("","")))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.CSVProfiler.scala/udf/73.22.Dataset-MaskBasedReport.map","Type: org.apache.spark.sql.Dataset[io.gzet.profilers.field.MaskBasedReport]
Call: map

{
          report => (""field.pop.check"", report.metricValue, Map(Tags.FIELD_IDX -> report.field.toString, Tags.MASK -> report.mask, Tags.EXTRA -> locally {
            val _t_m_p_15 = report.description
            _t_m_p_15.map(l => s""[$l]"")
          }.mkString("","")))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.CSVProfiler.scala/udf/81.22.Dataset-MaskBasedReport.map","Type: org.apache.spark.sql.Dataset[io.gzet.profilers.field.MaskBasedReport]
Call: map

{
          report => (""field.class.freq"", report.metricValue, Map(Tags.FIELD_IDX -> report.field.toString, Tags.MASK -> report.mask, Tags.EXTRA -> locally {
            val _t_m_p_17 = report.description
            _t_m_p_17.map(l => s""[$l]"")
          }.mkString("","")))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.field.CardinalityProfiler.scala/udf/16.21.Dataset-Field, Long).map","Type: org.apache.spark.sql.Dataset[(io.gzet.profilers.Utils.Field, Long)]
Call: map

{
          case (field, count) =>
            (field.idx, Map(field.value -> count))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.field.CardinalityProfiler.scala/udf/36.19.Dataset-(Int, Long).map","Type: org.apache.spark.sql.Dataset[(Int, Long)]
Call: map

{
        case (column, distinctValues) =>
          val cardinality = distinctValues / total.value.toDouble
          (column, cardinality)
      }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.field.CardinalityProfiler.scala/udf/44.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(column: Int, cardinality: Double, description: mutable.WrappedArray[String]) =>
          CardinalityReport(column, cardinality, description.toArray)
      }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.field.EmptinessProfiler.scala/udf/13.23.Dataset-Field.map","Type: org.apache.spark.sql.Dataset[io.gzet.profilers.Utils.Field]
Call: map

f => (f.idx, StringUtils.isNotEmpty(f.value))
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.field.EmptinessProfiler.scala/udf/18.21.Dataset-((Int, Boolean), Long).map","Type: org.apache.spark.sql.Dataset[((Int, Boolean), Long)]
Call: map

{
          case ((column, isNotEmpty), count) =>
            (column, Map(isNotEmpty -> count))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.field.MaskBasedProfiler.scala/udf/109.21.Dataset-((Int, String, String), Long).map","Type: org.apache.spark.sql.Dataset[((Int, String, String), Long)]
Call: map

{
          case ((field, mask, value), count) =>
            ((field, mask), Map(value -> count))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.field.MaskBasedProfiler.scala/udf/131.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(col: Int, mask: String, maskCount: Long, description: mutable.WrappedArray[String]) =>
          MaskBasedReport(mask, col, maskCount, description.toArray)
      }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.field.MaskBasedProfiler.scala/udf/88.19.Dataset-((Int, String), Long).map","Type: org.apache.spark.sql.Dataset[((Int, String), Long)]
Call: map

{
        case ((col, mask), count) =>
          (col, mask, count)
      }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.GdeltStructuralProfiler.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[String](0)
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.GdeltStructuralProfiler.scala/udf/34.21.Dataset-AsciiReport.map","Type: org.apache.spark.sql.Dataset[io.gzet.profilers.raw.AsciiReport]
Call: map

{
          report => (""row.ascii"", report.metricValue, Map(Tags.ASCII_NAME -> report.ascii, Tags.ASCII_BINARY -> report.binary))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.GdeltStructuralProfiler.scala/udf/65.22.Dataset-MaskBasedReport.map","Type: org.apache.spark.sql.Dataset[io.gzet.profilers.field.MaskBasedReport]
Call: map

{
          report => (""field.pop.check"", report.metricValue, Map(Tags.FIELD_IDX -> report.field.toString, Tags.MASK -> report.mask, Tags.EXTRA -> locally {
            val _t_m_p_13 = report.description
            _t_m_p_13.map(l => s""[$l]"")
          }.mkString("","")))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter04.profilers.src.main.scala.io.gzet.profilers.GdeltStructuralProfiler.scala/udf/73.22.Dataset-MaskBasedReport.map","Type: org.apache.spark.sql.Dataset[io.gzet.profilers.field.MaskBasedReport]
Call: map

{
          report => (""field.class.freq"", report.metricValue, Map(Tags.FIELD_IDX -> report.field.toString, Tags.MASK -> report.mask, Tags.EXTRA -> locally {
            val _t_m_p_15 = report.description
            _t_m_p_15.map(l => s""[$l]"")
          }.mkString("","")))
        }
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter11.src.main.scala.io.gzet.timeseries.TwitterWord2Vec.scala/udf/233.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterText($""body"")
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter11.src.main.scala.io.gzet.timeseries.TwitterWord2Vec.scala/udf/235.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

filterEmojis($""emojis"")
"
"udf/spark_repos_6/37_PacktPublishing_Mastering-Spark-for-Data-Science/..Chapter12.src.main.scala.io.gzet.mainClass.scala/udf/28.31.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

hlc
"
"udf/spark_repos_6/3_BenFradet_spark-ml-examples/..chapter4.src.main.scala.io.github.benfradet.smia.chapter4.DataPreparation.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""type"" === ""IssuesEvent""
"
"udf/spark_repos_6/3_BenFradet_spark-ml-examples/..chapter5.src.main.scala.io.github.benfradet.smia.chapter5.DataPreparation.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""type"" === ""IssuesEvent""
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/13.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Signal.isParsable(_)
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/15.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

Signal.isConsiderable(_)
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/17.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

parts => Signal(timestamp = new Timestamp(parts.getString(0).toLong), rssi = parts.getString(1).toInt, stationId = parts.getString(2).toInt, tagId = parts.getString(3).toInt)
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/25.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Tag.isParsable(_)
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/27.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

Tag.isConsiderable(_)
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/29.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

parts => Tag(id = parts.getString(0).toInt, activeFrom = new Timestamp(parts.getString(1).toLong), activeTo = new Timestamp(parts.getString(2).toLong))
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/36.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Station.isParsable(_)
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/38.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

parts => Station(id = parts.getString(0).toInt, zoneId = parts.getString(1).toInt)
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/45.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Zone.isParsable(_)
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.datasource.UnstructuredDatasource.scala/udf/47.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

parts => Zone(id = parts.getString(0).toInt, name = parts.getString(1))
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.jobs.AnalysisJob.scala/udf/44.24.Dataset-(String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: filter

_._2 >= thirtyMin
"
"udf/spark_repos_6/3_fpopic_wt-interview-challenge/..src.main.scala.com.wt.conference.jobs.AnalysisJob.scala/udf/46.19.Dataset-(String, Long).map","Type: org.apache.spark.sql.Dataset[(String, Long)]
Call: map

_._1
"
"udf/spark_repos_6/3_gsjunior86_SIS/..src.main.scala.br.gsj.spark.kmeans.ImageSegmentation.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""prediction"") === i
"
"udf/spark_repos_6/3_JerryChii_spark-practice/..spark-sql.src.main.scala.com.chii.spark.examples.sql.SparkSQLExample.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_6/3_JerryChii_spark-practice/..spark-sql.src.main.scala.com.chii.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_6/3_JerryChii_spark-practice/..spark-sql.src.main.scala.com.chii.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_6/3_JerryChii_spark-practice/..spark-sql.src.main.scala.com.chii.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_6/3_JerryChii_spark-practice/..spark-sql.src.main.scala.com.chii.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_6/3_JerryChii_spark-practice/..spark-sql.src.main.scala.com.chii.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_6/3_JerryChii_spark-practice/..spark-sql.src.main.scala.com.chii.spark.examples.sql.SQLDataSourceExample.scala/udf/23.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_6/3_knight76_Scala-and-Spark-for-Big-Data-Analytics/..src.main.scala.chapter21.SpamFilteringDemo.scala/udf/27.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 1.0d
"
"udf/spark_repos_6/3_knight76_Scala-and-Spark-for-Big-Data-Analytics/..src.main.scala.chapter21.SpamFilteringDemo.scala/udf/31.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

word => word.length() > 1
"
"udf/spark_repos_6/3_knight76_Scala-and-Spark-for-Big-Data-Analytics/..src.main.scala.chapter21.SpamFilteringDemo.scala/udf/39.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"" === 0.0d
"
"udf/spark_repos_6/3_knight76_Scala-and-Spark-for-Big-Data-Analytics/..src.main.scala.chapter21.SpamFilteringDemo.scala/udf/43.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

word => word.length() > 1
"
"udf/spark_repos_6/3_knight76_Scala-and-Spark-for-Big-Data-Analytics/..src.main.scala.chapter21.SpamFilteringDemo.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_6/3_ljm0_BitcoinCriminality-DataProcessing-Visualization/..0data_pre-process_scala.src.main.scala.org.zuinnote.spark.bitcoin.example.SparkScalaBitcoinTransactionGraph.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""dest_address"".equalTo(""bitcoinaddress_"" + central_addreess) || $""source_address"".equalTo(""bitcoinaddress_"" + central_addreess)
"
"udf/spark_repos_6/3_mthambipillai_cert-anomaly-ids/..src.main.scala.detection.Ensembler.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(scoreColName).geq(threshold)
"
"udf/spark_repos_6/3_mthambipillai_cert-anomaly-ids/..src.main.scala.detection.kmeans.KMeansDetector.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

withScores(""km_score"").geq(lit(threshold))
"
"udf/spark_repos_6/3_mthambipillai_cert-anomaly-ids/..src.main.scala.detection.lof.LOFDetector.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""lof"").geq(lofThreshold)
"
"udf/spark_repos_6/3_mthambipillai_cert-anomaly-ids/..src.main.scala.features.FeaturesExtractor.scala/udf/52.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""timestamp"") >= lowB.value && col(""timestamp"") < highB.value
"
"udf/spark_repos_6/3_mthambipillai_cert-anomaly-ids/..src.main.scala.features.PpidBinaryResolver.scala/udf/20.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""timestamp"") >= lowB.value && col(""timestamp"") < highB.value
"
"udf/spark_repos_6/3_opentargets_genetics-pipe/..src.main.scala.ot.geckopipe.index.V2GIndex.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""gene_id"") isInCollection geneIDsBc.value
"
"udf/spark_repos_6/3_opentargets_genetics-pipe/..src.main.scala.ot.geckopipe.index.VariantIndex.scala/udf/49.23.Dataset-VIRow).map","Type: org.apache.spark.sql.Dataset[((String, Long, String, String), ot.geckopipe.index.VariantIndex.VIRow)]
Call: map

_._2
"
"udf/spark_repos_6/3_opentargets_genetics-pipe/..src.main.scala.ot.geckopipe.index.VariantIndex.scala/udf/54.23.Dataset-VIRow).map","Type: org.apache.spark.sql.Dataset[((String, Long, String, String), ot.geckopipe.index.VariantIndex.VIRow)]
Call: map

_._2
"
"udf/spark_repos_6/3_opentargets_genetics-pipe/..src.main.scala.ot.geckopipe.Main.scala/udf/36.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!isnan(col(""coloc_h3""))
"
"udf/spark_repos_6/3_opentargets_genetics-pipe/..src.main.scala.ot.geckopipe.VEP.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""v2g_score"").isNotNull
"
"udf/spark_repos_6/3_opentargets_genetics-pipe/..src.main.scala.ot.geckopipe.VEP.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getAs[String](0), r.getAs[Double](1))
"
"udf/spark_repos_6/3_sbaresearch_cps-state-replication/..src.main.scala.org.sba_research.cpsstatereplication.CpsStateReplicationApp.scala/udf/198.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
              val timestamp = row.getAs[String](""timestamp"")
              val name = row.getAs[String](""name"")
              val candy = row.getAs[String](""candy"")
              RfidReaderLog(name, timestamp, candy)
            }
"
"udf/spark_repos_6/3_sbaresearch_cps-state-replication/..src.main.scala.org.sba_research.cpsstatereplication.CpsStateReplicationApp.scala/udf/243.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""layers.mqtt"".isNotNull && isMqttRequest && isMqttPublish
"
"udf/spark_repos_6/3_sbaresearch_cps-state-replication/..src.main.scala.org.sba_research.cpsstatereplication.CpsStateReplicationApp.scala/udf/254.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""layers.mqtt"".isNotNull && isMqttRequest && isMqttPublish
"
"udf/spark_repos_6/3_sbaresearch_cps-state-replication/..src.main.scala.org.sba_research.cpsstatereplication.CpsStateReplicationApp.scala/udf/295.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""name"".isNotNull && $""name"" === name
"
"udf/spark_repos_6/3_sbaresearch_cps-state-replication/..src.main.scala.org.sba_research.cpsstatereplication.CpsStateReplicationApp.scala/udf/302.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""name"".isNotNull && $""name"" === name
"
"udf/spark_repos_6/3_sbaresearch_cps-state-replication/..src.main.scala.org.sba_research.cpsstatereplication.CpsStateReplicationApp.scala/udf/62.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""layers.modbus"".isNotNull && isModbusRequest
"
"udf/spark_repos_6/3_sdgordon67_udemy-scala-spark/..SparkScalaCourse.src.com.sundogsoftware.spark.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_6/3_TomLous_spark-valuta-forecast/..src.main.scala.dataset.ForexDataset.scala/udf/16.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size('tmp) === lit(8)
"
"udf/spark_repos_6/3_TomLous_spark-valuta-forecast/..src.main.scala.dataset.ForexDataset.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""<VOL>"".isNotNull
"
"udf/spark_repos_6/3_TomLous_spark-valuta-forecast/..src.main.scala.dataset.ForexLabeledPointDataset.scala/udf/15.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size('featureVector) === windowSize
"
"udf/spark_repos_6/3_TomLous_spark-valuta-forecast/..src.main.scala.job.convert.ConvertCSVToParquet.scala/udf/18.24.Dataset-ForexRecord.filter","Type: org.apache.spark.sql.Dataset[model.ForexRecord]
Call: filter

year('timestamp) === lit(yr)
"
"udf/spark_repos_6/3_TomLous_spark-valuta-forecast/..src.main.scala.job.ml.StreamingLinearRegressionPredict.scala/udf/14.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size('featureVector) === windowSize
"
"udf/spark_repos_6/3_TomLous_spark-valuta-forecast/..src.main.scala.job.preview.PreviewParquet.scala/udf/14.20.Dataset-ForexRecord.filter","Type: org.apache.spark.sql.Dataset[model.ForexRecord]
Call: filter

'high.isNull || 'low.isNull || 'open.isNull || 'vol.isNull
"
"udf/spark_repos_6/3_TomLous_spark-valuta-forecast/..src.main.scala.job.preview.Preview.scala/udf/14.20.Dataset-ForexRecord.filter","Type: org.apache.spark.sql.Dataset[model.ForexRecord]
Call: filter

'high.isNull || 'low.isNull || 'open.isNull || 'vol.isNull
"
"udf/spark_repos_6/3_veeraravi_Spark-notes/..DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_6/4_303844828_PyXGBoost/..src.main.scala.com.jd.aitrade.pyspark.xgboost.scala.PyXGBoostClassifier.scala/udf/49.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => TrainData(row.getAs[org.apache.spark.ml.linalg.Vector](""tempFeatures"").toDense, row.getAs[Integer](labelCol))
"
"udf/spark_repos_6/46_pluralsight_hydra-spark/..core.src.main.scala.hydra.spark.operations.filters.RegexFilter.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_6/4_ada-discovery_ada-server/..src.main.scala.org.ada.server.services.ml.MachineLearningService.scala/udf/162.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
        val id = r(0).asInstanceOf[String]
        val clazz = r(1).asInstanceOf[Int]
        (id, clazz + 1)
      }
"
"udf/spark_repos_6/4_getsentry_sentry-spark/..src.test.scala.io.sentry.spark.listener.SentrySparkListenerSpec.scala/udf/137.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

line => {
          throw new IllegalStateException(""Exception thrown"")
          line.contains(""a"")
        }
"
"udf/spark_repos_6/4_getsentry_sentry-spark/..src.test.scala.io.sentry.spark.listener.SentryStreamingQueryListenerSpec.scala/udf/16.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

elem => elem + 3
"
"udf/spark_repos_6/4_ronald-smith-angel_dataset_deduplication_sparkml/..src.main.scala.com.sample.products.OperationsHelperLSH.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

validateEmptyVector(col(OperationsHelperLSH.ColumnFeaturesArray))
"
"udf/spark_repos_6/4_ronald-smith-angel_dataset_deduplication_sparkml/..src.main.scala.com.sample.products.OperationsHelperWindowStrategy.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(OperationsHelperWindowStrategy.ColumnRowNum) === 1
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/112.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(f: Float) => {
              val params = locally {
                val _t_m_p_14 = outisConf.getParameters(OutisConf.ANONYMIZER_FLOAT)
                _t_m_p_14.map({
                  case ColumnValue => f
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[Float]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/126.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(d: Double) => {
              val params = locally {
                val _t_m_p_16 = outisConf.getParameters(OutisConf.ANONYMIZER_DOUBLE)
                _t_m_p_16.map({
                  case ColumnValue => d
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[Double]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/140.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(bd: java.math.BigDecimal) => {
              val params = locally {
                val _t_m_p_18 = outisConf.getParameters(OutisConf.ANONYMIZER_BIGDECIMAL)
                _t_m_p_18.map({
                  case ColumnValue => bd
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[java.math.BigDecimal]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/154.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(d: Date) => {
              val params = locally {
                val _t_m_p_20 = outisConf.getParameters(OutisConf.ANONYMIZER_DATE)
                _t_m_p_20.map({
                  case ColumnValue => d
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[Date]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/168.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(ts: Timestamp) => {
              val params = locally {
                val _t_m_p_22 = outisConf.getParameters(OutisConf.ANONYMIZER_TIMESTAMP)
                _t_m_p_22.map({
                  case ColumnValue => ts
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[Timestamp]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/182.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String, pattern: String) => {
              val params = locally {
                val _t_m_p_24 = outisConf.getParameters(OutisConf.ANONYMIZER_DATE_STRING)
                _t_m_p_24.map({
                  case ColumnValue => s
                  case x => x
                })
              } :+ pattern :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[String]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/42.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => {
              val params = locally {
                val _t_m_p_4 = outisConf.getParameters(OutisConf.ANONYMIZER_STRING)
                _t_m_p_4.map({
                  case ColumnValue => s
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[String]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/56.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(b: Byte) => {
              val params = locally {
                val _t_m_p_6 = outisConf.getParameters(OutisConf.ANONYMIZER_BYTE)
                _t_m_p_6.map({
                  case ColumnValue => b
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[Byte]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/70.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: Short) => {
              val params = locally {
                val _t_m_p_8 = outisConf.getParameters(OutisConf.ANONYMIZER_BYTE)
                _t_m_p_8.map({
                  case ColumnValue => s
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[Short]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/84.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(i: Int) => {
              val params = locally {
                val _t_m_p_10 = outisConf.getParameters(OutisConf.ANONYMIZER_INT)
                _t_m_p_10.map({
                  case ColumnValue => i
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[Int]
            }
"
"udf/spark_repos_6/4_saagie_outis/..outis-core.src.main.scala.io.saagie.outis.core.job.AnonymizationJob.scala/udf/98.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(l: Long) => {
              val params = locally {
                val _t_m_p_12 = outisConf.getParameters(OutisConf.ANONYMIZER_LONG)
                _t_m_p_12.map({
                  case ColumnValue => l
                  case x => x
                })
              } :+ errorAccumulator
              t._2()(params: _*).asInstanceOf[Long]
            }
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.customerInsights.CIFunctions.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sale => sale.getAs[Timestamp](""timestamp"").toLocalDateTime.getYear == yearFilter
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.customerInsights.CIFunctions.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sale => sale.getAs[Timestamp](""timestamp"").toLocalDateTime.getMonth == monthFilter && sale.getAs[Timestamp](""timestamp"").toLocalDateTime.getYear == yearFilter
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.customerInsights.CIFunctions.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

customer => customer.getAs[Int](""cId"") == cid
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.customerInsights.CIFunctions.scala/udf/58.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

sale => ((sale.getAs[Int](""cId""), sale.getAs[Int](""pId"")), sale.getAs[Timestamp](""timestamp""))
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.customerInsights.IOUtilities.scala/udf/19.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(CIConstants.DELIMITER)
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.customerInsights.IOUtilities.scala/udf/29.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(CIConstants.DELIMITER)
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.customerInsights.IOUtilities.scala/udf/39.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(CIConstants.DELIMITER)
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.customerInsights.IOUtilities.scala/udf/9.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(CIConstants.DELIMITER)
"
"udf/spark_repos_6/5_ambarishHazarnis_spark-practice/..src.main.scala.probelms.trips.TripsDataFrame.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.split(""\t"")
"
"udf/spark_repos_6/5_ansrivas_yelp_dataset/..src.main.scala.com.ansrivas.SparkJob.scala/udf/15.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""categories_flattened"" === ""Health & Medical""
"
"udf/spark_repos_6/5_ansrivas_yelp_dataset/..src.main.scala.com.ansrivas.SparkJob.scala/udf/19.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_open"" === ""1""
"
"udf/spark_repos_6/5_flashbook_flashbot/..backup.MarketData.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""product_id"".isin(locally {
        val _t_m_p_7 = ps.toList
        _t_m_p_7.map(productToStr)
      }: _*)
"
"udf/spark_repos_6/5_flashbook_flashbot/..backup.MarketData.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""product"" === productToStr(p)
"
"udf/spark_repos_6/5_newfront_odsc-east-realish-predictions/..spark-utilities.src.main.scala.com.twilio.open.odsc.realish.DataFrameUtils.scala/udf/18.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
            case Row(s: T) => s
          }
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.examples.Graphs.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""j"") !== n - 1
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.examples.Graphs.scala/udf/109.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""i"") !== n - 1
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.GraphFrame.scala/udf/156.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""degree"") >= threshold
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.GraphFrame.scala/udf/185.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isHub(col(joinCol))
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.GraphFrame.scala/udf/188.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isHub(col(joinCol))
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.GraphFrame.scala/udf/192.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isHub(col(joinCol))
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.GraphFrame.scala/udf/195.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isHub(col(joinCol))
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.BFS.scala/udf/102.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

toVExpr
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.BFS.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

from
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.BFS.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

to
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.BFS.scala/udf/53.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

to
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.BFS.scala/udf/65.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

efExpr
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.BFS.scala/udf/82.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fromAExpr
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.BFS.scala/udf/96.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

previousVertexChecks
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.ConnectedComponents.scala/udf/133.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(SRC) !== col(DST)
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.ConnectedComponents.scala/udf/63.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(SRC) !== col(DST)
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.ConnectedComponents.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(CNT) > broadcastThreshold
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.GraphXConversions.scala/udf/162.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(GraphFrame.ID) === vertexId
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..graphframes-dist.graphframes-0.4.0-spark2.1-s_2.11.src.main.scala.org.graphframes.lib.TriangleCount.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$LONG_SRC != $LONG_DST""
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/20.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.binary _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/24.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.binaryWithSubType _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/28.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.dbPointer _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/32.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.javaScript _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/36.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.javaScriptWithScope _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/40.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.maxKey _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/44.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.minKey _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/48.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.objectId _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/52.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.regularExpression _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/56.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.regularExpressionWithOptions _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/60.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.symbol _
"
"udf/spark_repos_6/5_thuongdinh-agilityio_docker-spark-anaconda/..mongo-spark-connector.mongo-spark-connector_2.10_2.0.0.src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/64.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.timestamp _
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/112.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""totalnum"", ""totaltime"", ""maxtime"", ""mintime""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/147.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""totalnum"", ""create_time""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/170.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""totalnum"", ""create_time""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/193.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""totalnum"", ""create_time""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/215.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""totalnum""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/247.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""create_time"", ""num""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/283.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""create_time"", ""num""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/319.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""create_time"", ""num""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/35.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""totalnum"", ""totaltime"", ""maxtime"", ""mintime"", ""create_time""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/354.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""num"", ""remoteaddr""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/61.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""totalnum"", ""totaltime"", ""maxtime"", ""mintime"", ""create_time""))
"
"udf/spark_repos_6/66_cloudframeworks-smack_user-guide-smack/..source.src.main.scala.com.smack.spark.rpc.service.RankDataService.scala/udf/87.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => x.getValuesMap[Any](List(""totalnum"", ""totaltime"", ""maxtime"", ""mintime"", ""create_time""))
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkml.YelpRecommender.scala/udf/26.19.Dataset-Business.map","Type: org.apache.spark.sql.Dataset[com.alvin.niagara.model.Business]
Call: map

_.business_id
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkml.YelpRecommender.scala/udf/31.19.Dataset-User.map","Type: org.apache.spark.sql.Dataset[com.alvin.niagara.model.User]
Call: map

_.user_id
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkml.YelpRecommender.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""useful"") > 3 && col(""date"") > ""2007-01-01""
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkml.YelpRecommender.scala/udf/48.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""userid"" === userId
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkml.YelpRecommender.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""businessid"" isin (recommendedBuzzIDs: _*)
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkstream.SparkBatchApp.scala/udf/14.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

{
          (l: String) => l.contains(""<row "")
        }
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkstream.SparkService.scala/udf/24.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.getYearMonth _
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkstream.SparkService.scala/udf/49.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => PostTags(row.getAs[Long](""postid""), row.getAs[Int](""typeid""), row.getAs[Seq[String]](""tags""), row.getAs[Long](""creationdate""))
      }
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkstream.SparkService.scala/udf/56.20.Dataset-PostTags.filter","Type: org.apache.spark.sql.Dataset[com.alvin.niagara.model.PostTags]
Call: filter

{
      (post: PostTags) => post.typeid == 1 && post.tags.contains(tag)
    }
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkstream.SparkService.scala/udf/65.21.Dataset-PostTags.map","Type: org.apache.spark.sql.Dataset[com.alvin.niagara.model.PostTags]
Call: map

post => (Util.getYearMonth(post.creationdate), 1)
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkstream.SparkService.scala/udf/75.20.Dataset-PostTags.filter","Type: org.apache.spark.sql.Dataset[com.alvin.niagara.model.PostTags]
Call: filter

{
      (post: PostTags) => Util.getYearMonth(post.creationdate) == month
    }
"
"udf/spark_repos_6/6_AlvinCJin_Niagara/..src.main.scala.com.alvin.niagara.sparkstream.SparkService.scala/udf/83.19.Dataset-PostTags.map","Type: org.apache.spark.sql.Dataset[com.alvin.niagara.model.PostTags]
Call: map

post => (Util.getYearMonth(post.creationdate), 1)
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.FraudDetectionTraining.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.FraudDetectionTraining.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.IntialImportToCassandra.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.IntialImportToCassandra.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.RealTimeFraudDetection.StructuredStreamingFraudDetection.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1.0d
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.spark.jobs.RealTimeFraudDetection.StructuredStreamingFraudDetection.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" =!= 1.0d
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.testing.SamplePipeline.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 1
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.testing.SamplePipeline.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_fraud"" === 0
"
"udf/spark_repos_6/6_pramoddatamantra_FraudDetection/..src.main.scala.com.datamantra.testing.Streaming.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""partition"".isNotNull
"
"udf/spark_repos_6/6_swapniel99_coursera-scala-spark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/104.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

row => TimeUsageRow(row._1._1, row._1._2, row._1._3, row._2, row._3, row._4)
"
"udf/spark_repos_6/6_swapniel99_coursera-scala-spark/..timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/98.18.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (row: Row) => TimeUsageRow(row.getAs[String](""working""), row.getAs[String](""sex""), row.getAs[String](""age""), row.getAs[Double](""primaryNeeds""), row.getAs[Double](""work""), row.getAs[Double](""other"")) }
"
"udf/spark_repos_6/6_sylvesterdj_LearningScala/..SparkStreamingPOC.src.main.scala.streaming.MultiStreamTODO.scala/udf/17.17.Dataset-RateData.map","Type: org.apache.spark.sql.Dataset[entity.RateData]
Call: map

triggerException(_)
"
"udf/spark_repos_6/7_rchillyard_CSYE7200_Old/..spark-example.src.main.scala.edu.neu.coe.csye7200.WordCount.scala/udf/55.22.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

(_, 1)
"
"udf/spark_repos_6/7_rchillyard_CSYE7200_Old/..spark-example.src.main.scala.edu.neu.coe.csye7200.WordCount.scala/udf/57.20.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

Word.tupled
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.BucketedRandomProjectionLSHExample.scala/udf/15.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""state"") === pp(0)
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.BucketedRandomProjectionLSHExample.scala/udf/19.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""state"") === pp(1)
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.CustomizedLSH.scala/udf/47.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sameBucketWithKeyUDF(col($(outputCol)))
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.CustomizedLSH.scala/udf/57.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hashDistCol <= hashThreshold
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.CustomizedLSH.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(distCol) < threshold
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.ExtractCandidates.scala/udf/105.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

custom_predicate_ex
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.ExtractCandidates.scala/udf/94.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

custom_predicate
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.ExtractCandidates.scala/udf/99.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

custom_predicate
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.ExtractMinHashLSH.scala/udf/28.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""state"") === pp(0)
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.ExtractMinHashLSH.scala/udf/32.22.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col(""state"") === pp(1)
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.ExtractMinHashLSH.scala/udf/48.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""docversion"" === vv
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.ExtractMinHashLSH.scala/udf/50.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

Utils.compactSelector_udf(col(""content""))
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.ExtractMinHashLSH.scala/udf/52.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

Utils.lengthSelector_udf(col(""content""))
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.stats.AnalysisUtils.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""docversion"") === ""Introduced""
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.stats.AnalysisUtils.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""docversion"") === ""Introduced""
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.stats.AnalysisUtils.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

processed_data(""similarity"") <= threshold
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.stats.AnalysisUtils.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

processed_data(""similarity"") >= threshold
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.WordCount.scala/udf/19.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""docversion"" === vv
"
"udf/spark_repos_6/9_ASvyatkovskiy_ScaBillMatch/..src.main.scala.org.princeton.billmatch.WordCount.scala/udf/21.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

Utils.lengthSelector_udf(col(""content""))
"
"udf/spark_repos_6/9_srbaird_mc-var-spark/..src.main.scala.factors.RiskFactorSourceFromFile.scala/udf/44.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(sortColumn).geq(fromDate)
"
"udf/spark_repos_6/9_srbaird_mc-var-spark/..src.main.scala.factors.RiskFactorSourceFromFile.scala/udf/46.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(sortColumn).leq(toDate)
"
"udf/spark_repos_6/9_srbaird_mc-var-spark/..src.main.scala.factors.RiskFactorSourceFromFile.scala/udf/53.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(sortColumn).geq(fromDate)
"
"udf/spark_repos_6/9_srbaird_mc-var-spark/..src.main.scala.prices.InstrumentPriceSourceFromFile.scala/udf/55.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(keyColumn).geq(fromDate)
"
"udf/spark_repos_6/9_srbaird_mc-var-spark/..src.main.scala.prices.InstrumentPriceSourceFromFile.scala/udf/57.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(keyColumn).leq(toDate)
"
"udf/spark_repos_6/9_srbaird_mc-var-spark/..src.main.scala.prices.InstrumentPriceSourceFromFile.scala/udf/65.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(keyColumn).geq(fromDate)
"
"udf/spark_repos_6/9_zhenchao125_sparkmall0906/..sparkmall0906-offline.src.main.scala.com.atguigu.sparkmall0906.offline.app.AreaClickApp.scala/udf/11.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AreaClickCountUDAF
"
"udf/spark_repos_7/10_linwt_Imooc-SparkSQL/..Imooc_SparkSQL.src.main.scala.main.TopN.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""courseType"" === ""article""
"
"udf/spark_repos_7/10_linwt_Imooc-SparkSQL/..Imooc_SparkSQL.src.main.scala.main.TopN.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""courseType"" === ""article""
"
"udf/spark_repos_7/10_linwt_Imooc-SparkSQL/..Imooc_SparkSQL.src.main.scala.main.TopN.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""courseType"" === ""article""
"
"udf/spark_repos_7/10_linwt_Imooc-SparkSQL/..Imooc_SparkSQL.src.main.scala.main.TopNYarn.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""courseType"" === ""article""
"
"udf/spark_repos_7/10_linwt_Imooc-SparkSQL/..Imooc_SparkSQL.src.main.scala.main.TopNYarn.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""courseType"" === ""article""
"
"udf/spark_repos_7/10_linwt_Imooc-SparkSQL/..Imooc_SparkSQL.src.main.scala.main.TopNYarn.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""day"" === day && $""courseType"" === ""article""
"
"udf/spark_repos_7/127_memsql_memsql-spark-connector/..src.test.scala.com.memsql.spark.SanityTest.scala/udf/34.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""id"") > 1
"
"udf/spark_repos_7/128_tribbloid_spookystuff/..core.src.test.scala.com.tribbloids.spookystuff.spike.SlowRDDSpike.scala/udf/27.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

{ ii => 
          Thread.sleep(400)
          println(f""repartitioned - $ii"")
          ii
        }
"
"udf/spark_repos_7/14_eleflow_uberdata/..iuberdata_core.src.main.scala.eleflow.uberdata.ForecastPredictor.scala/udf/236.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$labelCol > 0""
"
"udf/spark_repos_7/14_libaoquan95_aasPractice/..c2.Into.src.main.scala.net.libaoquan.aas.into.Into.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_match"" === false
"
"udf/spark_repos_7/14_libaoquan95_aasPractice/..c2.Into.src.main.scala.net.libaoquan.aas.into.Into.scala/udf/40.19.Dataset-MatchData.map","Type: org.apache.spark.sql.Dataset[net.libaoquan.aas.into.MatchData]
Call: map

{
        md => (scoreMatchData(md), md.is_match)
      }
"
"udf/spark_repos_7/14_libaoquan95_aasPractice/..c3.recommend.src.main.scala.net.libaoquan.aas.recommend.recommend.scala/udf/47.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val Array(userId, artistID, count) = locally {
          val _t_m_p_4 = line.split(' ')
          _t_m_p_4.map(_.toInt)
        }
        val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)
        (userId, artistID, count)
      }
"
"udf/spark_repos_7/15_WhiteFangBuck_workshop/..src.main.scala.com.cloudera.workshop.solutions.classifier.SpamClassification.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_7/15_WhiteFangBuck_workshop/..src.main.scala.com.cloudera.workshop.solutions.classifier.TelcoChurnPrediction.scala/udf/37.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getDouble(0), row.getDouble(1))
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..plugins-builtin.src.main.scala.za.co.absa.enceladus.plugins.builtin.errorsender.mq.KafkaErrorSenderPluginImpl.scala/udf/54.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(col(""errCol"")) > 0
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..plugins-builtin.src.main.scala.za.co.absa.enceladus.plugins.builtin.errorsender.mq.KafkaErrorSenderPluginImpl.scala/udf/56.21.Dataset-SingleErrorStardardized.map","Type: org.apache.spark.sql.Dataset[za.co.absa.enceladus.plugins.builtin.errorsender.mq.KafkaErrorSenderPluginImpl.SingleErrorStardardized]
Call: map

_.toDceError(params)
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..plugins-builtin.src.main.scala.za.co.absa.enceladus.plugins.builtin.errorsender.mq.KafkaErrorSenderPluginImpl.scala/udf/58.22.Dataset-DceError.filter","Type: org.apache.spark.sql.Dataset[za.co.absa.enceladus.plugins.builtin.errorsender.DceError]
Call: filter

entry => allowedErrorCodes.contains(entry.errorCode)
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.performance.PerformanceMetricTools.scala/udf/88.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(errCol) > 0
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.performance.PerformanceMetricTools.scala/udf/92.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(errCol) === 0
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.udf.UDFLibrary.scala/udf/11.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (errCol: String, rawValue: String) => ErrorMessage.stdCastErr(errCol, rawValue)
    }
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.udf.UDFLibrary.scala/udf/17.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (errCol: String) => ErrorMessage.stdNullErr(errCol)
    }
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.udf.UDFLibrary.scala/udf/23.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (errRow: String) => ErrorMessage.stdSchemaError(errRow)
    }
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.udf.UDFLibrary.scala/udf/29.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (errCol: String, rawValues: Seq[String], mappings: Seq[Mapping]) => ErrorMessage.confMappingErr(errCol, rawValues, mappings)
    }
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.udf.UDFLibrary.scala/udf/35.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (errCol: String, rawValue: String) => ErrorMessage.confCastErr(errCol, rawValue)
    }
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.udf.UDFLibrary.scala/udf/41.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (errCol: String, rawValue: String) => ErrorMessage.confNegErr(errCol, rawValue)
    }
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.udf.UDFLibrary.scala/udf/47.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (errCol: String, rawValue: String) => ErrorMessage.confLitErr(errCol, rawValue)
    }
"
"udf/spark_repos_7/18_AbsaOSS_enceladus/..utils.src.main.scala.za.co.absa.enceladus.utils.udf.UDFLibrary.scala/udf/53.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arr: mutable.WrappedArray[ErrorMessage]) => if (arr != null) {
      locally {
        val _t_m_p_9 = arr.distinct
        _t_m_p_9.filter { (a: AnyRef) => a != null }
      }
    } else {
      Seq[ErrorMessage]()
    }
"
"udf/spark_repos_7/1_996739940_online-education/..education-etl.src.main.scala.com.atguigu.service.AdlMemberService.scala/udf/18.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys = item._1.split(""_"")
        val appregurl = keys(0)
        val website = keys(1)
        (appregurl, item._2, website)
      }
"
"udf/spark_repos_7/1_996739940_online-education/..education-etl.src.main.scala.com.atguigu.service.AdlMemberService.scala/udf/33.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys = item._1.split(""_"")
        val sitename = keys(0)
        val website = keys(1)
        (sitename, item._2, website)
      }
"
"udf/spark_repos_7/1_996739940_online-education/..education-etl.src.main.scala.com.atguigu.service.AdlMemberService.scala/udf/48.19.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys = item._1.split(""_"")
        val regsourcename = keys(0)
        val website = keys(1)
        (regsourcename, item._2, website)
      }
"
"udf/spark_repos_7/1_996739940_online-education/..education-etl.src.main.scala.com.atguigu.service.AdlMemberService.scala/udf/63.20.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys = item._1.split(""_"")
        val adname = keys(0)
        val website = keys(1)
        (adname, item._2, website)
      }
"
"udf/spark_repos_7/1_996739940_online-education/..education-etl.src.main.scala.com.atguigu.service.AdlMemberService.scala/udf/78.20.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys = item._1.split(""_"")
        val memberlevel = keys(0)
        val website = keys(1)
        (memberlevel, item._2, website)
      }
"
"udf/spark_repos_7/1_996739940_online-education/..education-etl.src.main.scala.com.atguigu.service.AdlMemberService.scala/udf/93.20.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

item => {
        val keys = item._1.split(""_"")
        val vip_level = keys(0)
        val website = keys(1)
        (vip_level, item._2, website)
      }
"
"udf/spark_repos_7/19_AbsaOSS_atum/..examples.src.main.scala.za.co.absa.atum.examples.SampleMeasurements1.scala/udf/11.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""total_response_size"" > 1000
"
"udf/spark_repos_7/1_aastha0304_GameRecommendation/..src.main.scala.game_recommendation.GameRecommendation.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""rank_desc"" <= 6
"
"udf/spark_repos_7/1_aastha0304_GameRecommendation/..src.main.scala.game_recommendation.GameRecommendation.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

func = row => (row.getAs[String](""user_id""), row.getAs[String](""game_id""), gameEventScores.get(row.getAs[String](""event_type"")).unwrapped.asInstanceOf[Int])
"
"udf/spark_repos_7/1_aastha0304_GameRecommendation/..src.main.scala.game_recommendation.GameRecommendation.scala/udf/47.19.Dataset-PlayerData.map","Type: org.apache.spark.sql.Dataset[game_recommendation.PlayerData]
Call: map

row => row.event_type.get match {
        case ""AchievementLikedEvent"" =>
          (row.payload.get.achievement_owner_id, row.payload.get.follower_id, playerEventScores.get(row.event_type.get).unwrapped.asInstanceOf[Int], row.event_time)
        case ""AchievementCommentedEvent"" =>
          (row.payload.get.achievement_owner_id, row.payload.get.follower_id, playerEventScores.get(row.event_type.get).unwrapped.asInstanceOf[Int], row.event_time)
        case ""ProfileVisitedEvent"" =>
          (row.payload.get.visitor_profile_id, row.payload.get.user_id, playerEventScores.get(row.event_type.get).unwrapped.asInstanceOf[Int], row.event_time)
        case ""AchievementSharedEvent"" =>
          (row.payload.get.follower_id, row.payload.get.achievement_owner_id, playerEventScores.get(row.event_type.get).unwrapped.asInstanceOf[Int], row.event_time)
        case ""DirectMessageSentEvent"" =>
          (row.payload.get.target_profile_id, row.payload.get.source_profile_id, playerEventScores.get(row.event_type.get).unwrapped.asInstanceOf[Int], row.event_time)
      }
"
"udf/spark_repos_7/1_abhijitbiswas_SPARK-SCALA/..AmazonDataAnalysis.src.com.upx.amazonData.Operation.DataAnalysisImpl.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x != columnRow
"
"udf/spark_repos_7/1_abhijitbiswas_SPARK-SCALA/..DuplicateDistinctRecordOperation.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""AcctCount"" === 1
"
"udf/spark_repos_7/1_abhijitbiswas_SPARK-SCALA/..DuplicateDistinctRecordOperation.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""AcctCount"" > 1
"
"udf/spark_repos_7/1_achinnasamy_scalatrainingintellij/..src.com.dmac.sparkdataset.SparkTwoDataSet.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

each => each.length
"
"udf/spark_repos_7/1_achinnasamy_scalatrainingintellij/..src.com.dmac.spark.StructuredStreamingMultiplier.scala/udf/10.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

Integer.parseInt(_) * 3
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.SparkSQLExample.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.SparkSQLExample.scala/udf/37.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.SparkSQLExample.scala/udf/56.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.SparkSQLExample.scala/udf/65.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.SparkSQLExample.scala/udf/98.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.SqlRddDemo.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val word = line.split("","")
        val nation = word(0).toString
        val nation1 = word(1).toString
        (nation, nation1)
      }
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.SqlRddDemo.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val word: Array[String] = line.split("","")
        val id = word(0).toLong
        val name = word(1).toString
        val country = word(2).toString
        (id, name, country)
      }
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.TempViewDemo.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

l => {
        val fields = l.split("","")
        val id = fields(0).toInt
        val name = fields(1)
        (id, name)
      }
"
"udf/spark_repos_7/1_andvoidlei_spark-demo/..src.main.scala.examples.sparkSQL.TempViewDemo.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

l => {
        val fields = l.split("","")
        val id = fields(0).toInt
        val subject = fields(1)
        val score = fields(2).toInt
        (id, subject, score)
      }
"
"udf/spark_repos_7/1_anish749_geo-search-spark/..src.main.scala.com.anish.spark.geosearch.Indexer.scala/udf/20.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""lat"" >= -90.0d && $""lat"" <= 90.0d
"
"udf/spark_repos_7/1_anish749_geo-search-spark/..src.main.scala.com.anish.spark.geosearch.Indexer.scala/udf/22.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""lon"" >= -180.0d && $""lon"" <= 180.0d
"
"udf/spark_repos_7/1_anish749_geo-search-spark/..src.main.scala.com.anish.spark.geosearch.Search.scala/udf/22.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

toCoordinate
"
"udf/spark_repos_7/1_ankitshukla1107_SparkScala/..DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_7/1_ankitshukla1107_SparkScala/..SparkScalaCourse.src.com.xebia.spark.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.xebia.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_7/1_AnubhavGuha_CSE511-Project-Phase1/..src.main.scala.cse512.SpatialQuery.scala/udf/25.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_7/1_AnubhavGuha_CSE511-Project-Phase1/..src.main.scala.cse512.SpatialQuery.scala/udf/39.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_7/1_AnubhavGuha_CSE511-Project-Phase1/..src.main.scala.cse512.SpatialQuery.scala/udf/50.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => {
        var fixed_point: Array[Double] = locally {
          val _t_m_p_4 = pointString2.split("","")
          _t_m_p_4.map(_.toDouble)
        }
        var dist_point: Array[Double] = locally {
          val _t_m_p_5 = pointString1.split("","")
          _t_m_p_5.map(_.toDouble)
        }
        StWithin(fixed_point, dist_point, distance)
      }
"
"udf/spark_repos_7/1_AnubhavGuha_CSE511-Project-Phase1/..src.main.scala.cse512.SpatialQuery.scala/udf/73.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pointString1: String, pointString2: String, distance: Double) => {
        var point_P1: Array[Double] = locally {
          val _t_m_p_7 = pointString1.split("","")
          _t_m_p_7.map(_.toDouble)
        }
        var point_P2: Array[Double] = locally {
          val _t_m_p_8 = pointString2.split("","")
          _t_m_p_8.map(_.toDouble)
        }
        StWithin(point_P1, point_P2, distance)
      }
"
"udf/spark_repos_7/1_AravindRamanan_spark-code---social-media-activity/..social_media_activity.src.social_media_activity.facebook_tweets.fb_tweets.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""data_dt"" === ""2019-06-01""
"
"udf/spark_repos_7/1_AravindRamanan_spark-code---social-media-activity/..social_media_activity.src.social_media_activity.facebook_tweets.fb_tweets.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""data_dt"" === ""2019-06-01""
"
"udf/spark_repos_7/1_AravindRamanan_spark-code---social-media-activity/..social_media_activity.src.social_media_activity.facebook_tweets.fb_tweets.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""data_dt"" === ""2019-06-01""
"
"udf/spark_repos_7/1_aravinthsci_Learning-Spark/..src.main.scala.com.aravinth.spark.dataframe_operations.DataFrameOperations.scala/udf/43.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => toQuestion(row)
"
"udf/spark_repos_7/1_aravinthsci_Learning-Spark/..src.main.scala.com.aravinth.spark.spark_sql_operations.SparkSQL_Tutorial.scala/udf/58.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

prefixStackoverflow _
"
"udf/spark_repos_7/1_arpadtamasi_ml-error-classification/..src.main.scala.jobs.Agglomerative.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""framework_name"" === frameworkName && $""class_name"" === className && $""message"".isNotNull
"
"udf/spark_repos_7/1_arpadtamasi_ml-error-classification/..src.main.scala.jobs.ApplyPCA.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""framework_name"" === frameworkName && $""class_name"" === className && $""message"".isNotNull
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..courses.apache-spark-quick-start-guide.src.test.scala.com.backwards.spark.dataframe.DataFrameSpec.scala/udf/12.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" < 5
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..courses.apache-spark-quick-start-guide.src.test.scala.com.backwards.spark.dataframe.DataFrameSpec.scala/udf/26.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => s""IP: ${row(0)}""
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..courses.apache-spark-quick-start-guide.src.test.scala.com.backwards.spark.dataframe.DataFrameSpec.scala/udf/36.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => s""IP:${row(0)}""
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..courses.spark-and-hadoop.src.main.scala.com.backwards.spark.SimpleExampleJob.scala/udf/13.20.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""b"")
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..courses.spark-and-hadoop.src.main.scala.com.backwards.spark.SimpleExampleJob.scala/udf/9.20.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.contains(""a"")
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..src.main.scala.com.backwards.spark.GithubAllArchiveApp.scala/udf/41.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isEmp
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..src.main.scala.com.backwards.spark.GithubAllArchiveApp.scala/udf/45.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

sqlFunc($""login"")
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..src.main.scala.com.backwards.spark.GithubArchiveApp.scala/udf/44.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isEmp
"
"udf/spark_repos_7/1_backwards-limited_spark-backwards/..src.main.scala.com.backwards.spark.GithubArchiveApp.scala/udf/48.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

isEmployee($""login"")
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text16.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r != pheader
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text16.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r != psheader
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text16.scala/udf/21.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r != sheader
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text17.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r != pheader
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text17.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r != psheader
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text17.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r != sheader
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text3.scala/udf/17.22.Dataset-Prd.filter","Type: org.apache.spark.sql.Dataset[Prd]
Call: filter

_.productCode.isEmpty
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text3.scala/udf/22.22.Dataset-Prd.filter","Type: org.apache.spark.sql.Dataset[Prd]
Call: filter

_.name.startsWith(""Pen "")
"
"udf/spark_repos_7/1_br7roy_spark-exercise/..src.main.scala.com.rust.scala.cdh.Text3.scala/udf/27.22.Dataset-Prd.filter","Type: org.apache.spark.sql.Dataset[Prd]
Call: filter

r => r.name.startsWith(""Pen "")
"
"udf/spark_repos_7/1_burakkose_word2vec-playlist-generation/..src.main.scala.net.koseburak.recommendation.model.PlaylistModel.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(playlist: Seq[String]) =>
          (playlist.head, playlist.tail)
      }
"
"udf/spark_repos_7/1_burakkose_word2vec-playlist-generation/..src.main.scala.net.koseburak.recommendation.model.PlaylistModel.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(target: String, recommendations: Seq[String]) =>
          if (recommendations.toSet(target)) 1 else 0
      }
"
"udf/spark_repos_7/1_burakkose_word2vec-playlist-generation/..src.main.scala.net.koseburak.recommendation.util.DataUtils.scala/udf/10.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

parseRow
"
"udf/spark_repos_7/1_caicai5555_spark_sf/..src.main.scala.com.daoke360.task.spark_sql.AreaTop3ProductTask.scala/udf/91.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(city: String, clickCount: Long) => city + "":"" + clickCount
"
"udf/spark_repos_7/1_caicai5555_spark_sf/..src.main.scala.com.daoke360.task.spark_sql.AreaTop3ProductTask.scala/udf/95.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new GroupConcatCityUDAF
"
"udf/spark_repos_7/1_carolynduby_TwitterStructuredStreaming/..src.main.scala.TwitterStreamHive.scala/udf/22.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

length($""hashtags"") > 0
"
"udf/spark_repos_7/1_castiel-vr_sales-report-data-challenge/..src.main.scala.it.castielvr.challenge.itemsales.engine.KpiEngine.scala/udf/33.19.Dataset-KpiSales).map","Type: org.apache.spark.sql.Dataset[((String, String), it.castielvr.challenge.itemsales.model.KpiSales)]
Call: map

_._2
"
"udf/spark_repos_7/1_castiel-vr_sales-report-data-challenge/..src.main.scala.it.castielvr.challenge.itemsales.engine.KpiEngine.scala/udf/9.22.Dataset-InputSales.filter","Type: org.apache.spark.sql.Dataset[it.castielvr.challenge.itemsales.model.InputSales]
Call: filter

is => is.arrivalDay >= startPartition && is.arrivalDay <= endPartition
"
"udf/spark_repos_7/1_castiel-vr_sales-report-data-challenge/..src.main.scala.it.castielvr.challenge.itemsales.io.impl.KpiIoImpl.scala/udf/12.19.Dataset-KpiSalesDTO.map","Type: org.apache.spark.sql.Dataset[it.castielvr.challenge.itemsales.io.dto.KpiSalesDTO]
Call: map

k => KpiSales(k)
"
"udf/spark_repos_7/1_castiel-vr_sales-report-data-challenge/..src.main.scala.it.castielvr.challenge.itemsales.io.impl.KpiIoImpl.scala/udf/20.19.Dataset-KpiSales.map","Type: org.apache.spark.sql.Dataset[it.castielvr.challenge.itemsales.model.KpiSales]
Call: map

k => KpiSalesDTO(k)
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/135.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val Array(userID, artistID, count) = locally {
          val _t_m_p_14 = line.split(' ')
          _t_m_p_14.map(_.toInt)
        }
        val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)
        (userID, finalArtistID, count)
      }
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/26.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val Array(user, _*) = line.split(' ')
        user.toInt
      }
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/40.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""user"" === userID
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (existingArtistIDs: _*)
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin userID
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (recommendedArtistIDs: _*)
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/64.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val Array(user, artist, _*) = line.split(' ')
        (user.toInt, artist.toInt)
      }
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin artistID
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""artist"" isin artistID
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/92.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""user"" isin topUserIDForArtistID
"
"udf/spark_repos_7/1_chaicoffee08_randomartistrecommender/..audioscrobbler_recommender.ch03-recommender-chaiedit.src.main.scala.com.chai.datascience.recommender.RunRecommender.scala/udf/99.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (recommendedArtistIDs: _*)
"
"udf/spark_repos_7/1_chenshiwei_cassandra_spark_sql/..core.src.main.java.uyun.whale.sql.core.Configuration.scala/udf/66.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

functions.overrideFunc
"
"udf/spark_repos_7/1_chenshiwei_cassandra_spark_sql/..core.src.main.java.uyun.whale.sql.core.Configuration.scala/udf/70.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

functions.earliest
"
"udf/spark_repos_7/1_chenshiwei_cassandra_spark_sql/..core.src.main.java.uyun.whale.sql.core.Configuration.scala/udf/74.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

functions.windowTimestamp
"
"udf/spark_repos_7/1_DanferWang_Spark-L-P/..SparkSql.UDAF.scala/udf/14.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

avgAge
"
"udf/spark_repos_7/1_deadwind4_gumiho/..batch.src.main.scala.org.gumiho.batch.spark.Sql.scala/udf/36.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_7/1_deadwind4_gumiho/..batch.src.main.scala.org.gumiho.batch.spark.StructuredStreaming.scala/udf/19.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

x => {
        val msg = JSON.parseObject(x._2)
        (msg.getInteger(""index""), msg.getInteger(""value""))
      }
"
"udf/spark_repos_7/1_deadwind4_gumiho/..batch.src.main.scala.org.gumiho.batch.spark.StructuredStreaming.scala/udf/38.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => Msg(x, Timestamp.valueOf(LocalDateTime.now()))
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.Data.scala/udf/22.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
        case Row(id: Long, title: String) =>
          (id, title)
      }
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.Data.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(title: String, v: org.apache.spark.ml.linalg.SparseVector, id: Long) =>
          (id, title, v(termId))
      }
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.Data.scala/udf/8.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      case Row(id: Long, title: String) =>
        (id, title)
    }
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.lda.mllib.LDAQueryEngine.scala/udf/10.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      case Row(id: Long, title: String) =>
        (id, title)
    }
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.package.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(id: Long, v: ml_Vector, t: String) =>
          (id, (mllib_Vectors.fromML(v), t))
      }
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.package.scala/udf/46.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[String](0)
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.package.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getAs[Double](0)
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.preprocessing.DocTermMatrixWriter.scala/udf/13.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      r => (r.getAs[String](0), r.getAs[String](1))
    }
"
"udf/spark_repos_7/1_derlin_bda-lsa-project/..src.main.scala.bda.lsa.preprocessing.TextToIDF.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(title: String, words: mutable.WrappedArray[String]) =>
          (title, locally {
            val _t_m_p_2 = filterWords(words)
            _t_m_p_2.map(_.toLowerCase)
          })
      }
"
"udf/spark_repos_7/1_DnLKnR_SENG5709/..Assignment02.src.main.scala.com.spark.assignment2.Assignment2.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""avg_playtime"").gt(lit(0))
"
"udf/spark_repos_7/1_DnLKnR_SENG5709/..Assignment02.src.main.scala.com.spark.assignment2.Assignment2.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""Type"").like(""game"")
"
"udf/spark_repos_7/1_DnLKnR_SENG5709/..Assignment02.src.main.scala.com.spark.assignment2.Assignment2.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dfAvgGames1(""appid"").isNotNull
"
"udf/spark_repos_7/1_DnLKnR_SENG5709/..Assignment02.src.main.scala.com.spark.assignment2.Assignment2.scala/udf/94.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dfAvgGames1(""appid"").isNull
"
"udf/spark_repos_7/1_DnLKnR_SENG5709/..Assignment02.src.main.scala.com.spark.drivers.DataDriver.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""appid"").equalTo(lit(appId))
"
"udf/spark_repos_7/1_DnLKnR_SENG5709/..Assignment02.src.main.scala.com.spark.drivers.DataDriver.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""steamid"").equalTo(lit(steamId))
"
"udf/spark_repos_7/1_dounine_billioncomputer/..compute.src.main.scala.com.dounine.compute.indicators.common.Come.scala/udf/17.24.Dataset-LogCase.filter","Type: org.apache.spark.sql.Dataset[com.dounine.compute.Structs.LogCase]
Call: filter

_.uid != null
"
"udf/spark_repos_7/1_DSNFZ_CSYE7200_FINAL_PROJECT/..src.main.scala.com.edu.neu.csye7200.finalproject.util.DataUtil.scala/udf/13.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

row => row != header
"
"udf/spark_repos_7/1_DSNFZ_CSYE7200_FINAL_PROJECT/..src.main.scala.com.edu.neu.csye7200.finalproject.util.DataUtil.scala/udf/39.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

row => row != header
"
"udf/spark_repos_7/1_emmalanguage_emma-tutorial/..emma-tutorial-spark.src.main.scala.org.example.emma.tutorial.snippets.SparkSnippets.scala/udf/163.20.Dataset-(String, Long, Long).map","Type: org.apache.spark.sql.Dataset[(String, Long, Long)]
Call: map

{
        case (k, x, y) =>
          k -> (x, y)
      }
"
"udf/spark_repos_7/1_emystein_spark-cassandra-tests/..src.main.scala.org.demo.LongTweetsFilter.scala/udf/6.22.Dataset-Tweet.filter","Type: org.apache.spark.sql.Dataset[org.demo.Tweet]
Call: filter

_.text.length > 144
"
"udf/spark_repos_7/1_gaborgsomogyi_spark-structured-kafka-stress-app/..src.main.scala.com.cloudera.spark.examples.StructuredKafkaStress.scala/udf/46.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ row => 
        if (new Random().nextInt() % 500 == 0) {
          throw new Exception(""Artificial exception"")
        }
        row
      }
"
"udf/spark_repos_7/1_Gelerion_spark-streaming-in-depth/..src.main.scala.com.gelerion.spark.streaming.in.depth.kafka.iot.SensorDataFileGenerator.scala/udf/14.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{ id => 
        val sensorType = sensorTypes(Random.nextInt(sensorTypes.size))
        SensorReference(id, sensorType.sensorType, sensorType.unit, sensorType.minRange, sensorType.maxRange)
      }
"
"udf/spark_repos_7/1_Gelerion_spark-streaming-in-depth/..src.main.scala.com.gelerion.spark.streaming.in.depth.kafka.iot.SensorDataKafkaGenerator.scala/udf/14.21.Dataset-Rate.map","Type: org.apache.spark.sql.Dataset[com.gelerion.spark.streaming.in.depth.kafka.Rate]
Call: map

_ => SensorData.randomGen(sensorCount)
"
"udf/spark_repos_7/1_Gelerion_spark-streaming-in-depth/..src.main.scala.com.gelerion.spark.streaming.in.depth.kafka.iot.SensorDataKafkaGenerator.scala/udf/16.19.Dataset-SensorData.map","Type: org.apache.spark.sql.Dataset[com.gelerion.spark.streaming.in.depth.kafka.SensorData]
Call: map

_.asKafkaRecord
"
"udf/spark_repos_7/1_Gelerion_spark-streaming-in-depth/..src.main.scala.com.gelerion.spark.streaming.in.depth.logs.ApacheLogsAnalysis.scala/udf/51.22.Dataset-WebLog.filter","Type: org.apache.spark.sql.Dataset[com.gelerion.spark.streaming.in.depth.logs.WebLog]
Call: filter

{
        log => log.request match {
          case urlExtractor(url) =>
            val ext = url.takeRight(5).dropWhile(c => c != '.')
            allowedExtensions.contains(ext)
          case _ =>
            false
        }
      }
"
"udf/spark_repos_7/1_Gelerion_spark-streaming-in-depth/..src.main.scala.com.gelerion.spark.streaming.in.depth.stateful.StatefulStream.scala/udf/12.19.Dataset-Rate.map","Type: org.apache.spark.sql.Dataset[com.gelerion.spark.streaming.in.depth.stateful.Rate]
Call: map

{
        case Rate(ts, value) =>
          WeatherEvent.generate(ts)
      }
"
"udf/spark_repos_7/1_Gelerion_spark-streaming-in-depth/..src.main.scala.com.gelerion.spark.streaming.in.depth.tcp_web_server.TcpWebServer.scala/udf/60.19.Dataset-WebLog.map","Type: org.apache.spark.sql.Dataset[com.gelerion.spark.streaming.in.depth.logs.WebLog]
Call: map

weblog => weblog.copy(timestamp = new Timestamp(weblog.timestamp.getTime + offset))
"
"udf/spark_repos_7/1_grj001_ECPro/..warehouse.sparksql.src.main.scala.com.zhiyou.warehouse.sparksql.ActionLogSparkSql.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val info = x.getString(0).split(""\\|"")
        (info(0), info(1), info(2), info(3), info(4), info(5), info(6), info(7))
      }
"
"udf/spark_repos_7/1_grj001_ECPro/..warehouse.sparksql.src.main.scala.com.zhiyou.warehouse.sparksql.ActionLogSparkSql.scala/udf/23.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(dateTime: String) => {
        val dateTime1 = new Timestamp(dateTime.toLong)
        dateTime1.toString
      }
"
"udf/spark_repos_7/1_grj001_ECPro/..warehouse.sparksql.src.main.scala.com.zhiyou.warehouse.sparksql.ActionLogSparkSql.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val info = x.getString(0).split(""\\|"")
        (info(0), info(1), info(2), info(3), info(4))
      }
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.CassandraSource.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""timestamp"" > System.currentTimeMillis / 1000 - seconds
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.CassandraSource.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""userid"" === userId
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.CassandraSource.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!$""itemid"".isin(itemIdsNotToIncludeSet: _*)
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.cf.CFJob.scala/udf/21.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""prediction"").isNotNull
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.cf.CFPredictionServiceRunner.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(isnan($""rating""))
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.cf.CFPredictionServiceRunner.scala/udf/105.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getInt(0)
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.cf.CFPredictionServiceRunner.scala/udf/90.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(isnan($""prediction""))
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.contentbased.CBFJob.scala/udf/17.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""userid"" === userId
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.contentbased.CBFJob.scala/udf/23.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""prediction"").isNotNull
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.contentbased.CBPredictionService.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""label"".isin(doubleItemIDsNotRatedByUser: _*)
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.contentbased.RegressionSelector.scala/udf/27.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" >= 4.0d
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.contentbased.RegressionSelector.scala/udf/31.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" >= 4.0d
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.FeatureExtractor.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val id = row.getInt(0)
        val featureValues = locally {
          val _t_m_p_4 = 1 until row.length
          _t_m_p_4.map(idx => row.getDouble(idx))
        }
        val featureValuesArr: Array[Double] = featureValues.toArray
        (id, Vectors.dense(featureValuesArr))
      }
"
"udf/spark_repos_7/1_GRpro_recommender_as_a_service/..service.src.main.scala.gr.ml.analytics.service.FeatureExtractor.scala/udf/54.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val id = row.getInt(0)
        val featureValues = locally {
          val _t_m_p_9 = featureColumnNames
          _t_m_p_9.map(name => {
            val ratio = featureRatios(name)
            val value = row.getAs[Double](name)
            value * ratio
          })
        }
        val featureValuesArr: Array[Double] = featureValues.toArray
        (id, Vectors.dense(featureValuesArr))
      }
"
"udf/spark_repos_7/1_harshjangid1015_test_spark_2/..src.main.scala.spark_learning.DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[spark_learning.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_7/1_hcloli_spark-ml-movies/..src.main.scala.com.tikal.spark.ml.MoviesRecommendations.scala/udf/14.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => {
      val splits = str.split(""::"")
      Movie(splits(0).toInt, splits(1), splits(2))
    }
"
"udf/spark_repos_7/1_hcloli_spark-ml-movies/..src.main.scala.com.tikal.spark.ml.MoviesRecommendations.scala/udf/36.17.Dataset-Movie.map","Type: org.apache.spark.sql.Dataset[com.tikal.spark.ml.Movie]
Call: map

movie => {
      val splittedGenre = movie.genre.split('|')
      val genreArray = Array.fill(bGenreMap.value.size) {
        0.0d
      }
      locally {
        val _t_m_p_5 = splittedGenre
        _t_m_p_5.foreach(genre => genreArray(bGenreMap.value(genre)) = 1.0d)
      }
      (movie.movieId, Vectors.dense(genreArray), movie.genre)
    }
"
"udf/spark_repos_7/1_hcloli_spark-ml-movies/..src.main.scala.com.tikal.spark.ml.MoviesRecommendations.scala/udf/55.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => {
      val splits = str.split(""::"")
      Rating(splits(0).toInt, splits(1).toInt, splits(2).toDouble, splits(3).toInt)
    }
"
"udf/spark_repos_7/1_hcloli_spark-ml-movies/..src.main.scala.com.tikal.spark.ml.MoviesRecommendations.scala/udf/68.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""userId"").equalTo(userId)
"
"udf/spark_repos_7/1_hcloli_spark-ml-movies/..src.main.scala.com.tikal.spark.ml.MoviesRecommendations.scala/udf/81.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""rating"").isNull
"
"udf/spark_repos_7/1_hcloli_spark-ml-movies/..src.main.scala.com.tikal.spark.ml.MoviesRecommendations.scala/udf/83.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""predicted_rating"").geq(minPredictedRating)
"
"udf/spark_repos_7/1_Headstorm_epoch/..src.main.scala.com.headstorm.spark.SparkApp.scala/udf/11.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

o => o
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/131.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""loc"" <= 9999
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/137.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""REV_DEPT"" === ""57""
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/141.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""REV_DEPT"" =!= ""57""
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/157.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""EFF_DT"".leq(date_add(current_date(), 1))
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/167.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SBT_LAUNCH_ID"".cast(IntegerType) === 0
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/176.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SBT_LAUNCH_ID"" === '0'
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/181.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""SBT_LAUNCH_ID"".cast(IntegerType) > 0
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/185.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""SBT_LAUNCH_ID"" === '0'
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/191.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!($""grp11_REV_STORE"".isNull and $""grp11_ITEM_ID"" === ""null"" and $""grp12_ITEM_ID"".isNull)
"
"udf/spark_repos_7/1_heenasalim_Project1-Heena_Deter_Authorization_Spark_Sql/..deter.scala/udf/35.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""ksn_purchase_status_cd"" === 'U'
"
"udf/spark_repos_7/1_hgf-anu_BAP_hgf/..src.main.scala.com.yaxin.exam.util.SparkHelper.scala/udf/48.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDFAgeRange.getAgeRange _
"
"udf/spark_repos_7/1_hgf-anu_BAP_hgf/..src.main.scala.com.yaxin.release.util.SparkHelper.scala/udf/49.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDFAgeRange.getAgeRange _
"
"udf/spark_repos_7/1_jayeshkapadnis_sample-spark-json/..src.main.scala.com.hashmapinc.processor.IOTDataProcessor.scala/udf/13.22.Dataset-DeviceIoTData.filter","Type: org.apache.spark.sql.Dataset[com.hashmapinc.processor.DeviceIoTData]
Call: filter

d => d.temp > temperature
"
"udf/spark_repos_7/1_jayeshkapadnis_sample-spark-json/..src.main.scala.com.hashmapinc.processor.IOTDataProcessor.scala/udf/15.17.Dataset-DeviceIoTData.map","Type: org.apache.spark.sql.Dataset[com.hashmapinc.processor.DeviceIoTData]
Call: map

d => (d.temp, d.humidity, d.cca3)
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LightGBMFeature.scala/udf/114.24.Dataset-UALProcessed.filter","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: filter

{
          ual => !outUserBroadCast.value.contains(ual.user)
        }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LightGBMFeature.scala/udf/118.19.Dataset-UALProcessed.map","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: map

{ ual => 
        val featureBuilder = new FeatureBuilder
        var startIndex = 1
        startIndex = BasicProfile.encode(featureBuilder, ual, startIndex)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, tsAvgFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.avg)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, tsMaxFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.max)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryDetailFieldBroadCast.value, queryDetailRateBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryStatFieldBroadCast.value, queryStatRateBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDurationFieldsBroadCast.value, appUsageDurationRateBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDayFieldsBroadCast.value, appUsageDayRateBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageTimeFieldsBroadCast.value, appUsageTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatInstallFieldsBroadCast.value, appStatInstallRateBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatOpenTimeFieldsBroadCast.value, appStatOpenTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 3)(MergedMethod.avg)
        startIndex = MissingValue.encode(featureBuilder, ual, startIndex, 3)
        FeatureEncoded(ual.user, startIndex - 1, ual.label + featureBuilder.getFeature())
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LightGBMFeature.scala/udf/175.25.Dataset-UALProcessed.filter","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: filter

{
          ual => !outUserBroadCast.value.contains(ual.user)
        }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LightGBMFeature.scala/udf/179.20.Dataset-UALProcessed.map","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: map

{ ual => 
        val featureBuilder = new FeatureBuilder
        var startIndex = 1
        startIndex = BasicProfile.encode(featureBuilder, ual, startIndex)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, hyAvgFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, hyMaxFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.max)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryDetailFieldBroadCast.value, queryDetailRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryStatFieldBroadCast.value, queryStatRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDurationFieldsBroadCast.value, appUsageDurationRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDayFieldsBroadCast.value, appUsageDayRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageTimeFieldsBroadCast.value, appUsageTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatInstallFieldsBroadCast.value, appStatInstallRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatOpenTimeFieldsBroadCast.value, appStatOpenTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = MissingValue.encode(featureBuilder, ual, startIndex, 6)
        FeatureEncoded(ual.user, startIndex - 1, ual.label + featureBuilder.getFeature())
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LightGBMFeature.scala/udf/236.25.Dataset-UALProcessed.filter","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: filter

{
          ual => !outUserBroadCast.value.contains(ual.user)
        }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LightGBMFeature.scala/udf/240.20.Dataset-UALProcessed.map","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: map

{ ual => 
        val featureBuilder = new FeatureBuilder
        var startIndex = 1
        startIndex = BasicProfile.encode(featureBuilder, ual, startIndex)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, osAvgFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.avg)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, osMaxFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.max)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryDetailFieldBroadCast.value, queryDetailRateBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryStatFieldBroadCast.value, queryStatRateBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDurationFieldsBroadCast.value, appUsageDurationRateBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDayFieldsBroadCast.value, appUsageDayRateBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageTimeFieldsBroadCast.value, appUsageTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatInstallFieldsBroadCast.value, appStatInstallRateBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatOpenTimeFieldsBroadCast.value, appStatOpenTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 9)(MergedMethod.avg)
        startIndex = MissingValue.encode(featureBuilder, ual, startIndex, 9)
        FeatureEncoded(ual.user, startIndex - 1, ual.label + featureBuilder.getFeature())
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LightGBMFeature.scala/udf/53.24.Dataset-UALProcessed.filter","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: filter

{
          ual => !outUserBroadCast.value.contains(ual.user)
        }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LightGBMFeature.scala/udf/57.19.Dataset-UALProcessed.map","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: map

{ ual => 
        val featureBuilder = new FeatureBuilder
        var startIndex = 1
        startIndex = BasicProfile.encode(featureBuilder, ual, startIndex)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, needFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, needFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.max)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryDetailFieldBroadCast.value, queryDetailRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryStatFieldBroadCast.value, queryStatRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDurationFieldsBroadCast.value, appUsageDurationRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDayFieldsBroadCast.value, appUsageDayRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageTimeFieldsBroadCast.value, appUsageTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatInstallFieldsBroadCast.value, appStatInstallRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatOpenTimeFieldsBroadCast.value, appStatOpenTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = MissingValue.encode(featureBuilder, ual, startIndex, 0)
        FeatureEncoded(ual.user, startIndex - 1, ual.label + featureBuilder.getFeature())
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.LRFeature.scala/udf/48.19.Dataset-UALProcessed.map","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: map

{ ual => 
        val featureBuilder = new FeatureBuilder
        var startIndex = 1
        startIndex = BasicProfile.encode(featureBuilder, ual, startIndex)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, needFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeature(featureBuilder, ual, startIndex, queryDetailRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeature(featureBuilder, ual, startIndex, queryStatRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeature(featureBuilder, ual, startIndex, appUsageDurationRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeature(featureBuilder, ual, startIndex, appUsageDayRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeature(featureBuilder, ual, startIndex, appUsageTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeature(featureBuilder, ual, startIndex, appStatInstallRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeRateFeature(featureBuilder, ual, startIndex, appStatOpenTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeCombineLogFeatures(featureBuilder, ual, startIndex, combineLogNeedFieldsBroadCast.value, combineLogFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeMaxChangeFeature(featureBuilder, ual, startIndex, maxChangeFieldsBroadCast.value, maxChangeMinMaxStatisticsBroadCast.value, 6)
        startIndex = MissingValue.encodeLR(featureBuilder, ual, startIndex, 6)
        FeatureEncoded(ual.user, startIndex - 1, ual.label + featureBuilder.getFeature())
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.XGBFeature.scala/udf/45.24.Dataset-UALProcessed.filter","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: filter

{
          ual => !outUserBroadCast.value.contains(ual.user)
        }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.feature_engineering.XGBFeature.scala/udf/49.19.Dataset-UALProcessed.map","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: map

{ ual => 
        val featureBuilder = new FeatureBuilder
        var startIndex = 1
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryDetailFieldBroadCast.value, queryDetailRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, queryStatFieldBroadCast.value, queryStatRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDurationFieldsBroadCast.value, appUsageDurationRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageDayFieldsBroadCast.value, appUsageDayRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appUsageTimeFieldsBroadCast.value, appUsageTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatInstallFieldsBroadCast.value, appStatInstallRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeRateFeatures(featureBuilder, ual, startIndex, appStatOpenTimeFieldsBroadCast.value, appStatOpenTimeRateBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, needFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.avg)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, needFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 0)(MergedMethod.max)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, hyAvgFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.avg)
        startIndex = encodeFeatures(featureBuilder, ual, startIndex, hyMaxFieldsBroadCast.value, minMaxStatisticsBroadCast.value, 6)(MergedMethod.max)
        startIndex = BasicProfile.encode(featureBuilder, ual, startIndex)
        startIndex = MissingValue.encode(featureBuilder, ual, startIndex, 0)
        startIndex = MissingValue.encode(featureBuilder, ual, startIndex, 6)
        FeatureEncoded(ual.user, startIndex - 1, ual.label + featureBuilder.getFeature())
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.others.MergeUserAction4Test.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val splits = line.split(""\t"", 3)
        val actions = splits(2)
        UserActions(splits.head, splits(1), actions)
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.others.MergeUserAction4Train.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val splits = line.split(""\t"", 3)
        Label(splits.head, splits(1), splits(2).toInt)
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.others.MergeUserAction4Train.scala/udf/22.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val splits = line.split(""\t"", 3)
        val actions = splits(2)
        UserActions(splits.head, splits(1), actions)
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.statistics.CategoryRateStatistics.scala/udf/19.23.Dataset-UALProcessed.map","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: map

{ ual => 
            val curActions = locally {
              val _t_m_p_4 = locally {
                val _t_m_p_5 = ual.actions.values
                _t_m_p_5.flatMap {
                  ca => ca
                }
              }
              _t_m_p_4.filter(_._1 == targetField)
            }.toSeq
            val v = if (curActions.isEmpty) -1.0d else curActions.head._2
            v -> ual.label
          }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.statistics.CategoryRateStatistics.scala/udf/33.24.Dataset-(Double, Int).filter","Type: org.apache.spark.sql.Dataset[(Double, Int)]
Call: filter

$""_1"" >= 0.0d
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.statistics.MaxChangeMinMaxStatistics.scala/udf/57.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
          val splits = line.split(""\t"")
          splits.head.toInt -> MinMax(splits(1).toDouble, splits(2).toDouble)
        }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.statistics.MinMaxStatistics.scala/udf/56.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val splits = line.split(""\t"")
        splits.head.toInt -> MinMax(splits(1).toDouble, splits(2).toDouble)
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.statistics.MissingColumnStatistics.scala/udf/45.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{ row => 
        val field = row.getAs[Int](""field"")
        val count = row.getAs[Long](""count"")
        val missV = (allCount - count) * 1.0d / allCount
        f""$field\t$missV%1.4f""
      }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.statistics.MissingRowStatistics.scala/udf/19.21.Dataset-UALProcessed.map","Type: org.apache.spark.sql.Dataset[com.xiaomi.miui.ad.others.UALProcessed]
Call: map

{ ual => 
          val curActions = locally {
            val _t_m_p_3 = locally {
              val _t_m_p_4 = locally {
                val _t_m_p_5 = ual.actions.values
                _t_m_p_5.flatMap {
                  ca => ca
                }
              }
              _t_m_p_4.filter {
                c => categorySeqBroadCast.value.contains(c._1) || c._2 > 0.0d
              }
            }
            _t_m_p_3.map(_._1)
          }.toSet
          val missV = curActions.size * 1.0d / 96116
          ual.user -> missV
        }
"
"udf/spark_repos_7/1_JimmyCai_xiaomi-dmc-risk-contest/..src.main.scala.com.xiaomi.miui.ad.statistics.MissingRowStatistics.scala/udf/38.19.Dataset-(String, Double).map","Type: org.apache.spark.sql.Dataset[(String, Double)]
Call: map

{
        row => f""${row._1}\t${row._2}%1.4f""
      }
"
"udf/spark_repos_7/1_jvk243_scala-spark-docker-compose-travisci/..src.main.scala.com.example.spark.sql.SparkSalesCountryExample.scala/udf/14.19.Dataset-SalesDetails.map","Type: org.apache.spark.sql.Dataset[com.example.spark.utils.SalesDetails]
Call: map

x => x.Price
"
"udf/spark_repos_7/1_Kaiiim_Big-Data-Analysis-with-Scala-and-Spark/..week3.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/120.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case ((working, sex, age), primaryNeeds, work, other) =>
          TimeUsageRow(working, sex, age, primaryNeeds, work, other)
      }
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/102.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 11
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/106.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 12
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/110.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 12
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/18.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 1
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/22.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 1
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/26.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 2
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/30.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 2
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/34.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 3
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/38.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 3
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/42.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 4
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/46.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 4
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/50.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 5
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/54.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 5
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/58.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 6
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/62.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 6
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/66.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 7
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/70.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 7
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/74.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 8
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/78.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 8
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/82.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 9
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/86.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 9
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/90.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 10
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/94.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 2 and $""mois"" === 10
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsAccidents.scala/udf/98.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sexe"" === 1 and $""mois"" === 11
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/103.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 8
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/107.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 8
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/111.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 8
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/115.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 9
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/119.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 9
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/123.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 9
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/127.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 10
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/131.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 10
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/135.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 10
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/139.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 11
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/143.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 11
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/147.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 11
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/151.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 12
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/155.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 12
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/159.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 12
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/19.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 1
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/23.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 1
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/27.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 1
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/31.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 2
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/35.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 2
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/39.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 2
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/43.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 3
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/47.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 3
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/51.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 3
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/55.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 4
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/59.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 4
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/63.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 4
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/67.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 5
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/71.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 5
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/75.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 5
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/79.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 6
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/83.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 6
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/87.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 6
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/91.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 0 and $""Vitessevent10mn"" < 5 and $""mois"" === 7
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/95.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 5.0d and $""Vitessevent10mn"" < 9.9d and $""mois"" === 7
"
"udf/spark_repos_7/1_kaisbenfadhel_BigDataAnalyticswithSparkScalaFX/..src.main.scala.analytics.PlotAnalyticsVent.scala/udf/99.21.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Vitessevent10mn"" > 10.0d and $""Vitessevent10mn"" < 14.9d and $""mois"" === 7
"
"udf/spark_repos_7/1_kaleeaswari_transformations/..src.main.scala.thoughtworks.wordcount.WordCountUtils.scala/udf/10.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.toLowerCase().replaceAll(""\"""", """").split("" |,|;|\\.|-"")
"
"udf/spark_repos_7/1_kefuzhu_Scala/..TalkingData.src.main.scala.com.talkingdata.dmp.etl.extract.TimeFeatureExtract.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sqlfunc(col(""tdid""))
"
"udf/spark_repos_7/1_kefuzhu_Scala/..TalkingData.src.main.scala.com.talkingdata.dmp.etl.update.FeatureFilter.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sqlfunc(col(""tdid""))
"
"udf/spark_repos_7/1_kefuzhu_Scala/..TalkingData.src.main.scala.HadoopRun.unicom_beijing.scala/udf/405.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ line => 
          val tdid = line.getAs[String](""deviceId"")
          val imei = line.getAs[Seq[String]](""imei"").mkString("","").split("","").mkString(""\t"")
          val imsi = line.getAs[Seq[String]](""imsi"").mkString("","").split("","").mkString(""\t"")
          (tdid, imei, imsi)
        }
"
"udf/spark_repos_7/1_kefuzhu_Scala/..TalkingData.src.main.scala.HadoopRun.unicom_beijing.scala/udf/509.26.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ line => 
              val tdid = line.getAs[String](""deviceId"")
              val imei = line.getAs[Seq[String]](""imei"").mkString("","").split("","").mkString(""\t"")
              val imsi = line.getAs[Seq[String]](""imsi"").mkString("","").split("","").mkString(""\t"")
              val first_date = line.getAs[Int](""first_date"").toString
              val active_date = line.getAs[Int](""active_date"").toString
              val change_date = line.getAs[Int](""change_date"").toString
              (tdid, imei, imsi, first_date, active_date, change_date)
            }
"
"udf/spark_repos_7/1_knoldus_spark-stream-to-stream-join.g8/..src.main.g8.src.main.scala.com.knoldus.api.StreamToStreamJoin.scala/udf/9.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => Timestamp.valueOf(str).getTime
"
"udf/spark_repos_7/1_knoldus_spark-stream-to-stream-join.g8/..src.main.g8.src.test.scala.StreamJoinSpec.scala/udf/50.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => Timestamp.valueOf(str).getTime
"
"udf/spark_repos_7/1_lmrzero_SparkTest/..src.main.scala.cn.edu360.day7.IpLoactionSQL2.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_7/1_lmrzero_SparkTest/..src.main.scala.cn.edu360.day7.IpLoactionSQL2.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_7/1_lmrzero_SparkTest/..src.main.scala.cn.edu360.day7.IpLoactionSQL2.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(ipNum: Long) => {
        val ipRulesInExecutor: Array[(Long, Long, String)] = broadcastRef.value
        val index = MyUtils.binarySearch(ipRulesInExecutor, ipNum)
        var province = ""未知""
        if (index != -1) {
          province = ipRulesInExecutor(index)._3
        }
        province
      }
"
"udf/spark_repos_7/1_lmrzero_SparkTest/..src.main.scala.cn.edu360.day7.IpLoactionSQL.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_7/1_lmrzero_SparkTest/..src.main.scala.cn.edu360.day7.IpLoactionSQL.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_7/1_lmrzero_SparkTest/..src.main.scala.cn.edu360.day7.JdbcDataSource.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" <= 13
"
"udf/spark_repos_7/1_lmrzero_SparkTest/..src.main.scala.cn.edu360.day7.joinTest.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

l => {
        val fields = l.split("","")
        val ename = fields(0)
        val cname = fields(1)
        (ename, cname)
      }
"
"udf/spark_repos_7/1_lmrzero_SparkTest/..src.main.scala.cn.edu360.day7.joinTest.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split("","")
        val id = fields(0).toLong
        val name = fields(1)
        val nation = fields(2)
        (id, name, nation)
      }
"
"udf/spark_repos_7/1_lookuut_raif-competition/..src.main.scala.com.spark.raif.solutions.XGBoostBinaryLogistic.scala/udf/62.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          case Row(probabilities: Vector, customer_id: String, prediction: Double, label: Double) =>
            (customer_id, (1 - probabilities(0), label))
        }
"
"udf/spark_repos_7/1_lookuut_raif-competition/..src.main.scala.com.spark.raif.solutions.XGBoostBinaryLogistic.scala/udf/84.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          case Row(probabilities: Vector, customer_id: String, prediction: Double, pointx: Double, pointy: Double) =>
            (customer_id, (1 - probabilities(0), pointx, pointy))
        }
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/33.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/35.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/40.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/42.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!($""label"" === ($""prediction""))
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/47.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/49.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!($""label"" === ($""prediction""))
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareLogisticRegression.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataset(""label"") === 0.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareRandomForest.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareRandomForest.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareRandomForest.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareRandomForest.scala/udf/39.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""label"" === ($""prediction"")
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareRandomForest.scala/udf/44.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 0.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareRandomForest.scala/udf/46.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!($""label"" === ($""prediction""))
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareRandomForest.scala/udf/51.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_7/1_Loriszirah_ADcare/..src.main.scala.adcare.ADcareRandomForest.scala/udf/53.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!($""label"" === ($""prediction""))
"
"udf/spark_repos_7/1_manhcompany_heavy/..etl.src.main.scala.com.heavy.etl.udfs.StringUdf.scala/udf/14.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

func
"
"udf/spark_repos_7/1_MarkCLewis_CSCI3395-F19/..src.main.scala.sparksql.TempDataTypeSafe.scala/udf/19.20.Dataset-Season).filter","Type: org.apache.spark.sql.Dataset[(sparksql.TempData, sparksql.Season)]
Call: filter

t => t._2.season == ""summer""
"
"udf/spark_repos_7/1_MarkCLewis_CSCI3395-F19/..src.main.scala.sparksql.TempDataTypeSafe.scala/udf/24.17.Dataset-Season).map","Type: org.apache.spark.sql.Dataset[(sparksql.TempData, sparksql.Season)]
Call: map

{
      case (td, s) =>
        JoinedStuff(td.year, s.season, td.tmax)
    }
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/40.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/92.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/1_mrchristine_spark-examples-dbc/..src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/29.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/20.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.binary _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/24.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.binaryWithSubType _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/28.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.dbPointer _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/32.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.javaScript _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/36.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.javaScriptWithScope _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/40.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.maxKey _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/44.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.minKey _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/48.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.objectId _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/52.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.regularExpression _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/56.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.regularExpressionWithOptions _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/60.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.symbol _
"
"udf/spark_repos_7/1_nboumaza_repo-spark-connectors/..src.main.scala.com.c12e.repos.spark.mongo.UDF.scala/udf/64.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.timestamp _
"
"udf/spark_repos_7/1_nikolayskovpin_hbc-etl-dim-branch/..src.main.scala.com.hbc.etl.dim_brand.staging.Stage2Brand.scala/udf/19.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hist.to_time"" =!= ""3000-01-01 00:00:00+00""
"
"udf/spark_repos_7/1_nikolayskovpin_hbc-etl-dim-branch/..src.main.scala.com.hbc.etl.dim_brand.staging.Stage2Brand.scala/udf/21.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""hist.dw_batch_id"" <= lastId
"
"udf/spark_repos_7/1_oavioz_Spark-Kafka-Streaming-Structured/..src.main.scala.kafka.KafkaSource.scala/udf/13.22.Dataset-SimpleSongAggregationKafka.filter","Type: org.apache.spark.sql.Dataset[radio.SimpleSongAggregationKafka]
Call: filter

_.radioCount != null
"
"udf/spark_repos_7/1_objektwerks_spark/..src.test.scala.spark.DataSourceTest.scala/udf/86.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => AvgAgeByRole(row.getString(0), row.getDouble(1))
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.impl.SampleImpl.scala/udf/10.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

strata(prn) >= startPoint
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.impl.StratificationImpl.scala/udf/12.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

frameDf(sic07) >= sic07LowerClass && frameDf(sic07) <= sic07UpperClass
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.impl.StratificationImpl.scala/udf/14.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

frameDf(bounds) >= boundsLowerRange && frameDf(bounds) <= boundsUpperRange
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.impl.StratificationImpl.scala/udf/21.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

frameDf(sic07) >= sic07LowerClass && frameDf(sic07) <= sic07UpperClass
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.PayeCalculator.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.isNull(payeRefs)
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sample.scala/udf/27.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

stratificationPropsDf(""seltype"") === ""P"" || stratificationPropsDf(""seltype"") === ""C""
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/110.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(group) isin ""461""
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/114.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(col(group) isin ""461"")
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/119.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not(col(group) isin ""469"")
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/123.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(group) isin ""469""
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/130.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(group) isin groupVal
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/139.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(col(group).isin(""478"", ""479""))
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/143.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(group).isin(""478"", ""479"")
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/150.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(group).isin(theGroup)
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/157.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(group).isin(""471"")
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/161.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

not(col(group).isin(""471""))
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/168.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(group) isin filter
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/177.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(group).isin(""478"")
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/181.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(group).isin(""479"")
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(ern) isin row
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.methods.Sic.scala/udf/91.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(subdivision) isin dupCheckedDF.first.getString(0)
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.model.CommonFrameAndPropertiesFieldsCasting.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

castedUnitDf(sic07).isNull || castedUnitDf(prn).isNull
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.model.CommonFrameAndPropertiesFieldsCasting.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

castedStratifiedDF(cellNumber).isNull || castedStratifiedDF(prn).isNull
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.model.CommonFrameAndPropertiesFieldsCasting.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

castedPayeDF(payeEmployees).isNull || castedPayeDF(paye_jobs).isNull
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.model.CommonFrameAndPropertiesFieldsCasting.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

castedVatDF(payeEmployees).isNull || castedVatDF(paye_jobs).isNull || castedVatDF(ent).isNull
"
"udf/spark_repos_7/1_ONSdigital_registers-sml/..src.main.scala.uk.gov.ons.registers.TransformDataFrames.scala/udf/21.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.getAs[Int](cellNumber) == thisCellNumber
"
"udf/spark_repos_7/1_PabloJabat_TrafficMonitoring/..src.main.scala.com.trafficmonitoring.MonitoringApp.scala/udf/28.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

a => (a.getAs[Int](""wayID""), a.getAs[Long](""count""), a.getAs[String](""wayData""))
"
"udf/spark_repos_7/1_pan5431333_optree4s/..src.main.scala.org.optree4s.OptimalTreeModel.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val x = locally {
          val _t_m_p_2 = xCols
          _t_m_p_2.map(row.getAs[Double])
        }
        val y = row.getAs[Double](yCol)
        x -> y
      }
"
"udf/spark_repos_7/1_pan5431333_optree4s/..src.main.scala.org.optree4s.OptimalTreeModel.scala/udf/36.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val x = locally {
            val _t_m_p_5 = xCols
            _t_m_p_5.map(row.getAs[Double])
          }
          x
        }
"
"udf/spark_repos_7/1_praveenmpm88_hello-world/..SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_7/1_rahul168_courserepo/..course4.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/108.20.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

{
        case (a, b, c, d) =>
          TimeUsageRow(a._1, a._2, a._3, Math.round(b * 10.0d) / 10.0d, Math.round(c * 10.0d) / 10.0d, Math.round(d * 10.0d) / 10.0d)
      }
"
"udf/spark_repos_7/1_sadikovi_spark-hosvd/..src.main.scala.com.github.sadikovi.spark.hosvd.CoordinateBlock.scala/udf/14.19.Dataset-RowStripe.map","Type: org.apache.spark.sql.Dataset[com.github.sadikovi.spark.hosvd.RowStripe]
Call: map

{ row => 
        val vector = Vectors.sparse(n, locally {
          val _t_m_p_2 = row.cols
          _t_m_p_2.map {
            cl => (cl.j.toInt, cl.value)
          }
        })
        IndexedRow(row.i, vector)
      }
"
"udf/spark_repos_7/1_sadikovi_spark-hosvd/..src.main.scala.com.github.sadikovi.spark.hosvd.DistributedTensor.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""k"") === layer
"
"udf/spark_repos_7/1_SmorSmor_Release/..src.main.scala.com.release.util.SparkHelper.scala/udf/32.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

QFUdf.getAgeRange _
"
"udf/spark_repos_7/1_SmorSmor_Release/..src.main.scala.com.release.util.SparkHelper.scala/udf/36.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

QFUdf.getScoue _
"
"udf/spark_repos_7/1_SmorSmor_Release/..src.main.scala.com.release.util.SparkHelper.scala/udf/40.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

QFUdf.getLevel _
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.machinelearning.ML_Songs_Example.scala/udf/205.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => LabeledPoint(x(0).asInstanceOf[Double], Vectors.dense(x(1).asInstanceOf[org.apache.spark.ml.linalg.Vector].toArray))
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.machinelearning.ML_Songs_Example.scala/udf/211.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => getLabeledPrediction(finalWeights, LabeledPoint(x(0).asInstanceOf[Double], x(1).asInstanceOf[org.apache.spark.ml.linalg.Vector]))
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.machinelearning.ML_Songs_Example.scala/udf/254.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => twoWayInteractions(LabeledPoint(x(0).asInstanceOf[Double], Vectors.dense(x(1).asInstanceOf[org.apache.spark.ml.linalg.Vector].toArray)))
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.machinelearning.ML_Songs_Example.scala/udf/258.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => twoWayInteractions(LabeledPoint(x(0).asInstanceOf[Double], Vectors.dense(x(1).asInstanceOf[org.apache.spark.ml.linalg.Vector].toArray)))
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.machinelearning.ML_Songs_Example.scala/udf/262.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => twoWayInteractions(LabeledPoint(x(0).asInstanceOf[Double], Vectors.dense(x(1).asInstanceOf[org.apache.spark.ml.linalg.Vector].toArray)))
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.Spark_Experiment_NASA_HTTP.scala/udf/34.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

clearedDF(""host"").like(""unknown"") || clearedDF(""timestamp"").like(unknownLabel) || clearedDF(""path"").like(unknownLabel) || clearedDF(""status"").like(unknownLabel) || clearedDF(""content_size"").like(unknownLabel)
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.Spark_Experiment_NASA_HTTP.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

hostSumDF(""count"") > 10
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.Spark_Experiment_NASA_HTTP.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!parsedLogsDF(""status"").equalTo(""200"")
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.Spark_Experiment_NASA_HTTP.scala/udf/77.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

parsedLogsDF(""status"").equalTo(""404"")
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.SparkMoviesExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""primaryName"" === ""Brad Pitt""
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.SparkWikiExample.scala/udf/17.22.Dataset-PageView.filter","Type: org.apache.spark.sql.Dataset[spark.sample.project.SparkWikiExample.PageView]
Call: filter

x => x.domainCode == ""en""
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.SparkWikiExample.scala/udf/22.22.Dataset-PageView.filter","Type: org.apache.spark.sql.Dataset[spark.sample.project.SparkWikiExample.PageView]
Call: filter

x => x.countViews.getOrElse(0) > 20000
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.Spark_Words_Experiment.scala/udf/32.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_ != """"
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.Spark_Words_Experiment.scala/udf/36.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

w => (w, 1)
"
"udf/spark_repos_7/1_steveSK_sparkExperiments/..src.main.scala.spark.sample.project.Spark_Words_Experiment.scala/udf/43.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

w => WordCount(w, 1)
"
"udf/spark_repos_7/1_sunGibson_OD_Analyze/..src.main.scala.com.triton.aio.OD_car_analyze.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

GPS_DATA(""EMPTY"") === 0 || GPS_DATA(""EMPTY"") === 1
"
"udf/spark_repos_7/1_symat_spark-api-comparison/..src.main.scala.com.epam.meetup.batch.DataSetExample.scala/udf/18.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => new ActorMovieRelation(row.getString(0), row.getString(1), row.getString(2))
"
"udf/spark_repos_7/1_symat_spark-api-comparison/..src.main.scala.com.epam.meetup.batch.DataSetExample.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => new ActorMovieRelation(row.getString(0), row.getString(1), row.getString(2))
"
"udf/spark_repos_7/1_symat_spark-api-comparison/..src.main.scala.com.epam.meetup.batch.DataSetExample.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => new MovieRating(row.getString(0), row.getString(1).toInt, row.getString(2).toFloat, row.getString(3), row.getString(4))
"
"udf/spark_repos_7/1_symat_spark-api-comparison/..src.main.scala.com.epam.meetup.batch.DataSetExample.scala/udf/35.19.Dataset-MovieRating).map","Type: org.apache.spark.sql.Dataset[(com.epam.meetup.batch.DataSetExample.ActorMovieRelation, com.epam.meetup.batch.DataSetExample.MovieRating)]
Call: map

a => new ActorRating(a._1.name, a._2.rating)
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-realtime.realtime1.src.main.scala.com.tian.realtime1.RealtimeApp1.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr = line.split("","")
        val date = new Date(arr(0).toLong)
        AdsInfo(arr(0).toLong, new Timestamp(arr(0).toLong), dayFormatter.format(date), hmFormatter.format(date), arr(1), arr(2), arr(3), arr(4))
      }
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.duplicate.DuplicateDemo.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val arr: Array[String] = line.split("","")
        (arr(0), Timestamp.valueOf(arr(1)), arr(2))
      }
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.join.StreamingStatic.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val splits = line.split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.join.StreamingStreaming.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val splits: Array[String] = line.split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.join.StreamingStreaming.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val splits: Array[String] = line.split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.opt.TypeOption.scala/udf/12.24.Dataset-People.filter","Type: org.apache.spark.sql.Dataset[com.tian.opt.People]
Call: filter

_.age > 20
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.opt.TypeOption.scala/udf/14.19.Dataset-People.map","Type: org.apache.spark.sql.Dataset[com.tian.opt.People]
Call: map

_.name
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.output.KafkaSinkBatch.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0) + "","" + row.getLong(1)
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.output.KafkaSinkStreaming.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0) + "","" + row.getLong(1)
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark-review.structured-streaming.src.main.scala.com.tian.window.Window2.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val splits: Array[String] = line.split("","")
        (splits(0), splits(1))
      }
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark.spark-sql-proj.src.main.scala.com.tian.sqlproj.SqlPractice.scala/udf/7.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityRemarkUDAF
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark.spark-sql.src.main.scala.com.tian.day01.udf.MyAvgDemo.scala/udf/7.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAvg
"
"udf/spark_repos_7/1_Tiankx1003_BigData-Spark/..spark.spark-sql.src.main.scala.com.tian.day01.udf.MySumDemo.scala/udf/7.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MySum
"
"udf/spark_repos_7/1_wangshubing1_spark-study-scala/..src.main.scala.cn.spark.study.sql.DataFrameOperation.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 18
"
"udf/spark_repos_7/1_wangshubing1_spark-study-scala/..src.main.scala.cn.spark.study.sql.SparkExampleUDF.scala/udf/27.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length
"
"udf/spark_repos_7/1_wangshubing1_spark-study-scala/..src.main.scala.cn.spark.study.sql.SparkExampleUDF.scala/udf/31.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAdult _
"
"udf/spark_repos_7/1_wangshubing1_spark-study-scala/..src.main.scala.cn.spark.study.sql.UDAF.scala/udf/21.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new StringCount
"
"udf/spark_repos_7/1_wangshubing1_spark-study-scala/..src.main.scala.cn.spark.study.sql.UDF.scala/udf/21.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length()
"
"udf/spark_repos_7/1_wardziniak_aviation-analyzer/..data-analyzer.src.main.scala.com.wardziniak.aviation.analyzer.app.AnalyzerApp.scala/udf/14.17.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[AnalyzerApp.this.Person]
Call: map

p => p.copy(age = p.age + 7)
"
"udf/spark_repos_7/1_wardziniak_aviation-analyzer/..data-analyzer.src.main.scala.com.wardziniak.aviation.analyzer.app.JdbcSampleApp.scala/udf/16.17.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[com.wardziniak.aviation.analyzer.app.JdbcSampleApp.Person]
Call: map

p => p.copy(age = p.age + 7)
"
"udf/spark_repos_7/1_wardziniak_aviation-analyzer/..data-analyzer.src.main.scala.com.wardziniak.aviation.analyzer.app.PocStructureStreamKafkaApp.scala/udf/14.17.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[com.wardziniak.aviation.analyzer.app.PocStructureStreamKafkaApp.Person]
Call: map

p => p.copy(age = p.age + 7)
"
"udf/spark_repos_7/1_wardziniak_aviation-analyzer/..data-analyzer.src.main.scala.com.wardziniak.aviation.analyzer.app.StructureStreamApp.scala/udf/20.17.Dataset-Person.map","Type: org.apache.spark.sql.Dataset[com.wardziniak.aviation.analyzer.app.StructureStreamApp.Person]
Call: map

p => p.copy(age = p.age + 4)
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.IpLoactionSQL2.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.IpLoactionSQL2.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.IpLoactionSQL2.scala/udf/34.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(ipNum: Long) => {
        val ipRulesInExecutor: Array[(Long, Long, String)] = broadcastRef.value
        val index = MyUtils.binarySearch(ipRulesInExecutor, ipNum)
        var province = ""未知""
        if (index != -1) {
          province = ipRulesInExecutor(index)._3
        }
        province
      }
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.IpLoactionSQL.scala/udf/10.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split(""[|]"")
        val startNum = fields(2).toLong
        val endNum = fields(3).toLong
        val province = fields(6)
        (startNum, endNum, province)
      }
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.IpLoactionSQL.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

log => {
        val fields = log.split(""[|]"")
        val ip = fields(1)
        val ipNum = MyUtils.ip2Long(ip)
        ipNum
      }
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.JdbcDataSource.scala/udf/9.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" <= 13
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.JoinTest.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

l => {
        val fields = l.split("","")
        val ename = fields(0)
        val cname = fields(1)
        (ename, cname)
      }
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.JoinTest.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val fields = line.split("","")
        val id = fields(0).toLong
        val name = fields(1)
        val nationCode = fields(2)
        (id, name, nationCode)
      }
"
"udf/spark_repos_7/1_wjwujun_spark-work/..src.scala.sql.SQLFavTeacher.scala/udf/11.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val tIndex = line.lastIndexOf(""/"") + 1
        val teacher = line.substring(tIndex)
        val host = new URL(line).getHost
        val sIndex = host.indexOf(""."")
        val subject = host.substring(0, sIndex)
        (subject, teacher)
      }
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.BoundBox.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""LON"").isNotNull.and(col(""LAT"").isNotNull)
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/119.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""TYPE"") === OsmEntity.RELATION
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/121.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

size(col(""RELATION_NODES"")) > 0
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/129.29.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""TYPE"") === OsmEntity.RELATION
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/131.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

size(col(""RELATION_WAYS"")) > 0
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/140.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === OsmEntity.NODE
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/142.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => referencedNodes_bc.value.contains(row.getAs[Long](""ID""))
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/147.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === OsmEntity.WAY
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/149.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => referencedWays_bc.value.contains(row.getAs[Long](""ID""))
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === OsmEntity.NODE && col(""LON"") >= envelope.getMinX && col(""LON"") <= envelope.getMaxX && col(""LAT"") >= envelope.getMinY && col(""LAT"") <= envelope.getMaxY
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/159.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""TYPE"") === OsmEntity.WAY
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/167.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === OsmEntity.NODE
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/169.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => allNodes_bc.value.contains(row.getAs[Long](""ID""))
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/26.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

polygonFilterUdf(col(""LON""), col(""LAT""))
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === OsmEntity.WAY
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/39.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => Option(row.getAs[Seq[Long]](""WAY"")).exists(_.exists(nodes_ids_bc.value.contains(_)))
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === OsmEntity.RELATION
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/51.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => {
        val node_match = Option(row.getAs[Seq[Long]](""RELATION_NODES"")).exists(_.exists(nodes_ids_bc.value.contains(_)))
        val way_match = Option(row.getAs[Seq[Long]](""RELATION_WAYS"")).exists(_.exists(ways_ids_bc.value.contains(_)))
        node_match || way_match
      }
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/71.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === OsmEntity.RELATION
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.Extract.scala/udf/73.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => selectedIds_bc.value.contains(row.getAs[Long](""ID""))
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.geometry.ResolveMultipolygon.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""MEMBER"")(""TYPE"") === OsmEntity.WAY
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.geometry.ResolveMultipolygon.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === OsmEntity.WAY
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.geometry.WayGeometry.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""TYPE"") === 0
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.render.Renderer.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

geometryValidator(col(""geometry""))
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.render.Renderer.scala/udf/67.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""ZOOM"") >= col(""minZoom"")
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.render.Renderer.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isRenderableUdf(col(""symbolizer""), col(""ZOOM""), col(""PROJECTED""), col(""parameters""))
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.WriteOsmosis.scala/udf/20.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""TYPE"") === OsmEntity.NODE
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.WriteOsmosis.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""TYPE"") === OsmEntity.WAY
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.WriteOsmosis.scala/udf/28.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""TYPE"") === OsmEntity.WAY
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.WriteOsmosis.scala/udf/32.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""TYPE"") === OsmEntity.RELATION
"
"udf/spark_repos_7/1_woltapp_spark-osm-tools/..src.main.scala.com.wolt.osm.spark.WriteOsmosis.scala/udf/36.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""TYPE"") === OsmEntity.RELATION
"
"udf/spark_repos_7/1_xtxxtxxtx_commerce/..areaStat.src.main.scala.area.AreaTop3Stat.scala/udf/21.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(v1: Long, v2: String, split: String) => v1 + split + v2
"
"udf/spark_repos_7/1_xtxxtxxtx_commerce/..areaStat.src.main.scala.area.AreaTop3Stat.scala/udf/25.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new GroupConcatDistinct
"
"udf/spark_repos_7/1_xtxxtxxtx_commerce/..areaStat.src.main.scala.area.AreaTop3Stat.scala/udf/30.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(json: String, field: String) => {
        val jSONObject = JSONObject.fromObject(json)
        jSONObject.getString(field)
      }
"
"udf/spark_repos_7/1_yuedingmengxiangqidong_sparkmall0901_test/..sparkmall-offline.src.main.scala.com.atguigu.sparkmall0901.offline.app.AreaCountApp.scala/udf/13.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new CityRationUDAF
"
"udf/spark_repos_7/1_zhengruifeng_SparkFM/..src.main.scala.org.apache.spark.ml.fm.DistributedFM.scala/udf/123.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val vec = row.getAs[Vector](0)
          require(vec.size == n)
          var prevIndex = Int.MinValue
          vec.foreachActive({
            case (i, v) =>
              require(!v.isNaN)
              require(0 <= i && i < n)
              require(prevIndex <= i)
              prevIndex = i
          })
          true
        }
"
"udf/spark_repos_7/1_zzy_bigdata4job/..recmd.scala/udf/527.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

salaryMinColumn >= salaryMinRjjs && salaryMaxColumn <= salaryMaxRjjs && (experienceColumn.contains(""无工作经验"") || experienceColumn.contains(""无经验"") || experienceColumn.contains(""在读"") || experienceColumn.contains(""应届"") || experienceColumn.contains(""1年以下""))
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.ap_raw_lzw.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

deviceInfoDF(""gb_code"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.ap_raw_lzw.scala/udf/72.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFapraw(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.ifaas_face_lzw.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFifaas_face(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.ifaas_warning_lzw.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFifaas_warning(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.rzx_device_lzw.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFrzx_device(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.rzx_feature_lzw.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

deviceInfoDF(""gb_code"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.rzx_feature_lzw.scala/udf/71.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFrzx_feature(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.rzx_location_lzw.scala/udf/71.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFrzx_location(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.sensordoor_face_lzw.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFsensordoor_face(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.sensordoor_idcard_lzw.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFsensordoor_idcard(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.st_alarm_lzw.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFSt(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.st_status_lzw.scala/udf/69.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFSt(""gbNo"") =!= """"
"
"udf/spark_repos_7/2_0xqq_sibat_shenyundata_analyze/..shenyundataaseess.ty_imsi_lzw.scala/udf/67.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

parquetDFty_imsi(""gbNo"") =!= """"
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-11.src.main.scala.chapter11.analysis.streaming.StreamingAnalysis_Bak.scala/udf/16.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

t => {
        val gson = new Gson()
        val answer = gson.fromJson(t._2, classOf[Answer])
        answer
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-11.src.main.scala.chapter11.analysis.streaming.StreamingAnalysis.scala/udf/19.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

t => {
        val gson = new Gson()
        val answer = gson.fromJson(t._2, classOf[Answer])
        answer
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_3_2.scala/udf/13.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sex"" === ""Female""
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_3_2.scala/udf/15.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""age"" < 25
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_3_2.scala/udf/22.24.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[chapter7.Chapter7_3_2.User]
Call: filter

_.sex == ""Female""
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_3_2.scala/udf/24.22.Dataset-User.filter","Type: org.apache.spark.sql.Dataset[chapter7.Chapter7_3_2.User]
Call: filter

_.age < 25
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_5_1_1.scala/udf/14.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sex"" === ""Female""
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_5_1_1.scala/udf/16.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""age"" < 25
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_5_1_1.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""sex"" === ""Female"" && $""age"" < 25
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_5_1_1.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[String](""name"").toLowerCase
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_6_1.scala/udf/18.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(column: String) => column.toUpperCase
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_6_2.scala/udf/18.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(column: Any) => column.toString.toDouble
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-7.src.main.scala.chapter7.Chapter7_6_2.scala/udf/22.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AverageUDAF
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_10_2.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => (s, s.reverse)
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_10_3.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0) + "","" + row.getLong(1).toString
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_10_4.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0) + "","" + row.getLong(1).toString
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_5_3.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf.parse(arr(0))
        (new Timestamp(date.getTime), arr(1))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_5_3_SQL.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf.parse(arr(0))
        (new Timestamp(date.getTime), arr(1))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_6_1.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf.parse(arr(0))
        (new Timestamp(date.getTime), arr(1))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_6_2.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf.parse(arr(0))
        (new Timestamp(date.getTime), arr(1))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_7.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf.parse(arr(1))
        (arr(0), new Timestamp(date.getTime), arr(2))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_8_1.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        (arr(0), arr(1).toInt)
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_8_2.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf.parse(arr(2))
        (arr(0), arr(1), new Timestamp(date.getTime))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_8_2.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf.parse(arr(2))
        (arr(0), arr(1).toInt, new Timestamp(date.getTime))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_9_1.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf1.parse(arr(0))
        (new Timestamp(date.getTime), arr(1))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_9_1.scala/udf/56.22.Dataset-(String, String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, String, Long)]
Call: filter

_ != null
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.Chapter9_9_2.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        val date = sdf1.parse(arr(0))
        (new Timestamp(date.getTime), arr(1))
      }
"
"udf/spark_repos_7/24_Zjinji_SparkSyllabusBook/..chapter-9.src.main.scala.chapter9.WordCountTest.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => {
        val arr = s.split("","")
        (arr(0), new Timestamp(arr(1).toLong), arr(2))
      }
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.ml.spark.Classification.scala/udf/100.22.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
          x => (x.getString(1), x.getDouble(0))
        }
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.ml.spark.Classification.scala/udf/72.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

{
          x => (x.getString(0), (x.getString(2), x.getDouble(1)))
        }
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.ml.spark.SparkML.scala/udf/37.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(col.name).isNull
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.models.SeriesMissingValueModel.scala/udf/103.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

rowIdVar === sprowid
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.models.SeriesMissingValueModel.scala/udf/121.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(col(series.id) <= lit(m2sp(key))).and(not(isnull(col(colName.id)))).and(rowIdVar =!= sprowid)
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.models.SeriesMissingValueModel.scala/udf/142.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

(col(series.id) >= lit(m2sp(key))).and(not(isnull(col(colName.id)))).and(rowIdVar =!= sprowid)
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.models.SparkClassifierModel.scala/udf/52.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isnull(col(column.id))
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.statistics.DetectSeries.scala/udf/152.27.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

not(isnull(col(seriesColumnName.id)))
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.mimir.statistics.DetectSeries.scala/udf/169.24.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (ID(row.getString(1)), row.getDouble(0))
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.org.apache.spark.sql.execution.datasources.ubodin.csv.CSVUtils.scala/udf/10.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

length(trim($""value"")) > 0
"
"udf/spark_repos_7/25_UBOdin_mimir/..src.main.scala.org.apache.spark.sql.execution.datasources.ubodin.csv.CSVUtils.scala/udf/15.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

!$""value"".startsWith(options.comment.toString)
"
"udf/spark_repos_7/2_adaltas_spark-mllib-streaming/..src.main.scala.com.adaltas.taxistreaming.clustering.MainKmeans.scala/udf/47.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""startHour"") === h
"
"udf/spark_repos_7/2_adaltas_spark-mllib-streaming/..src.main.scala.com.adaltas.taxistreaming.processing.TaxiProcessingBatch.scala/udf/15.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""startLon"") >= lonWest && col(""startLon"") <= lonEast && col(""startLat"") >= latSouth && col(""startLat"") <= latNorth && col(""endLon"") >= lonWest && col(""endLon"") <= lonEast && col(""endLat"") >= latSouth && col(""endLat"") <= latNorth
"
"udf/spark_repos_7/2_adaltas_spark-mllib-streaming/..src.main.scala.com.adaltas.taxistreaming.processing.TaxiProcessingBatch.scala/udf/17.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""isStart"") === ""END""
"
"udf/spark_repos_7/2_adaltas_spark-mllib-streaming/..src.main.scala.com.adaltas.taxistreaming.processing.TaxiProcessingBatch.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""tip"") > 0
"
"udf/spark_repos_7/2_adaltas_spark-mllib-streaming/..src.main.scala.com.adaltas.taxistreaming.processing.TaxiProcessing.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""startLon"") >= lonWest && col(""startLon"") <= lonEast && col(""startLat"") >= latSouth && col(""startLat"") <= latNorth && col(""endLon"") >= lonWest && col(""endLon"") <= lonEast && col(""endLat"") >= latSouth && col(""endLat"") <= latNorth
"
"udf/spark_repos_7/2_adaltas_spark-mllib-streaming/..src.main.scala.com.adaltas.taxistreaming.processing.TaxiProcessing.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""isStart"") === ""END""
"
"udf/spark_repos_7/2_bkosaraju_curate-dataset/..curation-app.src.main.scala.au.com.telstra.in.oan.dataCuration.functions.ConvNonStdDateTimes.scala/udf/25.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""dqValidityFlag"").equalTo(lit(""Y""))
"
"udf/spark_repos_7/2_bkosaraju_curate-dataset/..curation-app.src.main.scala.au.com.telstra.in.oan.dataCuration.functions.ConvNonStdDateTimes.scala/udf/32.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""dqValidityFlag"").equalTo(lit(""Y""))
"
"udf/spark_repos_7/2_bkosaraju_curate-dataset/..curation-app.src.main.scala.au.com.telstra.in.oan.dataCuration.functions.ConvNonStdDateTimes.scala/udf/35.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""dqValidityFlag"").equalTo(lit(""N""))
"
"udf/spark_repos_7/2_bkosaraju_curate-dataset/..curation-app.src.main.scala.au.com.telstra.in.oan.dataCuration.functions.SchemaFlattener.scala/udf/42.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(clmn).isNull
"
"udf/spark_repos_7/2_bkosaraju_curate-dataset/..curation-app.src.main.scala.au.com.telstra.in.oan.dataCuration.functions.SchemaFlattener.scala/udf/45.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

size(col(clmn)).equalTo(0)
"
"udf/spark_repos_7/2_bkosaraju_curate-dataset/..curation-app.src.main.scala.au.com.telstra.in.oan.dataCuration.functions.SchemaFlattener.scala/udf/48.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(clmn).isNotNull
"
"udf/spark_repos_7/2_ChitturiPadma_Spark_New_Libraries_Research/..SpatialData_Analysis.Magellan.UberTraffic_Analysis.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""point"" within ($""polygon"")
"
"udf/spark_repos_7/2_ChitturiPadma_Spark_New_Libraries_Research/..SpatialData_Analysis.Magellan.UberTraffic_Analysis.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""point"" within ($""polygon"")
"
"udf/spark_repos_7/2_ChitturiPadma_Spark_New_Libraries_Research/..SpatialData_Analysis.Magellan.UberTraffic_Analysis.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""point"" intersects ($""polygon"")
"
"udf/spark_repos_7/2_fangwendong_gbdt_leaf_spark/..src.main.scala.org.apache.spark.ml.mleap.gbdt.GbdtLeaf.scala/udf/65.24.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

not(col($(validationIndicatorCol)))
"
"udf/spark_repos_7/2_fangwendong_gbdt_leaf_spark/..src.main.scala.org.apache.spark.ml.mleap.gbdt.GbdtLeaf.scala/udf/68.24.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

col($(validationIndicatorCol))
"
"udf/spark_repos_7/2_fpopic_gg-interview-challenge/..src.main.scala.com.fp.GumGumJoiner.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val pageViewId = pvPattern.findFirstMatchIn(line).get.group(1)
        val numImpressions = 1
        Impression(pageViewId, numImpressions)
      }
"
"udf/spark_repos_7/2_fpopic_gg-interview-challenge/..src.main.scala.com.fp.GumGumJoiner.scala/udf/30.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

ePattern.findFirstIn(_).isDefined
"
"udf/spark_repos_7/2_fpopic_gg-interview-challenge/..src.main.scala.com.fp.GumGumJoiner.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val pageViewId = pvPattern.findFirstMatchIn(line).get.group(1)
        val eventType = ePattern.findFirstMatchIn(line).get.group(1)
        val numViews = if (eventType == ""view"") 1 else 0
        val numClicks = 1 - numViews
        AdEvent(pageViewId, numViews, numClicks)
      }
"
"udf/spark_repos_7/2_fpopic_gg-interview-challenge/..src.main.scala.com.fp.GumGumJoiner.scala/udf/42.19.Dataset-PageViewStats.map","Type: org.apache.spark.sql.Dataset[com.fp.PageViewStats]
Call: map

{
        case PageViewStats(pageViewId, numImpressions, numViews, numClicks) =>
          s""$pageViewId\t$numImpressions\t$numViews\t$numClicks""
      }
"
"udf/spark_repos_7/2_fpopic_gg-interview-challenge/..src.test.scala.com.fp.GumGumJoinerTest.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val pageViewId = pvPattern.findFirstMatchIn(line).get.group(1)
        val numImpressions = 1
        Impression(pageViewId, numImpressions)
      }
"
"udf/spark_repos_7/2_fpopic_gg-interview-challenge/..src.test.scala.com.fp.GumGumJoinerTest.scala/udf/33.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

ePattern.findFirstIn(_).isDefined
"
"udf/spark_repos_7/2_fpopic_gg-interview-challenge/..src.test.scala.com.fp.GumGumJoinerTest.scala/udf/35.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val pageViewId = pvPattern.findFirstMatchIn(line).get.group(1)
        val eventType = ePattern.findFirstMatchIn(line).get.group(1)
        val numViews = if (eventType == ""view"") 1 else 0
        val numClicks = 1 - numViews
        AdEvent(pageViewId, numViews, numClicks)
      }
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/43.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/62.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/66.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/71.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/95.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/2_geekyouth_spark-examples_2.1.1/..src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/34.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_7/2_gilbutITbook_006908/..ch03.scala.org.sia.chapter03App.GitHubDay.scala/udf/20.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isEmp
"
"udf/spark_repos_7/2_gilbutITbook_006908/..ch03.scala.org.sia.chapter03App.GitHubDay.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

sqlFunc($""login"")
"
"udf/spark_repos_7/2_GlassyWing_components-recommend/..src.main.scala.org.manlier.recommend.LinearItemCFModel.scala/udf/110.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getInt(2), row.getInt(1), row.getDouble(3))
"
"udf/spark_repos_7/2_GlassyWing_components-recommend/..src.main.scala.org.manlier.recommend.LinearItemCFModel.scala/udf/13.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => History(row.getString(0).toInt, row.getString(1).toInt, row.getString(2).toInt, row.getString(3).toFloat)
"
"udf/spark_repos_7/2_GlassyWing_components-recommend/..src.main.scala.org.manlier.recommend.LinearItemCFModel.scala/udf/58.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        val compId = row.getInt(0)
        val followCompId1 = row.getInt(1)
        val followCompId2 = row.getInt(2)
        val size = row.getLong(3)
        val numRaters1 = row.getLong(4)
        val numRaters2 = row.getLong(5)
        val cooc = cooccurrence(size, numRaters1, numRaters2)
        (compId, followCompId1, followCompId2, cooc)
      }
"
"udf/spark_repos_7/2_GlassyWing_components-recommend/..src.test.scala.others.ALSRecommendNewTest.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_7/2_lassalleloan_anomaly-detection/..intelligent-web-application-firewall.src.main.scala.com.loanlassalle.intelligentwaf.AnomalyDetector.scala/udf/172.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

distanceToCentroid(model, _) >= threshold
"
"udf/spark_repos_7/2_lassalleloan_anomaly-detection/..intelligent-web-application-firewall.src.main.scala.com.loanlassalle.intelligentwaf.AnomalyDetector.scala/udf/184.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[String](""label"").equals(""anomaly"")
"
"udf/spark_repos_7/2_lassalleloan_anomaly-detection/..intelligent-web-application-firewall.src.main.scala.com.loanlassalle.intelligentwaf.AnomalyDetector.scala/udf/205.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

distanceToCentroid(model, _)
"
"udf/spark_repos_7/2_lassalleloan_anomaly-detection/..intelligent-web-application-firewall.src.main.scala.com.loanlassalle.intelligentwaf.AnomalyDetector.scala/udf/228.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val label = row.getAs[String](""label"").equals(""normal"").compareTo(false).toDouble
        val prediction = row.getAs[Int](""prediction"").toDouble
        prediction -> label
      }
"
"udf/spark_repos_7/2_lassalleloan_anomaly-detection/..intelligent-web-application-firewall.src.main.scala.com.loanlassalle.intelligentwaf.IntelligentWaf.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[String](""label"").equals(""anomaly"")
"
"udf/spark_repos_7/2_lassalleloan_anomaly-detection/..intelligent-web-application-firewall.src.main.scala.com.loanlassalle.intelligentwaf.IntelligentWaf.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getAs[String](""label"").equals(""anomaly"")
"
"udf/spark_repos_7/2_lxw2_Spark_Pro/..Spark.src.main.scala.it.luke.sql.spark_sql_json.spark_01_parquet_json.scala/udf/13.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

item => {
          val bool = item.getString(0).startsWith(""i"")
          if (bool) {
            false
          } else {
            true
          }
        }
"
"udf/spark_repos_7/2_lxw2_Spark_Pro/..Spark.src.main.scala.it.luke.sql.spark_sql_json.spark_01_parquet_json.scala/udf/22.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

item => {
        val arr = item.getString(0).split("","")
        val id = arr(0)
        val name = arr(1)
        (id, name)
      }
"
"udf/spark_repos_7/2_lxw2_Spark_Pro/..Spark.src.main.scala.it.luke.sql.Spark_sql_pro.scala/udf/35.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udf.fixID _
"
"udf/spark_repos_7/2_lxw2_Spark_Pro/..Spark.src.main.scala.it.luke.sql.spark_sql_tran.spark_03_filter.scala/udf/12.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[it.luke.sql.spark_sql_tran.Person]
Call: filter

p => p.age >= 30
"
"udf/spark_repos_7/2_lxw2_Spark_Pro/..Spark.src.main.scala.it.luke.sql.spark_sql_tran.spark_11_udf.scala/udf/10.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

addPrfix _
"
"udf/spark_repos_7/2_lxw2_Spark_Pro/..Spark.src.main.scala.it.luke.sql.spark_sql_tran.spark_16_NullProcess.scala/udf/12.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

item => {
          val bool: Boolean = item.startsWith(""id"")
          if (bool) {
            false
          } else {
            true
          }
        }
"
"udf/spark_repos_7/2_lxw2_Spark_Pro/..Spark.src.main.scala.it.luke.sql.spark_sql_tran.spark_16_NullProcess.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

item => {
        val arr: Array[String] = item.split(""\\|"")
        val id = arr(0)
        val name = arr(1)
        (id, name)
      }
"
"udf/spark_repos_7/2_min281004_saprkmall/..sparkmall-offline.src.main.scala.com.sha.sparkmall.offline.app.AreaTop3ProductApp.scala/udf/15.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

cityRemarkUDAF
"
"udf/spark_repos_7/2_multivacplatform_multivac-wikipedia/..build_pageviews.src.main.scala.dataframe_helpers.ParquetHelper.scala/udf/11.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""year"" === year
"
"udf/spark_repos_7/2_multivacplatform_multivac-wikipedia/..build_pageviews.src.main.scala.dataframe_helpers.ParquetHelper.scala/udf/13.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""month"" === month
"
"udf/spark_repos_7/2_multivacplatform_multivac-wikipedia/..build_pageviews.src.main.scala.dataframe_helpers.ParquetHelper.scala/udf/15.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""day"" === day
"
"udf/spark_repos_7/2_nathan-gs_eventhubs-reingest/..src.main.scala.gs.nathan.eventhubsreingest.sql.udfs.ColumnToPartition.scala/udf/6.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

this.function
"
"udf/spark_repos_7/2_nathan-gs_eventhubs-reingest/..src.main.scala.gs.nathan.eventhubsreingest.sql.udfs.RandomPartition.scala/udf/7.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

this.RandomPartition
"
"udf/spark_repos_7/2_nathan-gs_eventhubs-reingest/..src.main.scala.gs.nathan.eventhubsreingest.sql.udfs.ToTimestamp.scala/udf/9.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ToTimestamp
"
"udf/spark_repos_7/2_ONI-MIKO_spring-boot-spark/..spring-boot-spark-starters.spring-boot-spark-starter-data.src.main.scala.com.baosight.spark.starter.data.mapper.core.SparkSqlRegisterManager.scala/udf/28.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

entry._2
"
"udf/spark_repos_7/2_pavanpkulkarni_Spark_Streaming_Examples/..src.main.scala.com.pavanpkulkarni.sparkstreaming.FileToFileStreaming.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => Customer(x.getString(0), x.getString(1), x.getString(2))
"
"udf/spark_repos_7/2_phact_streaming-ml-product-recommendation/..src.main.scala.com.datastax.powertools.analytics.SparkMLProductRecommendationBatchJob.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

predictions(""user"") === 2664
"
"udf/spark_repos_7/2_phact_streaming-ml-product-recommendation/..src.main.scala.com.datastax.powertools.analytics.SparkMLProductRecommendations.scala/udf/49.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => ""item: "" + x.getLong(0) + "" user: "" + x.getLong(1) + "" rating: "" + x.getFloat(2)
"
"udf/spark_repos_7/2_phact_streaming-ml-product-recommendation/..src.main.scala.com.datastax.powertools.analytics.SparkMLProductRecommendationStreamingJob.scala/udf/45.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => ""item: "" + x.getLong(0) + "" user: "" + x.getLong(1) + "" rating: "" + x.getFloat(2)
"
"udf/spark_repos_7/2_ShikharSundriyal_SparkInterviewQuestions/..src.main.scala.com.ss.SparkQuestion1.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

year($""yoj"").between(2017, 2019)
"
"udf/spark_repos_7/2_ShikharSundriyal_SparkInterviewQuestions/..src.main.scala.com.ss.SparkQuestion1.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""amount"" > 50
"
"udf/spark_repos_7/2_soerenreichardt_cypher-for-apache-flink/..morpheus-examples.src.main.scala.org.opencypher.morpheus.integration.yelp.YelpHelpers.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""city"" === city
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.aa_excl_oreilly.LoadHive.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getInt(0)
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.aa_excl_oreilly.SparkSQLTwitter.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.aa_excl_oreilly.SparkSQLTwitter.scala/udf/39.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(_: String).length
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.SQLDataSourceExample.scala/udf/36.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.resources.official_examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.dataframe.Basic.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.dataframe.DatasetConversion.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.dataframe.UDF.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

myNameFilter($""name"")
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.dataframe.UDF.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

salesFilter($""sales"", lit(2000.0d))
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.dataframe.UDF.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

stateFilter($""state"", array(lit(""CA""), lit(""MA""), lit(""NY""), lit(""NJ"")))
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.dataframe.UDF.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

multipleFilter($""state"", $""discount"", struct(lit(""CA""), lit(100.0d)))
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/58.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

struct _
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/67.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAtomic _
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/71.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

arrayLength _
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.JSON.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => s.replaceAllLiterally(""$"", """")
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.UDAF2.scala/udf/48.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.UDAF_Multi.scala/udf/43.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mystats
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.UDAF.scala/udf/41.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.UDF.scala/udf/16.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

westernState _
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.UDF.scala/udf/27.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

manyCustomers _
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.UDF.scala/udf/47.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

stateRegion _
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.UDF.scala/udf/62.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

discountRatio _
"
"udf/spark_repos_7/2_steklopod_Spark-examples/..src.main.scala.sql.UDF.scala/udf/76.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeStruct _
"
"udf/spark_repos_7/2_TalentOrigin_spark-course/..src.main.scala.advanced.spark.SparkUDFExample2.scala/udf/10.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

averageUDF
"
"udf/spark_repos_7/2_TalentOrigin_spark-course/..src.main.scala.transformations.SparkReference.scala/udf/14.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

toUpperUDF
"
"udf/spark_repos_7/2_TalentOrigin_spark-course/..src.main.scala.transformations.SparkReference.scala/udf/18.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

fetchFirstGenreUDF
"
"udf/spark_repos_7/2_TalentOrigin_spark-course/..src.main.scala.transformations.SparkReference.scala/udf/22.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

concatStringsUDF
"
"udf/spark_repos_7/2_TalentOrigin_spark-course/..src.main.scala.transformations.SparkReference.scala/udf/26.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

columnLengthsUDF
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.app.TaxiODForParquetGPS.scala/udf/15.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""date"") === dateArray(i)
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.app.TaxiODForParquetGPS.scala/udf/17.21.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
          val uptime = row.getString(row.fieldIndex(""upTime""))
          val downtime = row.getString(row.fieldIndex(""downTime""))
          val upTime = sdf.format(sdf1.parse(uptime))
          val downTime = sdf.format(sdf1.parse(downtime))
          var color = row.getString(row.fieldIndex(""color""))
          val company = row.getString(row.fieldIndex(""company""))
          if (color.equals(""null"")) {
            color = company match {
              case ""1"" => ""红的""
              case ""2"" => ""绿的""
              case ""3"" => ""蓝的""
            }
          }
          TaxiDealClean(row.getString(row.fieldIndex(""carId"")), row.getString(row.fieldIndex(""date"")), upTime, downTime, row.getDouble(row.fieldIndex(""singlePrice"")), row.getDouble(row.fieldIndex(""runningDistance"")), row.getString(row.fieldIndex(""runTime"")), row.getDouble(row.fieldIndex(""sumPrice"")), row.getDouble(row.fieldIndex(""emptyDistance"")), color, company)
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.ReadTaxiGPSToHDFS.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""date"") === formatDate
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDataCleanUtils.scala/udf/127.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
          val split = line.split("","")
          Tuple14.apply(split(0), split(1).toDouble, split(2).toDouble, split(3), split(4), split(5), split(6), split(7), split(8), split(9), split(10), split(11), split(12), split(13))
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDataCleanUtils.scala/udf/139.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val sdf1 = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"")
        val sdf2 = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"")
        val time = {
          val time = row.getString(row.fieldIndex(""time""))
          sdf2.format(sdf1.parse(time))
        }
        val bd = TaxiData(row.getString(row.fieldIndex(""carId"")), row.getDouble(row.fieldIndex(""lon"")), row.getDouble(row.fieldIndex(""lat"")), time, row.getString(row.fieldIndex(""SBH"")), row.getString(row.fieldIndex(""speed"")), row.getString(row.fieldIndex(""direction"")), row.getString(row.fieldIndex(""locationStatus"")), row.getString(row.fieldIndex(""X"")), row.getString(row.fieldIndex(""SMICarid"")), row.getString(row.fieldIndex(""carStatus"")), row.getString(row.fieldIndex(""carColor"")))
        bd
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDataCleanUtils.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""lon"") =!= 0.0d && col(""lat"") =!= 0.0d
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDataCleanUtils.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""locationStatus"") === lit(""0"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDataCleanUtils.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""lon"") < lit(135.05d) && col(""lat"") < lit(53.55d) && col(""lon"") > lit(73.66d) && col(""lat"") > lit(3.86d)
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDataCleanUtils.scala/udf/55.22.Dataset-TaxiData.filter","Type: org.apache.spark.sql.Dataset[taxi.TaxiData]
Call: filter

_ != null
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDataInfo.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val d = row.getInt(row.fieldIndex(""value""))
        TaxiDataInfo(d / 10, d / 20, d, d, ""sum"")
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDealCleanUtils.scala/udf/16.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""upTime"") =!= null && col(""downTime"") =!= null
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDealCleanUtils.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        var carId = row.getString(row.fieldIndex(""carId""))
        if (carId.length > 7) {
          carId = carId.substring(0, 7)
        }
        val bd = TaxiDeal(carId, row.getString(row.fieldIndex(""upTime"")), row.getString(row.fieldIndex(""downTime"")), row.getDouble(row.fieldIndex(""singlePrice"")), row.getDouble(row.fieldIndex(""runningDistance"")), row.getString(row.fieldIndex(""runTime"")), row.getDouble(row.fieldIndex(""sumPrice"")), row.getDouble(row.fieldIndex(""emptyDistance"")), row.getString(row.fieldIndex(""consumptionId"")), row.getString(row.fieldIndex(""consumptionPre"")), row.getString(row.fieldIndex(""consumptionAft"")), row.getString(row.fieldIndex(""cutNumber"")), row.getString(row.fieldIndex(""cutTime"")), row.getString(row.fieldIndex(""overDistance"")), row.getString(row.fieldIndex(""overNumber"")), row.getString(row.fieldIndex(""license"")), row.getString(row.fieldIndex(""dealId"")))
        bd
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDealCleanUtils.scala/udf/39.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val sdf1 = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"")
        val sdf2 = new SimpleDateFormat(""yyyy-MM-dd'T'HH:mm:ss.sss'Z'"")
        val time1 = {
          val time1 = row.getString(row.fieldIndex(""upTime""))
          sdf2.format(sdf1.parse(time1))
        }
        val time2 = {
          val time2 = row.getString(row.fieldIndex(""downTime""))
          sdf2.format(sdf1.parse(time2))
        }
        val bd = TaxiDeal(row.getString(row.fieldIndex(""carId"")), time1, time2, row.getDouble(row.fieldIndex(""singlePrice"")), row.getDouble(row.fieldIndex(""runningDistance"")), row.getString(row.fieldIndex(""runTime"")), row.getDouble(row.fieldIndex(""sumPrice"")), row.getDouble(row.fieldIndex(""emptyDistance"")), row.getString(row.fieldIndex(""consumptionId"")), row.getString(row.fieldIndex(""consumptionPre"")), row.getString(row.fieldIndex(""consumptionAft"")), row.getString(row.fieldIndex(""cutNumber"")), row.getString(row.fieldIndex(""cutTime"")), row.getString(row.fieldIndex(""overDistance"")), row.getString(row.fieldIndex(""overNumber"")), row.getString(row.fieldIndex(""license"")), row.getString(row.fieldIndex(""dealId"")))
        bd
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDealCleanUtils.scala/udf/62.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => try {
          val time1 = {
            val time1 = row.getString(row.fieldIndex(""upTime""))
            sdf2.format(sdf1.parse(time1))
          }
          val time2 = {
            val time2 = row.getString(row.fieldIndex(""downTime""))
            sdf2.format(sdf1.parse(time2))
          }
          val bd = TaxiDeal(row.getString(row.fieldIndex(""carId"")), time1, time2, row.getDouble(row.fieldIndex(""singlePrice"")), row.getDouble(row.fieldIndex(""runningDistance"")), row.getString(row.fieldIndex(""runTime"")), row.getDouble(row.fieldIndex(""sumPrice"")), row.getDouble(row.fieldIndex(""emptyDistance"")), row.getString(row.fieldIndex(""consumptionId"")), row.getString(row.fieldIndex(""consumptionPre"")), row.getString(row.fieldIndex(""consumptionAft"")), row.getString(row.fieldIndex(""cutNumber"")), row.getString(row.fieldIndex(""cutTime"")), row.getString(row.fieldIndex(""overDistance"")), row.getString(row.fieldIndex(""overNumber"")), row.getString(row.fieldIndex(""license"")), row.getString(row.fieldIndex(""dealId"")))
          Some(bd)
        } catch {
          case e: Exception => None
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDealCleanUtils.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""runningDistance"") > 1000 && col(""sumPrice"") > 100
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDealCleanUtils.scala/udf/90.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val date = row.getString(row.fieldIndex(""upTime"")).split(""T"")(0)
        val bd = TaxiDealClean(row.getString(row.fieldIndex(""carId"")), date, row.getString(row.fieldIndex(""upTime"")), row.getString(row.fieldIndex(""downTime"")), row.getDouble(row.fieldIndex(""singlePrice"")), row.getDouble(row.fieldIndex(""runningDistance"")), row.getString(row.fieldIndex(""runTime"")), row.getDouble(row.fieldIndex(""sumPrice"")), row.getDouble(row.fieldIndex(""emptyDistance"")), row.getString(row.fieldIndex(""color"")), row.getString(row.fieldIndex(""company"")))
        bd
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDealTest.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => {
        val Array(carId, color, company) = str.split("","")
        new TaxiStatic(carId, color, company)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiDealTest.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.replaceAll("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiFunc.scala/udf/59.22.Dataset-TaxiOD.filter","Type: org.apache.spark.sql.Dataset[taxi.TaxiOD]
Call: filter

od => od.distance > 300.0d && od.upDif < 30.0d && od.downDif < 30.0d
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiODTest.scala/udf/59.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

str => {
        val Array(carId, color, company) = str.split("","")
        TaxiStatic(carId, color, company)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiODTest.scala/udf/68.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.replaceAll("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiODTest.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => line.replaceAll("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..CalTaxi.src.main.scala.taxi.TaxiTotal.scala/udf/14.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

s => try {
          val uptime = s.getString(s.fieldIndex(""upTime""))
          val downtime = s.getString(s.fieldIndex(""downTime""))
          val carId = s.getString(s.fieldIndex(""carId""))
          val singlePrice = s.getDouble(s.fieldIndex(""singlePrice""))
          val runningDistance = s.getDouble(s.fieldIndex(""runningDistance""))
          val sumPrice = s.getDouble(s.fieldIndex(""sumPrice""))
          val emptyDistance = s.getDouble(s.fieldIndex(""emptyDistance""))
          val runTime = s.getString(s.fieldIndex(""runTime""))
          val carType = s.getInt(s.fieldIndex(""carType""))
          val upTime = sdf2.format(sdf1.parse(uptime))
          val downTime = sdf2.format(sdf1.parse(downtime))
          val date = upTime.split("" "")(0)
          val company = carType.toString
          val color = company match {
            case ""1"" => ""红的""
            case ""2"" => ""绿的""
            case ""3"" => ""蓝的""
          }
          TaxiDealClean(carId, date, upTime, downTime, singlePrice, runningDistance, runTime, sumPrice, emptyDistance, color, company)
        } catch {
          case e: Exception =>
            TaxiDealClean(""1"", ""1"", ""1"", ""1"", 1.1d, 1.1d, ""1"", 1.1d, 1.1d, ""1"", ""1"")
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.BusArrivalApp2.scala/udf/40.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val route = row.getString(row.fieldIndex(""route""))
        val lineName = row.getString(row.fieldIndex(""lineName""))
        val direct = row.getString(row.fieldIndex(""direct""))
        val stationId = row.getString(row.fieldIndex(""stationId""))
        val stationName = row.getString(row.fieldIndex(""stationName""))
        val stationSeqId = row.getInt(row.fieldIndex(""stationSeqId""))
        val stationLon = row.getString(row.fieldIndex(""stationLon""))
        val stationLat = row.getString(row.fieldIndex(""stationLat""))
        val Array(lat, lon) = LocationUtil.gcj02_To_84(stationLat.toDouble, stationLon.toDouble).split("","")
        StationData(route, lineName, direct, stationId, stationName, stationSeqId.toInt, lon.toDouble, lat.toDouble, 0)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.BusArrivalApp.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val lineId = row.getString(row.fieldIndex(""lineId""))
        val direct = row.getString(row.fieldIndex(""direct""))
        val order = row.getInt(row.fieldIndex(""order""))
        val stationLon = row.getString(row.fieldIndex(""lon""))
        val stationLat = row.getString(row.fieldIndex(""lat""))
        val Array(lat, lon) = LocationUtil.gcj02_To_84(stationLat.toDouble, stationLon.toDouble).split("","")
        LineCheckPoint(lon.toDouble, lat.toDouble, lineId, order, direct)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.BusArrivalApp.scala/udf/55.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val route = row.getString(row.fieldIndex(""route""))
        val lineName = row.getString(row.fieldIndex(""lineName""))
        val direct = row.getString(row.fieldIndex(""direct""))
        val stationId = row.getString(row.fieldIndex(""stationId""))
        val stationName = row.getString(row.fieldIndex(""stationName""))
        val stationSeqId = row.getInt(row.fieldIndex(""stationSeqId""))
        val stationLon = row.getString(row.fieldIndex(""stationLon""))
        val stationLat = row.getString(row.fieldIndex(""stationLat""))
        val Array(lat, lon) = LocationUtil.gcj02_To_84(stationLat.toDouble, stationLon.toDouble).split("","")
        StationData(route, lineName, direct, stationId, stationName, stationSeqId.toInt, lon.toDouble, lat.toDouble, 0)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.BusArrivalBaseGPSApp.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val route = row.getString(row.fieldIndex(""route""))
        val lineName = row.getString(row.fieldIndex(""lineName""))
        val direct = row.getString(row.fieldIndex(""direct""))
        val stationId = row.getString(row.fieldIndex(""stationId""))
        val stationName = row.getString(row.fieldIndex(""stationName""))
        val stationSeqId = row.getInt(row.fieldIndex(""stationSeqId""))
        val stationLon = row.getString(row.fieldIndex(""stationLon""))
        val stationLat = row.getString(row.fieldIndex(""stationLat""))
        val Array(lat, lon) = LocationUtil.gcj02_To_84(stationLat.toDouble, stationLon.toDouble).split("","")
        StationData(route, lineName, direct, stationId, stationName, stationSeqId.toInt, lon.toDouble, lat.toDouble, 0)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.BusArrivalBaseGPSApp.scala/udf/41.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

s => s.split("","").length > 2 && !s.split("","")(1).equals(""00"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.BusArrivalBaseGPSApp.scala/udf/43.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

func = s => try {
          val split = s.replaceAll("",,"", "",null,"").split("","")
          val carId: String = split(3)
          val route: String = split(4)
          val lon: Double = split(8).toDouble
          val lat: Double = split(9).toDouble
          val upTime: String = DateUtil.timeFormat(split(11))
          val arrStation = split(16)
          var nextStation = ""null""
          try {
            nextStation = split(18)
          } catch {
            case e: Exception =>
          }
          BusData1(carId, route, lon, lat, upTime, ""null"", arrStation, ""name"", 0, ""up=1>2:"" + nextStation)
        } catch {
          case e: Exception =>
            println(s)
            BusData1(""null"", ""null"", 0.0d, 0.0d, ""null"", ""null"", ""null"", ""name"", 0, ""up=1>2"")
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.ExportBusArrivalApp.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val carId = line.getString(line.fieldIndex(""_c0"")).split(""\\|"")(1)
        val tripId = line.getString(line.fieldIndex(""_c1""))
        val route = line.getString(line.fieldIndex(""_c2""))
        val direct = line.getString(line.fieldIndex(""_c3""))
        val seqId = line.getString(line.fieldIndex(""_c4""))
        val index = ""0"" * (3 - seqId.length) + seqId
        val arrivalStation = line.getString(line.fieldIndex(""_c5""))
        val arrivalTime = line.getString(line.fieldIndex(""_c6"")).split(""\\."")(0).replace(""T"", "" "")
        val leaTime = line.getString(line.fieldIndex(""_c7"")).split(""\\."")(0).replace(""T"", "" "")
        val prefixStationId = line.getString(line.fieldIndex(""_c8""))
        val nextStationId = line.getString(line.fieldIndex(""_c9""))
        var preStationId = ""null""
        var preStopId = ""null""
        var nextStationSeqId = ""null""
        var nextStopId = ""null""
        var arrStationId = ""null""
        var arrStopId = ""null""
        try {
          val arrRow = bMap.value(arrivalStation)
          arrStationId = arrRow.getString(arrRow.fieldIndex(""stationSeqId""))
          arrStopId = arrRow.getInt(arrRow.fieldIndex(""stopId"")).toString
          if (!prefixStationId.equals(""null"")) {
            val preRow = bMap.value(prefixStationId)
            preStationId = preRow.getString(preRow.fieldIndex(""stationSeqId""))
            preStopId = preRow.getInt(preRow.fieldIndex(""stopId"")).toString
          }
          if (!nextStationId.equals(""null"")) {
            val nextRow = bMap.value(nextStationId)
            nextStationSeqId = nextRow.getString(nextRow.fieldIndex(""stationSeqId""))
            nextStopId = nextRow.getInt(nextRow.fieldIndex(""stopId"")).toString
          }
        } catch {
          case e: Exception =>
        }
        tripId + "","" + route + "","" + direct + "","" + carId + "","" + index + "","" + arrStationId + "","" + arrStopId + "","" + arrivalTime + "","" + leaTime + "","" + preStationId + "","" + preStopId + "","" + nextStationSeqId + "","" + nextStopId
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.PassengerFlows.scala/udf/10.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.split("","").length > 12
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.PassengerFlows.scala/udf/12.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ s => 
          val split = s.split("","")
          val o = split(6)
          val d = split(12)
          val Array(hour, minute) = split(5).substring(split(5).indexOf(""T"") + 1, split(5).indexOf(""T"") + 6).split("":"")
          val time = hour.toDouble + minute.toDouble / 60.0d
          (o + "","" + d + "","" + time, 1)
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.apps.PassengerFlows.scala/udf/21.22.Dataset-(String, Int).filter","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: filter

_._1.contains(""XBUS_00024315"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BMWAPP.scala/udf/32.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => s.replaceAll(""\"""", """").replaceAll("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BMWAPP.scala/udf/34.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.split("","").length > 8
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BMWAPP.scala/udf/36.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
          val split = line.split("","")
          val id = split(0)
          val gpsTime = split(8).replaceAll(""\"""", """")
          val lon = split(2).toDouble
          val lat = split(3).toDouble
          val imsi = split(4)
          val speed = split(5).toDouble
          val seqId = split(6)
          val direct = split(7)
          val systemTime = split(1).replaceAll(""\"""", """")
          val date = gpsTime.split("" "")(0)
          (id, systemTime, lon, lat, imsi, speed, seqId, direct, gpsTime, date)
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusDataCleanUtils.scala/udf/139.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""upTime"") =!= ""errorDate""
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusDataCleanUtils.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterLength(col(""value""))
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusDataCleanUtils.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""lon"") =!= 0.0d && col(""lat"") =!= 0.0d
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusDataCleanUtils.scala/udf/49.22.Dataset-BusData.filter","Type: org.apache.spark.sql.Dataset[cn.sibat.bus.BusData]
Call: filter

_ != null
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusDataCleanUtils.scala/udf/56.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""lon"") < lit(135.05d) && col(""lat"") < lit(53.55d) && col(""lon"") > lit(73.66d) && col(""lat"") > lit(3.86d)
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusDataCleanUtils.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""status"") === lit(""0"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusToStationCheck.scala/udf/127.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusToStationCheck.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""timeDiff"") =!= Int.MaxValue
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/105.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""line"") === ""M4413""
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/107.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => {
        var key1: String = ""null""
        var key2: String = ""null""
        val direction = row.getString(row.fieldIndex(""direction""))
        val date = row.getString(row.fieldIndex(""date""))
        val oStationName = row.getString(row.fieldIndex(""o_station_name""))
        val dStationName = row.getString(row.fieldIndex(""d_station_name""))
        if (siteListForM441.contains(oStationName) && canceledSiteListForM441.contains(dStationName)) {
          key1 = oStationName + ""_"" + dStationName
        } else if (siteListForM441.contains(dStationName) && canceledSiteListForM441.contains(oStationName)) {
          key2 = oStationName + ""_"" + dStationName
        }
        (direction, key1, key2, date)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/130.26.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replace("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/132.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

row => {
            val arr = row.split("","")
            val line = arr(1)
            val carId = arr(2)
            val direction = arr(3)
            val oTime = arr(5)
            val oStationName = arr(7)
            val dTime = arr(11)
            val dStationName = arr(13)
            val date = oTime.substring(0, 10)
            BusOD(line, carId, direction, oTime, oStationName, dTime, dStationName, date)
          }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/145.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          var key1 = ""null""
          var key2 = ""null""
          val direction = row.getString(row.fieldIndex(""direction""))
          val line = row.getString(row.fieldIndex(""line""))
          val date = row.getString(row.fieldIndex(""date""))
          val oStationName = row.getString(row.fieldIndex(""o_station_name""))
          val dStationName = row.getString(row.fieldIndex(""d_station_name""))
          if (newSiteListForM441.contains(oStationName) && addedSiteListForM441.contains(dStationName)) key1 = oStationName + ""_"" + dStationName else if (newSiteListForM441.contains(dStationName) && addedSiteListForM441.contains(oStationName)) key2 = oStationName + ""_"" + dStationName
          (direction, key1, key2, line, date)
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/157.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""upLineOd"") =!= ""null"" || col(""downLineOd"") =!= ""null""
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/167.26.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replace("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/169.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

row => {
            val arr = row.split("","")
            val line = arr(1)
            val carId = arr(2)
            val direction = arr(3)
            val oTime = arr(5)
            val oStationName = arr(7)
            val dTime = arr(11)
            val dStationName = arr(13)
            val date = oTime.substring(0, 10)
            BusOD(line, carId, direction, oTime, oStationName, dTime, dStationName, date)
          }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/182.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          var key = ""null""
          val direction = row.getString(row.fieldIndex(""direction""))
          val line = row.getString(row.fieldIndex(""line""))
          val date = row.getString(row.fieldIndex(""date""))
          val oStationName = row.getString(row.fieldIndex(""o_station_name""))
          val dStationName = row.getString(row.fieldIndex(""d_station_name""))
          if (upLineRouteListForM441.contains(oStationName) && upLineRouteListForM441.contains(dStationName)) key = oStationName + ""_"" + dStationName else if (downLineRouteListForM441.contains(oStationName) && downLineRouteListForM441.contains(dStationName)) key = oStationName + ""_"" + dStationName
          (key, date)
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/193.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""od"") =!= ""null""
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/205.26.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replace("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/207.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

row => {
            val arr = row.split("","")
            val line = arr(1)
            val carId = arr(2)
            val direction = arr(3)
            val oTime = arr(5)
            var oTimePeriod = ""null""
            if (oTime.substring(11, 13) >= ""08"" && oTime.substring(11, 13) <= ""09"") oTimePeriod = ""earlyPeak"" else if (oTime.substring(11, 13) >= ""18"" && oTime.substring(11, 13) <= ""19"") oTimePeriod = ""latePeak"" else oTimePeriod = ""flatPeak""
            val oStationName = arr(7)
            val dTime = arr(11)
            val dStationName = arr(13)
            val date = oTime.substring(0, 10)
            BusO(line, carId, direction, oTime, oTimePeriod, oStationName, date)
          }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/222.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getString(row.fieldIndex(""line"")).matches(""M4543"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/224.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

_.mkString("","")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/60.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replace("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/62.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

row => {
            val arr = row.split("","")
            val cardId = arr(0)
            val line = arr(1)
            val oStationName = arr(7)
            val dStationName = arr(13)
            val oTime = arr(5)
            val date = oTime.substring(0, 9)
            UserOD(cardId, line, oStationName, dStationName, date)
          }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/73.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""line"") === ""M4413""
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/75.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

row => {
        val oStationName = row.getString(row.fieldIndex(""oStationName""))
        val dStationName = row.getString(row.fieldIndex(""dStationName""))
        canceledSiteListForM441.contains(oStationName) || canceledSiteListForM441.contains(dStationName)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/90.25.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.replace("",,"", "",null,"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.BusVolume.scala/udf/92.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

row => {
            val arr = row.split("","")
            val line = arr(1)
            val carId = arr(2)
            val direction = arr(3)
            val oTime = arr(5)
            val oStationName = arr(7)
            val dTime = arr(11)
            val dStationName = arr(13)
            val date = oTime.substring(0, 9)
            BusOD(line, carId, direction, oTime, oStationName, dTime, dStationName, date)
          }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.graph.BusNetwork.scala/udf/37.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{
        s => (hashId(s.split("","")(2)), s.split("","")(2))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.graph.BusNetwork.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(topics: Seq[_], cnt: Long) =>
          val ids = locally {
            val _t_m_p_5 = locally {
              val _t_m_p_6 = topics
              _t_m_p_6.map(_.toString)
            }
            _t_m_p_5.map(hashId)
          }.sorted
          Edge(ids(0), ids(1), cnt)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.RoadInformation.scala/udf/1357.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""carId"") === carId && dateContains(col(""upTime""))
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..DataPlatform.TestBus.scala/udf/21.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ str => 
        val Array(route, direct, stationId, stationName, stationSeqId, stationLat, stationLon) = str.split("","")
        import cn.sibat.bus.utils.LocationUtil
        val Array(lat, lon) = LocationUtil.gcj02_To_84(stationLat.toDouble, stationLon.toDouble).split("","")
        StationData(route, ""74"", direct, stationId, stationName, stationSeqId.toInt, lon.toDouble, lat.toDouble, 0)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.ACC_analysis.Acc_Cal.scala/udf/85.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val o_time = line.getString(line.fieldIndex(""o_time""))
        new TimeUtils().timeChange(o_time, size)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.BusD_first.scala/udf/130.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

x => x.getString(x.fieldIndex(""time"")) != null
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.BusD_first.scala/udf/135.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

x => x.getString(x.fieldIndex(""time"")) == null
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.BusD_first.scala/udf/160.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""time_diff"") > 0
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.BusD_Forth.scala/udf/15.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""o_time"").isNull
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.BusD_Third.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""o_time"").isNull
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.BusD_Third.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""o_time"").isNull
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/146.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
        val isFestival = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
        (isFestival, Row.getString(Row.fieldIndex(""station"")), Row.getInt(Row.fieldIndex(""InFlow"")), Row.getInt(Row.fieldIndex(""OutFlow"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/195.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
        val isFestival = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
        (isFestival, Row.getString(Row.fieldIndex(""o_station"")), Row.getString(Row.fieldIndex(""d_station"")), Row.getLong(Row.fieldIndex(""count"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/234.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
          val isFestival = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
          (isFestival, Row.getString(Row.fieldIndex(""station"")), Row.getInt(Row.fieldIndex(""InFlow"")), Row.getInt(Row.fieldIndex(""OutFlow"")))
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/262.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => {
        val newtime = x.getLong(x.fieldIndex(""time"")) + ""min""
        (x.getString(x.fieldIndex(""date"")), newtime, x.getLong(x.fieldIndex(""num"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/283.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => {
        val newtime = x.getLong(x.fieldIndex(""time"")) + ""min""
        (x.getString(x.fieldIndex(""date"")), x.getString(x.fieldIndex(""zone"")), newtime, x.getLong(x.fieldIndex(""num"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/42.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
        val isHoliday = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
        (isHoliday, Row.getLong(Row.fieldIndex(""count"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/65.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val o_time = line.getString(line.fieldIndex(""o_time""))
        new TimeUtils().timeChange(o_time, size)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/94.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
          val isFestival = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
          (isFestival, Row.getString(Row.fieldIndex(""o_station_name"")), Row.getString(Row.fieldIndex(""d_station_name"")), Row.getLong(Row.fieldIndex(""count"")))
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Bus.Cal_Bus.scala/udf/99.22.Dataset-(String, String, String, Long).filter","Type: org.apache.spark.sql.Dataset[(String, String, String, Long)]
Call: filter

x => !x._1.matches(""ErrorFormat"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.Cal_subway.scala/udf/141.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
        val isFestival = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
        (isFestival, Row.getString(Row.fieldIndex(""station"")), Row.getInt(Row.fieldIndex(""InFlow"")), Row.getInt(Row.fieldIndex(""OutFlow"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.Cal_subway.scala/udf/224.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
        val isFestival = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
        (isFestival, Row.getString(Row.fieldIndex(""o_station"")), Row.getString(Row.fieldIndex(""d_station"")), Row.getLong(Row.fieldIndex(""count"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.Cal_subway.scala/udf/266.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
          val isFestival = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
          (isFestival, Row.getString(Row.fieldIndex(""station"")), Row.getInt(Row.fieldIndex(""InFlow"")), Row.getInt(Row.fieldIndex(""OutFlow"")))
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.Cal_subway.scala/udf/288.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => {
        val newtime = x.getLong(x.fieldIndex(""time"")) + ""min""
        (x.getString(x.fieldIndex(""date"")), newtime, x.getLong(x.fieldIndex(""num"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.Cal_subway.scala/udf/309.20.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => {
        val newtime = x.getLong(x.fieldIndex(""time"")) + ""min""
        (x.getString(x.fieldIndex(""date"")), x.getString(x.fieldIndex(""zone"")), newtime, x.getLong(x.fieldIndex(""num"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.Cal_subway.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
        val isHoliday = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
        (isHoliday, Row.getLong(Row.fieldIndex(""count"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.Cal_subway.scala/udf/66.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val o_time = line.getString(line.fieldIndex(""o_time""))
        new TimeUtils().timeChange(o_time, size)
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.Cal_subway.scala/udf/94.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

Row => {
        val isFestival = TimeUtils().isFestival(Row.getString(Row.fieldIndex(""date"")), ""yyyy-MM-dd"", Holiday)
        (isFestival, Row.getString(Row.fieldIndex(""o_station"")), Row.getString(Row.fieldIndex(""d_station"")), Row.getLong(Row.fieldIndex(""count"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.MetroOD.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""timeDiff"") < 3
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.MetroOD.scala/udf/61.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""siteName"") =!= col(""outSiteName"")
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.section.Cal_Section.scala/udf/118.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val time = line.getString(line.fieldIndex(""date"")) + ""T"" + line.getString(line.fieldIndex(""time""))
        val newTime = new TimeUtils().timeChange(time, size)
        (newTime, line.getString(line.fieldIndex(""O"")), line.getString(line.fieldIndex(""D"")), line.getInt(line.fieldIndex(""flow"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.section.Cal_Section.scala/udf/176.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

line => {
        val time = line.getString(line.fieldIndex(""date"")) + ""T"" + line.getString(line.fieldIndex(""time""))
        val newTime = new TimeUtils().timeChange(time, size)
        (newTime, line.getString(line.fieldIndex(""O"")), line.getInt(line.fieldIndex(""transfer"")))
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Cal_public_transit.Subway.section.meanDisPrice.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val line = row.getString(row.fieldIndex(""line"")) match {
          case ""2680"" => ""一号线(上行)""
          case ""2681"" => ""一号线(下行)""
          case ""2600"" => ""二号线(上行)""
          case ""2601"" => ""二号线(下行)""
          case ""2610"" => ""三号线(上行)""
          case ""2611"" => ""三号线(下行)""
          case ""2620"" => ""四号线(上行)""
          case ""2621"" => ""四号线(下行)""
          case ""2630"" => ""五号线(上行)""
          case ""2631"" => ""五号线(下行)""
          case ""2410"" => ""十一号线(上行)""
          case ""2411"" => ""十一号线(下行)""
          case ""2650"" => ""七号线(上行)""
          case ""2651"" => ""七号线(下行)""
          case ""2670"" => ""九号线(上行)""
          case ""2671"" => ""九号线(下行)""
          case _ => ""NoMatch""
        }
        List(row.getString(row.fieldIndex(""date"")), line, row.getDouble(2).formatted(""%.2f""), row.getDouble(3).formatted(""%.2f"")).mkString("","")
      }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.cn.sibat.QF_rate.scala/udf/170.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
          val judge = row.getDouble(row.fieldIndex(""shortPathTime"")).equals(row.getDouble(row.fieldIndex(""shortTime"")))
          (row.getString(row.fieldIndex(""o_station"")), row.getString(row.fieldIndex(""d_station"")), row.getDouble(row.fieldIndex(""shortPathTime"")), row.getDouble(row.fieldIndex(""shortTime"")), judge)
        }
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.cn.sibat.QualityOfData.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""stime"").isNotNull
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.cn.sibat.QualityOfData.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""starttime"").isNotNull
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.cn.sibat.QualityOfData.scala/udf/91.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""timeStamp"").isNotNull
"
"udf/spark_repos_7/2_WangMuLing1028_Scala/..IhaveADream.src.main.scala.Delay.Cal_delay.scala/udf/165.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val carId = row.getString(row.fieldIndex(""carID""))
        val upTime = row.getString(row.fieldIndex(""upTime""))
        val receiveTime1 = row.getString(2)
        val receiveTime2 = row.getString(3)
        val timeDiff = time_diff(receiveTime1, receiveTime2)
        (carId, upTime, receiveTime1, receiveTime2, timeDiff)
      }
"
"udf/spark_repos_7/2_zhenchao125_spark0105/..spark-sql.src.main.scala.com.atguigu.MyAggre.scala/udf/12.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new MyAvg
"
"udf/spark_repos_7/2_zhenchao125_sparkmall0105/..spark-offline.src.main.scala.com.atguigu.sparkoffline.app.AreaProductClick.scala/udf/9.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AreaClickUDAF
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..DF_Operate.src.main.scala.preOperator.garbage_msg_clf.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0).split("" "")
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..DF_Operate.src.main.scala.preOperator.garbage_msg_clf.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0).split("" "")
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..DF_Operate.src.main.scala.preOperator.garbage_msg_clf.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

email => LabeledPoint(1, email.getAs[Vector](1))
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..DF_Operate.src.main.scala.preOperator.garbage_msg_clf.scala/udf/31.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

email => LabeledPoint(0, email.getAs[Vector](1))
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..DF_Operate.src.main.scala.user_define_Functions.untyped_udaf.scala/udf/15.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new mySum
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..DF_Operate.src.main.scala.user_define_Functions.untyped_udaf.scala/udf/19.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new myAvg
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..DF_Operate.src.main.scala.user_define_Functions.untyped_udaf.scala/udf/23.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new percentile(Array(0.1d, 0.5d, 0.51d))
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.continous.continous.scala/udf/29.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

x => {
        val tuple = new Tuple1(x._1 + ""\t"" + x._2)
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.continous.microBatch.scala/udf/30.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

x => {
        val tuple = new Tuple1(x._1 + ""\t"" + x._2)
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.continous.readKafka.scala/udf/24.19.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

x => {
        val str: String = x._1.split(""\t"")(0)
        val delayMs = x._2.getTime() - Timestamp.valueOf(str).getTime()
        writeToFile(delayMs.toString, resultDataPath)
        val tuple: (String, Timestamp, Timestamp, Long) = (x._1, x._2, new Timestamp(System.currentTimeMillis()), delayMs)
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.continous.test.scala/udf/13.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.execution.streaming.FileStreamSource.Timestamp, Long)]
Call: map

x => {
        println(x)
        x._1.toString() + x._2
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.foreachSink.writeToMysql.scala/udf/34.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

oldTuple => {
        userIdList = map.keySet.toList
        userId = getRandomUserId(userIdList)
        ip = map.get(userId).get
        val tuple = (oldTuple._1, oldTuple._2, batchId + 1, userId, getUUID(), getUUID(), randomData(5), randomData(3), ip)
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.foreachSink.writeToMysql.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.get(7).equals(""view"")
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.foreachSink.writeToRedis.scala/udf/35.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

oldTuple => {
        userIdList = map.keySet.toList
        userId = getRandomUserId(userIdList)
        ip = map.get(userId).get
        val tuple = (oldTuple._1, oldTuple._2, batchId + 1, userId, getUUID(), getUUID(), randomData(5), randomData(3), ip)
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.foreachSink.writeToRedis.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.get(7).equals(""view"")
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.join.streamJoinStatic.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

x => {
        val arr: Array[String] = x.split("" "")
        val tuple = (arr(0).toInt, arr(1), arr(2))
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.join.streamJoinStatic.scala/udf/23.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

x => getData()
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.join.streamJoinStream.scala/udf/23.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

x => getCourse()
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.join.streamJoinStream.scala/udf/29.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

x => getStudent()
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.MicroBatchVSContinous.continuousTrigger.scala/udf/32.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

oldTuple => {
        userIdList = map.keySet.toList
        userId = getRandomUserId(userIdList)
        ip = map.get(userId).get
        val tuple = (oldTuple._1, oldTuple._2, batchId + 1, userId, getUUID(), getUUID(), randomData(5), randomData(3), ip)
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.MicroBatchVSContinous.continuousTrigger.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

_.get(7).equals(""view"")
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.statefulOperations.statefulOperation.scala/udf/23.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

x => (x._1, x._2, batchId)
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.test.addBatchId_1.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val bdValue = bd.value
        (x.getTimestamp(0), x.getLong(1), bdValue)
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.test.addBatchId_2.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val bdValue = bd.value
        (x.getTimestamp(0), x.getLong(1), bdValue)
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.test.SourceOfKafka.scala/udf/11.19.Dataset-(String, String).map","Type: org.apache.spark.sql.Dataset[(String, String)]
Call: map

_._2
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.test.StaticData.scala/udf/36.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

oldTuple => {
        val timestamp: Timestamp = new Timestamp(System.currentTimeMillis())
        val newTuple: (Timestamp, Long, Timestamp) = (oldTuple._1, oldTuple._2, timestamp)
        newTuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.test.writeToFile.scala/udf/13.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(org.apache.spark.sql.execution.streaming.FileStreamSource.Timestamp, Long)]
Call: map

x => x._1.toString() + """" + x._2
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.windowOperator.RateOfSource.scala/udf/38.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

x => {
        val arr: Array[String] = dateFormat(System.currentTimeMillis())
        val tuple = (x._1, x._2, arr(3), arr(4), arr(5), arr(6))
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.windowOperator.RateWithWindows.scala/udf/49.19.Dataset-Timestamp, Long).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, Long)]
Call: map

oldTuple => {
        userIdList = map.keySet.toList
        userId = getRandomUserId(userIdList)
        ip = map.get(userId).get
        val tuple = (oldTuple._1, oldTuple._2, batchId + 1, userId, getUUID(), getUUID(), randomData(5), randomData(3), ip)
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.windowOperator.RateWithWindows.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.get(7).equals(""view"")
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.windowOperator.readKafka.scala/udf/39.19.Dataset-Timestamp, String).map","Type: org.apache.spark.sql.Dataset[(java.sql.Timestamp, String)]
Call: map

oldTuple => {
        userIdList = map.keySet.toList
        userId = getRandomUserId(userIdList)
        ip = map.get(userId).get
        val tuple = (oldTuple._1, oldTuple._2, batchId + 1, userId, getUUID(), getUUID(), randomData(5), randomData(3), ip)
        tuple
      }
"
"udf/spark_repos_7/2_ZhiYinZhang_sparkStudy/..structuredStreaming.src.main.scala.windowOperator.readKafka.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!_.get(7).equals(""view"")
"
"udf/spark_repos_7/31_Gaglia88_sparker/..old_versions.sparker.src.main.scala-2.11.BlockBuildingMethods.LSHSpark.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isNoneZeroVector(col(""features""))
"
"udf/spark_repos_7/31_Gaglia88_sparker/..old_versions.sparker.src.main.scala-2.11.BlockBuildingMethods.LSHSpark.scala/udf/96.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isNoneZeroVector(col(""features""))
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.DumpParseMerge.scala/udf/19.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""namespace"" === WikipediaNamespace.Page
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.DumpParseMerge.scala/udf/23.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""namespace"" === WikipediaNamespace.Category
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.DumpParseMerge.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""namespace"" === WikipediaNamespace.Category
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.DumpProcessor.scala/udf/31.22.Dataset-WikipediaPage.filter","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.WikipediaPage]
Call: filter

p => p.namespace == ns && (keepRedirect || !p.isRedirect)
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.DumpProcessor.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""from"" !== ($""dest"")
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.DumpProcessor.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""isRedirect"" === false
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PagecountProcessor.scala/udf/119.22.Dataset-WikipediaPageLang.filter","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.WikipediaPageLang]
Call: filter

p => cfg.getBoolean(""pagecountProcessor.keepRedirects"") || !p.isRedirect
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PageCountStatsLoader.scala/udf/37.22.Dataset-PageVisitRow.filter","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitRow]
Call: filter

p => p.visit_time.after(startTime) && p.visit_time.before(endTime)
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PageCountStatsLoader.scala/udf/57.22.Dataset-PageVisitRow.filter","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitRow]
Call: filter

r => r.languageCode == language
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PageCountStatsLoader.scala/udf/77.22.Dataset-PageVisitRow.filter","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitRow]
Call: filter

r => r.languageCode == language
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/103.24.Dataset-PageVisitThrGroup.map","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitThrGroup]
Call: map

p => (p, p.visits.count(v => v._2 > p.threshold))
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/105.25.Dataset-PageVisitThrGroup, Int).filter","Type: org.apache.spark.sql.Dataset[(ch.epfl.lts2.wikipedia.PageVisitThrGroup, Int)]
Call: filter

k => k._2 > burstCount
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/107.20.Dataset-PageVisitThrGroup, Int).map","Type: org.apache.spark.sql.Dataset[(ch.epfl.lts2.wikipedia.PageVisitThrGroup, Int)]
Call: map

p => p._1.page_id
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/125.30.Dataset-PageVisitGroup.map","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitGroup]
Call: map

p => PageVisitElapsedGroup(p.page_id, locally {
                  val _t_m_p_20 = p.visits
                  _t_m_p_20.map(v => (Duration.between(startTime, v._1.toLocalDateTime).toHours.toInt, v._2.toDouble))
                })
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/130.28.Dataset-PageVisitElapsedGroup.map","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitElapsedGroup]
Call: map

p => (p.page_id, new VectorBuilder(locally {
                val _t_m_p_21 = p.visits
                _t_m_p_21.map(f => f._1)
              }.toArray, locally {
                val _t_m_p_22 = p.visits
                _t_m_p_22.map(f => f._2)
              }.toArray, p.visits.size, totalHours).toDenseVector.toArray)
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/176.24.Dataset-PageVisitGroup.map","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitGroup]
Call: map

p => (p, locally {
            val _t_m_p_27 = TimeSeriesUtils.densifyVisitList(p.visits, startDate.atStartOfDay, totalHours).grouped(24)
            _t_m_p_27.map(_.sum)
          }.max)
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/181.25.Dataset-PageVisitGroup, Double).filter","Type: org.apache.spark.sql.Dataset[(ch.epfl.lts2.wikipedia.PageVisitGroup, Double)]
Call: filter

_._2 >= dailyMinThreshold
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/183.20.Dataset-PageVisitGroup, Double).map","Type: org.apache.spark.sql.Dataset[(ch.epfl.lts2.wikipedia.PageVisitGroup, Double)]
Call: map

p => (p._1.page_id, p._1.visits)
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/35.19.Dataset-PageVisitGroup.map","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitGroup]
Call: map

p => PageVisitGroup(p.page_id, locally {
        val _t_m_p_2 = p.visits
        _t_m_p_2.filter(v => v._1.after(startTs))
      })
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/67.19.Dataset-PageStatRow.map","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageStatRow]
Call: map

p => PageRowThreshold(p.page_id, p.mean + burstRate * scala.math.sqrt(p.variance))
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/77.23.Dataset-PageVisitGroup.map","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitGroup]
Call: map

p => PageVisitElapsedGroup(p.page_id, locally {
            val _t_m_p_8 = p.visits
            _t_m_p_8.map(v => (Duration.between(startDate.atStartOfDay, v._1.toLocalDateTime).toHours.toInt, v._2.toDouble))
          })
"
"udf/spark_repos_7/33_epfl-lts2_sparkwiki/..src.main.scala.ch.epfl.lts2.wikipedia.PeakFinder.scala/udf/82.21.Dataset-PageVisitElapsedGroup.map","Type: org.apache.spark.sql.Dataset[ch.epfl.lts2.wikipedia.PageVisitElapsedGroup]
Call: map

p => (p.page_id, meanAndVariance(new VectorBuilder(locally {
          val _t_m_p_9 = p.visits
          _t_m_p_9.map(f => f._1)
        }.toArray, locally {
          val _t_m_p_10 = p.visits
          _t_m_p_10.map(f => f._2)
        }.toArray, p.visits.size, totalHours).toDenseVector))
"
"udf/spark_repos_7/34_JCDecaux_setl/..src.main.scala.com.jcdecaux.setl.storage.repository.SparkRepository.scala/udf/82.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sql
"
"udf/spark_repos_7/34_JCDecaux_setl/..src.main.scala.com.jcdecaux.setl.util.FilterImplicits.scala/udf/15.24.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

condition.toSqlRequest
"
"udf/spark_repos_7/34_JCDecaux_setl/..src.main.scala.com.jcdecaux.setl.util.FilterImplicits.scala/udf/9.24.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

conditions.toSqlRequest
"
"udf/spark_repos_7/34_JCDecaux_setl/..src.main.scala.com.jcdecaux.setl.workflow.DeliverableDispatcher.scala/udf/107.24.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

condition
"
"udf/spark_repos_7/34_JCDecaux_setl/..src.main.scala.com.jcdecaux.setl.workflow.DeliverableDispatcher.scala/udf/128.30.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

condition
"
"udf/spark_repos_7/3_bpatni_spark-dynamic-funnel-analysis/..src.main.scala.us.patni.clickstream.EventTimestampFunnelAnalysis.scala/udf/51.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(userEvents: String, funnelEvents: String) => udfValidateFunnelEvents
"
"udf/spark_repos_7/3_ChemaGit_big-data-developer-with-apache-hadoop/..big-data-developer-with-apache-hadoop.spark.spark-sql.DataFrames.scala/udf/101.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""country"" === ""Switzerland""
"
"udf/spark_repos_7/3_ChemaGit_big-data-developer-with-apache-hadoop/..big-data-developer-with-apache-hadoop.spark.spark-sql.DataFrames.scala/udf/48.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 25 && $""city"" === ""Sydney""
"
"udf/spark_repos_7/3_ChemaGit_big-data-developer-with-apache-hadoop/..big-data-developer-with-apache-hadoop.spark.spark-sql.DataFrames.scala/udf/97.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""hasDebt"" && ($""hasFinancialDependents"")
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.blog.spark.ml.DecisionTreeExample.scala/udf/16.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_2 = arr(1).split(' ')
            _t_m_p_2.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.blog.spark.ml.featureprocessing.StringIndexerDemo.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_2 = arr(1).split(' ')
            _t_m_p_2.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.blog.spark.ml.featureprocessing.StringIndexerDemo.scala/udf/33.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_4 = arr(1).split(' ')
            _t_m_p_4.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.blog.spark.udf.UdfDemo.scala/udf/35.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length()
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.blog.spark.udf.UdfDemo.scala/udf/39.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAdult _
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.blog.spark.udf.UdfDemo.scala/udf/51.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.length()
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.blog.spark.udf.UdfDemo.scala/udf/55.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAdult _
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.learning.spark.df.TestDf.scala/udf/26.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""col1"") === df(""col2"")
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.learning.spark.ml.featureprocessing.ReadData.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_2 = arr(1).split(' ')
            _t_m_p_2.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.learning.spark.ml.ReadDataDemo.scala/udf/122.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0).toDouble, Vectors.dense(locally {
            val _t_m_p_15 = arr(1).split(' ')
            _t_m_p_15.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.learning.spark.ml.ReadDataDemo.scala/udf/133.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0).toDouble, Vectors.dense(locally {
            val _t_m_p_17 = arr(1).split(' ')
            _t_m_p_17.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.learning.spark.ml.StatisticsDemo.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = locally {
            val _t_m_p_6 = line.split(""\t"")
            _t_m_p_6.map(_.toDouble)
          }
          (arr(0), arr(1), arr(2), arr(3), arr(4))
      }
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.learning.spark.ml.VectorIndexerDemo.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(line: String) =>
          var arr = line.split(',')
          (arr(0), Vectors.dense(locally {
            val _t_m_p_2 = arr(1).split(' ')
            _t_m_p_2.map(_.toDouble)
          }))
      }
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.learning.spark.sql.ReadDemo.scala/udf/16.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/3_dongkelun_spark-scala/..src.main.scala.com.dkl.learning.spark.sql.ReadDemo.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_7/3_fediazgon_sparkml-flights-delay/..src.main.scala.fdiazgon.utils.DataPreparation.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Diverted"" === 0
"
"udf/spark_repos_7/3_fediazgon_sparkml-flights-delay/..src.main.scala.fdiazgon.utils.DataPreparation.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""Cancelled"" === 0
"
"udf/spark_repos_7/3_fediazgon_sparkml-flights-delay/..src.main.scala.fdiazgon.utils.DataPreparation.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""ArrDelay"".isNull
"
"udf/spark_repos_7/3_fediazgon_sparkml-flights-delay/..src.main.scala.fdiazgon.utils.DataPreparation.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""ArrDelay"".isNotNull
"
"udf/spark_repos_7/3_jiayuasu_CSE512-Project-Hotspot-Analysis-Template/..src.main.scala.cse512.HotcellAnalysis.scala/udf/16.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 0)
"
"udf/spark_repos_7/3_jiayuasu_CSE512-Project-Hotspot-Analysis-Template/..src.main.scala.cse512.HotcellAnalysis.scala/udf/20.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupPoint: String) => HotcellUtils.CalculateCoordinate(pickupPoint, 1)
"
"udf/spark_repos_7/3_jiayuasu_CSE512-Project-Hotspot-Analysis-Template/..src.main.scala.cse512.HotcellAnalysis.scala/udf/24.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(pickupTime: String) => HotcellUtils.CalculateCoordinate(pickupTime, 2)
"
"udf/spark_repos_7/3_jiayuasu_CSE512-Project-Hotspot-Analysis-Template/..src.main.scala.cse512.HotzoneAnalysis.scala/udf/13.32.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(string: String) => string.replace(""("", """").replace("")"", """")
"
"udf/spark_repos_7/3_jiayuasu_CSE512-Project-Hotspot-Analysis-Template/..src.main.scala.cse512.HotzoneAnalysis.scala/udf/21.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(queryRectangle: String, pointString: String) => HotzoneUtils.ST_Contains(queryRectangle, pointString)
"
"udf/spark_repos_7/3_jiazou-bigdata_SparkBench/..perf-bench.src.main.scala.edu.rice.bench.DFEmployeeSelection.scala/udf/16.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""name"" === ""Frank""
"
"udf/spark_repos_7/3_jiazou-bigdata_SparkBench/..perf-bench.src.main.scala.edu.rice.bench.ParquetDFEmployeeSelection.scala/udf/16.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""name"" === ""Frank""
"
"udf/spark_repos_7/3_jiazou-bigdata_SparkBench/..perf-bench.src.main.scala.edu.rice.bench.ParquetDSEmployeeSelection.scala/udf/17.24.Dataset-Employee.filter","Type: org.apache.spark.sql.Dataset[edu.rice.bench.Employee]
Call: filter

_.name == ""Frank""
"
"udf/spark_repos_7/3_jiazou-bigdata_SparkBench/..perf-bench.src.main.scala.edu.rice.bench.ParquetDSEmployeeSelection.scala/udf/19.19.Dataset-Employee.map","Type: org.apache.spark.sql.Dataset[edu.rice.bench.Employee]
Call: map

_.name
"
"udf/spark_repos_7/3_jsnowacki_streaming-ml-talk/..src.main.scala.com.sigdelta.spark.streaming.KMeansStreamingJob.scala/udf/19.19.Dataset-Point.map","Type: org.apache.spark.sql.Dataset[com.sigdelta.spark.streaming.Point]
Call: map

point => point.toPointVector
"
"udf/spark_repos_7/3_jsnowacki_streaming-ml-talk/..src.main.scala.com.sigdelta.spark.streaming.KMeansStreamingJob.scala/udf/25.19.Dataset-PointVector.map","Type: org.apache.spark.sql.Dataset[com.sigdelta.spark.streaming.PointVector]
Call: map

{ (pointVector: PointVector) => 
        val model = skm.getModel
        val point = Point.fromVector(pointVector.features)
        val label = model.predict(pointVector.features)
        val center = Point.fromVector(model.centers(label))
        PointCenter(point, center, label).toJson
      }
"
"udf/spark_repos_7/3_ONSdigital_sbr-idbr-data-load/..src.main.scala.spark.calculations.DataFrameHelper.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""max_$level == sum_$level""
"
"udf/spark_repos_7/3_scalaLearn_myetl/..src.main.scala.com.github.package.scala/udf/18.32.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date_format(rec_upd_ts,'yyyy-MM-dd')='$dt'""
"
"udf/spark_repos_7/3_scalaLearn_myetl/..src.main.scala.com.github.package.scala/udf/23.32.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date_format(REC_UPD_TS,'yyyy-MM-dd')='$dt'""
"
"udf/spark_repos_7/3_scalaLearn_myetl/..src.main.scala.com.github.package.scala/udf/35.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date_format(rec_upd_ts,'yyyy-MM-ddHH:mm:ss')>='$startTime' and date_format(rec_upd_ts,'yyyy-MM-ddHH:mm:ss')<='$endTime'""
"
"udf/spark_repos_7/3_scalaLearn_myetl/..src.main.scala.com.github.package.scala/udf/40.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date_format(REC_UPD_TS,'yyyy-MM-ddHH:mm:ss')>='$startTime' and date_format(REC_UPD_TS,'yyyy-MM-ddHH:mm:ss')<='$endTime'""
"
"udf/spark_repos_7/3_scalaLearn_myetl/..src.main.scala.com.github.package.scala/udf/49.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""date_format(updateAt,'yyyy-MM-dd')='$dt'""
"
"udf/spark_repos_7/3_ScrapCodes_SS-on-kube/..demo.src.main.scala.org.codait.sb.demo.SparkStreamingMLPipeline.scala/udf/73.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ tweetJson => 
        val client = new OkHttpClient()
        val parser = new JSONParser()
        val resp = post(s""${args(2)}"", tweetJson, client)
        (parseTweetJson(tweetJson, parser), parseResponseJson(resp, parser))
      }
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{
        line => execute(cfg)(line) match {
          case Right(r) =>
            SparkSuccess(r)
          case e @ Left(RecoveryError(UnrecoverableBadRowType(_), _, _)) =>
            SparkUnrecoverable(e.left.get)
          case Left(e) =>
            SparkFailure(e)
        }
      }
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/56.24.Dataset-SparkResult.filter","Type: org.apache.spark.sql.Dataset[com.snowplowanalytics.snowplow.event.recovery.SparkResult]
Call: filter

_.isInstanceOf[SparkSuccess]
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/58.19.Dataset-SparkResult.map","Type: org.apache.spark.sql.Dataset[com.snowplowanalytics.snowplow.event.recovery.SparkResult]
Call: map

_.message
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/63.24.Dataset-SparkResult.filter","Type: org.apache.spark.sql.Dataset[com.snowplowanalytics.snowplow.event.recovery.SparkResult]
Call: filter

_.isInstanceOf[SparkUnrecoverable]
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/65.19.Dataset-SparkResult.map","Type: org.apache.spark.sql.Dataset[com.snowplowanalytics.snowplow.event.recovery.SparkResult]
Call: map

_.message
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/70.24.Dataset-SparkResult.filter","Type: org.apache.spark.sql.Dataset[com.snowplowanalytics.snowplow.event.recovery.SparkResult]
Call: filter

_.isInstanceOf[SparkFailure]
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/72.19.Dataset-SparkResult.map","Type: org.apache.spark.sql.Dataset[com.snowplowanalytics.snowplow.event.recovery.SparkResult]
Call: map

_.message
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/76.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ x => 
        summary.successful.add(1)
        x
      }
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/84.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ x => 
          summary.failed.add(1)
          x
        }
"
"udf/spark_repos_7/3_snowplow-incubator_snowplow-event-recovery/..spark.src.main.scala.com.snowplowanalytics.snowplow.event.recovery.RecoveryJob.scala/udf/93.22.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ x => 
          summary.unrecoverable.add(1)
          x
        }
"
"udf/spark_repos_7/3_spark-notebook_sample-generated-projects/..simple-spark-sources.src.main.scala.App.scala/udf/62.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

transform
"
"udf/spark_repos_7/3_spark-notebook_sample-generated-projects/..simple-spark-sources.src.main.scala.App.scala/udf/68.21.Dataset-(Int, Int).map","Type: org.apache.spark.sql.Dataset[(Int, Int)]
Call: map

_._2
"
"udf/spark_repos_7/3_xhanshawn_CUR-Reader/..src.main.scala.com.github.xhanshawn.utils.CURPartLoader.scala/udf/21.19.Dataset-CURPart.map","Type: org.apache.spark.sql.Dataset[com.github.xhanshawn.reader.CURPart]
Call: map

_.fullPath
"
"udf/spark_repos_7/3_xhanshawn_CUR-Reader/..src.main.scala.com.github.xhanshawn.utils.CURPartLoader.scala/udf/29.19.Dataset-CURPart.map","Type: org.apache.spark.sql.Dataset[com.github.xhanshawn.reader.CURPart]
Call: map

_.fullPath
"
"udf/spark_repos_7/3_xhanshawn_CUR-Reader/..src.main.scala.com.github.xhanshawn.utils.CURPartLoader.scala/udf/34.19.Dataset-CURPart.map","Type: org.apache.spark.sql.Dataset[com.github.xhanshawn.reader.CURPart]
Call: map

part => downloadGZFromS3(part.bucket, part.reportKey)
"
"udf/spark_repos_7/3_xhanshawn_CUR-Reader/..src.main.scala.com.github.xhanshawn.utils.ManifestLoader.scala/udf/17.19.Dataset-CURPath.map","Type: org.apache.spark.sql.Dataset[com.github.xhanshawn.reader.CURPath]
Call: map

path => path.manifestPath
"
"udf/spark_repos_7/3_xhanshawn_CUR-Reader/..src.main.scala.com.github.xhanshawn.utils.ManifestLoader.scala/udf/27.22.Dataset-CURPath.filter","Type: org.apache.spark.sql.Dataset[com.github.xhanshawn.reader.CURPath]
Call: filter

_.fromS3
"
"udf/spark_repos_7/3_xhanshawn_CUR-Reader/..src.main.scala.com.github.xhanshawn.utils.ManifestLoader.scala/udf/31.19.Dataset-CURPath.map","Type: org.apache.spark.sql.Dataset[com.github.xhanshawn.reader.CURPath]
Call: map

path => readFromS3ByString(path.bucket, path.manifestKey)
"
"udf/spark_repos_7/3_xhanshawn_CUR-Reader/..src.main.scala.com.github.xhanshawn.utils.ManifestLoader.scala/udf/40.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

readFromS3ByString(bucket, _)
"
"udf/spark_repos_7/3_xhanshawn_CUR-Reader/..src.main.scala.com.github.xhanshawn.utils.ManifestLoader.scala/udf/8.22.Dataset-CURPath.filter","Type: org.apache.spark.sql.Dataset[com.github.xhanshawn.reader.CURPath]
Call: filter

_.useAWSAPI
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter12_movie.Movie_Users_Analyzer_DateFrame.scala/udf/63.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter12_movie.Movie_Users_Analyzer_DateSet.scala/udf/71.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter12_movie.Movie_Users_Analyzer_DateSet.scala/udf/76.23.Dataset-Rating.filter","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.Movie_Users_Analyzer_DateSet.Rating]
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""' and typed = 0""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""'""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/45.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, 1)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/51.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, -1)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""' and typed = 1""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/64.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, 1)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/70.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, -1)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/80.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => ConsumedOnce(log.logID, log.userID, log.consumed)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/86.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => ConsumedOnce(log.logID, log.userID, -log.consumed)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/92.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

userInfo(""registeredTime"") >= ""2016-10-01"" && userInfo(""registeredTime"") <= ""2016-10-14"" && userLog(""time"") >= userInfo(""registeredTime"") && userLog(""time"") <= date_add(userInfo(""registeredTime""), 14) && userLog(""typed"") === 0
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..chapter_code.chapter14_electronic_commerce.EB_Users_Analyzer_DateSet.scala/udf/97.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

userInfo(""registeredTime"") >= ""2016-10-01"" && userInfo(""registeredTime"") <= ""2016-10-14"" && userLog(""time"") >= userInfo(""registeredTime"") && userLog(""time"") <= date_add(userInfo(""registeredTime""), 14) && userLog(""typed"") === 1
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""' and typed = 0""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""'""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/45.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, 1)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/51.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, -1)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""time >= '"" + startTime + ""' and time <= '"" + endTime + ""' and typed = 1""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/64.19.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, 1)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/70.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => LogOnce(log.logID, log.userID, -1)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/80.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => ConsumedOnce(log.logID, log.userID, log.consumed)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/86.20.Dataset-UserLog.map","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.UserLog]
Call: map

log => ConsumedOnce(log.logID, log.userID, -log.consumed)
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/92.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

userInfo(""registeredTime"") >= ""2016-10-01"" && userInfo(""registeredTime"") <= ""2016-10-14"" && userLog(""time"") >= userInfo(""registeredTime"") && userLog(""time"") <= date_add(userInfo(""registeredTime""), 14) && userLog(""typed"") === 0
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.EB_Users_Analyzer_DateSet.scala/udf/97.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

userInfo(""registeredTime"") >= ""2016-10-01"" && userInfo(""registeredTime"") <= ""2016-10-14"" && userLog(""time"") >= userInfo(""registeredTime"") && userLog(""time"") <= date_add(userInfo(""registeredTime""), 14) && userLog(""typed"") === 1
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.Movie_Users_Analyzer_DateFrame.scala/udf/63.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.Movie_Users_Analyzer_DateSet.scala/udf/70.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_7/40_duanzhihua_code-of-spark-big-data-business-trilogy/..SparkApps.src.main.scala.com.dt.spark.sparksql.Movie_Users_Analyzer_DateSet.scala/udf/75.23.Dataset-Rating.filter","Type: org.apache.spark.sql.Dataset[com.dt.spark.sparksql.Movie_Users_Analyzer_DateSet.Rating]
Call: filter

s"" MovieID = 1193""
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.odkl.analysis.spark.util.SQLOperations.scala/udf/62.77.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(positive: Long, total: Long) => {
        if (total <= 0) throw new NotStrictlyPositiveException(LocalizedFormats.NUMBER_OF_TRIALS, total)
        if (positive < 0) throw new NotPositiveException(LocalizedFormats.NEGATIVE_NUMBER_OF_SUCCESSES, positive)
        if (positive > total) throw new NumberIsTooLargeException(LocalizedFormats.NUMBER_OF_SUCCESS_LARGER_THAN_POPULATION_SIZE, positive, total, true)
        val result = if (total <= 0) 0.0d else {
          val mean = positive.toDouble / total.toDouble
          val factor = 1.0d / (1 + 1.0d / total * zSquared)
          val modifiedSuccessRatio = mean + 1.0d / (2 * total) * zSquared
          val difference = z * FastMath.sqrt(1.0d / total * mean * (1 - mean) + 1.0d / (4 * FastMath.pow(total, 2)) * zSquared)
          factor * (modifiedSuccessRatio - difference)
        }
        if (result < minBound) 0 else result
      }
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.CombinedModel.scala/udf/218.31.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

data($(descriminantColumn)) === functions.lit(m._1)
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.CombinedModel.scala/udf/300.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

typeCol === x._1
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.CombinedModel.scala/udf/83.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataset($(typeColumn)) === t
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.CrossValidator.scala/udf/66.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""${$(numFoldsColumn)} = -1""
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.Evaluator.scala/udf/60.24.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

dataset($(isTestColumn)) === true
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.Evaluator.scala/udf/64.24.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

dataset($(isTestColumn)) === false
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.Evaluator.scala/udf/75.24.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

dataset($(isTestColumn)) === false
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.MultinominalExtractor.scala/udf/46.28.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

dataset($(inputCol)).isNotNull
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.NullToDefaultReplacer.scala/udf/28.30.Dataset-_.filter","Type: org.apache.spark.sql.Dataset[_]
Call: filter

dataset(field.name).isNotNull
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.ml.odkl.UnwrappedStage.scala/udf/319.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$lColumn = 1 OR RANDOM < $percent""
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.repro.MlFlowReproContext.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'value.isNotNull
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.repro.MlFlowReproContext.scala/udf/74.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""x-value"".isNull
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.main.scala.org.apache.spark.repro.MlFlowReproContext.scala/udf/94.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'value.isNotNull
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.test.scala.org.apache.spark.ml.odkl.EvaluationsSpec.scala/udf/192.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

metrics(""foldNum"") === -1 && metrics(""isTest"") === false
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.test.scala.org.apache.spark.ml.odkl.EvaluationsSpec.scala/udf/221.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

weightsFrame(""foldNum"") === -1
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.test.scala.org.apache.spark.ml.odkl.EvaluationsSpec.scala/udf/236.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

weightsFrame(""foldNum"") === 0
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.test.scala.org.apache.spark.ml.odkl.EvaluationsSpec.scala/udf/265.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

metrics(""foldNum"") === -1 && metrics(""isTest"") === false
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.test.scala.org.apache.spark.ml.odkl.EvaluationsSpec.scala/udf/294.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

weightsFrame(""foldNum"") === -1
"
"udf/spark_repos_7/40_odnoklassniki_pravda-ml/..src.test.scala.org.apache.spark.ml.odkl.EvaluationsSpec.scala/udf/309.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

weightsFrame(""foldNum"") === 0
"
"udf/spark_repos_7/45_jeanineharb_Big-Data-Analysis-with-Scala-and-Spark/..week-4.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/89.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => TimeUsageRow(r.getAs(""working""), r.getAs(""sex""), r.getAs(""age""), r.getAs(""primaryNeeds""), r.getAs(""work""), r.getAs(""other""))
"
"udf/spark_repos_7/45_jeanineharb_Big-Data-Analysis-with-Scala-and-Spark/..week-4.timeusage.src.main.scala.timeusage.TimeUsage.scala/udf/95.19.Dataset-((String, String, String), Double, Double, Double).map","Type: org.apache.spark.sql.Dataset[((String, String, String), Double, Double, Double)]
Call: map

k => TimeUsageRow(k._1._1, k._1._2, k._1._3, k._2, k._3, k._4)
"
"udf/spark_repos_7/47_uosdmlab_spark-nkp/..src.main.scala.com.github.uosdmlab.nkp.Dictionary.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(word: String, cost: String) =>
          s""$word,$cost""
        case Row(word: String, null) =>
          word
      }
"
"udf/spark_repos_7/48_CODAIT_aardpfark/..src.test.scala.com.ibm.aardpfark.spark.ml.classification.LinearSVCSuite.scala/udf/17.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      case Row(p: Double, raw: Vector) =>
        (p, raw.toArray)
    }
"
"udf/spark_repos_7/48_CODAIT_aardpfark/..src.test.scala.com.ibm.aardpfark.spark.ml.classification.LinearSVCSuite.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(p: Double, raw: Vector) =>
          (p, raw.toArray)
      }
"
"udf/spark_repos_7/48_CODAIT_aardpfark/..src.test.scala.com.ibm.aardpfark.spark.ml.classification.LogisticRegressionSuite.scala/udf/10.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(p: Double, raw: Vector, pr: Vector) =>
          (p, raw.toArray, pr.toArray)
      }
"
"udf/spark_repos_7/48_CODAIT_aardpfark/..src.test.scala.com.ibm.aardpfark.spark.ml.classification.NaiveBayesSuite.scala/udf/43.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      case Row(p: Double, raw: Vector, pr: Vector) =>
        (p, raw.toArray, pr.toArray)
    }
"
"udf/spark_repos_7/48_CODAIT_aardpfark/..src.test.scala.com.ibm.aardpfark.spark.ml.classification.NaiveBayesSuite.scala/udf/54.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(p: Double, raw: Vector, pr: Vector) =>
          (p, raw.toArray, pr.toArray)
      }
"
"udf/spark_repos_7/48_CODAIT_aardpfark/..src.test.scala.com.ibm.aardpfark.spark.ml.classification.NaiveBayesSuite.scala/udf/67.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(p: Double, raw: Vector, pr: Vector) =>
          (p, raw.toArray, pr.toArray)
      }
"
"udf/spark_repos_7/48_CODAIT_aardpfark/..src.test.scala.com.ibm.aardpfark.spark.ml.classification.NaiveBayesSuite.scala/udf/80.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(p: Double, raw: Vector, pr: Vector) =>
          (p, raw.toArray, pr.toArray)
      }
"
"udf/spark_repos_7/4_archivesunleashed_twut/..src.main.scala.io.archivesunleashed.package.scala/udf/46.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""video_info"").isNotNull
"
"udf/spark_repos_7/4_archivesunleashed_twut/..src.main.scala.io.archivesunleashed.package.scala/udf/54.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""video_info"").isNotNull
"
"udf/spark_repos_7/4_archivesunleashed_twut/..src.main.scala.io.archivesunleashed.package.scala/udf/62.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""possibly_sensitive"").isNull
"
"udf/spark_repos_7/4_archivesunleashed_twut/..src.main.scala.io.archivesunleashed.package.scala/udf/64.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""retweeted_status.possibly_sensitive"").isNull
"
"udf/spark_repos_7/4_archivesunleashed_twut/..src.main.scala.io.archivesunleashed.package.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""retweeted_status"").isNull
"
"udf/spark_repos_7/4_archivesunleashed_twut/..src.main.scala.io.archivesunleashed.package.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""user.verified"") === true
"
"udf/spark_repos_7/4_dvirgiln_bigdata/..enron-spark.src.main.scala.com.enron.services.SparkEnronService.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""_DocType"") === ""Message""
"
"udf/spark_repos_7/4_dvirgiln_bigdata/..weather.src.main.scala.com.enron.services.SparkEnronService.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""_DocType"") === ""Message""
"
"udf/spark_repos_7/4_ferrarisf50_Apache-Spark-2.0-with-Scala---Hands-On-with-Big-Data-/..DataFrames.scala/udf/28.22.Dataset-Person.filter","Type: org.apache.spark.sql.Dataset[com.sundogsoftware.spark.DataFrames.Person]
Call: filter

people(""age"") < 21
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.cn.lhfei.spark.df.DataframePlay.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.cn.lhfei.spark.df.DataframePlay.scala/udf/48.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.cn.lhfei.spark.df.DataframePlay.scala/udf/52.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.cn.lhfei.spark.df.DataframePlay.scala/udf/58.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.cn.lhfei.spark.df.DataframePlay.scala/udf/82.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.ml.ALSExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.ml.FPGrowthExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

t => t.split("" "")
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.hive.SparkHiveExample.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/41.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/64.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/69.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.SparkSQLExample.scala/udf/93.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.SQLDataSourceExample.scala/udf/37.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/4_lhfei_spark-in-action/..spark-2.x.src.main.scala.org.apache.spark.examples.sql.UserDefinedUntypedAggregation.scala/udf/33.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

MyAverage
"
"udf/spark_repos_7/4_yilong2001_yl-spark-sql/..example.src.main.scala.com.spark.sql.examples.others.UdfDemo.scala/udf/174.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new BasePrediction(""/Users/yilong/work/bigdata/code/mygithub/yl-spark-sql/datas/models/lrmodel.m"").predict _
"
"udf/spark_repos_7/4_yilong2001_yl-spark-sql/..example.src.main.scala.com.spark.sql.examples.sql.AISql.scala/udf/102.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AISql.getTypeCode _
"
"udf/spark_repos_7/4_yilong2001_yl-spark-sql/..example.src.main.scala.com.spark.sql.examples.sql.AISql.scala/udf/86.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AISql.getOldBalanceOrigValue _
"
"udf/spark_repos_7/4_yilong2001_yl-spark-sql/..example.src.main.scala.com.spark.sql.examples.sql.AISql.scala/udf/90.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AISql.getNewBalanceOrigValue _
"
"udf/spark_repos_7/4_yilong2001_yl-spark-sql/..example.src.main.scala.com.spark.sql.examples.sql.AISql.scala/udf/94.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AISql.getOldBalanceDestValue _
"
"udf/spark_repos_7/4_yilong2001_yl-spark-sql/..example.src.main.scala.com.spark.sql.examples.sql.AISql.scala/udf/98.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

AISql.getNewBalanceDestValue _
"
"udf/spark_repos_7/4_yilong2001_yl-spark-sql-on-hbase/..src.main.scala.com.example.spark.demo.client.WindowsSql.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

b(""count"").isNull
"
"udf/spark_repos_7/4_yilong2001_yl-spark-sql/..sql.src.main.scala.com.spark.dialect.sql.core.execution.command.mlearn.scala/udf/15.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new LogisticRegressionPrediction(path).predict _
"
"udf/spark_repos_7/50_xubo245_CarbonDataLearning/..src.test.scala.org.github.xubo245.carbonDataLearning.example.CarbonDataFrameExample.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""number"" > 31
"
"udf/spark_repos_7/54_datastax_SparkBuildExamples/..scala.sbt.dse.src.main.scala.com.datastax.spark.example.WriteRead.scala/udf/15.17.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

x => (x, x)
"
"udf/spark_repos_7/55_alibaba_SparkCube/..src.main.scala.com.alibaba.sparkcube.CubeManager.scala/udf/294.33.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

build.expr
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SparkDirectSQLExample.scala/udf/37.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SparkDirectSQLExample.scala/udf/56.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SparkDirectSQLExample.scala/udf/60.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SparkDirectSQLExample.scala/udf/65.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SparkDirectSQLExample.scala/udf/89.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SQLExample.scala/udf/40.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SQLExample.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SQLExample.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SQLExample.scala/udf/68.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_7/5_direct-spark-sql_direct-spark-sql/..src.test.scala.org.apache.spark.examples.sql.SQLExample.scala/udf/92.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_7/5_dlpkmr98_jsonToDF/..src.main.scala.com.org.dilip.spark.DataframeEx1.scala/udf/11.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

locally {
      val _t_m_p_2 = _.toSeq.toArray
      _t_m_p_2.map(_.toString())
    }
"
"udf/spark_repos_7/5_Smallhi_example/..src.main.scala.org.hhl.example.HbaseSuit.scala/udf/115.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => rowKeyByMD5(x.getString(1), x.getString(1))
"
"udf/spark_repos_7/5_Smallhi_example/..src.main.scala.org.hhl.example.HbaseSuit.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val sid = x.getString(0)
        val id = x.getString(1)
        val idType = x.getString(3)
        (sid, id, idType)
      }
"
"udf/spark_repos_7/5_Smallhi_example/..src.main.scala.org.hhl.example.HbaseSuit.scala/udf/52.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val sid = x.getString(0)
        val id = x.getString(1)
        val idtype = x.getString(3)
        rowKeyByMD5(id, idtype)
      }
"
"udf/spark_repos_7/5_Smallhi_example/..src.main.scala.org.hhl.example.HbaseSuit.scala/udf/78.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val id = x.getString(1)
        val idType = x.getString(3)
        rowKeyByMD5(id, idType)
      }
"
"udf/spark_repos_7/5_Smallhi_example/..src.main.scala.org.hhl.example.HbaseSuit.scala/udf/92.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val sid = x.getString(0)
        val id = x.getString(1)
        val idType = x.getString(3)
        (sid, id, idType)
      }
"
"udf/spark_repos_7/62_AbsaOSS_cobrix/..examples.examples-collection.src.main.scala.com.example.spark.cobol.examples.apps.CobolSparkExample2.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SEGMENT_ID"" === ""C""
"
"udf/spark_repos_7/62_AbsaOSS_cobrix/..examples.examples-collection.src.main.scala.com.example.spark.cobol.examples.apps.CobolSparkExample2.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SEGMENT_ID"" === ""P""
"
"udf/spark_repos_7/62_AbsaOSS_cobrix/..examples.examples-collection.src.main.scala.com.example.spark.cobol.examples.apps.CobolSparkExample3.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SEGMENT_ID"" === ""C""
"
"udf/spark_repos_7/62_AbsaOSS_cobrix/..examples.examples-collection.src.main.scala.com.example.spark.cobol.examples.apps.CobolSparkExample3.scala/udf/38.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SEGMENT_ID"" === ""P""
"
"udf/spark_repos_7/62_AbsaOSS_cobrix/..examples.spark-cobol-app.src.main.scala.com.example.spark.cobol.app.SparkCobolApp.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SEGMENT_ID"" === ""C""
"
"udf/spark_repos_7/62_AbsaOSS_cobrix/..examples.spark-cobol-app.src.main.scala.com.example.spark.cobol.app.SparkCobolApp.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""SEGMENT_ID"" === ""P""
"
"udf/spark_repos_7/67_linkedin_isolation-forest/..isolation-forest.src.main.scala.com.linkedin.relevance.isolationforest.IsolationForest.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => DataPoint(locally {
        val _t_m_p_2 = row.getAs[Vector]($(featuresCol)).toArray
        _t_m_p_2.map(x => x.toFloat)
      })
"
"udf/spark_repos_7/67_linkedin_isolation-forest/..isolation-forest.src.main.scala.com.linkedin.relevance.isolationforest.IsolationForest.scala/udf/75.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => OutlierScore(row.getAs[Double]($(scoreCol)))
"
"udf/spark_repos_7/67_linkedin_isolation-forest/..isolation-forest.src.main.scala.com.linkedin.relevance.isolationforest.IsolationForest.scala/udf/82.23.Dataset-OutlierScore.map","Type: org.apache.spark.sql.Dataset[com.linkedin.relevance.isolationforest.Utils.OutlierScore]
Call: map

outlierScore => if (outlierScore.score >= outlierScoreThreshold) 1.0d else 0.0d
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.CodelessInitExampleJob.scala/udf/7.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""total_response_size"" > 1000
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.CodelessInitExampleJob.scala/udf/9.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""count_views"" > 10
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.Example1Job.scala/udf/11.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""count_views"" > 10
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.Example1Job.scala/udf/9.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""total_response_size"" > 1000
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.Example2Job.scala/udf/13.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""domain_code"".eqNullSafe(""aa"")
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.Example2Job.scala/udf/17.20.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""count_views"" > 10
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.Example2Job.scala/udf/24.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""domain_code"".eqNullSafe(""aa"")
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.Example2Job.scala/udf/9.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""total_response_size"" > 10000
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batch.Example3Job.scala/udf/10.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""_subject"" === lit(""astronomy"")
"
"udf/spark_repos_7/6_AbsaOSS_spline-spark-agent/..examples.src.main.scala.za.co.absa.spline.example.batchWithDependencies.MareksJob.scala/udf/10.20.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""metric"" === ""GDP per capita (current US$)""
"
"udf/spark_repos_7/6_bastihaase_Insight18b-SparkSQL-Array/..sparksql_performance_tests.src.main.scala.Performance_Tests.SparkSQL_Performance_Concat.scala/udf/34.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arr1: Seq[String], arr2: Seq[String]) => (Option(arr1), Option(arr2)) match {
        case (Some(x), Some(y)) =>
          x ++ y
        case _ =>
          Seq()
      }
"
"udf/spark_repos_7/6_bastihaase_Insight18b-SparkSQL-Array/..sparksql_performance_tests.src.main.scala.Performance_Tests.SparkSQL_Performance_Group_By_Size.scala/udf/34.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arr1: Seq[String], arr2: Seq[String]) => (Option(arr1), Option(arr2)) match {
        case (Some(x), Some(y)) =>
          x.intersect(y)
        case _ =>
          Seq()
      }
"
"udf/spark_repos_7/6_bastihaase_Insight18b-SparkSQL-Array/..sparksql_performance_tests.src.main.scala.Performance_Tests.SparkSQL_Performance_Join.scala/udf/40.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arr1: Seq[String], arr2: Seq[String]) => (Option(arr1), Option(arr2)) match {
        case (Some(x), Some(y)) =>
          x.intersect(y)
        case _ =>
          Seq()
      }
"
"udf/spark_repos_7/6_bastihaase_Insight18b-SparkSQL-Array/..sparksql_performance_tests.src.main.scala.Performance_Tests.SparkSQL_Performance.scala/udf/33.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arr1: Seq[String], arr2: Seq[String]) => (Option(arr1), Option(arr2)) match {
        case (Some(x), Some(y)) =>
          x.intersect(y)
        case _ =>
          Seq()
      }
"
"udf/spark_repos_7/6_bastihaase_Insight18b-SparkSQL-Array/..sparksql_performance_tests.src.main.scala.Performance_Tests.SparkSQL_performance_show.scala/udf/33.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arr1: Seq[String], arr2: Seq[String]) => (Option(arr1), Option(arr2)) match {
        case (Some(x), Some(y)) =>
          x.intersect(y)
        case _ =>
          Seq()
      }
"
"udf/spark_repos_7/6_bastihaase_Insight18b-SparkSQL-Array/..spark-streaming.src.main.scala.Streaming.Streaming.scala/udf/40.44.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(arr1: Seq[String], arr2: Seq[String]) => (Option(arr1), Option(arr2)) match {
        case (Some(x), Some(y)) =>
          x.intersect(y)
        case _ =>
          Seq()
      }
"
"udf/spark_repos_7/6_binbln_spark-electronic-commerce-analysis/..sparkmall-offline.src.main.scala.com.xzb.sparkmall.offline.app.AreaClickApp.scala/udf/9.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

new AreaClickCountUDAF
"
"udf/spark_repos_7/6_rchillyard_Scalaprof/..SparkExample.src.main.scala.edu.neu.csye._7200.WordCount.scala/udf/55.22.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

(_, 1)
"
"udf/spark_repos_7/6_rchillyard_Scalaprof/..SparkExample.src.main.scala.edu.neu.csye._7200.WordCount.scala/udf/57.20.Dataset-(String, Int).map","Type: org.apache.spark.sql.Dataset[(String, Int)]
Call: map

Word.tupled
"
"udf/spark_repos_7/6_udavPit_spark-user-feedback/..src.main.scala.feedback.Feedback.scala/udf/115.19.Dataset-(String, Double, Double).map","Type: org.apache.spark.sql.Dataset[(String, Double, Double)]
Call: map

{
        case (managerName, time, satisfaction) =>
          FeedbackGroupedRow(managerName, time, satisfaction)
      }
"
"udf/spark_repos_7/7_franktheunicorn_predict-pr-comments/..sparkproject.src.main.scala.com.holdenkarau.predict.pr.comments.dataprep.DataFetch.scala/udf/75.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!($""pull_request_url"" === ""null"")
"
"udf/spark_repos_7/7_franktheunicorn_predict-pr-comments/..sparkproject.src.main.scala.com.holdenkarau.predict.pr.comments.ml.Featurizer.scala/udf/28.26.Dataset-PreparedData.filter","Type: org.apache.spark.sql.Dataset[com.holdenkarau.predict.pr.comments.sparkProject.ml.PreparedData]
Call: filter

$""extension"" === ""py""
"
"udf/spark_repos_7/7_franktheunicorn_predict-pr-comments/..sparkproject.src.main.scala.com.holdenkarau.predict.pr.comments.ml.Serving.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""prediction"" === 1.0d
"
"udf/spark_repos_7/7_olafurpg_scala-repos/..repos.spark.dev.audit-release.sbt_app_sql.src.main.scala.SqlApp.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_7/7_olafurpg_scala-repos/..repos.spark.sql.core.src.main.scala.org.apache.spark.sql.DataFrameNaFunctions.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Column(predicate)
"
"udf/spark_repos_7/7_olafurpg_scala-repos/..repos.spark.sql.core.src.main.scala.org.apache.spark.sql.DataFrameStatFunctions.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

f(c, r)
"
"udf/spark_repos_7/7_olafurpg_scala-repos/..repos.spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.BucketedReadSuite.scala/udf/78.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCondition
"
"udf/spark_repos_7/7_olafurpg_scala-repos/..repos.spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.hadoopFsRelationSuites.scala/udf/31.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 === 2
"
"udf/spark_repos_7/7_olafurpg_scala-repos/..repos.spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.hadoopFsRelationSuites.scala/udf/35.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1
"
"udf/spark_repos_7/7_olafurpg_scala-repos/..repos.spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.hadoopFsRelationSuites.scala/udf/39.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_7/7_olafurpg_scala-repos/..repos.spark.sql.hive.src.test.scala.org.apache.spark.sql.sources.hadoopFsRelationSuites.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'a > 1 && 'p1 < 2
"
"udf/spark_repos_7/7_phatak-dev_spark-3.0-examples/..src.main.scala.com.madhukaraphatak.spark.core.plugins.dynamicconfig.DynamicConfigExample.scala/udf/13.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

value => value + Configuration.getConfig
"
"udf/spark_repos_7/7_sympho410_sparkStreamingETL/..src.main.scala.ingestion.transformations.ColumnsHandler.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterCond
"
"udf/spark_repos_7/82_TedBear42_spark_training/..src.main.scala.com.malaska.spark.training.streaming.structured.CountingInAStreamDatasetExpGroupBy.scala/udf/23.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => MessageBuilder.build(line)
"
"udf/spark_repos_7/82_TedBear42_spark_training/..src.main.scala.com.malaska.spark.training.streaming.structured.CountingInAStreamExpQueringResults.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => MessageBuilder.build(line)
"
"udf/spark_repos_7/82_TedBear42_spark_training/..src.main.scala.com.malaska.spark.training.streaming.structured.CountingInAStreamExpWindowing.scala/udf/26.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, org.apache.spark.sql.execution.streaming.FileStreamSource.Timestamp)]
Call: map

line => MessageBuilder.build(line._1, line._2)
"
"udf/spark_repos_7/82_TedBear42_spark_training/..src.main.scala.com.malaska.spark.training.streaming.structured.CountingInAStreamExpWindowing.scala/udf/28.22.Dataset-Message.filter","Type: org.apache.spark.sql.Dataset[com.malaska.spark.training.streaming.Message]
Call: filter

r => r != null
"
"udf/spark_repos_7/82_TedBear42_spark_training/..src.main.scala.com.malaska.spark.training.streaming.structured.CountingInAStreamMapWithState.scala/udf/24.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

word => WordCountEvent(word, 1)
"
"udf/spark_repos_7/82_TedBear42_spark_training/..src.main.scala.com.malaska.spark.training.streaming.structured.EnrichmentInAStream.scala/udf/20.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => MessageBuilder.build(line)
"
"udf/spark_repos_7/82_TedBear42_spark_training/..src.main.scala.com.malaska.spark.training.streaming.structured.EnrichmentInAStream.scala/udf/24.19.Dataset-Message.map","Type: org.apache.spark.sql.Dataset[com.malaska.spark.training.streaming.Message]
Call: map

message => message.toString.toUpperCase()
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/15.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

replaceEmptyStr _
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x == 1900
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/24.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x != 1900
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieDataFillingBadValues.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.convertYear _
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.MovieData.scala/udf/8.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

Util.convertYear _
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.RatingData.scala/udf/49.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

getCurrentHour _
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter04.scala.2.0.0.src.main.scala.org.sparksamples.RatingData.scala/udf/55.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

assignTod _
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter05.2.0.0.scala-spark-app.src.main.scala.com.spark.recommendation.FeatureExtraction.scala/udf/15.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter08.scala.2.0.0.src.main.scala.org.sparksamples.kmeans.BisectingKMeans.scala/udf/26.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""prediction == "" + i
"
"udf/spark_repos_7/83_PacktPublishing_Machine-Learning-with-Spark-Second-Edition/..Chapter08.scala.2.0.0.src.main.scala.org.sparksamples.kmeans.MovieLensKMeans.scala/udf/30.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

""prediction == "" + i
"
"udf/spark_repos_7/87_holdenk_spark-validator/..src.main.scala.com.holdenkarau.spark.validator.HistoricData.scala/udf/31.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => HistoricData(x.getSeq[Map[String, Long]](1).reduceLeft(_ ++ _), x.getTimestamp(0))
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark_23.src.main.scala.com.vishnuviswanath.spark.streaming.ContinuousKafkaStreaming.scala/udf/19.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => CarEvent(r.getString(0))
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark_23.src.main.scala.com.vishnuviswanath.spark.streaming.KafkaSourceStreaming.scala/udf/20.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => CarEvent(r.getString(0))
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark_23.src.main.scala.com.vishnuviswanath.spark.streaming.StreamingAggregations.scala/udf/17.22.Dataset-CarEvent.filter","Type: org.apache.spark.sql.Dataset[com.vishnuviswanath.spark.streaming.StreamingAggregations.CarEvent]
Call: filter

_.speed.exists(_ > 70)
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.KaggleTitanic.scala/udf/24.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sex: String, age: Double) => if (age < 15) ""Child"" else sex
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.KaggleTitanic.scala/udf/28.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sib: Int, par: Int) => if (sib + par > 3) 1.0d else 0.0d
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.KaggleTitanic.scala/udf/32.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(n: Int) => n.toDouble
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.KaggleTitanic.scala/udf/36.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(name: String) => {
        val pattern = ""(Dr|Mrs?|Ms|Miss|Master|Rev|Capt|Mlle|Col|Major|Sir|Lady|Mme|Don)\\."".r
        val matchedStr = pattern.findFirstIn(name)
        var title = matchedStr match {
          case Some(s) =>
            matchedStr.getOrElse(""Other."")
          case None =>
            ""Other.""
        }
        if (title.equals(""Don."") || title.equals(""Major."") || title.equals(""Capt."")) title = ""Sir.""
        if (title.equals(""Mlle."") || title.equals(""Mme."")) title = ""Miss.""
        title
      }
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.KaggleTitanic.scala/udf/72.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => (row.get(0).asInstanceOf[Double], row.get(1).asInstanceOf[Double])
      }
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.KaggleTitanic.scala/udf/88.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(n: Int) => 0.0d
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.KaggleTitanic.scala/udf/95.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => (row.get(0).asInstanceOf[Int], row.get(1).asInstanceOf[Double].toInt)
      }
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicOverfit.scala/udf/16.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(n: Int) => n.toDouble
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicOverfit.scala/udf/38.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (row: Row) => 
        val features = Array[Double](row(1).asInstanceOf[Double], row(2).asInstanceOf[Double], row(3).asInstanceOf[Double], row(4).asInstanceOf[Double], row(5).asInstanceOf[Double], row(6).asInstanceOf[Double], row(7).asInstanceOf[Double], row(8).asInstanceOf[Double])
        LabeledPoint(row(0).asInstanceOf[Double], Vectors.dense(features))
      }
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicOverfit.scala/udf/50.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (row: Row) => 
        val features = Array[Double](row(1).asInstanceOf[Double], row(2).asInstanceOf[Double], row(3).asInstanceOf[Double], row(4).asInstanceOf[Double], row(5).asInstanceOf[Double], row(6).asInstanceOf[Double], row(7).asInstanceOf[Double], row(8).asInstanceOf[Double])
        LabeledPoint(row(0).asInstanceOf[Double], Vectors.dense(features))
      }
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicUnderfit.scala/udf/16.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(n: Int) => n.toDouble
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicUnderfit.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (row: Row) => 
        val features = Array[Double](row(1).asInstanceOf[Double], row(2).asInstanceOf[Double])
        LabeledPoint(row(0).asInstanceOf[Double], Vectors.dense(features))
      }
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicUnderfit.scala/udf/45.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (row: Row) => 
        val features = Array[Double](row(1).asInstanceOf[Double], row(2).asInstanceOf[Double])
        LabeledPoint(row(0).asInstanceOf[Double], Vectors.dense(features))
      }
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicWithPipeline.scala/udf/104.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(n: Int) => 0.0d
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicWithPipeline.scala/udf/63.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.get(0).asInstanceOf[Double].toInt, r.get(1).asInstanceOf[Double].toInt)
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicWithPipeline.scala/udf/75.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sib: Int, par: Int) => if (sib + par > 3) 1.0d else 0.0d
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicWithPipeline.scala/udf/79.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(name: String) => {
        val pattern = ""(Dr|Mrs?|Ms|Miss|Master|Rev|Capt|Mlle)\\."".r
        val matchedStr = pattern.findFirstIn(name)
        matchedStr match {
          case Some(s) =>
            matchedStr.getOrElse("""")
          case None =>
            ""Mr""
        }
      }
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicWithPipeline.scala/udf/92.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(sex: String, age: Double) => if (age < 15) ""Child"" else sex
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.kaggle.titanic.TitanicWithPipeline.scala/udf/96.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(n: Int) => n.toDouble
"
"udf/spark_repos_7/96_soniclavier_bigdata-notebook/..spark.src.main.scala.com.vishnu.spark.sql.FromJson.scala/udf/14.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(x: String) => findLength(x)
"
"udf/spark_repos_7/9_shafiquejamal_apache-spark-2-scala-starter-template/..src.main.scala.com.example.CreatingDatasets.scala/udf/12.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_7/9_shafiquejamal_apache-spark-2-scala-starter-template/..src.main.scala.com.example.ProgrammingGuideSQL.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_8/adam/..adam-core.src.main.scala.org.bdgenomics.adam.rdd.GenomicDataset.scala/udf/1073.26.Dataset-U.filter","Type: org.apache.spark.sql.Dataset[U]
Call: filter

referenceRegionsToDatasetQueryString(querys)
"
"udf/spark_repos_8/crossdata/..core.src.main.scala.org.apache.spark.sql.crossdata.XDContext.scala/udf/103.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(ms: Long) => {
        Thread.sleep(ms)
        1
      }
"
"udf/spark_repos_8/crossdata/..core.src.main.scala.org.apache.spark.sql.crossdata.XDContext.scala/udf/95.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

gc
"
"udf/spark_repos_8/crossdata/..core.src.main.scala.org.apache.spark.sql.crossdata.XDContext.scala/udf/99.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(numberStr: String) => if (numberStr contains ""."") numberStr.toDouble else numberStr.toLong
"
"udf/spark_repos_8/deequ/..src.main.scala.com.amazon.deequ.analyzers.GroupingAnalyzers.scala/udf/52.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_8/deequ/..src.main.scala.com.amazon.deequ.analyzers.Histogram.scala/udf/52.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.Checkpoints.scala/udf/149.21.Dataset-SingleAction.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.SingleAction]
Call: map

{ action => 
          if (action.add != null) {
            numOfFiles.add(1)
          }
          action
        }
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.DeleteCommand.scala/udf/81.34.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(cond)
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.DeleteCommand.scala/udf/83.32.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

deletedRowUdf()
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.DeleteCommand.scala/udf/98.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(filterCond)
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.MergeIntoCommand.scala/udf/183.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(incrSourceRowCountExpr)
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.MergeIntoCommand.scala/udf/185.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

new Column(notMatchedClause.get.condition.getOrElse(Literal(true)))
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.UpdateCommand.scala/udf/74.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(updateCondition)
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.UpdateCommand.scala/udf/76.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

updatedRowUdf()
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.VacuumCommand.scala/udf/130.23.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ relativePath => 
            assert(!stringToPath(relativePath).isAbsolute, ""Shouldn't have any absolute paths for deletion here."")
            pathToString(DeltaFileOperations.absolutePath(basePath, relativePath))
          }
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.commands.VacuumCommand.scala/udf/142.26.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

f => stringToPath(f).toString
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.DeltaLog.scala/udf/283.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

columnFilter
"
"udf/spark_repos_8/delta/..src.main.scala.org.apache.spark.sql.delta.hooks.GenerateSymlinkManifest.scala/udf/57.26.Dataset-AddFile.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.delta.actions.AddFile]
Call: filter

{
            a => partitionsUpdated.contains(a.partitionValues)
          }
"
"udf/spark_repos_8/delta/..src.test.scala.org.apache.spark.sql.delta.SchemaValidationSuite.scala/udf/85.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""id"") <= 20
"
"udf/spark_repos_8/FiloDB/..stress.src.main.scala.filodb.stress.IngestionStress.scala/udf/21.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
      (t: Timestamp) => new DateTime(t).getHourOfDay
    }
"
"udf/spark_repos_8/first-edition/..ch03.scala.org.sia.chapter03App.GitHubDay.scala/udf/20.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isEmp
"
"udf/spark_repos_8/first-edition/..ch03.scala.org.sia.chapter03App.GitHubDay.scala/udf/24.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

sqlFunc($""login"")
"
"udf/spark_repos_8/flint/..src.main.scala.com.twosigma.flint.timeseries.TimeSeriesRDD.scala/udf/115.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(timeColumn) >= lit(nanos / nanoToSec).cast(TimestampType)
"
"udf/spark_repos_8/flint/..src.main.scala.com.twosigma.flint.timeseries.TimeSeriesRDD.scala/udf/124.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(timeColumn) < lit(nanos / nanoToSec).cast(TimestampType)
"
"udf/spark_repos_8/flint/..src.main.scala.com.twosigma.flint.timeseries.TimeSeriesRDD.scala/udf/134.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(timeColumn) >= nanos
"
"udf/spark_repos_8/flint/..src.main.scala.com.twosigma.flint.timeseries.TimeSeriesRDD.scala/udf/143.30.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(timeColumn) < nanos
"
"udf/spark_repos_8/flint/..src.main.scala.com.twosigma.flint.timeseries.TimeSeriesRDD.scala/udf/372.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_8/flint/..src.test.scala.org.apache.spark.sql.FlintTestData.scala/udf/47.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""v"") > 0
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/100.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Dimension
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/104.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Envelope
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/108.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_ExteriorRing
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/112.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeometryN
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/116.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeometryType
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/120.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_InteriorRingN
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/124.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsClosed
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/128.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsCollection
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/132.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsEmpty
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/136.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsRing
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/140.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsSimple
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/144.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_IsValid
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/148.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_NumGeometries
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/152.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_NumPoints
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/156.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PointN
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/160.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_X
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/164.46.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Y
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/92.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Boundary
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricAccessorFunctions.scala/udf/96.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CoordDim
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/15.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CastToPoint
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/19.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CastToPolygon
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/23.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CastToLineString
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/27.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_CastToGeometry
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricCastFunctions.scala/udf/31.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_ByteArray
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/101.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Point
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/105.64.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PointFromGeoHash
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/109.61.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PointFromText
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/113.60.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PointFromWKB
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/117.55.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Polygon
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/121.63.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_PolygonFromText
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/37.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromGeoHash
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/41.62.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromGeoHash
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/45.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKT
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/49.47.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKT
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/53.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKT
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/57.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeomFromWKB
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/61.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_LineFromText
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/65.60.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MLineFromText
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/69.62.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MPointFromText
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/73.61.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MPolyFromText
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/77.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakeBBOX
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/81.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakeBox2D
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/85.56.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakeLine
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/89.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakePoint
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/93.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakePointM
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricConstructorFunctions.scala/udf/97.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_MakePolygon
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/24.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AsBinary
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/28.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AsGeoJSON
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/32.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AsLatLonText
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/36.48.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AsText
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricOutputFunctions.scala/udf/40.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_GeoHash
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricProcessingFunctions.scala/udf/48.66.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_antimeridianSafeGeom
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricProcessingFunctions.scala/udf/52.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_antimeridianSafeGeom
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.GeometricProcessingFunctions.scala/udf/56.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_BufferPoint
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/100.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Centroid
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/104.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Distance
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/108.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Length
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/112.59.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_DistanceSphere
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/116.68.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AggregateDistanceSphere
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/120.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_LengthSphere
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/124.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ch
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/128.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Intersection
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/132.55.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Difference
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/44.53.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Translate
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/48.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Contains
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/52.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Covers
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/56.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Crosses
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/60.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Disjoint
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/64.50.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Equals
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/68.54.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Intersects
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/72.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Overlaps
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/76.52.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Touches
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/80.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Within
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/84.51.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Relate
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/88.55.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_RelateBool
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/92.49.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_Area
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-jts.src.main.scala.org.locationtech.geomesa.spark.jts.udf.SpatialRelationFunctions.scala/udf/96.57.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_ClosestPoint
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-sql.src.main.scala.org.locationtech.geomesa.spark.GeometricDistanceFunctions.scala/udf/21.60.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_DistanceSpheroid
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-sql.src.main.scala.org.locationtech.geomesa.spark.GeometricDistanceFunctions.scala/udf/25.69.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_AggregateDistanceSpheroid
"
"udf/spark_repos_8/geomesa/..geomesa-spark.geomesa-spark-sql.src.main.scala.org.locationtech.geomesa.spark.GeometricDistanceFunctions.scala/udf/29.58.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

ST_LengthSpheroid
"
"udf/spark_repos_8/gimel/..gimel-serde.gimel-deserializers.generic-deserializers.src.main.scala.com.paypal.gimel.deserializers.generic.JsonDynamicDeserializer.scala/udf/18.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          eachRow => locally {
            val _t_m_p_2 = eachRow.getAs(conf.columnToDeserialize).asInstanceOf[Array[Byte]]
            _t_m_p_2.map(_.toChar)
          }.mkString
        }
"
"udf/spark_repos_8/gimel/..gimel-serde.gimel-serializers.generic-serializers.src.main.scala.com.paypal.gimel.serializers.generic.StringSerializer.scala/udf/8.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        x => x.getAs(0).toString
      }
"
"udf/spark_repos_8/gimel/..gimel-serde.serde-common.src.main.scala.com.paypal.gimel.serde.common.avro.AvroUtils.scala/udf/135.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ eachRow => 
          val recordToDeserialize: Array[Byte] = eachRow.getAs(columnToDeserialize).asInstanceOf[Array[Byte]]
          val originalColumnsMap = locally {
            val _t_m_p_12 = originalFields
            _t_m_p_12.map { field => 
              val index = eachRow.fieldIndex(field)
              if (eachRow.isNullAt(index)) {
                field -> ""null""
              } else {
                field -> eachRow.getAs(field).toString
              }
            }
          }
          val deserializedGenericRecord: GenericRecord = bytesToGenericRecordWithSchemaRecon(recordToDeserialize, avroSchemaString, avroSchemaString)
          val newDeserializedGenericRecord: GenericRecord = copyToGenericRecord(deserializedGenericRecord, avroSchemaString, newAvroSchemaString)
          locally {
            val _t_m_p_13 = originalColumnsMap
            _t_m_p_13.foreach {
              kv => newDeserializedGenericRecord.put(kv._1, kv._2)
            }
          }
          val avroSchemaObj: Schema = (new Schema.Parser).parse(newAvroSchemaString)
          val converter = AvroToSQLSchemaConverter.createConverterToSQL(avroSchemaObj)
          converter(newDeserializedGenericRecord).asInstanceOf[Row]
        }
"
"udf/spark_repos_8/gimel/..gimel-serde.serde-common.src.main.scala.com.paypal.gimel.serde.common.avro.AvroUtils.scala/udf/233.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
          val avroSchema = (new Schema.Parser).parse(avroSchemaString)
          val fields = locally {
            val _t_m_p_20 = avroSchema.getFields.asScala
            _t_m_p_20.map {
              x => x.name()
            }
          }.toArray
          val cols: Map[String, Any] = row.getValuesMap(fields)
          val genericRecord: GenericRecord = new GenericData.Record(avroSchema)
          locally {
            val _t_m_p_21 = cols
            _t_m_p_21.foreach(x => genericRecord.put(x._1, x._2))
          }
          genericRecordToBytes(genericRecord, avroSchemaString)
        }
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.examples.Graphs.scala/udf/104.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""j"") =!= n - 1
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.examples.Graphs.scala/udf/108.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""i"") =!= n - 1
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.GraphFrame.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.GraphFrame.scala/udf/135.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

condition
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.GraphFrame.scala/udf/188.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""degree"") >= threshold
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.GraphFrame.scala/udf/217.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isHub(col(joinCol))
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.GraphFrame.scala/udf/220.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!isHub(col(joinCol))
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.GraphFrame.scala/udf/224.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isHub(col(joinCol))
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.GraphFrame.scala/udf/227.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

isHub(col(joinCol))
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.BFS.scala/udf/101.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

toVExpr
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.BFS.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

from
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.BFS.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

to
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.BFS.scala/udf/52.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

to
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.BFS.scala/udf/64.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

efExpr
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.BFS.scala/udf/81.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

fromAExpr
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.BFS.scala/udf/83.26.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

col(""a.id"") =!= col(""b.id"")
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.BFS.scala/udf/95.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

previousVertexChecks
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.ConnectedComponents.scala/udf/139.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(SRC) =!= col(DST)
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.ConnectedComponents.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(SRC) =!= col(DST)
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.ConnectedComponents.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(CNT) > broadcastThreshold
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.GraphXConversions.scala/udf/162.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(GraphFrame.ID) === vertexId
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.Pregel.scala/udf/76.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

Pregel.msg.isNotNull
"
"udf/spark_repos_8/graphframes/..src.main.scala.org.graphframes.lib.TriangleCount.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""$LONG_SRC != $LONG_DST""
"
"udf/spark_repos_8/high-performance-spark-examples/..src.main.scala.com.high-performance-spark-examples.dataframe.HappyPandas.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

pandaInfo(""happyPandas"") >= minHappyPandas
"
"udf/spark_repos_8/high-performance-spark-examples/..src.main.scala.com.high-performance-spark-examples.dataframe.HappyPandas.scala/udf/70.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

pandaInfo(""happy"") !== true
"
"udf/spark_repos_8/high-performance-spark-examples/..src.main.scala.com.high-performance-spark-examples.dataframe.HappyPandas.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

pandaInfo(""happy"").and(pandaInfo(""attributes"")(0) > pandaInfo(""attributes"")(1))
"
"udf/spark_repos_8/high-performance-spark-examples/..src.main.scala.com.high-performance-spark-examples.dataframe.HappyPandas.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

pandaInfo(""happyPandas"") >= pandaInfo(""totalPandas"") / 2
"
"udf/spark_repos_8/high-performance-spark-examples/..src.main.scala.com.high-performance-spark-examples.dataframe.MixedDataset.scala/udf/17.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""happy"" === true
"
"udf/spark_repos_8/high-performance-spark-examples/..src.main.scala.com.high-performance-spark-examples.dataframe.UDFs.scala/udf/36.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

avg
"
"udf/spark_repos_8/high-performance-spark-examples/..src.main.scala.com.high-performance-spark-examples.dataframe.UDFs.scala/udf/8.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: String) => s.length()
"
"udf/spark_repos_8/keystone/..src.main.scala.keystoneml.loaders.AmazonReviewsDataLoader.scala/udf/9.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (if (r.getAs[Double](0) >= threshold) 1 else 0, r.getAs[String](1))
"
"udf/spark_repos_8/learningSpark/..src.main.scala.com.zgw.spark.sparkSQL.SparkSqlUDAF.scala/udf/13.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

uadf
"
"udf/spark_repos_8/morpheus/..morpheus-examples.src.main.scala.org.opencypher.morpheus.integration.yelp.YelpHelpers.scala/udf/37.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""city"" === city
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter1.scala.src.main.scala.com.databricks.apps.logs.chapter1.LogAnalyzerSQL.scala/udf/16.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getLong(1))
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter1.scala.src.main.scala.com.databricks.apps.logs.chapter1.LogAnalyzerSQL.scala/udf/21.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter1.scala.src.main.scala.com.databricks.apps.logs.chapter1.LogAnalyzerSQL.scala/udf/26.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter1.scala.src.main.scala.com.databricks.apps.logs.chapter1.LogAnalyzerSQL.scala/udf/9.17.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

ApacheAccessLog.parseLogLine
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter1.scala.src.main.scala.com.databricks.apps.logs.chapter1.LogAnalyzerStreamingSQL.scala/udf/25.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getLong(1))
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter1.scala.src.main.scala.com.databricks.apps.logs.chapter1.LogAnalyzerStreamingSQL.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getString(0)
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter1.scala.src.main.scala.com.databricks.apps.logs.chapter1.LogAnalyzerStreamingSQL.scala/udf/35.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter2.scala.src.main.scala.com.databricks.apps.logs.LogAnalyzerRDD.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getLong(1))
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter2.scala.src.main.scala.com.databricks.apps.logs.LogAnalyzerRDD.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0)
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter2.scala.src.main.scala.com.databricks.apps.logs.LogAnalyzerRDD.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter3.scala.src.main.scala.com.databricks.apps.logs.LogAnalyzerRDD.scala/udf/11.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getLong(1))
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter3.scala.src.main.scala.com.databricks.apps.logs.LogAnalyzerRDD.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0)
"
"udf/spark_repos_8/reference-apps/..logs_analyzer.chapter3.scala.src.main.scala.com.databricks.apps.logs.LogAnalyzerRDD.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getString(0), row.getLong(1))
"
"udf/spark_repos_8/reference-apps/..twitter_classifier.scala.src.main.scala.com.databricks.apps.twitterClassifier.ExamineAndTrain.scala/udf/47.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.toString
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.AvroSourceKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/125.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.AvroSourceSuite.scala/udf/134.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""name000"", ""name001"", ""name002"", ""name003"", ""name004"") and !($""col0"" isin (""name001"", ""name002"", ""name003""))
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.CompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/114.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/42.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/87.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.DataTypeSuite.scala/udf/96.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/107.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/125.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/62.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/89.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixCompositeKeySuite.scala/udf/98.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_8/shc/..core.src.test.scala.org.apache.spark.sql.PhoenixSuite.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" isin (""row005"", ""row001"", ""row002"") and !($""col0"" isin (""row001"", ""row002""))
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/101.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" === ""name005"" || $""col1.name"" <= ""name005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.AvroSource.scala/udf/105.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col1.name"" <= ""name005"" || $""col1.name"".contains(""name007"")
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" > 40
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col01"" >= 40
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/64.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" >= ""row250"" && $""col01"" < 50
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/68.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row010""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row010""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row011""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" === ""row005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row010"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row010""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.CompositeKey.scala/udf/88.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col00"" <= ""row050"" && $""col00"" > ""row040"" || $""col00"" === ""row005"" || $""col00"" === ""row020"" || $""col00"" === ""r20"" || $""col00"" <= ""row005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/53.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataCoder.scala/udf/57.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/100.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -100
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" < 0
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/51.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/58.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -9
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= -9
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/72.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" >= 0
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/79.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > 10
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > -10 && $""col0"" <= 10
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.DataType.scala/udf/93.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= -10 || $""col0"" > 10
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/61.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/69.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/78.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/82.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" === ""row005"" || $""col0"" <= ""row005""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.HBaseSource.scala/udf/86.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" > ""row250""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/76.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.JoinTablesFrom2Clusters.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobAccessing2Clusters.scala/udf/79.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row120"" && $""col0"" > ""row090""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobAccessing2Clusters.scala/udf/83.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""col0"" <= ""row150"" && $""col0"" > ""row100""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/55.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""5""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/59.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""40""
"
"udf/spark_repos_8/shc/..examples.src.main.scala.org.apache.spark.sql.execution.datasources.hbase.LRJobForDataSources.scala/udf/63.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""key"" <= ""20"" && $""key"" >= ""1""
"
"udf/spark_repos_8/snappydata/..core.src.main.scala.org.apache.spark.sql.streaming.SnappySinkCallback.scala/udf/224.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(EVENT_TYPE_COLUMN).isin(INSERT, UPDATE, DELETE)
"
"udf/spark_repos_8/snappydata/..core.src.main.scala.org.apache.spark.sql.streaming.SnappySinkCallback.scala/udf/231.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(EVENT_TYPE_COLUMN) === DELETE
"
"udf/spark_repos_8/snappydata/..core.src.main.scala.org.apache.spark.sql.streaming.SnappySinkCallback.scala/udf/240.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(EVENT_TYPE_COLUMN).isin(upsertEventTypes: _*)
"
"udf/spark_repos_8/snappydata/..core.src.main.scala.org.apache.spark.sql.streaming.SnappySinkCallback.scala/udf/248.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(EVENT_TYPE_COLUMN) === INSERT
"
"udf/spark_repos_8/snappydata/..core.src.main.scala.org.apache.spark.sql.streaming.SnappySinkCallback.scala/udf/255.29.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataFrame(EVENT_TYPE_COLUMN) === UPDATE
"
"udf/spark_repos_8/snappydata/..core.src.test.scala.io.snappydata.app.SparkSQLMultiThreadingTest.scala/udf/126.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Count: "" + t(0)
"
"udf/spark_repos_8/snappydata/..core.src.test.scala.io.snappydata.app.SparkSQLMultiThreadingTest.scala/udf/137.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Count: "" + t(0)
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.complexdatatypes.MapTypeAPI.scala/udf/29.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncReadDF(""name"") === ""JxVJBxYlNT""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.complexdatatypes.SmartConnectorMapTypeAPI.scala/udf/33.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncReadDF(""name"") === ""JxVJBxYlNT""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.complexdatatypes.SmartConnectorStructTypeAPI.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncReadDF(""TestRecord"").getItem(""batStyle"") === ""LeftHand""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.complexdatatypes.SmartConnectorStructTypeAPI.scala/udf/36.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sparkReadDF(""TestRecord"").getField(""batStyle"") === ""LeftHand""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.complexdatatypes.StructTypeAPI.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncReadDF(""TestRecord"").getItem(""batStyle"") === ""LeftHand""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.complexdatatypes.StructTypeAPI.scala/udf/32.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sparkReadDF(""TestRecord"").getField(""batStyle"") === ""LeftHand""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.ExternalTablesAPINorthWind.scala/udf/112.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncOrderDetailsDF(""PrdId"").equalTo(3)
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.ExternalTablesAPINorthWind.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sparkOrderDetailsDF(""ProductID"").equalTo(3)
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.ExternalTablesAPINorthWind.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncEmpDF(""Title"") === ""Sales Representative""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.ExternalTablesAPINorthWind.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sparkEmpDF(""Title"") === ""Sales Representative""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.ExternalTablesAPINorthWind.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncEmpDF(""Title"") =!= ""Sales Representative""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.ExternalTablesAPINorthWind.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sparkEmpDF(""Title"") =!= ""Sales Representative""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.SmartConnectorExternalTableAPINW.scala/udf/112.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncOrderDetailsDF(""PrdId"").equalTo(3)
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.SmartConnectorExternalTableAPINW.scala/udf/116.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sparkOrderDetailsDF(""ProductID"").equalTo(3)
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.SmartConnectorExternalTableAPINW.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncEmpDF(""Title"") === ""Sales Representative""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.SmartConnectorExternalTableAPINW.scala/udf/71.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sparkEmpDF(""Title"") === ""Sales Representative""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.SmartConnectorExternalTableAPINW.scala/udf/80.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sncEmpDF(""Title"") =!= ""Sales Representative""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.externaltables.SmartConnectorExternalTableAPINW.scala/udf/84.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

sparkEmpDF(""Title"") =!= ""Sales Representative""
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.northwind.NWTestUtil.scala/udf/57.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

dataTypeConverter
"
"udf/spark_repos_8/snappydata/..dtests.src.test.scala.io.snappydata.hydra.SnappyTestUtils.scala/udf/37.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

dataTypeConverter
"
"udf/spark_repos_8/spark-avro/..src.test.scala.com.databricks.spark.avro.AvroSuite.scala/udf/518.22.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

i => s""record$i""
"
"udf/spark_repos_8/spark-ext/..sparkext-example.src.main.scala.com.collective.sparkext.example.SparkMlExtExample.scala/udf/59.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
      case Row(probability: DenseVector, label: Double) =>
        val predictedActionProbability = probability(1)
        (predictedActionProbability, label)
    }
"
"udf/spark_repos_8/spark-ext/..sparkext-mllib.src.main.scala.org.apache.spark.ml.feature.Binning.scala/udf/28.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col($(inputCol)).isNotNull
"
"udf/spark_repos_8/spark-ext/..sparkext-mllib.src.main.scala.org.apache.spark.ml.feature.GatherEncoder.scala/udf/122.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""count"") >= threshold
"
"udf/spark_repos_8/spark-ext/..sparkext-mllib.src.main.scala.org.apache.spark.ml.feature.StringToShortIndexer.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getString(0)
"
"udf/spark_repos_8/spark-ext/..sparkext-mllib.src.main.scala.org.apache.spark.mllib.evaluation.BinaryModelMetrics.scala/udf/11.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (r.getDouble(0), r.getDouble(1))
"
"udf/spark_repos_8/spark-ext/..sparkext-mllib.src.main.scala.org.apache.spark.ml.sampling.Downsampling.scala/udf/41.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(getLabelCol) === getPrimaryClass
"
"udf/spark_repos_8/spark-ext/..sparkext-mllib.src.main.scala.org.apache.spark.ml.sampling.Downsampling.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(getLabelCol) !== getPrimaryClass
"
"udf/spark_repos_8/spark-ext/..sparkext-mllib.src.main.scala.org.apache.spark.ml.sampling.Downsampling.scala/udf/80.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(getLabelCol) === getPrimaryClass
"
"udf/spark_repos_8/spark-ext/..sparkext-mllib.src.main.scala.org.apache.spark.ml.sampling.Downsampling.scala/udf/84.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(getLabelCol) !== getPrimaryClass
"
"udf/spark_repos_8/spark-knn/..spark-knn-core.src.test.scala.org.apache.spark.ml.knn.KNNSuite.scala/udf/160.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

{
        (v: Vector) => math.abs(v(0))
      }
"
"udf/spark_repos_8/spark-knn/..spark-knn-examples.src.main.scala.org.apache.spark.ml.classification.NaiveKNN.scala/udf/59.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => new VectorWithNorm(r.getAs[Vector](0))
"
"udf/spark_repos_8/SparkLearning/..SparkLearning1.src.main.scala.org.apache.spark.ml.Ensembles.EnsemblesSuite.scala/udf/125.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(p: Double, l: Double) =>
          (p, l)
      }
"
"udf/spark_repos_8/SparkLearning/..SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromJson.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""age"") > 21
"
"udf/spark_repos_8/SparkLearning/..SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromTextReflection.scala/udf/25.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_8/SparkLearning/..SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromTextReflection.scala/udf/32.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t.getAs[String](""name"")
"
"udf/spark_repos_8/SparkLearning/..SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromTextReflection.scala/udf/39.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_8/SparkLearning/..SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLExamplesFromTextSchema.scala/udf/33.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""Name: "" + t(0)
"
"udf/spark_repos_8/SparkLearning/..SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLReadParquetMethods.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""id"") > 12904870
"
"udf/spark_repos_8/SparkLearning/..SparkLearning1.src.main.scala.org.apache.spark.sql.learning.SparkSQLReadParquetMethods.scala/udf/28.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""id: "" + t(0) + "" and modified:"" + t(1)
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.DataFrameLearning.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/13.19.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

_ + 1
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/40.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager(0)
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/44.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => ""Name: "" + teenager.getAs[String](""name"")
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/50.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

teenager => teenager.getValuesMap[Any](List(""name"", ""age""))
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.DataSetLearning.scala/udf/78.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.HiveTablesLearning.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(key: Int, value: String) =>
          s""Key: $key, Value: $value""
      }
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.implicitsLearning.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""age"" > 21
"
"udf/spark_repos_8/SparkLearning/..SparkLearning2.1.src.main.scala.org.apache.spark.sql.learning.ParquetLearning.scala/udf/15.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

attributes => ""Name: "" + attributes(0)
"
"udf/spark_repos_8/sparkling-water/..core.src.main.scala.ai.h2o.sparkling.backend.converters.SupportedDataset.scala/udf/112.21.Dataset-Vector.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.ml.linalg.Vector]
Call: map

v => Tuple1(v)
"
"udf/spark_repos_8/sparkling-water/..core.src.main.scala.ai.h2o.sparkling.backend.converters.SupportedDataset.scala/udf/122.21.Dataset-Vector.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.mllib.linalg.Vector]
Call: map

v => Tuple1(v)
"
"udf/spark_repos_8/sparkling-water/..core.src.main.scala.ai.h2o.sparkling.backend.converters.SupportedDataset.scala/udf/97.21.Dataset-Timestamp.map","Type: org.apache.spark.sql.Dataset[java.sql.Timestamp]
Call: map

v => Tuple1(v)
"
"udf/spark_repos_8/sparkling-water/..core.src.test.scala.ai.h2o.sparkling.TestUtils.scala/udf/93.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (r: Row) => 
        val schema = schemaHolder.schema
        val localRandom = new Random(r.getLong(0))
        val values = locally {
          val _t_m_p_7 = schema.fields
          _t_m_p_7.map(f => generateValueForField(localRandom, f, settings))
        }
        new GenericRowWithSchema(values, schema)
      }
"
"udf/spark_repos_8/sparkling-water/..examples.src.main.scala.ai.h2o.sparkling.examples.AirlinesWithWeatherDemo.scala/udf/18.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'Dest === ""ORD""
"
"udf/spark_repos_8/sparkling-water/..examples.src.main.scala.ai.h2o.sparkling.examples.CityBikeSharingDemo.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'HourLocal === 12
"
"udf/spark_repos_8/sparkling-water/..examples.src.main.scala.ai.h2o.sparkling.examples.DeepLearningDemo.scala/udf/15.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'Dest === ""SFO""
"
"udf/spark_repos_8/sparkling-water/..examples.src.main.scala.ai.h2o.sparkling.examples.ProstateDemo.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'CAPSULE === 1
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.Annotation.scala/udf/47.19.Dataset-AnnotationContainer.map","Type: org.apache.spark.sql.Dataset[com.johnsnowlabs.nlp.Annotation.AnnotationContainer]
Call: map

_.__annotation
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.Annotation.scala/udf/78.19.Dataset-AnnotationContainer.map","Type: org.apache.spark.sql.Dataset[com.johnsnowlabs.nlp.Annotation.AnnotationContainer]
Call: map

_.__annotation
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.functions.scala/udf/18.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

func(col(column)).as(column, meta)
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.training.POS.scala/udf/59.26.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.nonEmpty
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.training.POS.scala/udf/61.21.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => lineToTaggedDocument(line, delimiter)
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.training.POS.scala/udf/63.19.Dataset-TaggedDocument.map","Type: org.apache.spark.sql.Dataset[com.johnsnowlabs.nlp.training.TaggedDocument]
Call: map

{
        case TaggedDocument(sentence, taggedTokens) =>
          Annotations(sentence, createDocumentAnnotation(sentence), createPosAnnotation(sentence, taggedTokens))
      }
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.util.io.ResourceHelper.scala/udf/270.29.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.nonEmpty
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.util.io.ResourceHelper.scala/udf/272.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => locally {
            val _t_m_p_27 = locally {
              val _t_m_p_28 = line.split(""\\s+"")
              _t_m_p_28.filter(kv => {
                val s = kv.split(er.options(""delimiter"").head)
                s.length == 2 && s(0).nonEmpty && s(1).nonEmpty
              })
            }
            _t_m_p_27.map(kv => {
              val p = kv.split(er.options(""delimiter"").head)
              TaggedWord(p(0), p(1))
            })
          }
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.util.io.ResourceHelper.scala/udf/302.29.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.nonEmpty
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.nlp.util.io.ResourceHelper.scala/udf/304.24.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => locally {
            val _t_m_p_32 = locally {
              val _t_m_p_33 = line.split(""\\s+"")
              _t_m_p_33.filter(kv => {
                val s = kv.split(er.options(""delimiter"").head)
                s.length == 2 && s(0).nonEmpty && s(1).nonEmpty
              })
            }
            _t_m_p_32.map(kv => {
              val p = kv.split(er.options(""delimiter"").head)
              TaggedWord(p(0), p(1))
            })
          }
"
"udf/spark_repos_8/spark-nlp/..src.main.scala.com.johnsnowlabs.util.CoNLLGenerator.scala/udf/37.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

OArray
"
"udf/spark_repos_8/spark-nlp/..src.test.scala.com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteBehaviors.scala/udf/83.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""prediction"") === 1
"
"udf/spark_repos_8/spark-nlp/..src.test.scala.com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteBehaviors.scala/udf/87.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""prediction"") === 0
"
"udf/spark_repos_8/Spark-Structured-Streaming-Examples/..src.main.scala.kafka.KafkaSource.scala/udf/14.22.Dataset-SimpleSongAggregationKafka.filter","Type: org.apache.spark.sql.Dataset[radio.SimpleSongAggregationKafka]
Call: filter

_.radioCount != null
"
"udf/spark_repos_8/spark-testing-base/..core.src.main.2.0.scala.com.holdenkarau.spark.testing.DataFrameSuiteBase.scala/udf/113.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(expectedCol) =!= col(actualCol)
"
"udf/spark_repos_8/spark-testing-base/..core.src.test.2.2.scala.com.holdenkarau.spark.testing.StructuredStreamingSampleTests.scala/udf/12.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

elem => elem + 3
"
"udf/spark_repos_8/spark-testing-base/..core.src.test.2.2.scala.com.holdenkarau.spark.testing.StructuredStreamingSampleTests.scala/udf/24.21.Dataset-Int.map","Type: org.apache.spark.sql.Dataset[Int]
Call: map

elem => elem.toString
"
"udf/spark_repos_8/Spark-The-Definitive-Guide/..project-templates.scala.src.main.scala.DataFrameExample.scala/udf/15.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

DFUtils.pointlessUDF(_: String): String
"
"udf/spark_repos_8/tispark/..core.src.main.scala.com.pingcap.tispark.utils.TiUtil.scala/udf/109.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

() => s""${TiSparkVersion.version}\n${TiSparkInfo.info}\n$timeZoneStr""
"
"udf/spark_repos_8/tispark/..core.src.main.scala.com.pingcap.tispark.utils.TiUtil.scala/udf/113.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: Long, frac: Int) => Converter.convertDurationToStr(value, frac)
"
"udf/spark_repos_8/tispark/..core.src.main.scala.com.pingcap.tispark.utils.TiUtil.scala/udf/117.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(value: String) => Converter.convertStrToDuration(value)
"
"udf/spark_repos_8/tispark/..core.src.main.scala.com.pingcap.tispark.write.TiBatchWriteTable.scala/udf/84.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.get(colOffset) == null
"
"udf/spark_repos_8/TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.evaluator.OPLogLoss.scala/udf/13.21.Dataset-Vector, Double).map","Type: org.apache.spark.sql.Dataset[(Double, org.apache.spark.ml.linalg.Vector, org.apache.spark.ml.linalg.Vector, Double)]
Call: map

{
          case (lbl, _, prob, _) =>
            new AveragedValue(count = 1L, value = -math.log(prob.toArray(lbl.toInt)))
        }
"
"udf/spark_repos_8/TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.feature.DecisionTreeNumericBucketizer.scala/udf/70.23.Dataset-(Double, Double).map","Type: org.apache.spark.sql.Dataset[(Double, Double)]
Call: map

{
            case (label, v) =>
              label -> Vectors.dense(Array(v))
          }
"
"udf/spark_repos_8/TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.DataBalancer.scala/udf/64.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

labelCol === labelVal
"
"udf/spark_repos_8/TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.DataCutter.scala/udf/103.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => !labelsKeptSet.contains(r.getDouble(labelColIdx))
"
"udf/spark_repos_8/TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.DataCutter.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => labelSet.contains(r.getDouble(labelColIdx))
"
"udf/spark_repos_8/TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.DataCutter.scala/udf/95.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

r => r.getLong(countColIdx).toDouble / totalValues >= minLabelFract
"
"udf/spark_repos_8/TransmogrifAI/..core.src.main.scala.com.salesforce.op.stages.impl.tuning.OpValidator.scala/udf/69.24.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

functions.col(label) === theClass
"
"udf/spark_repos_8/TransmogrifAI/..core.src.test.scala.com.salesforce.op.stages.impl.classification.FunctionalityForClassificationTests.scala/udf/15.23.Dataset-Vector, Double).map","Type: org.apache.spark.sql.Dataset[(Double, org.apache.spark.ml.linalg.Vector, org.apache.spark.ml.linalg.Vector, Double)]
Call: map

{
            case (lbl, _, prob, _) =>
              math.log(prob.toArray(lbl.toInt))
          }
"
"udf/spark_repos_8/TransmogrifAI/..core.src.test.scala.com.salesforce.op.stages.impl.tuning.DataCutterTest.scala/udf/143.17.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

_.getDouble(0)
"
"udf/spark_repos_8/TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.binary.BinaryEstimator.scala/udf/22.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (convertI1.fromSpark(r.get(0)).value, convertI2.fromSpark(r.get(1)).value)
"
"udf/spark_repos_8/TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.quaternary.QuaternaryEstimator.scala/udf/26.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (convertI1.fromSpark(r.get(0)).value, convertI2.fromSpark(r.get(1)).value, convertI3.fromSpark(r.get(2)).value, convertI4.fromSpark(r.get(3)).value)
"
"udf/spark_repos_8/TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.sequence.BinarySequenceEstimator.scala/udf/30.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ r => 
        val rowSeq = r.toSeq
        (convertI1.fromSpark(rowSeq.head).value, locally {
          val _t_m_p_3 = rowSeq.tail
          _t_m_p_3.map(seqIConvert.fromSpark(_).value)
        })
      }
"
"udf/spark_repos_8/TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.sequence.SequenceEstimator.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

locally {
        val _t_m_p_3 = _.toSeq
        _t_m_p_3.map(seqIConvert.fromSpark(_).value)
      }
"
"udf/spark_repos_8/TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.ternary.TernaryEstimator.scala/udf/24.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => (convertI1.fromSpark(r.get(0)).value, convertI2.fromSpark(r.get(1)).value, convertI3.fromSpark(r.get(2)).value)
"
"udf/spark_repos_8/TransmogrifAI/..features.src.main.scala.com.salesforce.op.stages.base.unary.UnaryEstimator.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

r => iConvert.fromSpark(r.get(0)).value
"
"udf/spark_repos_8/TransmogrifAI/..features.src.main.scala.com.salesforce.op.utils.spark.RichDataset.scala/udf/45.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => predicate(row.getAs[T](columnName))
"
"udf/spark_repos_8/TransmogrifAI/..features.src.main.scala.com.salesforce.op.utils.spark.RichDataset.scala/udf/49.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => predicate(row.getFeatureType(feature).value)
"
"udf/spark_repos_8/TransmogrifAI/..readers.src.main.scala.com.salesforce.op.readers.DataReader.scala/udf/107.25.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

record => (key(record), Seq(record))
"
"udf/spark_repos_8/TransmogrifAI/..utils.src.main.scala.com.salesforce.op.test.SparkMatchers.scala/udf/11.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{
          row => inv(predicate(row.getAs[T](columnName)))
        }
"
"udf/spark_repos_8/TransmogrifAI/..utils.src.main.scala.com.salesforce.op.utils.io.csv.CSVInOut.scala/udf/12.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.toSeq.collect({
        case null =>
          null
        case v =>
          v.toString
      })
"
"udf/spark_repos_9/aas/..ch02-intro.src.main.scala.com.cloudera.datascience.intro.RunIntro.scala/udf/30.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""is_match"" === false
"
"udf/spark_repos_9/aas/..ch02-intro.src.main.scala.com.cloudera.datascience.intro.RunIntro.scala/udf/46.19.Dataset-MatchData.map","Type: org.apache.spark.sql.Dataset[com.cloudera.datascience.intro.MatchData]
Call: map

{
        md => (scoreMatchData(md), md.is_match)
      }
"
"udf/spark_repos_9/aas/..ch03-recommender.src.main.scala.com.cloudera.datascience.recommender.RunRecommender.scala/udf/142.20.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val Array(userID, artistID, count) = locally {
          val _t_m_p_11 = line.split(' ')
          _t_m_p_11.map(_.toInt)
        }
        val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)
        (userID, finalArtistID, count)
      }
"
"udf/spark_repos_9/aas/..ch03-recommender.src.main.scala.com.cloudera.datascience.recommender.RunRecommender.scala/udf/189.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""positivePrediction"" > ($""negativePrediction"")
"
"udf/spark_repos_9/aas/..ch03-recommender.src.main.scala.com.cloudera.datascience.recommender.RunRecommender.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

{ line => 
        val Array(user, artist, _*) = line.split(' ')
        (user.toInt, artist.toInt)
      }
"
"udf/spark_repos_9/aas/..ch03-recommender.src.main.scala.com.cloudera.datascience.recommender.RunRecommender.scala/udf/43.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (badID, goodID)
"
"udf/spark_repos_9/aas/..ch03-recommender.src.main.scala.com.cloudera.datascience.recommender.RunRecommender.scala/udf/55.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

$""user"" === userID
"
"udf/spark_repos_9/aas/..ch03-recommender.src.main.scala.com.cloudera.datascience.recommender.RunRecommender.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (existingArtistIDs: _*)
"
"udf/spark_repos_9/aas/..ch03-recommender.src.main.scala.com.cloudera.datascience.recommender.RunRecommender.scala/udf/67.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""id"" isin (recommendedArtistIDs: _*)
"
"udf/spark_repos_9/aas/..ch04-rdf.src.main.scala.com.cloudera.datascience.rdf.RunRDF.scala/udf/73.19.Dataset-Double.map","Type: org.apache.spark.sql.Dataset[Double]
Call: map

_ / total
"
"udf/spark_repos_9/aas/..ch05-kmeans.src.main.scala.com.cloudera.datascience.kmeans.RunKMeans.scala/udf/200.20.Dataset-Vector).map","Type: org.apache.spark.sql.Dataset[(Int, org.apache.spark.ml.linalg.Vector)]
Call: map

{
        case (cluster, vec) =>
          Vectors.sqdist(centroids(cluster), vec)
      }
"
"udf/spark_repos_9/aas/..ch05-kmeans.src.main.scala.com.cloudera.datascience.kmeans.RunKMeans.scala/udf/208.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
        val cluster = row.getAs[Int](""cluster"")
        val vec = row.getAs[Vector](""scaledFeatureVector"")
        Vectors.sqdist(centroids(cluster), vec) >= threshold
      }
"
"udf/spark_repos_9/aas/..ch07-graph.src.main.scala.com.cloudera.datascience.graph.RunGraph.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

majorTopics
"
"udf/spark_repos_9/aas/..ch07-graph.src.main.scala.com.cloudera.datascience.graph.RunGraph.scala/udf/41.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(topic: String) =>
          (hashId(topic), topic)
      }
"
"udf/spark_repos_9/aas/..ch07-graph.src.main.scala.com.cloudera.datascience.graph.RunGraph.scala/udf/48.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(topics: Seq[_], cnt: Long) =>
          val ids = locally {
            val _t_m_p_6 = locally {
              val _t_m_p_7 = topics
              _t_m_p_7.map(_.toString)
            }
            _t_m_p_6.map(hashId)
          }.sorted
          Edge(ids(0), ids(1), cnt)
      }
"
"udf/spark_repos_9/aas/..ch07-graph.src.main.scala.com.cloudera.datascience.graph.RunGraph.scala/udf/90.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        case Row(topic: String, cnt: Long) =>
          (hashId(topic), cnt)
      }
"
"udf/spark_repos_9/aas/..ch08-geotime.src.main.scala.com.cloudera.datascience.geotime.RunGeoTime.scala/udf/30.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

hours
"
"udf/spark_repos_9/albedo/..src.main.scala.ws.vinta.albedo.ALSRecommenderBuilder.scala/udf/30.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getInt(0)
"
"udf/spark_repos_9/albedo/..src.main.scala.ws.vinta.albedo.ContentRecommenderBuilder.scala/udf/20.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getInt(0)
"
"udf/spark_repos_9/albedo/..src.main.scala.ws.vinta.albedo.CurationRecommenderBuilder.scala/udf/20.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getInt(0)
"
"udf/spark_repos_9/albedo/..src.main.scala.ws.vinta.albedo.LogisticRegressionRanker.scala/udf/187.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getInt(0)
"
"udf/spark_repos_9/albedo/..src.main.scala.ws.vinta.albedo.PopularityRecommenderBuilder.scala/udf/20.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

row => row.getInt(0)
"
"udf/spark_repos_9/bahir/..sql-cloudant.examples.src.main.scala.org.apache.spark.examples.sql.cloudant.CloudantApp.scala/udf/21.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""code: "" + t(0) + "",name:"" + t(1)
"
"udf/spark_repos_9/bahir/..sql-cloudant.examples.src.main.scala.org.apache.spark.examples.sql.cloudant.CloudantApp.scala/udf/39.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

t => ""flightSegmentId: "" + t(0) + "", scheduledDepartureTime: "" + t(1)
"
"udf/spark_repos_9/bahir/..sql-cloudant.examples.src.main.scala.org.apache.spark.examples.sql.cloudant.CloudantDFOption.scala/udf/13.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""_id"") >= ""CAA""
"
"udf/spark_repos_9/bahir/..sql-cloudant.examples.src.main.scala.org.apache.spark.examples.sql.cloudant.CloudantDFOption.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""_id"") >= ""CAA""
"
"udf/spark_repos_9/bahir/..sql-cloudant.examples.src.main.scala.org.apache.spark.examples.sql.cloudant.CloudantDFOption.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df2(""flightSegmentId"") > ""AA9""
"
"udf/spark_repos_9/bahir/..sql-cloudant.examples.src.main.scala.org.apache.spark.examples.sql.cloudant.CloudantDF.scala/udf/10.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""flightSegmentId"") === ""AA106""
"
"udf/spark_repos_9/bahir/..sql-cloudant.examples.src.main.scala.org.apache.spark.examples.sql.cloudant.CloudantDF.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df3(""flightSegmentId"") > ""AA9""
"
"udf/spark_repos_9/bahir/..sql-streaming-akka.examples.src.main.scala.org.apache.bahir.examples.sql.streaming.akka.AkkaStreamWordCount.scala/udf/16.21.Dataset-Timestamp).map","Type: org.apache.spark.sql.Dataset[(String, java.sql.Timestamp)]
Call: map

_._1
"
"udf/spark_repos_9/bahir/..sql-streaming-jdbc.examples.src.main.scala.org.apache.bahir.examples.sql.streaming.jdbc.JdbcSinkDemo.scala/udf/20.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

{
        value => Person(s""name_$value"", value.toInt % 30)
      }
"
"udf/spark_repos_9/carbondata/..examples.spark.src.main.scala.org.apache.carbondata.examples.CarbonDataFrameExample.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""number"" > 31
"
"udf/spark_repos_9/carbondata/..examples.spark.src.main.scala.org.apache.carbondata.examples.StreamingWithRowParserExample.scala/udf/93.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_9/carbondata/..integration.spark.src.test.scala.org.apache.spark.carbondata.TestStreamingTableQueryFilter.scala/udf/85.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_9/carbondata/..integration.spark.src.test.scala.org.apache.spark.carbondata.TestStreamingTableWithLongString.scala/udf/233.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_9/carbondata/..integration.spark.src.test.scala.org.apache.spark.carbondata.TestStreamingTableWithRowParser.scala/udf/251.27.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.split("","")
"
"udf/spark_repos_9/cloudflow/..core.cloudflow-spark.src.main.scala.cloudflow.spark.avro.SparkAvroDecoder.scala/udf/59.19.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

{ value => 
        val key = keyFun(value)
        val internalRow = encoder.toRow(value)
        val row = rowEncoder.fromRow(internalRow)
        val bytes = rowToBytes(row)
        EncodedKV(key, bytes)
      }
"
"udf/spark_repos_9/CM-Well/..tools.dataConsistencyTool.cmwell-spark-analysis.src.main.scala.cmwell.analytics.util.SetDifferenceAndFilter.scala/udf/15.22.Dataset-KeyFields.filter","Type: org.apache.spark.sql.Dataset[cmwell.analytics.util.KeyFields]
Call: filter

overallFilter
"
"udf/spark_repos_9/CM-Well/..tools.dataConsistencyTool.cmwell-spark-analysis.src.main.scala.cmwell.analytics.util.SetDifferenceAndFilter.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

uuids2(""uuid"").isNull
"
"udf/spark_repos_9/CTRmodel/..src.main.com.ggstar.evaluation.Evaluator.scala/udf/9.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => (row.apply(1).asInstanceOf[DenseVector](1), row.getAs[Int](""label"").toDouble)
      }
"
"udf/spark_repos_9/CTRmodel/..src.main.com.ggstar.features.FeatureEngineering.scala/udf/10.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), Vectors.dense(row.getAs[Seq[Double]](""user_embedding"").toArray), Vectors.dense(row.getAs[Seq[Double]](""item_embedding"").toArray), row.getAs[Int](""label""))
"
"udf/spark_repos_9/CTRmodel/..src.main.com.ggstar.features.FeatureEngineering.scala/udf/17.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val userEmbedding = row.getAs[DenseVector](""user_embedding"")
        val itemEmbedding = row.getAs[DenseVector](""item_embedding"")
        var aSquare = 0.0d
        var bSquare = 0.0d
        var abProduct = 0.0d
        for (i <- 0 until userEmbedding.size) {
          aSquare += userEmbedding(i) * userEmbedding(i)
          bSquare += itemEmbedding(i) * itemEmbedding(i)
          abProduct += userEmbedding(i) * itemEmbedding(i)
        }
        var innerProduct = 0.0d
        if (aSquare == 0 || bSquare == 0) {
          innerProduct = 0.0d
        } else {
          innerProduct = abProduct / (Math.sqrt(aSquare) * Math.sqrt(bSquare))
        }
        (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), innerProduct, row.getAs[Int](""label""))
      }
"
"udf/spark_repos_9/CTRmodel/..src.main.com.ggstar.features.FeatureEngineering.scala/udf/42.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => {
        val user_embedding = row.getAs[DenseVector](""user_embedding"")
        val item_embedding = row.getAs[DenseVector](""item_embedding"")
        val outerProductEmbedding: Array[Double] = Array.fill[Double](user_embedding.size)(0)
        for (i <- 0 until user_embedding.size) {
          outerProductEmbedding(i) = user_embedding(i) * item_embedding(i)
        }
        (row.getAs[Int](""user_id""), row.getAs[Int](""item_id""), row.getAs[Int](""category_id""), row.getAs[String](""content_type""), row.getAs[String](""timestamp""), row.getAs[Long](""user_item_click""), row.getAs[Double](""user_item_imp""), row.getAs[Double](""item_ctr""), row.getAs[Int](""is_new_user""), Vectors.dense(outerProductEmbedding), row.getAs[Int](""label""))
      }
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.AlwaysNullConstraint.scala/udf/7.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(columnName).isNotNull
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.ColumnColumnConstraint.scala/udf/8.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

constraintColumn
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.ConditionalColumnConstraint.scala/udf/8.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

!statement || implication
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.DateFormatConstraint.scala/udf/14.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

cannotBeDate(new Column(columnName))
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.JoinableConstraint.scala/udf/20.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(""count"") > 1
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.NeverNullConstraint.scala/udf/7.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(columnName).isNull
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.RegexConstraint.scala/udf/11.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

doesNotMatch(new Column(columnName))
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.StringColumnConstraint.scala/udf/7.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

constraintString
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.TypeConversionConstraint.scala/udf/14.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(castedColumnName).isNull && originalColumn.isNotNull
"
"udf/spark_repos_9/drunken-data-quality/..src.main.scala.de.frosner.ddq.constraints.UniqueKeyConstraint.scala/udf/12.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

new Column(""count"") > 1
"
"udf/spark_repos_9/frameless/..dataset.src.main.scala.frameless.TypedDatasetForwarded.scala/udf/68.19.Dataset-T.map","Type: org.apache.spark.sql.Dataset[T]
Call: map

func
"
"udf/spark_repos_9/frameless/..dataset.src.main.scala.frameless.TypedDatasetForwarded.scala/udf/80.23.Dataset-T.filter","Type: org.apache.spark.sql.Dataset[T]
Call: filter

func
"
"udf/spark_repos_9/frameless/..dataset.src.main.scala.frameless.TypedDataset.scala/udf/102.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

column.untyped
"
"udf/spark_repos_9/frameless/..dataset.src.main.scala.frameless.TypedDataset.scala/udf/279.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(column.value.name).isNotNull
"
"udf/spark_repos_9/frameless/..dataset.src.main.scala.frameless.TypedDataset.scala/udf/47.24.Dataset-Out.filter","Type: org.apache.spark.sql.Dataset[Out]
Call: filter

filterStr
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.GpuKMeansBatch.scala/udf/224.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => generateData(i, N, d, R)
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.GpuKMeansBatchSmall.scala/udf/219.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => generateData(i, N, d, R)
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.GpuKMeansFile.scala/udf/199.20.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => {
        val rowElem = x.getString(0).split("" "")
        val len = rowElem.length
        val buffer = new mutable.ListBuffer[Double]()
        locally {
          val _t_m_p_11 = 0 until len
          _t_m_p_11.foreach {
            idx => if (!rowElem(idx).isEmpty) buffer += rowElem(idx).toDouble
          }
        }
        DataPointKMeans(buffer.toArray)
      }
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.GpuKMeans.scala/udf/358.20.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => generateData(i, N, d, R)
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.GpuKMeans.scala/udf/368.20.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => generateDataMod(i, N, d, R)
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.SparkDSLRmod.scala/udf/53.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => generateData(i, N, D, R)
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.SparkDSLRmod.scala/udf/99.23.Dataset-DataPoint.map","Type: org.apache.spark.sql.Dataset[com.ibm.gpuenabler.SparkDSLRmod.DataPoint]
Call: map

{
            p => dmulvs(p.x, 1 / (1 + exp(-1 * ddotvv(wCPU, p.x))) - p.y)
          }
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.SparkDSLR.scala/udf/101.23.Dataset-DataPoint.map","Type: org.apache.spark.sql.Dataset[com.ibm.gpuenabler.SparkDSLR.DataPoint]
Call: map

{
            p => dmulvs(p.x, (1 / (1 + exp(-p.y * ddotvv(wCPU, p.x))) - 1) * p.y)
          }
"
"udf/spark_repos_9/GPUEnabler/..examples.src.main.scala.com.ibm.gpuenabler.SparkDSLR.scala/udf/53.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

i => generateData(i, N, D, R)
"
"udf/spark_repos_9/GPUEnabler/..gpu-enabler.src.test.scala.com.ibm.gpuenabler.CUDADSFunctionSuite.scala/udf/194.23.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_.toShort
"
"udf/spark_repos_9/GPUEnabler/..gpu-enabler.src.test.scala.com.ibm.gpuenabler.CUDADSFunctionSuite.scala/udf/261.24.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_.toInt
"
"udf/spark_repos_9/GPUEnabler/..gpu-enabler.src.test.scala.com.ibm.gpuenabler.CUDADSFunctionSuite.scala/udf/310.24.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_.toInt
"
"udf/spark_repos_9/GPUEnabler/..gpu-enabler.src.test.scala.com.ibm.gpuenabler.CUDADSFunctionSuite.scala/udf/816.24.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

x => MatrixData(x, x)
"
"udf/spark_repos_9/incubator-s2graph/..s2jobs.src.main.scala.org.apache.s2graph.s2jobs.task.custom.process.ALSModelProcess.scala/udf/43.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val id = row.getAs[Int](idCol)
        val vector = row.getAs[Seq[Float]](featuresCol)
        (Seq(id) ++ vector).mkString("" "")
      }
"
"udf/spark_repos_9/incubator-s2graph/..s2jobs.src.main.scala.org.apache.s2graph.s2jobs.udfs.Grok.scala/udf/29.30.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

f
"
"udf/spark_repos_9/incubator-s2graph/..s2jobs.src.main.scala.org.apache.s2graph.s2jobs.wal.process.BuildTopFeaturesProcess.scala/udf/86.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""count > ${param._minUserCount}""
"
"udf/spark_repos_9/incubator-s2graph/..s2jobs.src.main.scala.org.apache.s2graph.s2jobs.wal.process.BuildTopFeaturesProcess.scala/udf/90.20.Dataset-(String, String, Long).map","Type: org.apache.spark.sql.Dataset[(String, String, Long)]
Call: map

{
        case (dim, value, uv) =>
          (dim, uv) -> value
      }
"
"udf/spark_repos_9/incubator-s2graph/..s2jobs.src.main.scala.org.apache.s2graph.s2jobs.wal.process.FilterTopFeaturesProcess.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

filterUDF(col(""dim""), col(""rank""))
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.dataframe.Basic.scala/udf/22.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.dataframe.DatasetConversion.scala/udf/17.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

$""state"".equalTo(""CA"")
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.dataframe.UDF.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

myNameFilter($""name"")
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.dataframe.UDF.scala/udf/44.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

salesFilter($""sales"", lit(2000.0d))
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.dataframe.UDF.scala/udf/52.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

stateFilter($""state"", array(lit(""CA""), lit(""MA""), lit(""NY""), lit(""NJ"")))
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.dataframe.UDF.scala/udf/60.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

multipleFilter($""state"", $""discount"", struct(lit(""CA""), lit(100.0d)))
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/58.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

struct _
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/67.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

isAtomic _
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.experiments.SemiStructuredUtilUDF.scala/udf/71.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

arrayLength _
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.JSON.scala/udf/32.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

s => s.replaceAllLiterally(""$"", """")
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.UDAF2.scala/udf/48.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.UDAF_Multi.scala/udf/43.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mystats
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.UDAF.scala/udf/41.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

mysum
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.UDF.scala/udf/16.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

westernState _
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.UDF.scala/udf/27.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

manyCustomers _
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.UDF.scala/udf/47.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

stateRegion _
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.UDF.scala/udf/62.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

discountRatio _
"
"udf/spark_repos_9/LearningSpark/..src.main.scala.sql.UDF.scala/udf/76.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

makeStruct _
"
"udf/spark_repos_9/metorikku/..examples.udf.Example.scala/udf/10.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

udf[String, String](addZPrefix)
"
"udf/spark_repos_9/metorikku/..src.main.scala.com.yotpo.metorikku.output.writers.redis.RedisOutputWriter.scala/udf/38.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => row.getAs[Any](redisOutputOptions.keyColumn).toString -> JSONObject(row.getValuesMap(columns)).toString()
"
"udf/spark_repos_9/mleap/..mleap-spark-extension.src.main.scala.org.apache.spark.ml.mleap.feature.Imputer.scala/udf/41.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

inputColumn.isNotNull && inputColumn =!= $(missingValue)
"
"udf/spark_repos_9/mleap/..mleap-spark-extension.src.main.scala.org.apache.spark.ml.mleap.feature.Imputer.scala/udf/43.22.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

!inputColumn.isNaN
"
"udf/spark_repos_9/mleap/..mleap-spark.src.main.scala.org.apache.spark.ml.bundle.ops.recommendation.ALSOp.scala/udf/17.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getSeq[Float](1))
"
"udf/spark_repos_9/mleap/..mleap-spark.src.main.scala.org.apache.spark.ml.bundle.ops.recommendation.ALSOp.scala/udf/21.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getInt(0), row.getSeq[Float](1))
"
"udf/spark_repos_9/mleap/..mleap-spark-testkit.src.main.scala.org.apache.spark.ml.parity.SparkParityBase.scala/udf/43.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

parseRating
"
"udf/spark_repos_9/mmlspark/..src.main.scala.com.microsoft.ml.spark.core.utils.ClusterUtil.scala/udf/60.19.Dataset-Long.map","Type: org.apache.spark.sql.Dataset[Long]
Call: map

_ => java.lang.Runtime.getRuntime.availableProcessors
"
"udf/spark_repos_9/mmlspark/..src.main.scala.com.microsoft.ml.spark.featurize.AssembleFeatures.scala/udf/146.23.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => toBitSet(row.getAs[SparseVector](0).indices)
"
"udf/spark_repos_9/mmlspark/..src.main.scala.com.microsoft.ml.spark.featurize.text.MultiNGram.scala/udf/42.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val mergedNGrams = locally {
          val _t_m_p_5 = locally {
            val _t_m_p_6 = intermediateOutputCols
            _t_m_p_6.map(col => row.getAs[Seq[String]](col))
          }
          _t_m_p_5.reduce(_ ++ _)
        }
        Row.merge(row, Row(mergedNGrams))
      }
"
"udf/spark_repos_9/mmlspark/..src.main.scala.com.microsoft.ml.spark.io.IOImplicits.scala/udf/106.28.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""didReply"").isNull
"
"udf/spark_repos_9/mmlspark/..src.main.scala.com.microsoft.ml.spark.lightgbm.LightGBMBase.scala/udf/111.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => !x.getBoolean(x.fieldIndex(getValidationIndicatorCol))
"
"udf/spark_repos_9/mmlspark/..src.main.scala.com.microsoft.ml.spark.lightgbm.LightGBMBase.scala/udf/114.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

x => x.getBoolean(x.fieldIndex(getValidationIndicatorCol))
"
"udf/spark_repos_9/mmlspark/..src.main.scala.com.microsoft.ml.spark.train.ComputeModelStatistics.scala/udf/117.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
        row => Row.merge(row, Row(confusionMatrix.asML))
      }
"
"udf/spark_repos_9/mmlspark/..src.test.scala.com.microsoft.ml.spark.opencv.ImageTransformerSuite.scala/udf/25.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

row => row.getString(4).contains(""negative"") && row.getString(4).endsWith(""5.jpg"")
"
"udf/spark_repos_9/mmlspark/..src.test.scala.com.microsoft.ml.spark.recommendation.SARSpec.scala/udf/101.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

col(""userId"") === ""0003000098E85347""
"
"udf/spark_repos_9/mmlspark/..src.test.scala.com.microsoft.ml.spark.stages.MiniBatchTransformerSuite.scala/udf/26.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ x => 
        Thread.sleep(3 * delay.toLong)
        x
      }
"
"udf/spark_repos_9/mongo-spark/..examples.src.test.scala.tour.SparkStructuredStreams.scala/udf/19.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ (r: Row) => WordCount(r.getAs[String](0), r.getAs[Long](1)) }
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/20.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.binary _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/24.45.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.binaryWithSubType _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/28.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.dbPointer _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/32.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.javaScript _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/36.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.javaScriptWithScope _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/40.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.maxKey _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/44.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.minKey _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/48.36.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.objectId _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/52.42.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.regularExpression _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/56.34.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.regularExpressionWithOptions _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/60.35.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.symbol _
"
"udf/spark_repos_9/mongo-spark/..src.main.scala.com.mongodb.spark.sql.helpers.UDF.scala/udf/64.38.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

UDF.timestamp _
"
"udf/spark_repos_9/OAP/..oap-cache.oap.src.main.scala.org.apache.spark.sql.execution.datasources.oap.index.IndexUtils.scala/udf/311.25.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

disjunctivePartitionsFilter
"
"udf/spark_repos_9/piflow/..piflow-bundle.src.test.scala.cn.piflow.bundle.packone.test.scala/udf/14.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => str.replaceAll(""""""
"""""", ""   "")
"
"udf/spark_repos_9/piflow/..piflow-bundle.src.test.scala.cn.piflow.bundle.packone.test.scala/udf/22.37.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String) => XmlToJson.xmlParse(str.replaceAll(""""""
"""""", ""  ""))
"
"udf/spark_repos_9/piflow/..piflow-core.src.main.scala.cn.piflow.lib.etl.scala/udf/34.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => func.perform(Seq(x)).asInstanceOf[Row]
"
"udf/spark_repos_9/quill/..quill-spark.src.test.scala.io.getquill.context.spark.CaseClassQuerySpec.scala/udf/190.19.Dataset-AddressableContact.map","Type: org.apache.spark.sql.Dataset[io.getquill.context.spark.AddressableContact]
Call: map

ac => ContactSimplifiedRenamed(ac.firstName, ac.lastName, ac.firstName.reverse)
"
"udf/spark_repos_9/quill/..quill-spark.src.test.scala.io.getquill.context.spark.examples.TopHashtagsExample.scala/udf/40.24.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

_.startsWith(""#"")
"
"udf/spark_repos_9/quill/..quill-spark.src.test.scala.io.getquill.context.spark.examples.TopHashtagsExample.scala/udf/42.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

_.toLowerCase
"
"udf/spark_repos_9/sagemaker-spark/..sagemaker-spark-sdk.src.test.scala.com.amazonaws.services.sagemaker.sparksdk.transformation.LibSVMTransformationLocalFunctionalTests.scala/udf/27.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => new String(libsvmSerializer.serializeRow(row))
"
"udf/spark_repos_9/spark2.0-examples/..src.main.scala.com.madhukaraphatak.examples.sparktwo.DatasetVsDataFrame.scala/udf/13.19.Dataset-Sales.map","Type: org.apache.spark.sql.Dataset[com.madhukaraphatak.examples.sparktwo.DatasetVsDataFrame.Sales]
Call: map

_.itemId
"
"udf/spark_repos_9/spark2.0-examples/..src.main.scala.com.madhukaraphatak.examples.sparktwo.MysqlTransactionExample.scala/udf/9.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

value => if (value == ""3"") throw new IllegalArgumentException(""value cannot be 3"") else value
"
"udf/spark_repos_9/spark2.0-examples/..src.main.scala.com.madhukaraphatak.examples.sparktwo.RDDToDataSet.scala/udf/40.22.Dataset-String.filter","Type: org.apache.spark.sql.Dataset[String]
Call: filter

value => value == ""hello""
"
"udf/spark_repos_9/spark2.0-examples/..src.main.scala.com.madhukaraphatak.examples.sparktwo.streaming.EventTimeExample.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

value => {
        val columns = value.split("","")
        Stock(new Timestamp(columns(0).toLong), columns(1), columns(2).toDouble)
      }
"
"udf/spark_repos_9/spark2.0-examples/..src.main.scala.com.madhukaraphatak.examples.sparktwo.streaming.SessionisationExample.scala/udf/16.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

line => {
        val columns = line.split("","")
        val endSignal = Try(Some(columns(2))).getOrElse(None)
        Session(columns(0), columns(1).toDouble, endSignal)
      }
"
"udf/spark_repos_9/spark2.0-examples/..src.main.scala.com.madhukaraphatak.examples.sparktwo.streaming.StreamJoin.scala/udf/18.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

value => {
        val values = value.split("","")
        Sales(values(0), values(1), values(2), values(3).toDouble)
      }
"
"udf/spark_repos_9/spark2.0-examples/..src.main.scala.com.madhukaraphatak.examples.sparktwo.streaming.WaterMarkExample.scala/udf/14.19.Dataset-String.map","Type: org.apache.spark.sql.Dataset[String]
Call: map

value => {
        val columns = value.split("","")
        Stock(new Timestamp(columns(0).toLong), columns(1), columns(2).toDouble)
      }
"
"udf/spark_repos_9/spark-cassandra-connector/..connector.src.it.scala.com.datastax.bdp.spark.search.SearchAnalyticsIntegrationSpec.scala/udf/413.27.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df(""key"") === 1
"
"udf/spark_repos_9/spark-cassandra-connector/..connector.src.it.scala.com.datastax.bdp.spark.search.SearchAnalyticsIntegrationSpec.scala/udf/415.25.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(""~~funcolumn"") === ""hello""
"
"udf/spark_repos_9/spark-cassandra-connector/..connector.src.it.scala.com.datastax.bdp.spark.search.SearchAnalyticsIntegrationSpec.scala/udf/417.23.Dataset-Row.filter","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: filter

df(""MixEdCol"") === ""world""
"
"udf/spark_repos_9/spark-cassandra-connector/..connector.src.it.scala.org.apache.spark.sql.cassandra.execution.CassandraDirectJoinSpec.scala/udf/405.23.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'c % 2 === 1
"
"udf/spark_repos_9/SparkDemo/..src.main.scala.org.apache.spark.examples.practice.ml.TextCategoryV16.scala/udf/116.41.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(str: String, array: Seq[String]) => str.split("" "") ++ array
"
"udf/spark_repos_9/SparkDemo/..src.main.scala.org.apache.spark.examples.practice.ml.TextCategoryV16.scala/udf/128.27.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

x => (x.getAs[String](""category""), (x.getAs[String](""brand""), x.getAs[String](""title"")))
"
"udf/spark_repos_9/SparkDemo/..src.main.scala.org.apache.spark.examples.practice.sql.UserRetention.scala/udf/68.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => (r.getAs[String](APPKEY), r.getAs[String](GID), statDay)
"
"udf/spark_repos_9/SparkDemo/..src.main.scala.org.apache.spark.examples.practice.sql.UserRetention.scala/udf/74.19.Dataset-Row.map","Type: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
Call: map

r => (r.getAs[String](APPKEY), r.getAs[String](GID), statDay)
"
"udf/spark_repos_9/SparkDemo/..src.main.scala.org.apache.spark.examples.sql.DataFrameTest.scala/udf/43.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: Int) => if (s <= 20) ""lower"" else ""high""
"
"udf/spark_repos_9/SparkDemo/..src.main.scala.org.apache.spark.examples.sql.HiveOperationTest.scala/udf/21.33.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

(s: Int) => if (s <= 20) ""lower"" else ""high""
"
"udf/spark_repos_9/spark-solr/..src.main.scala.com.lucidworks.spark.example.query.WordCount.scala/udf/50.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

df.col(""type_s"").equalTo(""echo"")
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/39.26.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: filter

_.id % 100 != 0
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/41.24.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: filter

_.id % 101 != 0
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/43.22.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: filter

_.id % 102 != 0
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/45.20.Dataset-Data.filter","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: filter

_.id % 103 != 0
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/79.24.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: map

d => Data(d.id + 1L)
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/81.22.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: map

d => Data(d.id + 1L)
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/83.20.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: map

d => Data(d.id + 1L)
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.DatasetPerformance.scala/udf/85.18.Dataset-Data.map","Type: org.apache.spark.sql.Dataset[com.databricks.spark.sql.perf.Data]
Call: map

d => Data(d.id + 1L)
"
"udf/spark_repos_9/spark-sql-perf/..src.main.scala.com.databricks.spark.sql.perf.RunBenchmark.scala/udf/73.26.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

'thisRunTimeMs.isNotNull
"
"udf/spark_repos_9/sparktraining/..src.main.scala.org.training.spark.ml.CreditCardDataExploratory.scala/udf/8.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

{ row => 
        val label = row.toSeq.last.asInstanceOf[String].toDouble
        label.equals(1.0d)
      }
"
"udf/spark_repos_9/sparktraining/..src.main.scala.org.training.spark.ml.MLUtils.scala/udf/14.19.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{ row => 
        val seq = row.toSeq
        val data = locally {
          val _t_m_p_2 = seq.drop(1).dropRight(2)
          _t_m_p_2.map(_.asInstanceOf[String].toDouble)
        }
        (Vectors.dense(data.toArray), seq.last.asInstanceOf[String].toDouble)
      }
"
"udf/spark_repos_9/sparktraining/..src.main.scala.org.training.spark.ml.MLUtils.scala/udf/65.22.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

dataset(""label"") === 0
"
"udf/spark_repos_9/sparktraining/..src.main.scala.org.training.spark.sql.MovieUserAnalyzerWithDataFrame.scala/udf/49.24.DataFrame.filter","Type: org.apache.spark.sql.DataFrame
Call: filter

s""movieid = $MOVIE_ID""
"
"udf/spark_repos_9/sparktraining/..src.main.scala.org.training.spark.sql.SparkSQLSimpleExample.scala/udf/86.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

{
          u => (u.getAs[String](""userID"").toLong, u.getAs[String](""age"").toInt + 1)
        }
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.Join.scala/udf/120.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => (x.getAs[Long](""id""), locally {
          val _t_m_p_9 = x.getAs[Seq[Row]](""tree"")
          _t_m_p_9.map(r => (r.getDouble(0), r.getDouble(1), r.getDouble(2), r.getDouble(3), r.getLong(4), r.getLong(5)))
        })
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.Join.scala/udf/184.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => ((x.getAs[Double](""xmin""), x.getAs[Double](""ymin""), x.getAs[Double](""xmax""), x.getAs[Double](""ymax"")), x.getAs[Long](""id""))
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.Join.scala/udf/188.22.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => ((x.getAs[Double](""xmin""), x.getAs[Double](""ymin""), x.getAs[Double](""xmax""), x.getAs[Double](""ymax"")), x.getAs[Long](""id""))
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.Join.scala/udf/212.39.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

treeJoin _
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.Join.scala/udf/216.43.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

intersect _
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.Join.scala/udf/224.22.Dataset-(Long, Long).map","Type: org.apache.spark.sql.Dataset[(Long, Long)]
Call: map

x => Pair(x._1, x._2)
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.Join.scala/udf/84.21.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

x => ((x.getAs[Double](""xmin""), x.getAs[Double](""ymin""), x.getAs[Double](""xmax""), x.getAs[Double](""ymax"")), x.getAs[Long](""id""))
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.RangeQuery.scala/udf/101.40.UDFRegistration.register","Type: org.apache.spark.sql.UDFRegistration
Call: register

queryRtree _
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.RangeQuery.scala/udf/125.25.DataFrame.map","Type: org.apache.spark.sql.DataFrame
Call: map

row => (row.getLong(0), row.getString(1))
"
"udf/spark_repos_9/SpatialSpark/..src.main.scala.spatialspark.exp.RangeQuery.scala/udf/127.26.Dataset-(Long, String).filter","Type: org.apache.spark.sql.Dataset[(Long, String)]
Call: filter

x => (new WKTReader).read(x._2).intersects(env)
"
